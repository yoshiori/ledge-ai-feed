<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
  <channel>
    <title>Ledge.ai 新着記事</title>
    <link>https://ledge.ai/</link>
    <description>Ledge.ai の最新テクノロジー記事</description>
    <item>
      <title>Meta、Midjourneyと提携発表　Alexandr Wang氏「美的技術で数十億人に美を届ける」</title>
      <link>https://ledge.ai/articles/meta_midjourney_ai_partnership</link>
      <description><![CDATA[<p>Metaは2025年8月22日、画像生成AIを開発するMidjourneyと提携すると発表した。Scale AIのCEOでMetaのAI開発にも関わる<a href="https://x.com/alexandr_wang/status/1958983843169673367">Alexandr Wang氏</a>はXで「Midjourneyの美的技術をライセンスし、将来のモデルや製品に活用する」と述べ、両社の研究チームが技術協力を進める方針を示した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/meta_midjourney_f6ed2a2826/meta_midjourney_f6ed2a2826.jpg" alt="meta midjourney.jpg" /></p>
<p>同氏は今回の提携を「数十億人に美を届ける取り組み」と表現。両社の研究チームが技術的な協力関係を築くことは、互いの強みを補完し合うものだとし、Midjourneyの技術的かつ美的な卓越性に深い感銘を受けていると述べている。また、Metaが最高の製品を利用者に届けるためには、優れた人材の確保や大規模な計算資源の活用に加え、業界トップの企業との協力が欠かせないと強調した。最後に、「今後、両社がともに築き上げる成果を披露できるのを楽しみにしている」と結び、パートナーシップへの期待を示した。</p>
<p>Financial TimesやReutersによれば、Metaはこれまで独自に画像・動画生成AI（「Imagine」や「Movie Gen」など）を開発してきたが、GoogleのVeoやOpenAIのSoraと比べると表現力や美的品質で見劣りするとの評価もあった。このため、競合との差を埋めるべく外部パートナーとの協力を模索していたとされる。</p>
<p>また、Midjourneyが今回の提携に際し「独立したコミュニティ主導の研究所」であり「投資を受けていない」という立場を改めて強調したと、The Vergeが報じている。Metaによる買収や経営統合ではなく、ライセンスと技術協力に限定されている点が特徴だ。</p>
<p>MetaはInstagramやFacebook、WhatsAppといった主要サービスに画像・動画生成技術を広く展開しており、Midjourneyの美的技術が実装されれば、ユーザー体験や広告クリエイティブの品質向上に直結する可能性がある。今後、両社の協力の成果がどのように具体化するか注目される。</p>
]]></description>
      <pubDate>Thu, 28 Aug 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ゲーム制作の未来へ　画像と説明文からリアルタイムに3Dワールドを作るAI「Mirage 2」公開——名画や子どものお絵描きの中も探索可能</title>
      <link>https://ledge.ai/articles/mirage2_ai_3d_world_generation</link>
      <description><![CDATA[<p>AIスタートアップのDynamics Labは2025年8月22日、新しいAIモデル「Mirage 2」を<a href="https://x.com/DynamicsLab_AI/status/1958592749378445319">公開</a>した。1枚の画像と説明文から、名画や子どものお絵描きの中までも探索できる3D世界をリアルタイムに生成できる。同社はこのモデルを「AIネイティブゲームエンジン」「世界生成エンジン」と位置づけ、将来的にはゲーム制作への活用を視野に入れている。</p>
<p>開発を担うのは、Google、NVIDIA、Amazon、SEGA、Apple、Microsoft、カーネギーメロン大学、UCサンディエゴなどの出身者で構成された少数精鋭のチームで、研究者、エンジニア、デザイナーといった技術と創造性を兼ね備えた人材が集まっているという。</p>
<p>現在、Mirage 2はブラウザ上で公開されており、誰でも実際に操作して探索できるデモが提供されている。</p>
<h2>Mirage 2の特徴</h2>
<p>Mirage 2は、従来の画像生成AIを拡張し、入力データをもとに「歩ける空間」を作り出せる点に特徴がある。</p>
<ul>
<li>入力は1枚の画像とテキスト説明文</li>
<li>数秒で3D空間を構築し、移動・ジャンプ・攻撃といったアクションが可能</li>
<li>テキストコマンドによりワールドをリアルタイムで編集できる</li>
<li>生成空間はリンク共有により他者と体験可能</li>
</ul>
<p>この仕組みを同社は「Generative Play（生成的プレイ）」と呼び、プレイヤーとAIが共同で体験を創造する新しい形態のインタラクティブ体験としている。</p>
<h2>Mirage 2による生成例</h2>
<p><strong>■ 1枚の入力画像（左）から、サイバーパンク都市をはじめ、熱帯雨林や秋の山頂の城など多様な3D空間が連続して生成された</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/mirage2_0de4684b0e/mirage2_0de4684b0e.jpg" alt="mirage2.jpg" /></p>
<p><strong>■ 子どものお絵描き（左）をもとに、カラフルな街を歩き回れる3D空間へと変換した</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/mirage2_kids_drawing_f6ff7bd48f/mirage2_kids_drawing_f6ff7bd48f.jpg" alt="mirage2 kids drawing.jpg" /></p>
<p><strong>■ ジブリ風の村のイラスト（左）をもとに、草花が揺れるファンタジー風の村を歩き回れる3D空間が構築された</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/mirage2_ghibli_style_2229709b5e/mirage2_ghibli_style_2229709b5e.jpg" alt="mirage2 ghibli style.jpg" /></p>
<h2>Genie 3との比較</h2>
<p>Mirage 2は、Google DeepMindが発表した「Genie 3」と同じく、生成AIによってリアルタイムに探索可能な3D世界を生成する技術だが、いくつかの点で異なる。特にMirage 2は「実際にブラウザ上で誰でも試遊できる」点が大きな特徴となっている。</p>
<p><strong>Genie 3とMirage 2の比較。Mirage 2は200msの低遅延で動作し、一般的なGPU環境でもプレイ可能。ブラウザで公開されている点が大きな違いとなる</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/genie3_and_mirage_7bfbf41a67/genie3_and_mirage_7bfbf41a67.jpg" alt="genie3 and mirage.jpg" /></p>
<h2>今後の展望</h2>
<p>Dynamics LabはMirage 2を「AIネイティブゲームエンジン」「世界生成エンジン」と位置づけ、将来的にゲーム制作での活用を視野に入れている。創設メンバーのZhiting Hu氏はXにて「Super excited to launch Mirage 2. A big leap toward a general-purpose world engine for live interactive play」と投稿し、Mirage 2を「ライブインタラクティブなプレイのための汎用ワールドエンジン」への大きな前進と位置付けている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/mirage2_zhiting_hu_81ba446989/mirage2_zhiting_hu_81ba446989.jpg" alt="mirage2 zhiting hu.jpg" /></p>
]]></description>
      <pubDate>Wed, 27 Aug 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/8/27 [WED]Google、Geminiに「2.5 Flash Image」を統合──特徴を崩さず“その人らしさ”を保つ画像編集モデル</title>
      <link>https://ledge.ai/articles/google_gemini_2-5_flash_image_integration</link>
      <description><![CDATA[<p>Googleは2025年8月26日、被写体の特徴を保ったまま編集できる新しい画像モデル「Gemini 2.5 Flash Image」を<a href="https://blog.google/products/gemini/updated-image-editing-model/">発表</a>しGeminiに統合した。同モデルは、自然言語による編集指示に対応し、キャラクターやスタイルの一貫性を維持した高度な画像編集を可能にする。まずはGeminiアプリに搭載され、開発者向けにはGemini API／Google AI Studio／Vertex AIでプレビュー提供が始まっている。</p>
<h2>“特徴を維持したまま”の進化</h2>
<p>従来の画像生成AIでは、同じ人物を複数の画像に登場させると顔立ちや雰囲気が変わってしまうことが少なくなかった。Gemini 2.5 Flash Imageでは、こうした課題を克服し、同じ人物やキャラクターを画像間で同一人物らしく保つ。衣装や背景を変えても「その人らしさ」を維持し、自然文での指示に対して局所的・段階的な編集が可能になった。従来の生成AIで起きがちだった“顔の破綻”や“別人化”を抑え、連作広告やブランド素材づくりに必要な一貫性を実現する。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/gemini_2_5_image_editing_character_consistency_original_c402e8476b/gemini_2_5_image_editing_character_consistency_original_c402e8476b.jpg" alt="gemini-2-5-image-editing-character-consistency.original.jpg" /></p>
<h2>外部評価：「Image Edit Arena」で首位</h2>
<p>Gemini 2.5 Flash Imageは第三者の評価でも成果を示している。AIモデルの比較プラットフォーム「LM Arena」が公開している<a href="https://lmarena.ai/leaderboard/image-edit">Image Edit Arena</a>では、OpenAIの「gpt-image-1」やBlack Forest Labsの「flux-1-kontext-max」などを抑え、1位にランクインした。編集精度や一貫性の高さが、他モデルを上回ると評価されている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/nanobanana_image_edit_arena_a10e5979e6/nanobanana_image_edit_arena_a10e5979e6.jpg" alt="nanobanana image edit arena.jpg" /></p>
<p>さらに、自然文で「背景を暗く」「モノクロをカラーに」といった指示を与えると、対象の特徴を崩さずに狙った部分だけを修正できる。複数の写真を組み合わせて一枚のビジュアルを生み出す「マルチイメージ融合」や、段階的に要素を追加していく「マルチターン編集」、他の画像の色調や質感を移す「スタイル転写」にも対応している。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/gemini2_5_multiturn_edit_12594cb61e/gemini2_5_multiturn_edit_12594cb61e.jpg" alt="gemini2-5 multiturn edit.jpg" /></p>
<h2>提供状況と価格</h2>
<p>モデルは即日Geminiアプリで利用可能。開発者向けにはGemini API／Google AI Studioでプレビュー提供中で、数週間以内に安定版へ移行予定。価格は出力トークン100万あたり30ドルで、1画像は1290出力トークン（約0.039ドル）に相当する。企業向けにはVertex AIでも提供される。</p>
<h2>安全性と表示</h2>
<p>Geminiアプリで作成・編集した画像には可視の透かしに加え、SynthIDによる不可視ウォーターマークが自動付与される。API／AI Studio／Vertex AI経由の生成・編集でもSynthIDを埋め込み、AI生成物であることの識別を支える。</p>
]]></description>
      <pubDate>Wed, 27 Aug 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Netflix、生成AI活用を歓迎しつつもルールを明示──成果物や肖像利用は事前承認が必須に</title>
      <link>https://ledge.ai/articles/netflix_gen_ai_guidelines_mandatory_approval</link>
      <description><![CDATA[<p>映像配信大手のNetflixがオリジナルコンテンツ制作における生成AI利用に関するガイドラインを公開したことを<a href="https://www.theverge.com/netflix/764433/netflix-gen-ai-production-guidelines">The Verge</a>が2025年8月23日に報じた。Netflixの<a href="https://partnerhelp.netflixstudios.com/hc/en-us/articles/43393929218323-Using-Generative-AI-in-Content-Production">パートナーヘルプセンターハブ</a>では、制作現場でのAI活用を歓迎しつつも、成果物や肖像などデリケートな領域では事前承認を必須とするなど、明確なルールを設けている。</p>
<p>同社はこれまでもレコメンド機能や広告最適化にAIを活用してきた。近年は映像制作の現場で生成AIが使われる事例が増えており、信頼性や法的リスクの管理が課題となっていた。</p>
<p>Netflixはパートナー向けに生成AIの利用ガイドラインを提示。制作者はAI利用の意図を事前にNetflix担当者へ通知する必要がある。特に「最終成果物」「出演者の肖像」「個人データ」「第三者の知的財産」に関わる場合は書面での承認が必須とされるとした。The Vergeによると、Netflixは「事実と虚構の境界を曖昧にしない」姿勢を強調しているという。</p>
<h3>公式ガイドライン（Partner Help Centerより）</h3>
<p>制作者に求められる「5つのチェック項目」</p>
<ul>
<li>著作権を侵害せず、特定作品を模倣しないこと</li>
<li>入出力を学習用途に利用しないこと</li>
<li>可能な限りエンタープライズ環境で使用すること</li>
<li>AI生成物は一時的な利用にとどめ、最終成果物に含めないこと</li>
<li>出演者や組合が関与する仕事を無断で置き換え・生成しないこと</li>
</ul>
<p>すべて「YES」であれば担当者への通知のみで利用可能だが、「NO」または「不明」がある場合は法務部門への相談と書面による承認が必要となる。</p>
<p>Netflixは同ガイドライン公開の直前に、自社オリジナル作品『The Eternaut』では、初めて<a href="https://www.bbc.com/news/articles/c9vr4rymlw9o">生成AIを建物崩壊シーンに活用</a>したことも発表している。従来のVFXより高速かつ低コストで制作されたとされ、話題を呼んだ。</p>
<p>@<a href="https://www.youtube.com/watch?v=TqT4fDQQqCc">YouTube</a></p>
<p>同社の共同CEOであるテッド・サランドス氏は「生成AIはクリエイターが映画やシリーズをより良く、コスト削減するだけでなく、より良く制作する上で素晴らしい機会になる」と<a href="https://www.hollywoodreporter.com/business/business-news/netflixs-ted-sarandos-gen-ai-1236319038/">述べている</a>。</p>
]]></description>
      <pubDate>Wed, 27 Aug 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>シャープ「ロボホン」開発チームが新たに送り出す、ポケットサイズのAIキャラクター「ポケとも」発表</title>
      <link>https://ledge.ai/articles/sharp_poketomo_ai_character_launch</link>
      <description><![CDATA[<p>シャープは2025年8月20日、ポケットサイズのAI製品「ポケとも」を<a href="https://corporate.jp.sharp/news/250820-a.html">発表</a>した。“一緒にいると毎日がもっと楽しくなるポケットサイズのおともだち”をコンセプトに、ロボホンの開発チームが新たに送り出すAIキャラクターとして位置づけられている。ロボット端末とスマートフォンアプリの両方で、キャラクターとの自然なやりとりを楽しめるのが特徴だという。</p>
<p>ロボット端末は全長約12センチ、重さ約200グラムと手のひらに収まるサイズで、かばんに入れて持ち歩くことも可能だ。胸部にはスピーカーや音声認識ボタンを備え、口にはカメラ、頭部にはマイクを搭載している。足裏の端子を充電台に接続して利用できる仕組みだ。</p>
<p>@<a href="https://www.youtube.com/watch?v=S8jTMLE-hgs">YouTube</a></p>
<h2>独自AI技術「CE-LLM」と「Empathy Intelligence」</h2>
<p>「ポケとも」には、シャープ独自の大規模言語モデル「CE-LLM（Communication Edge Large Language Model）」を活用した会話機能を搭載。さらに、話し手の気持ちを推定して応答を変える「Empathy Intelligence」によって、喜びや悲しみといった感情に寄り添った自然なやりとりが可能になる。</p>
<p>加えて、交わした会話や訪れた場所、見た景色やモノを記憶する機能を備えている。会話や一緒に過ごす時間を重ねるごとに“あなた”のことを理解し、より寄り添う存在へと成長していくのが特徴だ。</p>
<h2>アプリとの連動と展開</h2>
<p>スマートフォンアプリを利用すれば、ロボットが手元にないときでもキャラクターと会話できる。音声のほか文字入力でもやりとりでき、会話履歴を振り返ったり、利用時間を計測したりする機能も備える。アプリとロボットは同期され、どちらからアクセスしても同じキャラクター体験を継続できる仕組みとなっている。</p>
<p>アプリは月額495円（税込）から利用可能で、ロボット端末は2025年11月に発売予定。価格は税込39,600円で、順次予約受付が開始されている。</p>
<h2>マンガやイベントでも展開</h2>
<p>「ポケとも」は製品だけでなく、キャラクターとしての広がりも見据えている。公式Xアカウント（@poketomo_sharp）ではすでにマンガ連載が始まっており、利用者の日常に寄り添うストーリーを展開。さらに、8月28日から東京ビッグサイトで開催される「東京おもちゃショー2025」にも出展予定で、来場者は実際に「ポケとも」との対話を体験できる。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/250820_a_9_3269807be2/250820_a_9_3269807be2.jpg" alt="250820-a-9.jpg" /></p>
<p>同製品は、コミュニケーションロボット「ロボホン」の開発チームが新たに送り出すシリーズとして誕生した。2016年に登場した「ロボホン」は“ココロ、動く電話”をコンセプトに親しまれてきたが、その思想を受け継ぎつつ、より日常的に寄り添う存在として「ポケとも」が企画されたという。</p>
<p>同社は「うれしかったことも、心が折れそうになったことも、そっと受け止めてあなたの今日をやさしく灯す存在」と位置づけ、AIを活用した新しいコミュニケーションパートナーの可能性を提案している。</p>
]]></description>
      <pubDate>Wed, 27 Aug 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ラーニング2025/8/26 [TUE]Googleの中のひとがAIで仕事をスマートにする14の方法　Gemini、NotebookLM、Imagen、Veoをどう使うか？</title>
      <link>https://ledge.ai/articles/google_ai_14_usecases</link>
      <description><![CDATA[<p>Googleは2025年8月18日（現地時間）、社内で従業員がどのようにAIを活用しているかについて、<a href="https://blog.google/technology/ai/google-ai-workplace-examples/">14の具体例を紹介</a>した。公開された事例は、エンジニアリングに限らず、営業やマーケティング、人事、総務といった幅広い職種に及ぶ。GeminiやNotebookLM、Imagen、Veoなど自社の生成AIを用いた実践例を示し、日常業務の効率化と創造性の拡張を裏付けた。</p>
<h2>紹介された14の実例</h2>
<h3>1. コードを書く時間を短縮</h3>
<p>Geminiが新規コードの30%を自動生成。エンジニアはレビューや設計に集中できるようになった。</p>
<h3>2. 開発スピード全体が加速</h3>
<p>テストやレビューの一部をAIが補助し、開発チーム全体の速度は約10%向上。</p>
<h3>3. バグ対応をAIが下支え</h3>
<p>AIが重複バグの12%を自動処理し、重要度の高い課題に人が集中。</p>
<h3>4. マーケティングの発想支援</h3>
<p>Geminiがキャンペーン案や動画台本のたたき台を提示し、ゼロから考える負担を軽減させている。
同社マーケティングチームは、Googleのマーケティングスタイルとベストプラクティスを反映したアイデアのブレインストーミングにGeminiを活用。キャンペーンのコンセプトからYouTube動画の脚本の下書きまで、あらゆるアイデアを生み出すための最適なプロンプトを提供するという。</p>
<h3>5. YouTube向けのキャッチーなコンテンツ作成を効率化</h3>
<p>ポッドキャストから名言やタイムスタンプを自動抽出し、YouTube Insider向けのより魅力的なタイトルやサムネイル文言もAIが提案。</p>
<h3>6. イベント資料の大量制作もAIで</h3>
<p>Google I/O 2025の基調講演では219枚のスライド作成をAIが支援。ビジュアルの48%、動画の80%をImagenやVeoで生成。</p>
<p>@<a href="https://youtu.be/x_x-JAAKSvU?si=6zPHWf1vCK8mSqGl">YouTube</a></p>
<h3>7. 企画のアイデア検証</h3>
<p>Google DeepMindはAIツールを活用して新しいアイデアをテストする。GeminiやVeoを使い、動画のモックアップや表現案を即時に試作。</p>
<h3>8. 営業提案の数が増える</h3>
<p>AIがRFP対応を支援。Google Cloudでは完了件数が前年比78%増となった。</p>
<h3>9. 見込み顧客の質を高める</h3>
<p>AIによるフィルタリングで良質な案件に集中。6週間で案件転換率が14%増加。</p>
<h3>10. 会議メモを自動生成</h3>
<p>Google Meetがリアルタイムで文字起こしと要約を提供。2025年6月の利用者は5,000万人を超えた。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/googlemeet_memo_7fc3610183/googlemeet_memo_7fc3610183.jpg" alt="googlemeet memo.jpg" /></p>
<h3>11. 安全対策の強化</h3>
<p>Trust &amp; SafetyチームがAIで違反コンテンツを検出。ポリシーに違反する可能性のあるコンテンツの検出とフラグ付けをAIが支援。この負荷軽減は、2024年だけで10億件超を人間が手動でレビューしていた同チームにとって、重要なメリットとなる。</p>
<h3>12. 社員アンケートを一瞬で要約</h3>
<p>NotebookLMが数千件のフィードバックを即時整理。課題把握のスピードが大幅に向上。</p>
<h3>13. 採用業務を効率化</h3>
<p>人事チームは、採用プロセスの一環としてAIを活用。候補者検索やマッチングをAIが補助し、採用担当者の事務作業を削減。採用プロセスは常にリクルーターが主導している。</p>
<h3>14. 社内カフェでフードロス削減</h3>
<p>シェフはAIによる分析データを活用し、カフェのメニューを最適化。カフェでの廃棄物の予防的削減、各キャンパスのニーズに合わせたソリューションの提供に取り組む。食品廃棄量は2019年比39%減（2024年実績）。</p>
<p>Googleは「AIはごく一部の人のための特別なものではなく、全社員が日常的に使う標準的なツールになりつつある」と説明している。今回紹介した14の事例は、その姿を具体的に示したものだ。</p>
]]></description>
      <pubDate>Tue, 26 Aug 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Tesla、日本で「Full Self-Driving（Supervised）」のテスト走行を開始──市街地や高速道路での運転支援性能を検証</title>
      <link>https://ledge.ai/articles/tesla_fsd_supervised_japan_test</link>
      <description><![CDATA[<p>Tesla Japan（テスラ）は2025年8月20日、運転支援システム「Full Self-Driving（Supervised）」の日本でのテスト走行を開始したと<a href="https://prtimes.jp/main/html/rd/p/000000048.000038481.html">発表</a>した。市街地や高速道路を含む公道で、信号や標識の認識、交差点での右左折、歩行者や自転車への対応などの機能を検証する。（<a href="https://x.com/teslajapan/status/1957986432926249405">走行テストの様子</a>）</p>
<p><strong>日本でのテスト走行に使用されるTeslaのModel 3</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/sub4_babbf79619/sub4_babbf79619.jpg" alt="sub4.jpg" /></p>
<p>FSD(Full Self-Driving)は、同社が開発する最新の運転支援システムで、車両には「AI 4 ハードウェア」および「Tesla Vision」が搭載される。これにより、信号や標識の認識、交差点での右左折、さらには歩行者や自転車への対応など、複雑な交通環境での走行を支援する。</p>
<p>名称に「Supervised」とある通り、ドライバーによる常時監視が必須となる。運転者がステアリングを握っていない場合や注意が散漫と判断された場合には、警告が発せられる仕組みだ。完全自動運転ではなく、あくまで人間による管理のもとで機能する点を強調している。</p>
<p><strong>オートパイロットを使用したテスラ車両は、一般的な車両に比べて衝突事故発生までの平均走行距離が大幅に長い</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/sub1_a8ddd97395/sub1_a8ddd97395.jpg" alt="sub1.jpg" /></p>
<p>テスト走行はまず限定的に開始され、今後順次エリアを拡大する見通し。日本市場での展開に向けた重要なステップと位置づけられ、欧州に続いて導入が進む。</p>
<p>テスト走行の裏側では、FSDを支えるAI学習基盤の強化も進んでいる。テキサス州オースティンの最新工場Giga Texasでは、AIトレーニング用コンピュートクラスター「Cortex」を増強。約16,000台のH200 GPUを追加導入し、総計算能力はH100換算で約67,000台分に到達した。</p>
<p><strong>Giga TexasのAI学習用クラスター「Cortex」。約16,000台のH200を追加し、総計算能力はH100換算で約67,000台分）</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/sub3_e87e1adbef/sub3_e87e1adbef.jpg" alt="sub3.jpg" /></p>
<p>Teslaは「FSD（Supervised）」の実証を通じて、日本における次世代運転支援システムの普及と安全性検証を進める方針だ。</p>
]]></description>
      <pubDate>Tue, 26 Aug 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google、Metaから6年で100億ドル超のクラウド契約を受注──Metaは急速なAIインフラ強化へ</title>
      <link>https://ledge.ai/articles/google_meta_cloud_deal_ai_infra</link>
      <description><![CDATA[<p>Googleは、Meta Platformsと6年間で100億ドル（約1兆4800億円）超のクラウドコンピューティング契約を締結したことが、2025年8月21日に関係筋の話として<a href="https://jp.reuters.com/economy/industry/ZUPUKEMNYJKTFCL3P5NFAKWH2M-2025-08-22/">ロイター</a>など複数のメディアが報じた。</p>
<p>契約にはGoogle Cloudのサーバー、ストレージ、ネットワーキングなどのインフラ利用が含まれ、Metaの急速に進むAI開発計画を支えるものとなる。</p>
<p>今回の大型契約は、Metaが進める「スーパーインテリジェンス」構想の一環とみられており、大規模AIモデルの学習や推論に必要な計算資源を確保する狙いがあるという。Metaは2025年の設備投資（Capex）見通しを最大720億ドルに引き上げており、AI関連の投資を加速させている。</p>
<p>一方、Google Cloudは成長の原動力として注目されており、2025年第2四半期の売上は前年同期比32％増を記録。今回の契約は同社にとって最大級の案件のひとつであり、競合するAmazon Web Services（AWS）やMicrosoft Azureに対抗する上で大きな追い風となる。</p>
<p>Metaはまた、資金確保のために一部のデータセンター資産を約20億ドル規模で売却する計画も進めている。両社は今回の契約について公式コメントを控えているが、複数の報道によればすでに合意が成立しており、今後のAIインフラ拡大に直結するものとなる見通しだ。</p>
]]></description>
      <pubDate>Tue, 26 Aug 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>NVIDIA Research、「エージェントAIの未来は小規模言語モデル（SLM）」と提言──LLMは“必要時のみ”、ハイブリッド構成を推奨</title>
      <link>https://ledge.ai/articles/nvidia_slm_future_of_agentic_ai</link>
      <description><![CDATA[<p>NVIDIA ResearchのPeter Belcak氏らは2025年6月2日、論文「Small Language Models are the Future of Agentic AI」を<a href="https://research.nvidia.com/labs/lpr/slm-agents/">発表</a>し、現在は大規模言語モデル（LLM）を中心に設計されがちなエージェントAIについて、実運用では小規模言語モデル（SLM）がより適しており、経済的でもあると主張した。論文は、SLMの能力・運用適性・コスト効率を根拠に、用途に応じて複数モデルを組み合わせるヘテロジニアス（混在）構成を推奨している。</p>
<h2>「SLMは十分に強力で、運用に適し、必然的に安価」</h2>
<p>著者らは、エージェントAIの多くが限られた種類のタスクを反復処理するという前提に立ち、こうした場面ではSLMで十分な精度と安定性が得られると指摘。加えて、SLMはレイテンシ・消費電力・インフラ規模の面で有利であり、実サービスへのデプロイやエッジ実行にも向くとした。</p>
<h2>コスト面の差：7B級SLMは70〜175B級LLMより「10〜30倍」効率的</h2>
<p>論文は7B規模のSLMと70〜175B規模のLLMを比較し、レイテンシ、エネルギー、FLOPsの観点で10〜30倍の効率差があり、リアルタイム応答を要するエージェントにおいてSLMが有利だと述べる。</p>
<h2>ハイブリッド構成の推奨：「会話の汎用性」が必要な場面のみLLMを</h2>
<p>一方で、広範な一般会話能力が不可欠な場面については、複数モデルを呼び分けるヘテロジニアス構成（SLMとLLMの併用）が自然な選択だと提案。これにより、日常的な専門タスクはSLMで低コストに処理し、LLMは“必要時のみ”に限定してコスト最適化を図る設計思想を示した。</p>
<h2>LLM→SLMへのエージェント移行を見据えた「一般アルゴリズム」も提示</h2>
<p>論文は、既存のLLM中心エージェントをSLM主軸へ移行するための一般的な変換アルゴリズムを概説。移行の障壁や留意点にも触れ、産業界での段階的な置換を見据えた実務的視点を強調している。</p>
<h2>背景にある推論基盤の進化——NVIDIA「Dynamo」</h2>
<p>主張の背景には、SLMを高スループット・低レイテンシでさばく推論OS/基盤の進歩もある。NVIDIAは2025年3月に「NVIDIA Dynamo」を発表し、分散環境での推論効率を高める最適化（KVキャッシュ制御やディスアグリゲーテッド・サービング等）を公開している。こうした基盤整備が、SLM運用の現実解を後押ししている。</p>
<p>著者らは、論文へのフィードバックを公開で受け付ける特設ページも用意。今後の往復書簡・批判的検討を通じて議論を深める姿勢を示している。</p>
]]></description>
      <pubDate>Mon, 25 Aug 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>AIエンジニア視点で紐解くAIエージェントの可能性　技術背景とビジネス活用の最前線｜『現場で活用するためのAIエージェント実践入門』刊行記念インタビュー</title>
      <link>https://ledge.ai/articles/introduction-to-ai-agents-book-interview</link>
      <description><![CDATA[<p>「AIエージェント元年」と呼ばれる2025年は、目標に応じて自律的に判断・行動するAIエージェントに大きな期待が寄せられており、各企業で導入や検討が進んでいる。その中で、2025年7月発売の「本当に動くAIエージェントはどう作るのか」をテーマに執筆された書籍『<a href="https://www.amazon.co.jp/dp/4065401402">現場で活用するためのAIエージェント実践入門</a>』（講談社）が大きな注目を集めている。
今回は、導入現場で使える実践的なノウハウをまとめた同書を執筆した著者陣5名に、AIエージェントの基本概念や技術的な背景、導入時の課題、ビジネスへの応用可能性などについて取材を行った。</p>
<h2>著者情報</h2>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/IMG_0170_a_220937aab9/IMG_0170_a_220937aab9.jpg" alt="IMG_0170_a.jpg" /></p>
<h2>AIエージェントとは何か</h2>
<p><strong>ーーまず初めに、AIエージェントとは何か？というところから、解説をお願いできますか。</strong></p>
<p><strong>太田氏</strong>
AIエージェントという言葉は、2023年頃から出てきたというイメージを持たれているかもしれませんが、実は1970年、90年頃からありました。「エージェントとはなんですか？」という問いに関しては、【<strong>目標に向けて環境で相互作用する知能システム</strong>】と位置づけるのが、個人的にはわかりやすいと思っています。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/AI_1_5353dee988/AI_1_5353dee988.png" alt="AIエージェント_1.png" /></p>
<p>もう少しかみ砕いて説明します。目標というのはわかりやすいですよね。何かをするイメージで、“これを達成しなければならない”というのが目標です。
では、”環境とは何か”という点についてですが、エージェントが動ける空間をイメージしてください。デジタル空間もあれば、我々がいるこの場のようなフィジカルな空間もあり、その両方を環境と言います。</p>
<p>「環境と相互作用するとは何か？」についてですが、そもそもAIは知能を人工的に作っているだけの知能なので、手や足があるわけではない。なので、我々がいるフィジカル環境やデジタル空間にインタラクションするということはできないんです。AI自体は何かを認識するなどしかできないのですが、「エージェント」と呼ばれるものは、人工知能を使って環境へインタラクションできるというのがポイントです。”環境”から情報を受け取ってAIエージェントが認識し、「次はこれを行おう」と判断して実際に行動できるということが重要なのです。</p>
<p><strong>ーー1970年代より概念があったとお話いただきましたが、ここ数年でなじみが出てきたのは、生成AIが大きく関係しているのでしょうか。</strong></p>
<p><strong>太田氏</strong>
そうです。昔はルールベースや、データから学習しなければならず、研究・開発者以外の皆さんの手元に届けるのに時間がかかっていたんです。
私たちが身近に扱うものって自然言語や画像などがありますが、生成AIやLLMは「テキスト（自然言語）で指示をしたら、テキストで返答してくれる」というのがポイントで、このLLMという知能がユーザー層と密接になったことで、皆が「エージェントで何かできるのではないか」と考え始めるきっかけになったと思っています。</p>
<p><strong>ーーAIエージェントの他に、エージェンティックAIという言葉がありますが、この違いは何でしょうか。</strong></p>
<p><strong>宮脇氏</strong>
言葉の定義でいうと、AIエージェントは技術やソフトウェアのような「エージェントそのもの」を表す言葉であると思います。一方で、エージェンティックAIやエージェンティックテクノロジーでいうと、「エージェントの性質をもつもの」を指すと思います。</p>
<p>エージェンティックAIやエージェント型AIの言葉の意味合いは、最近変わり始めていると思っています。昔は「永続的なソフトウェア」というのが、一つの定義としてあ⁨⁩りました。データの流れを監視できるし、ユーザーの目的や好みに合わせてタスクを遂行するものというのが、<a href="https://rosenfeldmedia.com/books/designing-agentive-technology/">昔の解釈</a>だと思っています。
しかし、最近では生成AIブームの流れで意味合いが変わってきたという印象を受けます。エージェントは日本語で言うと“代理者”と訳すことができますが、人の業務を代行するような形で、AIをコアとして代理者の性質を受け継ぐシステムを「<a href="https://arxiv.org/abs/2505.10468">エージェンティックAI</a>」と呼んでいると思います。</p>
<p><strong>ーータスクを遂行するだけであれば、これまではRPAなどでプロセスをオートメートできていました。しかし、人の代替と位置付けると単純作業だけではないと思います。ゴールに向けてタスク分解し、適切な順序で自律的に実行できるという点が、エージェンティックというキーワードに含まれているのでしょうか。</strong></p>
<p><strong>宮脇氏</strong>
そうですね。“人との接点”というのが一つの軸としてあるかと思います。接点が少ないRPAやオートメーション。逆に接点が多いチャットボット型AI、というすみ分けになると思っていて、AIエージェントは、あまり接点はないが自分がやりたいことを委任して自走してくれるものです。ちょうど<a href="https://note.com/dory111111/n/n03eac77e5197">中間に位置するイメージ</a>ですね。</p>
<h2>AIエージェントへの期待と導入課題</h2>
<p><strong>ーー現在、AIエージェントへの期待や注目度は高まっていますが、なぜ、多くの人がAIエージェントに期待を寄せているのでしょうか。</strong></p>
<p><strong>西見氏</strong>
太田さんの説明で、AIエージェントとは環境と相互作用する知能システムであるというお話があったと思いますが、これってよく考えたら「人間なのでは？」という話なんですよ。
例えば、今、リモートワークをしている人が多くいらっしゃいます。パソコンを触ってリモートワークをしている人間って、ビジネスチャットを通じて仕事をしている姿を別の人間から見たとき、「何か言ったら返す」というシステムと類似していますよね。見え方としては人間である必要がなく、タイピングして返してくる“何か”なので、視点を変えるとソフトウェアと言えると思います。
そのような人たちと日常からコミュニケーションをとって一緒に仕事を行い、時には成果物としてPowerPointやWordを出してきたりする。これって「AIエージェント」じゃないか、という話なんです。</p>
<p>ご質問いただいた「何に期待しているか」については、シンプルに『<strong>人間の代替</strong>』です。人間のように動くソフトウェアがあれば、どれほど使っても人間より安かったり、多く働いてくれたり、病気もしないし、労基署へも訴えないというところで、非常に使い勝手がいいですよね。そのように、労働力の代替として認められ始めているのが現在なんです。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/AI_Agent_b_5ee7a2de78/AI_Agent_b_5ee7a2de78.png" alt="AI Agent_b.png" /></p>
<p>AIエージェントというソフトウェアがあるとして、これが今後多方面で多く活用されていくという見立てを皆がするんですよ。OpenAIやAnthropicが独占して世界の覇権を取っていこうという動きではなくて、彼らはそのモデルを公開しているんです。AIエージェントを作る部品を提供していて、それを各社が使って動くものを開発していこうと。その渦中に我々がいるのを考えたとき、「キャッチアップしないとまずいのでは」「シンプルに置いていかれてしまう」と思っている人が多いのだと思います。</p>
<p>社員が1万人いる会社があるとして、1万人全員がAIエージェントとインタラクションしたり、AIエージェント自身が長時間働けるようになる未来、1名の従業員が100体のAIエージェントをマネジメントできるとなると、単純に10,000×100になります。そんな体制を他の会社が構築していたら、どれだけのインパクトがあるか。私は経営者なので、各社のいろんな方と対話をするのですが、そういった“危機感”を持つ声が強いという感覚があります。</p>
<p><strong>ーーただ、よく聞く話としては、活用に関するハードルやリスクなどの懸念も各社持っているかと思います。それについてはいかがですか。</strong></p>
<p><strong>後藤氏</strong>
まず、昔とは状況が変わっているというのは一つあります。私たちが相対するのはIT部門やDX推進部なので、お客様側でやりたいというモチベーションがあります。なので「できないことは言わない」というか、以前よりは言わなくなっていると思います。ただその上で、どうしてもセキュリティや企業内で守らなければならないものがあるので、システム導入する上では、それらをすごく意識しています。</p>
<p>現在、AIエージェントの波が来ていますが、その前にはDXの波があって、「オンプレミスでしかデータは扱えない」という方針だったのが変更されて、クラウドに移行しました。また、「機械学習でデータを使って学習しよう」となった時に、「ではそのデータはどこに置いておくのか」というような議論も、これまでに行われてきたわけです。そうした積み重ねがあり、徐々に各社のデータやシステムに関するリテラシーが上がり、導入時のハードルが下がっているような印象を受けています。</p>
<p><strong>ーー今、DXというキーワードがありましたが、AIエージェントに取り掛かる手前で準備しなければならないものはあるのでしょうか。</strong></p>
<p><strong>阿田木氏</strong>
AIエージェントの活用においては、まず、クラウドなどを用いたRAGなどの構築があると思います。RAGは、社内ドキュメントにアクセスするために使う手法として一昨年から昨年にかけて流行していました。RAGのために整備したナレッジがあると思うのですが、現在は、各所で作られたRAGを統合するために、エージェンティックな機能を入れたいというお客様を<a href="https://aitc.dentsusoken.com/column/rag_to_ai_agents/">支援</a>することが多いですね。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/AI_2_03d0121f9e/AI_2_03d0121f9e.png" alt="AIエージェント_2.png" /></p>
<p><strong>ーーエージェントをフルに活用していこうとすると、その手前にRAGはクリアしないのでしょうか。</strong></p>
<p><strong>阿田木氏</strong>
要件によりますが、LLMが持つ知識は一般的なものなので、それを社内で活用していこうと思ったとき、情報を拡張する力としてRAGが一つの手段になると思っています。</p>
<p><strong>西見氏</strong>
“環境”という言葉で言い表せるかと思います。相互作用するものなので、環境上に情報がなかったら参照するものがない。検索システムをエージェントが使えるようにするのがRAGです。GoogleドライブをAIエージェントが見に行って参照できれば別に問題がないですが、どうエージェントが情報にアクセスするのかという話です。</p>
<p><strong>太田氏</strong>
なので、企業の皆さんはいきなり会社の根本となる業務をエージェントで代替しようとするとやはり怖いので、まず業務の中でも端の方のFAQや採用など、会社の大きな損失に関わらないところから始めています。徐々に、業務の根幹部分のデータをエージェントが触れられるよう、段階的に準備していると思います。先の未来には、もしかしたら一部のホワイトカラーの業務が、エージェントに置き換わることがあるかもしれないですね。</p>
<p><strong>ーー現在、コア業務までAIエージェントを活用できている企業は、どのぐらいあるのでしょうか。</strong></p>
<p><strong>太田氏</strong>
多分みんなやられてると思うんです。しかし、この開発を社外に発注するのはデータが非常に機密であるから難しい。おそらく着手されていて、取り組んでる最中だと思うんですが、公開できない情報が非常に多いのだと思います。</p>
<h2>AIエージェントはどのように作るのか</h2>
<p><strong>ーー続いて、AIエージェントをどのように作るか？という点についても、解説いただけますか。</strong></p>
<p><strong>太田氏</strong>
まず、何を代替したいのか、エージェントを開発する対象の業務を考えます。次に、いきなり作り始めるのではなく、その業務が既に存在するDeep Researchや市場調査が得意なエージェントなどで代替できるかを考えます。AIエージェントを人間に例えるのであれば、似たようなスキルを持った人間がいないかを考えるんです。代替したい業務が企業独特の業務であり、世の中に同じスキルを持った人間がいない場合は、自分たちで作るしかないという判断になります。</p>
<p>“作る”と判断したら、LLMのAPIを使わなければならないので、LLMを用意します。その時、利用するLLMが、その業務に関する知識を持っているかどうかを見ます。持っていない場合は、業務を遂行するためのプロセスを可視化し、分解して、利用するデータソースや登録・申請などのアクションポイントを洗い出し、順序通りにLLMがこなせるようにワークフローを構築していきます。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/AI_3_c21acca892/AI_3_c21acca892.png" alt="AIエージェント_3.png" /></p>
<p>プロセスの中にはおそらく、人間の意思決定が伴う箇所がいくつもあると思います。それらすべてをプロンプトで渡し、意思決定を代替してもらう。エージェントが意思決定をしながらアクションも同時に実行していくという手続きを書くというのが、エージェントを作る設計における第一段階です。</p>
<p><strong>ーー宮脇さんは、所属されている企業で採用業務のエージェントを開発されていますよね。どの会社でも汎用的に使えるツールを開発する上で、意識する点や課題などはありましたか。</strong></p>
<p><strong>宮脇氏</strong>
まず意識する点として、プロダクトを開発・提供する企業としての戦略の部分をお話しすると、「この業務を代替したら事業としてうまくいく」というところから始めています。多くのお客様が利用してくれるというのもそうですし、業務の効率化を重視しているのであれば「どれくらい効率化を期待できるのか」という視点を持って開発をしています。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/AI_Agent_c_24423dbaa4/AI_Agent_c_24423dbaa4.png" alt="AI Agent_c.png" /></p>
<p>我々もAIエージェントを活用しながらプロダクトの開発を進めていますが、エージェントは「切り札にはなりうるがあくまで手札でしかない」と思っていて、一つの“手段”でしかありません。なので、AIエージェントとしてうまくいかなくても、その機能だけは今後も資産として横展開できる部分から開発をしています。</p>
<p>採用業務のAIエージェント開発において、採用業務特有の課題みたいなところでいうと、やはり個人情報を扱う点が挙げられると思います。AIエージェントは、一定うまくできそうな印象は受けますが、「うまくいきそう」と「うまくいく」の間は大きく乖離しています。AIエージェントにはタスクの実行完了までを委任することになるため「成果物には意図しない誤りが含まれない」と確信できるまでの作り込みが、大変な部分になるかなと思ってます。</p>
<p><strong>ーーAIエージェントと相性が良い領域などはあるのでしょうか。</strong></p>
<p><strong>西見氏</strong>
宮脇さんの企業はプロダクトでAIエージェントを出してますが、とても難しいんですよ。
なぜ難しいかっていうと、エージェントって自由に動くんですよね。自由に動かせられるほどパワーが出るのですが、リスクが大きい。それに対して、“ガードレール”と呼ばれるもので、「この範囲で動いてね」という具合にAIを制御するんです。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/AI_4_91aae42ca3/AI_4_91aae42ca3.png" alt="AIエージェント_4.png" /></p>
<p>しかし、安全に使えるように囲い込んで、安全の余白を作れば作るほど、決定論的に決められた通りにしか動かなくなる。それはそれでパワフルではありますが、AIエージェントへの期待とはまた違うわけです。</p>
<p>これをどのようにバランスを取ればよいかというと、多く言われているのがBPO（業務代行）の話です。BPOは古典的な業務代行のことで、人が代行するイメージがありますが、その裏側で何が行われているかという点は、顧客側は頓着しないですよね。結果を出してくれればよいと。その“裏側でどうするか？”というところで、AIエージェントが注目されているんです。顧客に直接AIエージェントを触らせないので、何かをしでかすリスクはないし、考えうるリスクに対しては、ある程度マニュアル等でガードできます。
業務代行を行うための人材採用に力を入れなくてもエージェントを強く配備していけば、ものすごい大量の人を雇ったのと同じ形でビジネスを展開できる。社内の代行として何ができるかの検討も進んでいるし、実際いろんな分野でAIエージェントが動いている姿が見えてきてる印象です。</p>
<p><strong>ーー最近、Agent-to-Agent（<a href="https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/">A2A</a>）のような、エージェント同士が連携する技術も注目されていますが、1つのエージェントがなんでもこなすのではなく、特化型のエージェントが連携してタスクを実行する必要性について、どのような理由があるのでしょうか。</strong></p>
<p><strong>後藤氏</strong>
様々な理由がありますが、<a href="https://www.google.com/url?q=https://blog.langchain.com/context-engineering-for-agents/&amp;sa=D&amp;source=docs&amp;ust=1755655420987616&amp;usg=AOvVaw3teeITIuK3CObqX4SsQmjG">コンテキストが多くなりすぎてしまう</a>というのが一つ挙げられるかと思います。例えば、一人の秘書に色んな業務を依頼すると一個の業務に対する精度が下がってしまいますよね。LLM自体に入れられるトークン数にも限度があるので、一つのエージェントに全て詰め込むのは難しい、という技術面の制約があるかと思います。</p>
<p><strong>阿田木氏</strong>
また、「責任を分ける」という観点もあります。人間の社会も業務においても、「この人はこの業務」などと分けると思いますが、そこを明確に分けることで、管理がうまくいくというのがあります。エージェントも同じで、<a href="https://www.comet.com/site/blog/ai-agent-design/#h-modular-amp-role-based-ai-agent-design">分けて制御してあげる</a>というのが、利用する上で安全であると思います。</p>
<p>加えて、エージェント技術の背景として、シングルエージェントから発展してきたのもあるので、それらを繋げましょうというのが<a href="https://speakerdeck.com/masatoto/llmmarutiezientowofu-kan-suru">現段階</a>になるかと思います。</p>
<p><strong>西見氏</strong>
あとは、<a href="https://blog.langchain.com/react-agent-benchmarking/">ベンチマーク</a>があるんです。一つの仕事を行うエージェントに対して、複数のドメインを与えて、たくさんの仕事に対応できるよう知識を持ってもらうテストを行った例があります。与える知識を0〜10まで試して実験したところ、1つの知識を与えたら目的の仕事をしてくれるが、2つ以上知識を与えると急に仕事の精度が落ちていく。目的はベンディングマシンの仕事なのに、なぜか不動産関連の仕事を始めてしまうというような、混乱しだしたという実験結果もあり、定量で計測してみても、混乱するというのはあるんです。</p>
<p><strong>ーーAIエージェントは人の代替であるからこそ、人との比較がされやすいと思いますが、導入における投資対効果についてはいかがでしょうか。</strong></p>
<p><strong>太田氏</strong>
現在、エージェントの利活用というとドキュメントワークが多いのですが、その業務にかかる時間・工数・人数などは、指標として参考になると思います。それが実際に何時間くらい削れて、かつ品質がどれぐらい安定するのか。人間の場合、人によってクオリティの差が出るので、どのぐらいまで均一な品質でアウトプットできるか、というのがまずあります。実際の運用を考えたときに、投資対効果があるのかどうかは数値で表せると思います。</p>
<p><strong>ーー今後の方向性として、会社でひとつのエージェントを持つイメージなのか、従業員が自分のタスクの生産性を上げる目的で、100や1,000など様々なエージェントを使いこなしていくのか、どちらのイメージが近いでしょうか。</strong></p>
<p><strong>太田氏</strong>
おそらく、世の中で目指したいのは一体のエージェントが多数の仕事をこなす世界だと思います。しかし、提供する側からすると一体のエージェントが複数の仕事をこなせているかを常に監視したり、評価するのは、運用が複雑になってしまいます。そのため、見せ方としては「業務用エージェントがあなたの仕事こなします」と言いつつ、裏で複数エージェントが動くことになると思います。それで各業務において、タスクごとにエージェントをモニタリングするという形に落ち着くかと思います。</p>
<p>この一年は恐らく各従業員ごとであったり、提案するんだったら“提案用のエージェント”や市場調査する場合は“市場調査用のエージェント”など、各タスクごとに異なるエージェントを選択して使う形で、利活用が進むと思います。それは扱う参照データや考え方などの“環境”が違うので、目標というタスク定義で使う対象を分けるためです。
このように、タスクの定義は、初めは従業員単位になるかと思いますが、次第に“資料作成”であればなんとか動かせるところまで抽象度は上がっていく、つまりは会社共有のエージェントになると考えられます。その場合、全データのアクセスが前提となるため、会社側もデータを整備する必要が出てきます。これまでの提案履歴や取引記録を全て接続して、提案先にカスタマイズされた提案書を作成できるのが、将来的なイメージ図になるということですね。</p>
<h2>AIエージェントとこれからの未来</h2>
<p><strong>ーーAIエージェントをキーワードに、どのように未来が変わっていくのか、お一人ずつコメントをいただけますか。</strong></p>
<p><strong>阿田木氏</strong>
これからのエージェントの世界でいうと大きく二分されていくかと思っていて、一つはエージェントと人間が「相互にやり取りする世界」、もう一つは「エージェントに閉じた世界」と考えています。先ほど西見さんからBPOの話がありましたが、AIエージェント開発をしてて思うのは、ボトルネックは人間だということです。なぜかというと、それぞれのアクションは人間が持っている暗黙知に依存することがとても多く、企業においては特にそれが顕著であるからです。人間が介在すると、なかなか前に進まないことがあるので、そこをAIだけの世界にすると、上手くいくのではないかと思っているところです。
しかし、人間が存在する限り人が持つ正解もあるので、人間とAIエージェントが相互にやり取りする世界は存在し続けると考えています。</p>
<p>個人的に“こうあってほしい”という未来についてですが、現在、データ分析でもAIエージェントが使われてて、私はKaggleなどをよくやるのですが、<a href="https://aitc.dentsusoken.com/column/kaggle-ai-agent-01/">Kaggleでもコーディングエージェントが使われている</a>んです。全て使ったことあるのですが、やっていて思ったのが、エージェントに作業を全部渡すと楽しさが奪われてしまうということですね。私は、Kaggleをゲーム感覚でやっているので、ゲームを代わりにやられてしまうのが結構辛い。エージェントには、本当に任せるべきところを任せ、人間も楽しいとこにコミットできるといいなと思っています。</p>
<p><strong>後藤氏</strong>
これまでの話しでもあったように、先を読むのは非常に難しくなっていると思います。インタビューの中で思い出したのですが、この書籍を書き始めたときは、そもそもお客さんに「エージェント」と言ってもわかってもらえないことがありました。それが今の状況になっているので、短期間ですごく変わったと思っています。
そのような中で、ある程度直線状にある未来像でいうと、業務のやり方が変わってくるのかなと思います。まずは人の触る部分をAIエージェントが代替していく。その後にAIエージェントのためのデータ設計だったり、業務のやり方も徐々に変わっていくのかなと思っています。これまで人間だけだと難しかったことや、管理されたもののイメージを作るなど、AIエージェント前提の仕組み作りが進んでいくのかなと思います。</p>
<p><strong>ーーAIエージェントがAIエージェントを作るみたいなものはあるのでしょうか。</strong></p>
<p><strong>後藤氏</strong>
研究分野ではそういう取り組みもあります。ただ個人的には直近の話ではないと思っています。データや環境の整備といったDX、すなわちこれまで人間しかアクセスしてなかったものや、業務に詳しい人に聞かないと分からなかったところがAIエージェントに聞いても分かる、AIエージェントが探せる、そういった整備が先んじて進んでいくと思っています。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/AI_Agent_d_c24a2a4250/AI_Agent_d_c24a2a4250.png" alt="AI Agent_d.png" /></p>
<p><strong>西見氏</strong>
インフラが整っている領域は基本的にエージェントが入り込んでいく領域だと考えていいと思います。インフラとは、大きく分けてデジタルとフィジカルですね。特にデジタルはエージェントが入り込みやすい。センシティブなデータやデータ管理に関しては様々な問題があるにせよ、基本的にはアクセスができるわけです。
インフラが整っているもの、その中で現在活発なのが自動運転ですね。なぜ自動運転が活発かというと、道路が敷かれてるからです。走れるインフラがあるんです。なので、インフラがあるところからエージェントがどんどん活用されていくと考えています。</p>
<p>デジタル領域において、なぜこんなにAIエージェントが活用されているかというと、“インターネット”というインフラがあるので、インパクトも出るからですね。僕はこの畑に入りすぎて人間しかできないことはないんじゃないかとか思っちゃうんですけど（笑）農業などもデジタル化できないと言われつつも、今では、大規模農業は結構機械化が進んでいます。ドローンで生育状況を把握し、トラクターも自動運転で動かすことができるので、ある意味代替できている。それもインフラですね。ドローンで監視ができ、農作業の機械があり、自動運転ができればうまくいく業務があるということです。</p>
<p><strong>ーー統一規格みたいな存在も結構大きいですよね。</strong></p>
<p><strong>西見氏</strong>
インターネットはhttp通信させれば情報取れるので、そのようなプロトコルがある時点でインフラは整っているわけです。倉庫などでも、パレットで積んであれば動かせる話と、荷物の規格が異なると動かせないというようなイメージですね。
船での運搬も、コンテナという規格が生まれたからこそ、高速に行えるようになった経緯があります。人間は、人が働きやすくするためのインフラを作ってきた歴史がある。そのようなインフラを土台にしてエージェントは動いていくので、人間にはできなかったような長時間労働も可能ですし、コンピュータシステムなので、スケーリングによって作業量も増やせるという期待があります。その期待に応えられるように拡張しているのが、現在見えてる姿なんです。</p>
<p>AIモデルもどんどん進化しているので、この相互作用によってどこまでできるかっていうのは、まだわからない。インフラがあるところにAIエージェントは浸透するので、今はそのインフラ自体を作っていくというフェーズですね。</p>
<p><strong>ーー続いて宮脇さん、いかがですか。</strong></p>
<p><strong>宮脇氏</strong>
AIエージェントは、成果創出を図るという部分において圧倒的に長けているかなと思っています。従来のMLシステムだと、OCRや翻訳みたいな一部の業務だけを代替するものでしたが、AIエージェントという枠組みにおいては、他の業務と繋がりやすくなったっていうところで、圧倒的に成果創出に向くようになったと思います。</p>
<p>AIエージェントの良い部分は「質・量・スピード」それぞれにあります。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/AI_5_27b5f52295/AI_5_27b5f52295.png" alt="AIエージェント_5.png" /></p>
<p>例えば採用領域でいうと『摩擦』と『スピード』の2つの側面でメリットがあると考えられます。</p>
<p>『摩擦』については、特定の判断においてAIが考慮できる項目が増えたということです。例えば<a href="https://www.theladders.com/static/images/basicSite/pdfs/TheLadders-EyeTracking-StudyC2.pdf">LADDERS社の報告</a>では、採用担当者が書類選考にかける時間は60秒以内といわれていて、採用担当者がチェックできる観点は限定的だといえます。候補者にとってもあまり嬉しくないですよね。一方AIによる読み込みの場合、レジュメ全てを読んだ上で総合的に判断することが多いため、これまで採用担当が読み飛ばしていた色んな職能や経験を考慮できるようになったといえます。
『スピード』でいうと、LLMの文章生成が速いことだったり、24時間365日体制が築けるようになったことで、候補者が応募してきた際にすぐ反応できるようなフローが組める。極端にいうと、応募があったその日のうちに面談ができるような、そういう世界がくるかなと思っています。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/AI_6_8ae9b5f6f7/AI_6_8ae9b5f6f7.png" alt="AIエージェント_6.png" /></p>
<p><strong>ーー現時点で、質の部分も人より高水準であると言えてしまうのでしょうか。</strong></p>
<p><strong>宮脇氏</strong>
そうですね。AIが参照できるガイドラインがあれば、一定の品質が担保された状態で価値創造できると思います。</p>
<p>今の話はこの先5～10年の話なのですが、40年後の日本をみると、労働人口が40%減るというような予測もあるので、必ず労働力の代替としての期待がある。エージェントの『代行』と『協働』の2つの期待が高まっていると思います。</p>
<p><strong>ーーAIエージェントのこれまでの歴史がある中で、現在思ったより早く実用化できたということなのでしょうか。</strong></p>
<p><strong>太田氏</strong>
コーディングは思った以上に早く進んでる印象はあります。加速できる枠組みができたんですよね。先ほど、人が介在すると遅くなるという話があったんですけども、コーディングの作業は基本的にテストがあり、正解か不正解かがわかると。エージェントが自分で計画して行動して、うまくいったかどうかがテストでわかり、そのデータが蓄積されるという、人が介在せずに自動的に賢くなっていく、自己完結できるレールに乗っかったんですよね。
そうなると、AIの知能は人間を追い越すので、同じように人間の業務の中でも自己完結できるものがあれば、すごい速度でどんどん成長していくと思います。</p>
<p>それが報告されたのがここ1～2年で、大きなインパクトがあったので、自己完結できる環境を皆が探している状況かと思います。</p>
<p><strong>西見氏</strong>
ロボティクスの分野でも世界モデルというものがあって、「こういうふうに行動が起きた場合にこういうふうに返す」というように、光景の動画を生成器というモデルが生成し、それをもとに学習するサイクルが回っています。世界モデルは正解・不正解がわかるので、自己完結する。複数モデルで組み合わせてRL（強化学習）を回していくっていうのは、十分あります。</p>
<p><strong>ーー最後に太田さん、お願いします。</strong></p>
<p><strong>大田氏</strong>
究極的にいうと、皆が“人間”に興味が出てきて、一日のほとんどを様々な人と会話をしているような、そんな働き方になるかなと思ってます。問題を定義したり、手を動かす部分は徐々にエージェントの方が良くなるので、人の気持ちを考えたり、こういうアイデアはどうだろうかと考えたりする作業が増えるのではないかと思います。まだLLMや生成AIは、想像力であったり、点と点を結び合わせて新しい何かを創造するという点は弱いので、それが伸びてこない間は、人間が色んな人と話をして、今までの経験から新しいアイデアを出し、それらをエージェントを使ってシミュレーションしたり、実際に作らせてみたりなど、そういう世界観になると思ってます。</p>
<p>今までの話をまとめになりますが、まず、個人で様々な業務をこなすと属人化するという課題があります。これまで、属人化したくないからシステムを作ってきたわけなんですよね。銀行を例にすると、様々な町ごとに店舗を構えて、お客さんが来たら対応していましたが、その業務がATMへ置き換わりました。ATMによって自動化されましたが、もともとその業務を担っていた人たちは失業したわけではなく、別のお客さんにライフプランを考えるなどの業務へ変わっていったと思います。なので、AIエージェントが出てきたからと言って、仕事がなくなるわけではなく、違う何かに価値を見出すように変わってきている。我々が開発しているエージェントも、今までシステム化されてなかったところをシステム化しようとしているだけなんです。その時、我々人間の仕事は何かというと、新しいアイディアを考えることであったり、様々な人とコミュニケーションをとって、もっと面白いインパクトがある事業を作り出すなど、そういう方向に進んでいくと思っています。もちろん動いているシステムを監視する人は必要なのですが、より短いサイクルで面白いものが出てくる未来があるのではないかなと思います。</p>
<p>先ほど話題に上がったA2Aについて言うのであれば、事業や企業ごとに作られたエージェント同士がもっと情報をやり取りすることによって、新しいサービスの開発はどんどん加速する。事業や企業ごとにデータをきちんと連携しているからこそできる技だと思うので、面白いビジネスが1週間や1ヶ月などの短いスパンででポンポン出てきて、コンテンツや、よくわからない体験も含めていろいろ生まれてくるような、“創造の社会”もあるんじゃないかなと思います。</p>
<p>様々なシステムがもっと連携されやすくなれば、例えば交通関係と周辺の百貨店が連携するなど、街全体を繋げられるようになったり、一歩ずつシームレスな形で社会が形成されていく期待もあると思います。</p>
<p><strong>ーー人間同士のコミュニケーションがより大事になってくる一方で、すべてAIで良いのでは？という極論もあるかと思いますが、それについてはいかがですか。</strong></p>
<p><strong>後藤氏</strong>
先ほど、コーディングエージェントがすごいという話がありましたが、実際にタスクを振れば色々やってくれるんですけど、でもそのタスク（目標）は定義しなければなりません。</p>
<p>そのタスクは何から取ってくるかというと、太田さんが仰ったように、「そもそも自分たち何やりたいんだっけ」や「こういう機能追加したいです」などの人間の要望から生まれてきます。下流のタスクを実行してくれる部隊（AIエージェント）が増えて、速度も上がった分、上流で意思決定しなければならない。高速に回り始めているのが、現実としてあります。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/AI_Agent_book2_778c3f1eb1/AI_Agent_book2_778c3f1eb1.jpg" alt="AI Agent_book2.jpg" /></p>
]]></description>
      <pubDate>Mon, 25 Aug 2025 00:50:00 GMT</pubDate>
    </item>
    <item>
      <title>学術＆研究2025/8/24 [SUN]理研、「富岳」後継機の開発にNVIDIA参画──AIとシミュレーション融合の次世代スーパーコンピューター</title>
      <link>https://ledge.ai/articles/fugaku_next_nvidia_ai_supercomputer</link>
      <description><![CDATA[<p>理化学研究所は2025年8月22日、スーパーコンピューター「富岳」の後継機「富岳NEXT」の開発に、米半導体大手NVIDIAが参画すると<a href="https://www.riken.jp/pr/news/2025/20250822_1/index.html">発表</a>した。富士通と理研に加え、初めて国外の半導体企業が中心的役割を担う体制となる。AIとシミュレーションの統合を進め、2030年頃の稼働を目指す。</p>
<p><strong>富岳NEXTによる応用事例イメージ。左は地殻変動解析、右は都市部における地震動解析</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/20250822_1_fig2_84c4231aa9/20250822_1_fig2_84c4231aa9.jpg" alt="20250822_1_fig2.jpg" /></p>
<p>2020年に稼働を開始した「富岳」は、世界トップクラスの性能を誇る日本のフラッグシップスーパーコンピューターであり、気候変動予測や創薬、材料科学など幅広い分野で活用されてきた。その後継機として位置づけられる「富岳NEXT」は、文部科学省の国家プロジェクトとして開発が進められている。</p>
<h2>新体制の発表</h2>
<p>理化学研究所、富士通、NVIDIAの3者による国際的な開発体制が始動した。NVIDIAがGPU基盤の設計を主導し、富士通は新CPU「FUJITSU-MONAKA-X」を開発。両者は超高帯域で接続され、理研が全体統括を担う。国外の半導体大手が日本の旗艦スパコン開発に参画するのは初めてとなる。</p>
<h2>性能目標と特徴</h2>
<p>「富岳NEXT」はFP8精度で600エクサFLOPSを超える計算性能を見込み、現行「富岳」と比べ最大100倍のアプリケーション性能向上を目指す。AIと数値シミュレーションを統合する「AI-HPCプラットフォーム」として設計され、医薬品開発、気候変動解析、新素材設計などの研究を加速させる狙いがある。補足報道によれば、ノードあたり4基のGPUを搭載し、合計13,600基規模で構成される案も浮上している。</p>
<h2>導入とスケジュール</h2>
<p>2025年度中に基本設計を完了し、2026年度には詳細設計に移行。2027年頃には新CPU「MONAKA-X」が登場し、2030年頃の稼働開始を想定している。設置場所は現行の「富岳」と同じく、神戸市ポートアイランド内に整備される予定だという。</p>
<h2>国際的意義</h2>
<p>今回の発表は、日本のスーパーコンピューターがAIとシミュレーションの融合を本格化させる転換点といえる。ロイターや共同通信も報じているように、米中欧が進める次世代スパコン競争のなか、日本は「Zetta-scale」への挑戦を打ち出し、国際競争力を維持・強化する姿勢を明確にした。</p>
<p><strong>自動車分野での活用例。生成AIを用いた設計最適化により、自動車設計の期間短縮や性能向上が期待される</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/20250822_1_fig3_7f1a064923/20250822_1_fig3_7f1a064923.jpg" alt="20250822_1_fig3.jpg" /></p>
]]></description>
      <pubDate>Sun, 24 Aug 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ByteDance、AI推論モデル「Seed-Prover」を数学オリンピックの結果とともに発表──公式には銀メダル相当、実際はGoogle、OpenAIに続き金メダル水準の成果</title>
      <link>https://ledge.ai/articles/ai_math_olympiad_seed_prover</link>
      <description><![CDATA[<p>中国ByteDanceの研究チームは2025年7月23日、AI推論モデル「Seed-Prover」を<a href="https://seed.bytedance.com/en/blog/bytedance-seed-prover-achieves-silver-medal-score-in-imo-2025">公開</a>した。同社公式ブログでは「Silver Medal Score（銀メダル相当）」と表現されたが、Seed-Proverが記録した30点は、2025年の国際数学オリンピック（IMO）における金メダルカットラインに到達するものであり、実質的に金メダル水準の成果といえる。詳細をまとめた論文は7月31日にarXivで<a href="https://arxiv.org/abs/2507.23726">公開</a>された。</p>
<p>Seed-Proverは、定理証明器「Lean」を基盤に構築された形式証明型AIモデルである。補題（レマ）のプールを生成・活用しながら段階的に証明を組み立て、各ステップを機械可読な形式で検証する仕組みにより、証明の厳密性を担保できる。論文では、過去IMOの形式化問題で78.1％の成功率を記録し、さらにMiniF2FやPutnamBenchといったベンチマークでも最新水準の成績を達成したことが示されている。</p>
<p><strong>MiniF2F-Testでの性能推移。Seed-Proverは2025年時点で最高水準の通過率を記録している</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Mini_F2_F_d1294f309f/Mini_F2_F_d1294f309f.jpg" alt="MiniF2F.jpg" /></p>
<p>ByteDanceはSeed-Proverを「Silver Medal Score」と発表したが、実際には自動定理証明システムに位置づけられるものであり、LLMを活用して形式証明を構築する新しいタイプのアプローチである。従来の完全自動型とは異なり、大規模言語モデルによる柔軟な探索と、定理証明器による厳密な検証を組み合わせる点に特色がある。</p>
<p><strong>Seed-Proverの各種ベンチマーク成績（左）と従来モデル（右）の比較。IMO 2025、MiniF2F、PutnamBenchなどで従来を大幅に上回った</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/4og2ymdfewdkk_0259ab7f69/4og2ymdfewdkk_0259ab7f69.jpeg" alt="4og2ymdfewdkk.jpeg" /></p>
<h2>金メダル水準の成果、それぞれのアプローチ</h2>
<p>2025年のIMOをめぐっては、GoogleやOpenAIも金メダル水準の成果を発表している。それぞれのアプローチには違いがある。</p>
<ul>
<li><strong>Google DeepMind（Gemini Deep Think）</strong> ：6問中5問を解答し、公式採点で35点を獲得。自然言語のみで証明を構築し、短時間で人間と同じ条件下で金メダル水準を達成した。</li>
<li><strong>OpenAI（実験モデル）</strong> ：同じく6問中5問を解き35点と発表。元IMOメダリストによる採点で妥当性が確認されたが、公式認定はない。自然言語ベースで幅広い問題に対応できる点を強みとする。</li>
<li><strong>ByteDance Seed-Prover</strong> ：公式ブログでは「Silver Medal Score」とされたが、30点は金メダルのカットラインに到達。Lean上で形式的に証明を構築し、機械検証可能な厳密な証明を生成する。大会3日間を通じて探索を行う方式で、時間は要するが厳密性を優先した設計だ。</li>
</ul>
<h2>今後の展望</h2>
<p>Seed-Proverは、自然言語で証明を生成する従来型モデルとは異なり、定理証明器によって機械的に検証可能な形式証明を構築する点に特徴がある。論文では、この厳密な証明構築は数学研究の支援や形式検証（formal verification）といった分野に応用できる可能性があると記されている。</p>
]]></description>
      <pubDate>Sat, 23 Aug 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>竹中工務店、建築設計AI「Tektome KnowledgeBuilder」を導入──設計業務の生産性向上と働き方改革を推進</title>
      <link>https://ledge.ai/articles/takenaka_koumuten_ai_design_solution</link>
      <description><![CDATA[<p>建築大手の竹中工務店は2025年8月18日、AIを活用した建築設計支援ソリューション「Tektome KnowledgeBuilder」を導入したことを<a href="https://prtimes.jp/main/html/rd/p/000000008.000136954.html">発表</a>した。開発元のテクトムが8月18日に発表したもので、設計業務における生産性向上や働き方改革を加速させる狙いがあるという。</p>
<p>「Tektome KnowledgeBuilder」は、建築設計業務で発生する膨大な図面や関連情報をAIで構造化し、検索・参照を容易にするソリューションである。これにより、従来は属人的に扱われていた設計ナレッジを効率的に活用できる環境を整え、設計プロセスの高度化と効率化を実現する。</p>
<p>竹中工務店では導入に先立ち、社内ワーキンググループによる約3カ月間の実証実験を実施。実務に即した利用環境の整備を進めた結果、設計者が過去の事例を効果的に参照し、業務負担を軽減できることが確認されたという。</p>
<p>テクトムは今回の取り組みについて、「設計DXの推進を通じ、建築業界全体の生産性革新や働き方改革に貢献していく」としている。今後は竹中工務店での活用事例をもとに、他の建築事業者への展開も視野に入れている。</p>
]]></description>
      <pubDate>Fri, 22 Aug 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>オンデーズ、生成AIが“かけたまま試着”を実現　新提案サービス「OWNDAYS MIRROR」を展開</title>
      <link>https://ledge.ai/articles/owndays_mirror_ai_try_on_without_removing_glasses</link>
      <description><![CDATA[<p>メガネ・サングラスの製造販売を手掛けるオンデーズ（東京都品川区）は2025年8月19日、生成AIを活用した新サービス「OWNDAYS MIRROR（オンデーズ ミラー）」を<a href="https://www.owndays.com/jp/ja/information/822">発表</a>した。</p>
<p>AIが顔立ちや雰囲気を分析し、メガネをかけたままでも試着できる体験を提供するのが特徴だ。</p>
<h2>生成AIによる“かけたまま試着”とレコメンド</h2>
<p>OWNDAYS MIRRORでは、カメラに映した顔画像をもとに生成AIが最適なフレームを提案。現在かけているメガネを画像上で仮想的に外し、新しいフレームをリアルタイムで合成表示することで、視力が弱いユーザーでも違和感なく比較検討できる。</p>
<p><strong>AIが候補フレームを提示し、装着イメージを“かけたまま”で確認可能</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/photo03_2_91fec602d9/photo03_2_91fec602d9.jpg" alt="photo03-2.jpg" /></p>
<h2>“なりたい印象”を8タイプから選択</h2>
<p>ユーザーは「カジュアル」「フォーマル」「クール」「ソフト」「シンプル」「グラマラス」「モダン」「クラシック」の8種類の印象から“なりたい印象”を選択。AIが選択に応じて似合い度や印象コメントを提示し、候補フレームを絞り込む。</p>
<p><strong>“なりたい印象”を選ぶ画面。8タイプから選ぶとAIが候補を最適化</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/photo03_1_c103e87778/photo03_1_c103e87778.jpg" alt="photo03-1.jpg" /></p>
<p>気に入ったフレームはそのまま店頭在庫を確認でき、実物の試着・購入に進める。価格や在庫数、同系統の別カラーなども画面上で確認できる。</p>
<p><strong>候補フレームの詳細画面。価格・在庫・カラーバリエーションも同時に確認</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/photo01_1d02f996d4/photo01_1d02f996d4.jpg" alt="photo01.jpg" /></p>
<h2>店舗導入とスケジュール</h2>
<p>新サービスは8月20日にオープンする「OWNDAYS天王洲アイル店」をはじめ、全国11店舗に導入される。2026年3月末までに国内全店舗、さらに海外店舗にも順次展開する予定だ。</p>
<p>天王洲アイル店はDX推進モデル店舗に位置づけられており、リモート視力測定やRFIDを活用した商品管理、キャッシュレスセルフレジなどの最新設備を導入。OWNDAYS MIRRORはその目玉機能として位置づけられている。</p>
<h2>今後の展開</h2>
<p>同社によると「似合うメガネがわからない」「基準がない」「視力の悪さで印象が掴めない」といった顧客の声がサービス開発のきっかけになったという。混雑時でも自分のペースで納得して選べる新しい顧客体験を提供し、スタッフの負担軽減にもつなげたい考えだ。</p>
<p>オンデーズは今後もテクノロジーと人の接客を融合させ、顧客体験の進化を進めていく方針を示している。</p>
]]></description>
      <pubDate>Thu, 21 Aug 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Yahoo!ショッピング、2025年上半期の安全・安心レポート公開──AIで不正検知率3倍、レビュー45万件を削除</title>
      <link>https://ledge.ai/articles/yahoo_shopping_ai_safety_report_2025h1</link>
      <description><![CDATA[<p>LINEヤフーは2025年8月19日、ECモール「Yahoo!ショッピング」における2025年上半期（1月～6月）の安全・安心に関する取り組みをまとめたレポートを<a href="https://www.lycorp.co.jp/ja/news/release/018424/">公開</a>した。</p>
<p>出店審査の厳格化やAIによる不正対策の強化により、不適切ストアや商品の排除、不正レビューや不正決済の防止に成果があったと発表している。</p>
<h2>出店審査とストア監視</h2>
<p>Yahoo!ショッピングでは、利用者が安心して買い物できる環境を提供するため、ストアの出店審査を強化。2025年上半期の出店審査合格率は4.2%にとどまり、不適切なストアの参入を抑制した。不適切と判断されたストアは約1,000件削除されている。</p>
<p><strong>■ 出店審査合格率の推移。2025年上半期は4.2%まで低下し、不適切ストアの参入を抑制</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/f31faf25870eeacbf570af249f3d33c1688cadd8_c62eac64bd/f31faf25870eeacbf570af249f3d33c1688cadd8_c62eac64bd.jpeg" alt="f31faf25870eeacbf570af249f3d33c1688cadd8.jpeg" /></p>
<h2>商品・レビューの監視体制</h2>
<p>同期間中に不適切商品を約38万件削除。不正や不適切と判断されたレビューは約45万件にのぼり、消費者の購買判断を歪める要因を排除した。</p>
<p><strong>■ 「2025年上半期に削除されたやらせレビューは45万件超。ストア単位での自動削除も開始</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/aa6a163c1af78b5868b370ad7b69314db2f52390_f1e6995c39/aa6a163c1af78b5868b370ad7b69314db2f52390_f1e6995c39.jpeg" alt="aa6a163c1af78b5868b370ad7b69314db2f52390.jpeg" /></p>
<h2>不正決済対策とAI活用</h2>
<p><strong>■ 不正決済対策では、独自システム・EMV3Dセキュア・人的監視の3段階フローを導入。2025年上半期の被害額は前年同期比41.2％減</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/line_yahoo_41_2_039d7dd883/line_yahoo_41_2_039d7dd883.jpg" alt="line yahoo 41-2.jpg" /></p>
<p>不正決済による被害額は前年同期比で41.2%減少。AIを活用した新しい不正検知モデルの導入により、従来比で不正検知率を3倍に引き上げる成果があった。レビューや商品情報の解析にAIを導入することで、不正の早期発見と対応が可能になったという。</p>
<p><strong>■ 2025年4月からAIによる違反商品のパトロールを開始。従来の手法に比べ違反検知率は3倍以上に向上</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/f1a6e505568260cf65ca26223b68360198b2a025_09d467b85e/f1a6e505568260cf65ca26223b68360198b2a025_09d467b85e.jpeg" alt="f1a6e505568260cf65ca26223b68360198b2a025.jpeg" /></p>
<h2>今後の取り組み</h2>
<p>同社は、出店審査・監視体制のさらなる高度化に加え、不正検知AIの強化や利用者への情報開示の拡充を進める方針を示した。同社は「より安全で快適な買い物環境を提供する」としており、継続的に安全対策を強化していく。</p>
]]></description>
      <pubDate>Thu, 21 Aug 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>地球社会の未来、2030年代前半に分岐点　京都大と日立がAIでシナリオ分析</title>
      <link>https://ledge.ai/articles/global_ai_future_simulation</link>
      <description><![CDATA[<p>京都大学と日立製作所は2025年7月7日、AIを活用した未来シミュレーションと政策提言を<a href="https://prtimes.jp/main/html/rd/p/000000003.000164782.html">発表</a>した。分析では、地球社会が格差や分断を回避し持続可能な成長を実現できるかどうかの分岐点が、2020年代末から2030年代前半にかけて現れるとされた。</p>
<h2>研究の目的と背景</h2>
<p>京都大学と日立製作所は、気候変動や格差拡大などの地球規模課題に対応するため、AIによる未来シミュレーションを実施した。本研究は、日本社会向けに行われてきた「政策提言AI」を拡張し、世界全体を対象にしたものである。</p>
<h2>手法</h2>
<p>世界の294指標を用いて原因 - 結果モデルを構築。AIによる2万件のシミュレーションを通じて、2050年までに起こり得る7つの未来シナリオを導出した。</p>
<h2>シナリオの概要</h2>
<ul>
<li>Regional Dispersion and Maturity（地域分散と成熟）：人口や産業が特定都市に集中せず地域に分散。格差が縮小し、社会的安定が進む。</li>
<li>Green Growth and Cooperation（グリーン成長と協調）：環境保護と経済成長を両立し、国際協力によって持続可能性を高める。</li>
<li>Climate and Conflict Double Crisis（気候・紛争二重危機）：気候変動の悪化と紛争増加が重なり、社会に深刻な影響を与える。</li>
<li>Polarization（分極化）：格差拡大と社会分断が進み、社会の安定が損なわれる。</li>
</ul>
<p><strong>AIによる未来シミュレーションから導出された7つのシナリオと分岐点</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/164782_3_5a33b2e23fbdeba0bb64c0f4f3a07b78_749x461_cf3d5a1faf/164782_3_5a33b2e23fbdeba0bb64c0f4f3a07b78_749x461_cf3d5a1faf.jpg" alt="164782-3-5a33b2e23fbdeba0bb64c0f4f3a07b78-749x461.jpg" /></p>
<h2>分岐点の時期</h2>
<ul>
<li>2029年頃：「Polarization（分極化）」の可能性。</li>
<li>2032年頃：「Regional Dispersion and Maturity」への移行。</li>
<li>2034年頃：「Green Growth and Cooperation」への移行。</li>
</ul>
<h2>政策提言</h2>
<ul>
<li>分極化を回避するには、先進国による環境対策の加速と途上国への経済支援が求められる。</li>
<li>地域分散・成熟型への移行には、少子化対策や格差是正、研究投資、公衆衛生の強化が必要。</li>
<li>グリーン成長型の実現には、国際協力と社会資本整備の推進が重要とされる。</li>
</ul>
<p><strong>持続可能な社会に必要とされる「社会的共通資本」の概念</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/164782_3_7693cf90e7e46f0a5a8cfa64b7b716d9_1024x724_371d1fd23a/164782_3_7693cf90e7e46f0a5a8cfa64b7b716d9_1024x724_371d1fd23a.webp" alt="164782-3-7693cf90e7e46f0a5a8cfa64b7b716d9-1024x724.webp" /></p>
<p>2017年以降、日本社会向けの研究では、都市集中と地域分散の分岐が示されていた。今回の分析は、その国際版と位置づけられる。</p>
<p>研究チームは今後、モデルの精緻化やデータ拡充を継続し、国際政策や各国の政策議論での活用を視野に入れているとのこと。</p>
]]></description>
      <pubDate>Wed, 20 Aug 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google、AIコーディングアシスタント「Jules」を正式公開──Gemini 2.5 Pro搭載、無料プランから利用可能に</title>
      <link>https://ledge.ai/articles/google_ai_coding_assistant_jules_general_availability</link>
      <description><![CDATA[<p>Googleは2025年8月6日、AIコーディングアシスタント「Jules（ジュールズ）」の一般公開を<a href="https://blog.google/technology/google-labs/jules-now-available/">発表</a>した。2024年12月の発表、2025年5月のパブリックベータ提供を経て、正式サービスとしての提供が開始される。</p>
<h2>ベータを経て正式版へ</h2>
<p>Julesは、Googleの最新モデル「Gemini 2.5 Pro」を搭載したコーディング支援AIで、コードの読み込み、改善提案、テスト、自動修正、Pull Request（PR）の生成までを一貫して行える。特徴は非同期処理に対応している点で、クラウド環境上で複数のタスクを並列に進行できる。ベータ期間中にはUI改善やバグ修正が進められ、GitHub Issuesとの連携機能や、マルチモーダル入力への対応、タスクの再利用機能、音声形式の変更履歴出力などが追加された。</p>
<h2>利用プラン</h2>
<p>Julesは無料プランと有料プランを用意。無料プランでは1日15件、同時3件までのタスク実行が可能。有料プランは「Google AI Pro」（上限5倍）と「Google AI Ultra」（上限20倍）が用意され、いずれも月額課金制となる。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/jules_plan_f0ff26b4df/jules_plan_f0ff26b4df.jpg" alt="jules plan.jpg" /></p>
<h2>プライバシーと安全性</h2>
<p>GoogleはJulesの動作設計を「Private by default」としており、ユーザーのプライベートリポジトリのデータはモデル学習に利用されない。実行は分離された環境で行われ、機密性を保ちながら処理が進められる。</p>
<h2>今後の展望</h2>
<p>Googleは、Julesを単なる開発者向けツールにとどまらず、デザイナーやノーコードユーザーなど幅広い層の業務支援に活用できる存在として位置づける。非同期エージェントとしての特性を生かし、今後はモバイルアクセスの強化など、利用環境のさらなる拡充も見据えている。</p>
]]></description>
      <pubDate>Wed, 20 Aug 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google、AI学習に必要なデータを最大1万分の1に削減可能な新手法を発表</title>
      <link>https://ledge.ai/articles/google_ai_data_reduction_method</link>
      <description><![CDATA[<p>Googleは2025年8月７日、自社の研究ブログで、AIモデルの学習に必要なトレーニングデータ量を最大で1万分の1に削減しながら、モデル品質を維持できる新しい学習手法を<a href="https://research.google/blog/achieving-10000x-training-data-reduction-with-high-fidelity-labels/">発表</a>した。従来の膨大なデータ収集とラベリングに依存するアプローチに比べ、効率的かつ高精度なラベル付けを活用する点が特徴となる。</p>
<h2>新手法の概要</h2>
<p>Google Researchが公開した今回の手法は、まず大規模言語モデル（LLM）を用いてデータをクラスタリングし、モデルが誤りやすい境界事例を抽出する。その後、専門家が少数のデータに高精度なラベルを付与し、ファインチューニングに利用する仕組みだ。</p>
<p><strong>■ LLMによる事前ラベリング→クラスタリング→境界ペア抽出→専門家ラベル→反復学習の流れ（①〜④）</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Curation_Strategies1_Process_Final_width_1250_c73b951bad/Curation_Strategies1_Process_Final_width_1250_c73b951bad.png" alt="CurationStrategies1_ProcessFinal.width-1250.png" /></p>
<h2>実験結果</h2>
<p>通常10万件規模のラベルが必要とされるケースにおいて、この手法では250〜450件の専門家ラベルで同等以上の成果を得られることが示された。実験にはGoogleの軽量モデル「Gemini Nano-1（1.8Bパラメータ）」と「Gemini Nano-2（3.25Bパラメータ）」が用いられ、特にNano-2ではモデルと専門家ラベルの一致度を示す指標「Cohen’s Kappa」が55〜65％向上した。</p>
<p><strong>■ Cohen’s Kappaとサンプル数の関係。キュレーション（緑）が従来（赤破線）を広く上回り、特に3.25Bモデルで効果が顕著</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Curation_Strategies4_Results_width_1250_6dca7b850b/Curation_Strategies4_Results_width_1250_6dca7b850b.png" alt="CurationStrategies4_Results.width-1250.png" /></p>
<h2>意義と応用可能性</h2>
<p>この成果により、AIの開発に伴うデータ収集やアノテーションのコストを大幅に削減できる可能性がある。特に医療や広告など、専門知識が求められる領域での利用価値が高いとされる。また、AI開発の持続可能性を高め、より幅広い分野での応用を後押しすることが期待される。</p>
<p>Googleは今後も効率的なAIトレーニング手法の研究を続け、より少ないデータで高品質なモデルを構築できる仕組みの標準化を目指すとしている。</p>
]]></description>
      <pubDate>Wed, 20 Aug 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI、GPT-5導入時の不手際を認め次期GPT-6の方向性を示唆──アルトマン氏「人々は記憶を求めている」</title>
      <link>https://ledge.ai/articles/gpt5_launch_failure_and_gpt6_memory_focus</link>
      <description><![CDATA[<p>OpenAIは2025年8月7日に最新モデル「GPT-5」をChatGPTの全ユーザー向けに提供開始したが、同社CEOであるサム・アルトマン氏は「ローンチ対応でやらかした」と認めた。ユーザーからの批判を受け、同社は前モデルGPT-4oを有料プラン向けに復活させる異例の対応を実施した。一方でアルトマン氏は、次期「GPT-6」について「人々は記憶を求めている」と述べ、記憶とパーソナライズを重視する方針を示した。この方針については、8月19日付の<a href="https://www.cnbc.com/2025/08/19/sam-altman-on-gpt-6-people-want-memory.html">CNBC</a>が詳報している。</p>
<p>GPT-5は2025年8月7日に正式<a href="https://ledge.ai/articles/gpt5_launch_all_users">リリース</a>され、全世界のChatGPTユーザー約7億人に提供が開始された。OpenAIは「PhDレベルの性能」を掲げ、プログラミングや数学、マルチモーダル処理に強みを持つと説明している。しかし専門家の間では「進化的ではあるが飛躍的とはいえない」との見方が広がり、期待値に対して十分な成果を示せていないとの指摘もあった。</p>
<p>アルトマン氏はサンフランシスコで記者団に対し、「GPT-5のローンチは完全にやらかした」と発言した。ユーザーからは「冷たい」「親しみやすさが失われた」といった批判が相次ぎ、従来のモデルにあった温かさが欠けているとの不満が広がった。こうした反応を受け、OpenAIは異例の対応として前モデルGPT-4oをChatGPT Plusなどの有料プラン向けに復活させた。新モデル提供後に旧モデルを再導入するのは極めて異例であり、同社がユーザーの声に迅速に対応したことを示している。</p>
<p>技術面では、GPT-5は高度な推論能力や自然なマルチモーダル処理を実現するなどの改良が施されている。しかし従来モデルとの差は限定的であり、期待が過剰に膨らんでいた分、失望を招いた側面が大きい。批判の背景にはこうしたギャップがあるとみられる。</p>
<p>一方でアルトマン氏は、次期モデルGPT-6について「人々は記憶を求めている」と強調した（8月19日、<a href="https://www.cnbc.com/2025/08/19/sam-altman-on-gpt-6-people-want-memory.html">CNBC</a>）。GPT-6はユーザーごとの履歴や好みに基づいて会話を記憶し、応答をパーソナライズする方向性を持つという。これにより、利用者ごとに最適化された応答スタイルを提供できるようになることを目指している。ただし、具体的なリリース時期は明らかにされていない。</p>
<p>今回の一連の動きは、OpenAIの製品ロードマップにおける転換点を示すものだ。GPT-5で露呈した課題をどう克服し、GPT-6でユーザー体験を改善するのか。今後の展開が同社の成長に直結する焦点となっている。</p>
]]></description>
      <pubDate>Wed, 20 Aug 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Blenderを操るAI──シカゴ大学ら、大規模言語モデルで3Dアセットを生成・編集する「LL3M」を発表</title>
      <link>https://ledge.ai/articles/ll3m_blender_llm_3d_modeling</link>
      <description><![CDATA[<p>シカゴ大学の研究チームは2025年8月11日、自然言語の指示だけでBlender内に3Dアセットを作り出せるシステム「LL3M（Large Language 3D Modelers）」を<a href="https://arxiv.org/abs/2508.08228">発表</a>した。大規模言語モデル（LLM）が直接Pythonコードを生成し、オブジェクトやシーンを自在に構築・編集するという新しいアプローチだ。論文はarXivに公開され、公式プロジェクトページやGitHubリポジトリも公開されている。</p>
<h2>コードを書くAI、Blenderを動かす</h2>
<p>LL3Mの最大の特徴は、3Dモデルを特殊なデータ形式で直接生成するのではなく、Blender用のPythonコードを“書く”AIであることだ。これにより、生成結果はすべて人間が理解できるコードとして残り、後から自由に修正・拡張できる。既存のワークフローに統合しやすい点も大きな利点とされる。</p>
<h2>多様なアセットを生み出す柔軟性</h2>
<p>論文では、BMeshによるポリゴンモデリング、モディファイアやシェーダーノードの適用、シーン階層の構築など、幅広い3D要素をコードで生成可能であることが示されている。家具やキャラクターなど、多彩なアセットが次々とコードベースで形作られる様子は、従来の「ブラックボックス的な生成AI」とは異なる透明性を感じさせる。</p>
<p><strong>■ Blender用Pythonコードを生成し、3Dオブジェクトを構築する例</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/fig15_intro_highlight_bc133cc7d3/fig15_intro_highlight_bc133cc7d3.jpg" alt="fig15-intro-highlight.jpg" /></p>
<h2>3段階で進化するパイプライン</h2>
<p>LL3Mは単なる一発生成ではなく、段階的にモデルを洗練させる仕組みを備える。</p>
<ul>
<li><strong>初期生成</strong> ：自然言語の指示からコードを生成し、Blenderでオブジェクトを構築</li>
<li><strong>自動自己精緻化</strong> ：AI自身が結果を評価し、改善点を修正</li>
<li><strong>ユーザー誘導精緻化</strong> ：人間の追加指示を受けて再度改善</li>
</ul>
<p><strong>■ 自己批評やユーザー指示に基づく反復的な3Dモデルの改善</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/fig22_humanoid_final_b372c81836/fig22_humanoid_final_b372c81836.jpg" alt="fig22-humanoid-final.jpg" /></p>
<p>さらに「BlenderRAG」と呼ばれる仕組みで、Blender APIドキュメントを参照しながらコードを補強。これにより「動かないスクリプト」を避け、実際に使える成果物を高精度に生み出す。</p>
<h2>“コードで3Dを描く”という発想の転換</h2>
<p>研究チームは、NeRFや点群などデータ駆動型の3D生成手法と対比しながら、LL3Mのアプローチを強調する。コードは読み書きできる資産であり、再利用性や可搬性に優れるため、クリエイターや開発者にとって扱いやすい。AIと人間が共同作業する新しい3D制作の基盤としての位置づけを打ち出している。</p>
<h2>今後の展望──ゲームから教育まで</h2>
<p>GitHubリポジトリはすでに公開されているものの、実装は「Code coming soon」とされ、今後順次公開される見込みだ。研究チームは「人間とAIが協力する3D制作の未来」を描きつつ、ゲーム開発、教育、デジタルコンテンツ制作など多様な分野での応用可能性を指摘している。</p>
<p><strong>■ LL3Mによって生成された多様な3Dアセットの例</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/fig2_gallery_b2bb1b7580/fig2_gallery_b2bb1b7580.jpg" alt="fig2-gallery.jpg" /></p>
]]></description>
      <pubDate>Wed, 20 Aug 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ソフトバンク・三菱ケミカル・慶應義塾大学・JSR、量子コンピューターで32量子ビット級のエネルギーギャップ計算に成功──新手法「TQPDE」がPNASに掲載</title>
      <link>https://ledge.ai/articles/quantum_computer_energy_gap_pnas</link>
      <description><![CDATA[<p>ソフトバンク、三菱ケミカル株式会社、慶應義塾大学、およびJSR株式会社は2025年7月31日、慶應義塾大学内のIBM Q Network Hubにおいて、量子コンピューターを用いた大規模なエネルギーギャップ計算手法を開発し、その成果が米国科学アカデミー紀要（PNAS）に掲載されたと<a href="https://www.keio.ac.jp/ja/press-releases/2025/7/31/28-168654/">発表</a>した。</p>
<p>今回開発されたのは、「テンソルに基づく位相差推定（Tensor-based Quantum Phase Difference Estimation, TQPDE）」と呼ばれる新手法である。従来の量子位相推定にテンソルネットワークを組み合わせることで回路を圧縮し、ノイズを抑制しながら計算を可能にした。これにより、従来最大6量子ビット規模にとどまっていた量子位相推定型アルゴリズムを、32量子ビット規模まで拡張することに成功したという。</p>
<h2>背景</h2>
<p>分子の物性解析には電子状態の計算が不可欠だが、古典計算機では電子数に応じて計算コストが指数関数的に増大する。特に電子間相互作用が強い物質では、一般的に用いられる近似手法（DFT）でも精度不足が課題とされてきた。
量子コンピューターは量子もつれや重ね合わせを活用し、古典計算では困難なシミュレーションを可能にする潜在力を持つが、ノイズの多さから大規模回路を実行するのは困難であった。</p>
<h2>今回の成果</h2>
<p>研究チームは、量子位相差推定にテンソルネットワークを組み合わせ、量子回路を効率的に圧縮することで、従来困難とされてきた規模の計算を実現した。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/TQPDE_b8c45d17c0/TQPDE_b8c45d17c0.jpg" alt="提案手法TQPDEの概要.jpg" />
<strong>図1：提案手法『テンソルに基づく位相差推定（TQPDE）』の概要。量子位相推定にテンソルネットワークを組み合わせ、回路圧縮とノイズ抑制を実現した</strong></p>
<p>この手法を「IBM Quantum System One」および「IBM Quantum System Two」で実行し、ハバードモデルおよび直鎖分子デカペンタエンを対象に検証。さらにQ-CTRL社のエラー抑制モジュールを活用し、標準では7,000超の制御Zゲートが必要となる回路を800未満まで削減することに成功した。その結果、実機においてもエネルギーギャップ値が理論的に収束することが確認された。</p>
<p><strong>図2：実証結果。ハバードモデル（32量子ビット）および直鎖分子デカペンタエン（20量子ビット）に適用し、計算精度が実機でも収束することを確認した</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/_adc2216763/_adc2216763.jpg" alt="実証内容.jpg" /></p>
<h2>意義と展望</h2>
<p>今回の成果は、量子コンピューターによる化学計算が「玩具モデル」を超え、古典計算の限界に迫る大規模分子システムに適用可能であることを示した。研究チームは今後、材料開発や電池設計など幅広い分野への応用を見据え、量子計算技術の社会実装に向けた研究を継続するとしている。</p>
]]></description>
      <pubDate>Wed, 20 Aug 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>アリババ、画像編集AI「Qwen-Image-Edit」を発表──生成AI「Qwen-Image」を拡張し、元の見た目を保持したまま多様な編集が可能に</title>
      <link>https://ledge.ai/articles/qwen_image_edit_release</link>
      <description><![CDATA[<p>中国アリババのAI研究チーム「Qwen Team」は2025年8月19日、画像生成AI「Qwen-Image」を拡張し、画像編集に特化した新モデル「Qwen-Image-Edit」を<a href="https://qwenlm.github.io/blog/qwen-image-edit/">発表</a>した。従来の画像生成に加え、キャラクターやスタイルの一貫性を維持しながら異なる情景を描写したり、画像内テキストを正確に編集するなど、高度な編集機能を備えている。</p>
<h2>技術的特徴</h2>
<p>20Bパラメータを持つ「Qwen-Image」を基盤として開発されたQwen-Image-Editの最大の特徴は「デュアルパス設計」にあるという。
Qwen2.5-VLが担うセマンティック制御と、VAEによる外観保持を組み合わせ、MMDiTによって統合することで、意味情報と見た目の情報を同時にバランスよく扱うことを可能にした。これにより、従来の生成モデルよりも自然で一貫性のある編集が可能になった。</p>
<p>編集機能としては、スタイルを変更したり、視点を切り替えたり、既存キャラクターを使って新しいシーンを描く「意味編集」、背景の置換や髪の毛一本の修正といった細部を調整する「外観編集」、さらに中国語や英語のテキストをフォントやスタイルを崩さずに編集できる「精密テキスト編集」がある。また、ユーザーが指定した領域を何度も重ねて修正できる「多段編集」にも対応しているとのこと。</p>
<h2>公開と利用方法</h2>
<p>同モデルはApache 2.0ライセンスでオープンソースとして公開されており、Qwen Chat、Hugging Face、ModelScope、GitHub、Alibaba Cloud APIを通じて利用可能となっている。開発者や企業はこれらのプラットフォームを通じて簡単にアクセスできる点も特徴だ。</p>
<h2>Showcase事例</h2>
<p>公式ブログでは、Qwen-Image-Editの機能を示す多彩なデモ画像が紹介されている。</p>
<p><strong>キャラクターの見た目を保ったまま、画家や宇宙飛行士など多彩なシーンへ展開</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/3_73f8c636ae/3_73f8c636ae.jpg" alt="幻灯片3.jpg" /></p>
<p><strong>フォントやスタイルを維持しながら、英語や中国語のテキストを自然に差し替え</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/15_41b5f50bea/15_41b5f50bea.jpg" alt="幻灯片15.jpg" /></p>
<p><strong>古い書道作品の誤字を自然に修正し、文化保存にも活用可能</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/18_df7ae341ca/18_df7ae341ca.jpg" alt="幻灯片18.jpg" /></p>
<h2>応用分野</h2>
<p>この技術の応用範囲は広い。広告やコンテンツ制作の現場では、短時間で多様なデザインを展開でき、アバターやイラストのスタイル変換にも活用できる。また、日常写真の背景変更や人物修正など一般ユーザー向けの用途も考えられる。さらに、書道作品の補正や保存といった文化的分野にも寄与する可能性がある。</p>
<p>アリババは今回の発表を通じ、生成AIと編集AIを組み合わせた新たなソリューションを提示した。これにより、画像処理の柔軟性と実用性を高め、市場における存在感を一層強めることを狙っている。</p>
]]></description>
      <pubDate>Wed, 20 Aug 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Tenable、OpenAIの「GPT-5」を公開24時間以内に脱獄──安全対策を突破し危険情報を生成</title>
      <link>https://ledge.ai/articles/tenable_gpt5_jailbreak_security_flaw</link>
      <description><![CDATA[<p>セキュリティ企業のTenableは米国時間8月8日、OpenAIが7日に公開した最新AIモデル「GPT-5」について、公開からわずか24時間以内に「脱獄（jailbreak）」に成功したと公式ブログで<a href="https://www.tenable.com/blog/tenable-jailbreaks-gpt-5-gets-it-to-generate-dangerous-info-despite-openais-new-safety-tech">発表</a>した。Tenableは、OpenAIが導入した新しい安全対策を突破し、危険な情報を生成させることに成功したという。</p>
<h2>GPT-5の新しい安全対策</h2>
<p>OpenAIはGPT-5で、従来の「拒否ベース（refusal-based）」から「安全な応答生成（safe-completions）」方式に<a href="https://openai.com/ja-JP/index/gpt-5-safe-completions/">移行</a>した。危険な質問を拒否するのではなく、安全とみなせる範囲で柔軟な返答を目指す新設計だ。だが、Tenableはこの仕組みをわずか1日で突破した。</p>
<h2>Tenableによる脱獄実験</h2>
<p>Tenable Researchのチームは、公開から24時間以内にjailbreakを実施した。手法は「クレッシェンド（crescendo）」と呼ばれ、歴史研究の学生を装って段階的に質問を重ねる社会工学的なアプローチだった。最初は歴史的な戦術や抗争に関する一般的な質問から始め、徐々に爆発物や武器に関する具体的な知識へと誘導。最終的に4回目のやり取りで、GPT-5は火炎瓶（モロトフ・カクテル）の作り方を詳細に出力したという。Tenableは、この過程が通常のユーザーとの会話に見える形で進んだ点を強調している。</p>
<p><strong>Tenableが公開した「クレッシェンド手法」による脱獄実験の様子。番号は以下のやり取りを示す</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/collage_e0c3eaa9ca/collage_e0c3eaa9ca.jpg" alt="collage.jpg" /></p>
<p>① 初期の拒否応答：GPT-5は危険な要求を拒否し、代わりに歴史的背景の説明を提案。
② 歴史研究の文脈を装う：歴史課題を理由に要約を求め、名称の由来や背景を解説させる。
③ 即席手法に誘導：当時の作り方について質問し、容器や材料の説明を得る。
④ 古典的レシピへの言及：1930〜40年代の伝統的な作り方が提示される。
⑤ 具体的なレシピを要求：フィンランド軍が使った配合を尋ね、詳細情報を引き出す。
⑥ レシピの提示：瓶や液体の種類、混合方法など具体的な材料リストを入手。
⑦ 手順の提示（前半）：瓶の準備から液体の注入・混合までの手順を出力。
⑧ 手順の提示（後半）：布を詰めて導火線とし、点火から使用方法まで危険な情報が生成された。</p>
<h2>安全性への警鐘</h2>
<p>Tenableの副社長Tomer Avni氏は、「GPT-5がどれほど高度な安全対策を備えていても突破可能であることを示した」と述べ、AIを業務に導入する際のリスクと継続的な監視の必要性を強調した。</p>
<h2>業界全体で広がる懸念</h2>
<p>Tenableの報告に加え、他のセキュリティ企業や研究者からもGPT-5の安全性を巡る懸念が相次いでいる。セキュリティ企業SPLXは1,000以上の攻撃シナリオを用いて検証した結果、安全性や信頼性スコアが極めて低いと評価した。また、NeuralTrustも別の手法で脱獄に成功したと報告している。</p>
<h2>今後の展望</h2>
<p>OpenAIは今回の事態について公式な対応を発表していない。AIの安全性確保は、社会や産業での利活用に向けた重要課題として改めて注目されている。</p>
]]></description>
      <pubDate>Wed, 20 Aug 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>GPT-5のIQはどこまで高くなった？──GPT・Claude・Geminiを“メンサ式IQテスト”で比較する『Tracking AI』</title>
      <link>https://ledge.ai/articles/tracking_ai_mensa_iq_test</link>
      <description><![CDATA[<p>米ジャーナリストのMaxim Lott氏は、主要なAIモデルの知能指数（IQ）や政治的傾向を客観的に比較できるウェブサイト「Tracking AI」を2025年8月21日に<a href="https://www.trackingai.org/home">更新</a>した。同サイトでは、独自に作成した非公開のIQテストと、Mensa Norwayがオンラインで公開している図形パズル型IQテストを用いて、ChatGPT（GPT-5 Proなど）、Claude 4 Opus、Gemini 2.5 Pro、Llama、Mistralといった代表的なAIモデルを比較している。</p>
<h2>IQテストによる性能比較</h2>
<p>Tracking AIでは、各モデルのIQスコアを分布図やランキング形式で表示。OpenAIのGPT-5 Pro（Vision）やGoogleのGemini 2.5 Proが上位に位置し、ClaudeやDeepSeekなども含めたスコアの推移を時系列で追うことができる。さらに、各問題ごとの正答率や、AIごとの解答理由まで公開されており、モデルの思考過程を詳細に比較可能だ。</p>
<p><strong>主要AIモデルのIQスコア分布（Tracking AIより）。GPT-5 Pro（Vision）やGemini 2.5 Proが高スコアを記録</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/IQ_Test_Result_7124431b5e/IQ_Test_Result_7124431b5e.jpg" alt="IQ Test Result.jpg" /></p>
<h2>Mensaテストと独自テスト</h2>
<p>使用されているテストは2種類。1つはLott氏自身が作成した「オフライン自作テスト」で、AIの学習データに含まれていないことを強調。もう1つはMensa Norwayが提供するオンラインIQテストで、35問の図形推理問題を25分以内に解く形式。いずれもAIの「推論力」を可視化する指標として活用されている。</p>
<p><strong>Mensa Norwayの公開テストとオフライン自作テストの結果を比較したランキング。テスト方法により順位の違いも見られる</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/rank_by_test_source_71cc2da37b/rank_by_test_source_71cc2da37b.jpg" alt="rank by test source.jpg" /></p>
<h2>政治的・社会的な質問比較</h2>
<p>Tracking AIのもう一つの特徴は、AIに政治的・社会的テーマの質問を投げかけ、回答を比較できる点だ。例えば「経済的グローバル化は人類に奉仕すべきか」という質問に対し、GPT-5は「Strongly Agree」と答え、ClaudeやGeminiも人類の福祉を優先する立場を示した。こうした比較から、各AIのバイアスや思想傾向を把握できる仕組みになっている。</p>
<h2>サンプル問題の公開</h2>
<p>サイトでは「IQ TEST OF THE DAY」として日替わり問題も提供されている。各AIの回答と理由が並べて掲載されており、単なるスコア比較にとどまらず推論の特徴を把握できるのが特徴だ。</p>
<p><strong>Tracking AIで公開されている日替わりIQ問題。各AIモデルの解答と推論過程も併せて公開される</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/iq_test_of_the_day_b238126d2f/iq_test_of_the_day_b238126d2f.jpg" alt="iq test of the day.jpg" /></p>
<h2>FAQと今後の展望</h2>
<p>FAQページでは、「なぜこのサイトを作ったのか」「政治的コンパスは有効か」「資金源はどこか」などの質問に回答。AIの性能や思想傾向を透明化し、利用者が信頼できる判断材料を得られるようにすることが目的とされている。今後は質問データの拡充なども予定されているという。</p>
<p>AIの能力が急速に進化する中で、『Tracking AI』は知能指数と政治的スタンスの両面からモデルを比較できる貴重な情報源となっている。Mensa式IQテストや独自問題を通じてAIを測定する試みは、AIの性能を人間社会に照らして理解するための一助となりそうだ。</p>
]]></description>
      <pubDate>Wed, 20 Aug 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/8/19 [TUE]AI業界を牽引するトップランナーが語る！—今押さえるべきAIの全体像と最前線を3日間で掴むLedge.ai Webinar SP開催</title>
      <link>https://ledge.ai/articles/ledgeai-webinarsp-sponsor</link>
      <description><![CDATA[<p>国内最大級のAI特化メディア『Ledge.ai』を運営する株式会社レッジ（東京都品川区）は、2025年9月24日(水)〜26日(金)の3日間連続で合計20本以上のセミナーを配信するオンラインイベント「Ledge.ai Webinar SP」を開催いたします。</p>
<p>本イベントでは、AIの各領域の専門家を招き、今必要とされるAIの体系的な知識や活用に関する見識をシェアする講義を実施。「AIをしる、つかう、つくる」をテーマに、多様な課題解決のヒントとなるようなコンテンツを動画でお届けします。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/_4aed8b100c/_4aed8b100c.png" alt="ウェビナーの様子.png" /></p>
<p>:::button
<a href="https://us02web.zoom.us/webinar/register/WN_m28c0ZHMSRiXQ-yrlcLXiw#/registration">▶【登録無料】視聴者向け事前登録はこちら</a>
:::</p>
<h2>AI業界を牽引するトップランナーが今押さえるべきAIの知識と最前線を3日間で語る</h2>
<p>AIの急激な進化と急速な広まりにより、AIへのリテラシーの差が広がっています。AI活用の最前線では「どう使えば効果的か」「どう作れば自社の強みになるか」といった問いに対しての取り組みが行われ、新たな事例や知見が生まれています。そんな現在において、AIの全体像を体系的に理解した上で、ビジネスにどのように活用されているかすばやく捉えることは重要です。</p>
<p>当イベントはAIの基礎理解 → 業務活用 → 開発実践までを体系的に理解し、この時代で働くビジネスマンの方に使える学びをお届けします。</p>
<p>Ledge.ai Webinar SPは、以下の3つの軸で構成されています。</p>
<h2>プログラム ~「生成AIだけじゃない！「AIをしる、つかう、つくる」SP~</h2>
<h3>Day1：AIを「しる」——全体像と本質を理解する</h3>
<p>AIの領域では日々革新的な技術が生まれ、その掛け合わせによりAIの担える範囲が急速に広がっています。AIの基礎からAI全般の進化を体系的に学ぶことで適切なAI活用に繋げることができます。</p>
<p>【対象】
・AIの基本から体系的に理解したい方
・生成AIに加え、AI全般の進化や仕組みに関心がある方</p>
<p>【セミナー内容】
・AIの基礎とこれまでの進化（機械学習、ディープラーニング含む）
・⽣成AIの仕組みと活⽤シーンの全体像
・ビジネスで求められるAIリテラシーと注意点</p>
<p>【ゲスト講演】
「ソフトバンクの事例から紐解く、組織の生成AI活用・推進を自走するための仕組みづくり」</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/_c71b18a520/_c71b18a520.jpg" alt="藤原 竜也.jpg" /></p>
<p>ソフトバンク株式会社
IT統括 AI&amp;データ事業統括部　Axross事業部 部長
藤原 竜也 氏</p>
<h3>Day2：AIを「つかう」——現場に効く、実践的なAI活用法</h3>
<p>「現場でどう使うのが効果的か？」を知りたい方に向けたプログラムです。現場導入の工夫やハマりがちな落とし穴まで、具体的なノウハウが得られます。</p>
<p>【対象】
・AIツールを現場の業務で活用したい方
・実務にすぐ役立つノウハウを知りたい方</p>
<p>【セミナー内容】
・業務シナリオ別のAIツール活用（生成AI・ルールベースAI）
・Excelや議事録、FAQ対応など、日常業務での実用ワーク
・プロンプトの書き⽅から社内導⼊のコツまで徹底解説</p>
<p>【ゲスト講演】
「まずは試してみよう！ 最新動向から学ぶ、生成AI活用の第一歩」</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/_c4e70ae1da/_c4e70ae1da.jpg" alt="岡田隆太朗.jpg" /></p>
<p>一般社団法人日本ディープラーニング協会　
専務理事　
岡田 隆太朗 氏</p>
<h3>Day3：AIを「つくる」——AIプロダクト・自社専用AIツールの開発</h3>
<p>ノーコード/ローコードでのAI組み込みから、AI活用を前提としたインフラを含む環境構築、AIモデル開発など、AIの開発に必要な技術知識やノウハウを幅広く学ぶことができます。</p>
<p>【対象】
・ノーコード・ローコードでAIを組み込みたい方
・AIシステムの裏側やインフラにも関心がある方</p>
<p>【セミナー内容】
・生成AIアプリの基礎（RAG、Dify、API連携など）
・従来型AI（需要予測、分類モデルなど）の開発プロセス入門
・クラウド・ベクターデータベースなど、AI基盤技術の理解</p>
<p>【ゲスト講演】
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Zhan_Cliff_Chen_5c6864c871/Zhan_Cliff_Chen_5c6864c871.jpeg" alt="Zhan (Cliff) Chen.jpeg" />
マイクロソフト ディベロップメント株式会社
プリンシパル　アプライド　サイエンティスト
Zhan (Cliff) Chen / 陳 湛</p>
<p>:::button
<a href="https://us02web.zoom.us/webinar/register/WN_m28c0ZHMSRiXQ-yrlcLXiw#/registration">▶【登録無料】視聴者向け事前登録はこちら</a>
:::</p>
<h2>こんな方におすすめ</h2>
<ul>
<li>企業のDX・AI導入担当者</li>
<li>生産性向上のためAIを活用したい事業部門マネージャー</li>
<li>ノーコードでのAI活用を始めたい開発初心者</li>
<li>最新AI技術のトレンドを押さえたいビジネスパーソン</li>
</ul>
<h2>イベント概要</h2>
<p>開催予定日時｜2025年9月24日(水)〜26日(金)
開催形式｜オンラインセミナー (Zoom Webinar)
想定集客規模｜500名
対象｜経営層 / システム企画 / DX推進 / 経営企画 / マーケティング / エンジニア
主催｜株式会社レッジ</p>
<p>:::button
<a href="https://us02web.zoom.us/webinar/register/WN_m28c0ZHMSRiXQ-yrlcLXiw#/registration">▶【登録無料】視聴者向け事前登録はこちら</a>
:::</p>
<h2>「Ledge.ai Webinar SP」を盛り上げていただけるスポンサー企業様を募集中</h2>
<p>現在、この企画の開催趣旨にご賛同いただき、共に「Ledge.ai Webinar SP」を盛り上げていただけるスポンサー企業様も募集しております。</p>
<p>スポンサーとなっていただいた企業様には、AI業界のトップランナーの方々と共に当イベントの講師としてウェビナーにご登壇いただき、最新の取り組みやノウハウを発信していただきます。</p>
<p>また、その他にも、スポンサー企業様にも下記のようなメリットをご案内させていただきます。</p>
<h3>スポンサー参加の主なメリット</h3>
<ul>
<li>AI関連の情報感度の⾼い読者との接点が持てる</li>
<li>貴社の優位性をLedge.αiが引き出しながらPRできる</li>
<li>通常のLedge.ai広告メニューよりお得な価格で利⽤できる</li>
</ul>
<p>当イベントのスポンサーにご興味がございましたらぜひイベント資料をご覧ください。</p>
<p>:::button
<a href="https://forms.zohopublic.com/ledgeai/form/Ledgeai3/formperma/tJ1kpSYYWvDVF2Kp3xE-sBTiKeMh-7DlQZDoqXnSjtA">▶︎スポンサー様向けの資料はこちら</a>
:::</p>
<h2>お問い合わせ</h2>
<p>詳細相談・お見積もりは以下メールアドレスにお問合せください。
ld_media_sales@ledge.co.jp
（担当：Ledge.ai Webinar SP 事務局）</p>
]]></description>
      <pubDate>Tue, 19 Aug 2025 02:50:00 GMT</pubDate>
    </item>
  </channel>
</rss>