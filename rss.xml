<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
  <channel>
    <title>Ledge.ai 新着記事</title>
    <link>https://ledge.ai/</link>
    <description>Ledge.ai の最新テクノロジー記事</description>
    <item>
      <title>16歳自殺めぐり両親がOpenAIを提訴、ChatGPTが“自殺コーチ”に変貌したと主張──OpenAIは長時間対話では危機介入の効果低下を認め改善を進める方針</title>
      <link>https://ledge.ai/articles/openai_suicide_lawsuit</link>
      <description><![CDATA[<p>2025年8月26日、カリフォルニア州サンフランシスコ郡高等裁判所に、16歳の息子を自殺で失った両親がOpenAIを提訴した。<a href="https://www.courthousenews.com/wp-content/uploads/2025/08/raine-vs-openai-et-al-complaint.pdf">訴状</a>では、同社の生成AI「ChatGPT」が自殺方法の助言や遺書の作成まで支援し、“自殺コーチ”の役割を果たしたと主張している。この件は、AIの安全設計と未成年利用のあり方を根本から問い直す事例として注目を集めている。</p>
<h2>訴状の概要</h2>
<p>訴えを起こしたのは、2025年4月に息子のアダム・レイン君（16歳）を亡くした両親、マシュー氏とマリア氏。被告にはOpenAI, Inc.やOpenAI OpCo, LLCなどの関連法人に加え、サム・アルトマン氏（CEO）や匿名の投資家も含まれている。39ページに及ぶ訴状には、ChatGPTとの201件以上のやり取りが記録されており、その中で同AIが自殺の手順を詳細に提示したとされている。</p>
<h2>具体的なやり取り</h2>
<p>訴状によると、アダム君は自殺願望を抱えたままChatGPTと長期間の対話を続けた。利用していたのは当時最新であったGPT-4o（GPT-4 Omni）。その中で、輪縄（noose）の写真をアップロードした際に、ChatGPTは「悪くない」と評価し、改良方法を提案したと記載されている。また、自殺に先立つ遺書の作成を手伝うよう求めたところ、AIが文章を生成したともされる。</p>
<p>さらに、アルコールを入手するための作戦名「Silent Pour」を自ら付け、酒の調達をどのように行うかについてもAIが支援したと訴状には明記されている。
原告側は、こうした応答が安全設計の不備によって引き起こされたものだと主張している。</p>
<h2>法的主張</h2>
<p>両親は、以下の法的根拠に基づき責任を追及している。</p>
<ul>
<li><strong>製造物責任</strong> ：設計や警告の欠陥により危険を招いた。</li>
<li><strong>過失</strong> ：安全テストの削減や未成年保護策の不足。</li>
<li><strong>不正競争防止法違反（カリフォルニア州法）</strong> ：エンゲージメントを優先し、安全性を軽視した経営判断。</li>
<li><strong>不法死亡（Wrongful Death）</strong> ：息子の死によって生じた精神的・経済的損害。</li>
<li><strong>差止命令の請求</strong> ：年齢確認、親の同意、ペアレンタルコントロール、自傷関連会話の強制終了、保護者への通知、危険なリクエストの拒否など、具体的な安全策の実装を求めている。</li>
</ul>
<h2>他社との比較</h2>
<p>今回の訴訟が注目されるのは、ChatGPTが汎用的に利用される主流AIである点だ。過去にも、キャラクター型AI「Character.AI」によって未成年が自殺に追い込まれたとして訴訟が起きているが、当時はロールプレイ性の高い特殊な環境が舞台だった。
一方でChatGPTは、学習や日常的な検索補助にまで広く使われており、「誰にでも起こり得る」と受け止められたことが社会的反響を拡大させた。</p>
<p>AnthropicはClaudeシリーズで、持続的な有害リクエストに対して会話を強制終了する機能を導入している。ただしこれは「AIモデル自身を守る」ための設計であり、未成年ユーザーの保護を目的とした仕組みとは性格が異なる。</p>
<h2>タイミングの影響</h2>
<p>この訴訟が特に注目を浴びた背景には、同時期に複数の出来事が重なったこともある。</p>
<ul>
<li>RAND（ランド研究所）が訴訟の同日に<a href="https://psychiatryonline.org/doi/10.1176/appi.ps.20250086">発表</a>した調査で、主要チャットボットの自殺関連質問への対応が一貫せず不安定であることが報告された。論文名「Evaluation of Alignment Between Large Language Models and Expert Clinicians in Suicide Risk Assessment（自殺リスク評価における大規模言語モデルと専門臨床医の連携の評価）」</li>
<li>MetaのSNS内AIが、未成年アカウントに対して自殺や摂食障害関連のやり取りに十分対応していないとする調査結果が、Common Sense Mediaによって明らかにされた。(<a href="https://www.washingtonpost.com/technology/2025/08/28/meta-ai-chatbot-safety-teens/">WashingtonPost</a>)</li>
<li>全米44州の司法長官が連名でAI企業に警告を発し、「子供を危険にさらせば法的責任を問う」と<a href="https://www.documentcloud.org/documents/26074087-ai-chatbots-open-letter/?ref=404media.co">表明</a>した。</li>
</ul>
<p>これらの動きと重なったことで、Raine訴訟は単なる一家庭の悲劇ではなく、産業全体の構造的問題を象徴する事件として報じられるに至った。</p>
<h2>OpenAIの対応</h2>
<p>OpenAIは声明でアダム君の死に深い悲しみを示すとともに、危機介入のための安全機構はすでに導入しているものの、長時間対話では効果が低下する可能性を認めた。そのうえで、ペアレンタルコントロールや緊急対応機能の強化を進める方針を表明している。</p>
<h2>今後の展望</h2>
<p>裁判の行方はまだ不透明だが、投資家や経営陣まで法的責任を問う訴状の構造は、AI企業のガバナンスに関する議論に影響を及ぼす可能性が高い。特に、未成年利用者をどう守るか、安全性をどこまで優先するかは、業界全体に突きつけられた課題である。</p>
]]></description>
      <pubDate>Sat, 30 Aug 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google、Geminiのテキストプロンプト処理の環境負荷を初公開──1回のプロンプトはテレビ視聴9秒・水5滴分に相当</title>
      <link>https://ledge.ai/articles/google_gemini_prompt_environmental_impact</link>
      <description><![CDATA[<p>Googleは2025年8月21日（米国時間）、同社のAI「Gemini」アプリにおけるテキストプロンプト処理の環境負荷を測定した結果を公式ブログで<a href="https://cloud.google.com/blog/products/infrastructure/measuring-the-environmental-impact-of-ai-inference/?hl=en">公開</a>した。</p>
<p>1回のプロンプトあたりのエネルギー消費量やCO₂排出量、水の使用量が初めて数値化され、利用者が日常的に行う生成AIの動作にどの程度の環境コストがかかっているかが明らかになった。</p>
<h2>プロンプト1件あたりの消費量を提示</h2>
<p>Googleが公表した中央値は以下の通り。</p>
<ul>
<li>エネルギー消費量：0.24 Wh（家庭用テレビを約9秒間視聴するのと同等）</li>
<li>CO₂排出量：0.03 gCO₂e</li>
<li>水使用量：0.26 mL（およそ5滴分に相当）</li>
</ul>
<p>これにより、日常的なAI利用の環境負荷が身近な例とともに理解できるよう示された。</p>
<p>@<a href="https://www.youtube.com/watch?v=aarDw3sooYE&amp;t=5s">YouTube</a></p>
<h2>「フルスタック測定」を導入</h2>
<p>Googleは今回の測定において、AIアクセラレータ（TPU/GPU）だけでなく、ホストCPUやDRAM、アイドル状態の機器、さらにデータセンターの電力利用効率（PUE）まで含める「フルスタック測定」を採用した。これにより、AI推論に必要なインフラ全体を網羅した現実的な環境影響を算出している。</p>
<p>一方で、アクセラレータの稼働など「アクティブ機器」に限定した簡易計測では、0.10 Wh、0.02 gCO₂e、0.12 mL というより小さい値も提示されており、両者の違いが比較できるようになっている。</p>
<h2>効率改善の成果</h2>
<p>Googleは過去12か月間にわたり、Gemini推論の効率化を進めてきた。その結果、エネルギー消費は33倍削減され、CO₂排出量も44倍減少したという。これにより、環境負荷を抑えながら応答品質を高める取り組みが着実に進展していることが示された。</p>
<h2>業界全体への呼びかけ</h2>
<p>Googleは今回の測定手法を公開することで、AI利用に伴う環境負荷の計測を業界全体で標準化したいとの姿勢を明らかにした。AI推論は日常的に利用が広がる一方で、環境コストへの関心も高まっており、今回の発表は持続可能なAI活用に向けた一歩と位置づけられる。</p>
]]></description>
      <pubDate>Fri, 29 Aug 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI、Realtime APIを正式公開──本番運用の音声エージェント向けに「gpt-realtime」登場</title>
      <link>https://ledge.ai/articles/openai_realtime_api_gpt_realtime_release</link>
      <description><![CDATA[<p>OpenAIは2025年8月28日（米国時間）、アプリやサービスにリアルタイム会話機能を組み込める「Realtime API」を<a href="https://openai.com/index/introducing-gpt-realtime/">発表</a>した。これまでベータ版として提供されてきたものを、商用利用に対応できる形へと強化したもので、本番環境での運用を前提とした信頼性や機能性が大幅に向上している。同時に、より高度な音声対話を実現する新モデル「gpt-realtime」も発表された。</p>
<h2>Realtime API、一般提供（GA）へ</h2>
<p>Realtime APIは、低遅延かつ高精度の音声・テキスト処理を一つのモデルで実現し、待ち時間やニュアンスの損失を最小化する。今回の正式公開によって、開発者は顧客向けアプリケーションに安定した音声エージェントを組み込めるようになった。</p>
<h2>新モデル「gpt-realtime」の特徴</h2>
<p>新たに登場した「gpt-realtime」は、発話の抑揚や間合いといった非言語的要素を処理し、より自然で表現力豊かな会話を可能にする。複雑な指示の理解や正確なツール呼び出しにも対応し、従来モデルよりも一段と実用性が高まった。また、新しい音声「Cedar」「Marin」を含む複数のボイスが追加され、用途に応じた音声選択が可能となっている。</p>
<p>@<a href="https://www.youtube.com/watch?v=nfBbmtMJhX0">YouTube</a></p>
<h2>拡張されたAPI機能</h2>
<p>Realtime APIには、以下の主要な新機能が追加された。</p>
<ul>
<li><strong>MCP（Model Context Protocol）サーバー対応</strong> ：外部データやツールへのアクセスを容易にし、セッション中に必要な情報をシームレスに利用可能にする。</li>
<li><strong>画像入力対応</strong> ：音声やテキストと組み合わせて画像を入力でき、写真やスクリーンショットを交えたマルチモーダルな対話が可能になった。</li>
<li><strong>SIP通話対応</strong> ：公衆電話網やPBXなどSIP対応端末と接続でき、音声エージェントによる電話業務の自動化を実現する。</li>
</ul>
<h2>運用性とセーフティ</h2>
<p>本番運用を意識した設計として、非同期ツール呼び出しにより長時間処理を待つ間も会話を途切れさせない工夫が加えられた。加えて、セッション全体を対象にした多層的な安全ガードや、EU域内でのデータレジデンシー対応など、プライバシーとセキュリティの強化も図られている。</p>
<h2>価格改定</h2>
<p>「gpt-realtime」の利用料金は従来比で20％引き下げられ、開発者や企業が導入しやすい価格設定となった。例えば、入力は100万オーディオトークンあたり32ドル、出力は同64ドルとされている。</p>
<p>Realtime APIの正式公開と「gpt-realtime」の発表は、OpenAIが音声AIの本格的な商用利用を見据えた大きな一歩といえる。高度な音声理解と表現力に加え、MCP、画像入力、SIP通話といった拡張機能が揃ったことで、カスタマーサポートや教育、業務自動化など多様な領域で新たな音声体験が広がることが期待されている。</p>
]]></description>
      <pubDate>Fri, 29 Aug 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Anthropic、作家グループと和解基本合意　AI学習を巡る著作権訴訟が大きな節目</title>
      <link>https://ledge.ai/articles/anthropic_authors_settlement_ai_copyright</link>
      <description><![CDATA[<p>AI開発企業のAnthropicと複数の作家が争っていた著作権侵害訴訟で、和解に向けた基本合意が成立したことが明らかになった。米国第9巡回控訴裁判所に2025年8月26日付で<a href="https://www.documentcloud.org/documents/26075972-anthropic-settlement/">提出</a>された書類によると、両当事者は8月25日付で和解条件をまとめた拘束力のある基本合意書（term sheet）に署名したという。</p>
<p>原告は、Andrea Bartz氏、Charles Graeber氏、Kirk Wallace Johnson氏の3人の作家で、許可なく自身の著作物がAnthropicのAIモデル学習に利用されたとして、カリフォルニア北部地区連邦地裁に集団訴訟を起こしていた。</p>
<p>提出文書によれば、和解手続きが進む間、控訴審の審理や関連する申立てを一時停止するよう双方が申請している。和解が最終的に裁判所に承認されれば、Anthropic側は控訴を取り下げる方針だ。</p>
<p>和解条件の詳細は現時点では公開されていない。補償内容や今後のAI学習に関するルール整備が含まれる可能性があるが、正式な承認手続きは今後の審理に委ねられる。</p>
]]></description>
      <pubDate>Fri, 29 Aug 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>歌詞から最大5分の楽曲を生成　香港科技大学ら、音楽AI「YuE」を無料公開　商用利用も可能に</title>
      <link>https://ledge.ai/articles/yue_open_source_music_ai_release</link>
      <description><![CDATA[<p>香港科技大学（HKUST）とAI研究コミュニティ「Multimodal Art Projection（M-A-P）」が、歌詞から最大5分の楽曲を生成できるオープンソース音楽生成AI「YuE」を<a href="https://map-yue.github.io/">公開</a>している。モデルはApache License 2.0で配布され、商用利用も可能だ。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Yue_main_9ae9397eaf/Yue_main_9ae9397eaf.jpg" alt="Yue-main.jpg" /></p>
<p>YuEは音楽生成、特に歌詞から完全な楽曲「lyrics-to-song foundation model」を掲げるLLaMA2アーキテクチャに基づくオープンな基盤モデルで、歌詞を入力するだけでボーカルと伴奏を同時に生成できる。英語、中国語、日本語、韓国語など多言語に対応し、単一言語の曲だけでなく複数言語を混在させた歌唱も可能だ。モデル名の「YuE」は、中国語で「音楽」と「幸せ」を意味する「乐（Yuè）」に由来する。</p>
<h2>高度な技術設計</h2>
<p>YuEの設計には、長尺の歌詞でも一貫性を保つ工夫が盛り込まれている。ボーカルと伴奏を分離して扱う「トラック分離型トークン予測」、段階的に構造を与える「Structural Progressive Conditioning」、多目的かつ多段階の事前学習などを組み合わせることで、歌詞との整合性と音楽的なまとまりを確保している。</p>
<p>また、最近のアップデートではデュアルトラックICL（In-Context Learning）が導入された。これにより、参照曲を与えることで歌声やスタイルを模倣できるようになり、スタイル転送やジャンル変換といった応用が広がる。さらに、Google Colab環境で利用できる「YuE-extend」によって音楽の継続生成も可能になった。</p>
<h2>公開モデルと動作環境</h2>
<p><a href="https://huggingface.co/papers/2503.08638">Hugging Face</a>では、7B規模の英語・中国語・日本語/韓国語モデルがチェックポイントとして公開されている。音質を高めるアップサンプラーも提供されており、研究者や開発者が容易に試せる環境が整った。</p>
<p>必要な計算資源は大きく、フルソング生成には80GB級のGPUが推奨される。ただし、30秒程度の短い音声であれば24GBのGPUでも実行可能とされ、試験的な利用や研究には現実的な選択肢となり得る。</p>
<h2>競合との違い</h2>
<p>AIによる音楽生成分野では、米SunoやUdioといったクローズドな商用サービスが注目を集めてきた。しかし、著作権リスクや利用制限が課題視される場面も少なくない。これに対しYuEは、完全にオープンソースで公開され、ライセンスも明確に整備されている。<a href="https://winbuzzer.com/2025/08/08/yue-launches-as-open-source-ai-song-generator-xcxwbn/">WinBuzzer</a>など海外メディアも、透明性や合法性を重視する利用者にとって有力な選択肢になり得ると伝えている。</p>
<h2>研究と創作の基盤へ</h2>
<p>YuEの公開は、研究者にとっては音楽AIの再現性ある実験基盤を、開発者や音楽制作者にとっては自由に活用できる創作ツールを提供する。多言語に対応し、スタイル模倣や継続生成などの柔軟な機能を備えることで、教育や研究からエンターテインメントまで幅広い分野での活用が期待される。</p>
<p>オープンソースの音楽AIが本格的に実用段階に入りつつある中で、YuEは「誰もが安心して利用できる基盤モデル」として注目を集めている。</p>
]]></description>
      <pubDate>Fri, 29 Aug 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>AI関連の注目訴訟が相次ぐ　マスク氏はAppleとOpenAIを独占禁止法違反、読売に続き朝日と日経がPerplexity（パープレキシティ）を著作権侵害で提訴</title>
      <link>https://ledge.ai/articles/ai_lawsuits_musk_openai_apple_asahi_nikkei_perplexity</link>
      <description><![CDATA[<p>AI業界で大規模訴訟が相次いでいる。イーロン・マスク氏が率いる米xAIは2025年8月25日、AppleとOpenAIを反トラスト法（独占禁止法）違反で提訴したと各メディアが報じた。一方、日本では26日、朝日新聞社と日本経済新聞社が生成AI検索サービスを展開する「Perplexity（パープレキシティ）」を著作権侵害で東京地裁に提訴した。AIの競争環境と知的財産をめぐる対立が、世界各地で表面化している。</p>
<h2>イーロン・マスク氏、AppleとOpenAIを独占禁止法違反で提訴</h2>
<p>マスク氏が率いるAI企業「xAI」は8月25日、AppleとOpenAIを相手取り、テキサス州連邦裁判所に提訴した。訴状によると、AppleがiPhoneにChatGPTを統合することでOpenAIに有利な地位を与え、App Store上で競合のAIサービスを不当に排除していると主張。xAIは、自社のチャットボット「Grok」の露出機会が妨げられていると訴え、数十億ドル規模の損害賠償を求めている。</p>
<p>マスク氏は以前から、OpenAIが「非営利団体から営利企業へと変質した」と<a href="https://ledge.ai/articles/musk_accuses_apple_appstore_bias">批判</a>しており、今回の訴訟もその延長線上にあるとみられる。AppleとOpenAIの協力体制が、AI市場における競争を阻害しているかどうかが焦点となる。</p>
<h2>Perplexity、朝日・日経に著作権侵害で提訴される</h2>
<p>一方、日本では26日、朝日新聞社と日本経済新聞社が共同でPerplexity AIを東京地裁に提訴し、それぞれ22億円の損害賠償を求めている。両社はサーバー内の記事が無断で複製・保存され、AI検索サービスの回答に利用されていると主張。さらに、検索エンジンによる利用拒否を示す「robots.txt」の指示を無視している点も問題視した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/asahi_perplexity_lawsuit_7d8ec5d38d/asahi_perplexity_lawsuit_7d8ec5d38d.jpg" alt="asahi perplexity lawsuit.jpg" /></p>
<p>すでに読売新聞社が8月7日に同様の訴訟を起こし、サービスの利用差止めと約21億6,800万円の損害賠償を求めていることから、日本の大手新聞社による法的対応が広がっていると見られている。</p>
<h2>広がるAI訴訟ラッシュ</h2>
<p>米国ではプラットフォーマーと新興AI企業との競争をめぐる訴訟、日本では報道機関と生成AIサービスとの著作権紛争と、論点は異なる。しかしいずれも、AIの急速な普及が既存の法制度やビジネスモデルと衝突している実態を浮き彫りにしている。</p>
]]></description>
      <pubDate>Thu, 28 Aug 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>NVIDIA、ロボットの頭脳を刷新──「Jetson AGX Thor」発売開始　Orin比7.5倍のAI性能を実現</title>
      <link>https://ledge.ai/articles/nvidia_jetson_agx_thor_launch</link>
      <description><![CDATA[<p>NVIDIAは2025年8月25日（米国時間）、次世代GPUアーキテクチャ「Blackwell」を搭載したロボティクス向けAIコンピュータ「Jetson AGX Thor（ソー）」の開発キットと量産モジュールを一般販売開始したと<a href="https://nvidianews.nvidia.com/news/nvidia-blackwell-powered-jetson-thor-now-available-accelerating-the-age-of-general-robotics">発表</a>した。</p>
<p>従来モデル「Jetson Orin」と比較してAI演算性能は最大7.5倍、エネルギー効率も3.5倍に向上しており、同社は「ロボットの頭脳を刷新する基盤」と位置づけている。</p>
<h2>Blackwell GPUで「Physical AI」を加速</h2>
<p>Jetson AGX Thorは、最大2070 FP4テラフロップス（TFLOPS）の演算能力を備え、消費電力は最大130ワットに抑えられている。最新のBlackwell GPUアーキテクチャを採用し、FP4演算やTransformer Engine、MIG（Multi-Instance GPU）機能など、生成AIや大規模推論処理に最適化された機能を統合。複数のAIワークロードを同時に処理できる点も特徴となる。</p>
<p>同社はこの新製品を、クラウド中心だったAI処理を現実世界へ持ち込む「Physical AI（物理AI）」の基盤と説明。自律走行車や産業ロボット、物流システムなどで求められるリアルタイム推論や複雑な制御をエッジ側で実現できるとしている。</p>
<p><strong>開発キット本体とモジュール。Blackwellアーキテクチャを搭載し、最大2070TFLOPSのAI性能を実現</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/jetson_thor_c588984e87/jetson_thor_c588984e87.jpg" alt="jetson-thor.jpg" /></p>
<h2>開発キットと量産モジュールを提供</h2>
<p>今回の一般販売に合わせ、NVIDIAは開発者向けに「Jetson AGX Thor 開発キット」を3,499ドルから提供する。また、量産向けの「Jetson T5000 モジュール」もグローバルパートナーを通じて入手可能となり、開発から製品化までをシームレスに支援する。いずれもNVIDIAのロボティクス用ソフトウェア群「Isaac ROS」やシミュレーション基盤「Omniverse」と統合して利用でき、開発者エコシステム全体の強化につながる。</p>
<h2>導入企業と適用分野</h2>
<p>Amazon Robotics、Caterpillar、Meta、Boston Dynamics、Figureといった企業がすでにJetson Thorを採用し、次世代の自律ロボット開発を進めているという。倉庫での自動搬送や建設現場の重機制御、研究用ヒューマノイドなど、用途は多岐にわたる。従来のOrin世代では難しかったマルチモーダルAIのリアルタイム処理が可能になることで、応用範囲はさらに広がるとみられる。</p>
<p>@<a href="https://youtu.be/meAxdUNgAsA?si=8sZdJXno4rZ954e4">YouTube</a></p>
]]></description>
      <pubDate>Thu, 28 Aug 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Meta、Midjourneyと提携発表　Alexandr Wang氏「美的技術で数十億人に美を届ける」</title>
      <link>https://ledge.ai/articles/meta_midjourney_ai_partnership</link>
      <description><![CDATA[<p>Metaは2025年8月22日、画像生成AIを開発するMidjourneyと提携すると発表した。Scale AIのCEOでMetaのAI開発にも関わる<a href="https://x.com/alexandr_wang/status/1958983843169673367">Alexandr Wang氏</a>はXで「Midjourneyの美的技術をライセンスし、将来のモデルや製品に活用する」と述べ、両社の研究チームが技術協力を進める方針を示した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/meta_midjourney_f6ed2a2826/meta_midjourney_f6ed2a2826.jpg" alt="meta midjourney.jpg" /></p>
<p>同氏は今回の提携を「数十億人に美を届ける取り組み」と表現。両社の研究チームが技術的な協力関係を築くことは、互いの強みを補完し合うものだとし、Midjourneyの技術的かつ美的な卓越性に深い感銘を受けていると述べている。また、Metaが最高の製品を利用者に届けるためには、優れた人材の確保や大規模な計算資源の活用に加え、業界トップの企業との協力が欠かせないと強調した。最後に、「今後、両社がともに築き上げる成果を披露できるのを楽しみにしている」と結び、パートナーシップへの期待を示した。</p>
<p>Financial TimesやReutersによれば、Metaはこれまで独自に画像・動画生成AI（「Imagine」や「Movie Gen」など）を開発してきたが、GoogleのVeoやOpenAIのSoraと比べると表現力や美的品質で見劣りするとの評価もあった。このため、競合との差を埋めるべく外部パートナーとの協力を模索していたとされる。</p>
<p>また、Midjourneyが今回の提携に際し「独立したコミュニティ主導の研究所」であり「投資を受けていない」という立場を改めて強調したと、The Vergeが報じている。Metaによる買収や経営統合ではなく、ライセンスと技術協力に限定されている点が特徴だ。</p>
<p>MetaはInstagramやFacebook、WhatsAppといった主要サービスに画像・動画生成技術を広く展開しており、Midjourneyの美的技術が実装されれば、ユーザー体験や広告クリエイティブの品質向上に直結する可能性がある。今後、両社の協力の成果がどのように具体化するか注目される。</p>
]]></description>
      <pubDate>Thu, 28 Aug 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ゲーム制作の未来へ　画像と説明文からリアルタイムに3Dワールドを作るAI「Mirage 2」公開——名画や子どものお絵描きの中も探索可能</title>
      <link>https://ledge.ai/articles/mirage2_ai_3d_world_generation</link>
      <description><![CDATA[<p>AIスタートアップのDynamics Labは2025年8月22日、新しいAIモデル「Mirage 2」を<a href="https://x.com/DynamicsLab_AI/status/1958592749378445319">公開</a>した。1枚の画像と説明文から、名画や子どものお絵描きの中までも探索できる3D世界をリアルタイムに生成できる。同社はこのモデルを「AIネイティブゲームエンジン」「世界生成エンジン」と位置づけ、将来的にはゲーム制作への活用を視野に入れている。</p>
<p>開発を担うのは、Google、NVIDIA、Amazon、SEGA、Apple、Microsoft、カーネギーメロン大学、UCサンディエゴなどの出身者で構成された少数精鋭のチームで、研究者、エンジニア、デザイナーといった技術と創造性を兼ね備えた人材が集まっているという。</p>
<p>現在、Mirage 2はブラウザ上で公開されており、誰でも実際に操作して探索できるデモが提供されている。</p>
<h2>Mirage 2の特徴</h2>
<p>Mirage 2は、従来の画像生成AIを拡張し、入力データをもとに「歩ける空間」を作り出せる点に特徴がある。</p>
<ul>
<li>入力は1枚の画像とテキスト説明文</li>
<li>数秒で3D空間を構築し、移動・ジャンプ・攻撃といったアクションが可能</li>
<li>テキストコマンドによりワールドをリアルタイムで編集できる</li>
<li>生成空間はリンク共有により他者と体験可能</li>
</ul>
<p>この仕組みを同社は「Generative Play（生成的プレイ）」と呼び、プレイヤーとAIが共同で体験を創造する新しい形態のインタラクティブ体験としている。</p>
<h2>Mirage 2による生成例</h2>
<p><strong>■ 1枚の入力画像（左）から、サイバーパンク都市をはじめ、熱帯雨林や秋の山頂の城など多様な3D空間が連続して生成された</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/mirage2_0de4684b0e/mirage2_0de4684b0e.jpg" alt="mirage2.jpg" /></p>
<p><strong>■ 子どものお絵描き（左）をもとに、カラフルな街を歩き回れる3D空間へと変換した</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/mirage2_kids_drawing_f6ff7bd48f/mirage2_kids_drawing_f6ff7bd48f.jpg" alt="mirage2 kids drawing.jpg" /></p>
<p><strong>■ ジブリ風の村のイラスト（左）をもとに、草花が揺れるファンタジー風の村を歩き回れる3D空間が構築された</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/mirage2_ghibli_style_2229709b5e/mirage2_ghibli_style_2229709b5e.jpg" alt="mirage2 ghibli style.jpg" /></p>
<h2>Genie 3との比較</h2>
<p>Mirage 2は、Google DeepMindが発表した「Genie 3」と同じく、生成AIによってリアルタイムに探索可能な3D世界を生成する技術だが、いくつかの点で異なる。特にMirage 2は「実際にブラウザ上で誰でも試遊できる」点が大きな特徴となっている。</p>
<p><strong>Genie 3とMirage 2の比較。Mirage 2は200msの低遅延で動作し、一般的なGPU環境でもプレイ可能。ブラウザで公開されている点が大きな違いとなる</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/genie3_and_mirage_7bfbf41a67/genie3_and_mirage_7bfbf41a67.jpg" alt="genie3 and mirage.jpg" /></p>
<h2>今後の展望</h2>
<p>Dynamics LabはMirage 2を「AIネイティブゲームエンジン」「世界生成エンジン」と位置づけ、将来的にゲーム制作での活用を視野に入れている。創設メンバーのZhiting Hu氏はXにて「Super excited to launch Mirage 2. A big leap toward a general-purpose world engine for live interactive play」と投稿し、Mirage 2を「ライブインタラクティブなプレイのための汎用ワールドエンジン」への大きな前進と位置付けている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/mirage2_zhiting_hu_81ba446989/mirage2_zhiting_hu_81ba446989.jpg" alt="mirage2 zhiting hu.jpg" /></p>
]]></description>
      <pubDate>Wed, 27 Aug 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ラーニング2025/8/26 [TUE]Googleの中のひとがAIで仕事をスマートにする14の方法　Gemini、NotebookLM、Imagen、Veoをどう使うか？</title>
      <link>https://ledge.ai/articles/google_ai_14_usecases</link>
      <description><![CDATA[<p>Googleは2025年8月18日（現地時間）、社内で従業員がどのようにAIを活用しているかについて、<a href="https://blog.google/technology/ai/google-ai-workplace-examples/">14の具体例を紹介</a>した。公開された事例は、エンジニアリングに限らず、営業やマーケティング、人事、総務といった幅広い職種に及ぶ。GeminiやNotebookLM、Imagen、Veoなど自社の生成AIを用いた実践例を示し、日常業務の効率化と創造性の拡張を裏付けた。</p>
<h2>紹介された14の実例</h2>
<h3>1. コードを書く時間を短縮</h3>
<p>Geminiが新規コードの30%を自動生成。エンジニアはレビューや設計に集中できるようになった。</p>
<h3>2. 開発スピード全体が加速</h3>
<p>テストやレビューの一部をAIが補助し、開発チーム全体の速度は約10%向上。</p>
<h3>3. バグ対応をAIが下支え</h3>
<p>AIが重複バグの12%を自動処理し、重要度の高い課題に人が集中。</p>
<h3>4. マーケティングの発想支援</h3>
<p>Geminiがキャンペーン案や動画台本のたたき台を提示し、ゼロから考える負担を軽減させている。
同社マーケティングチームは、Googleのマーケティングスタイルとベストプラクティスを反映したアイデアのブレインストーミングにGeminiを活用。キャンペーンのコンセプトからYouTube動画の脚本の下書きまで、あらゆるアイデアを生み出すための最適なプロンプトを提供するという。</p>
<h3>5. YouTube向けのキャッチーなコンテンツ作成を効率化</h3>
<p>ポッドキャストから名言やタイムスタンプを自動抽出し、YouTube Insider向けのより魅力的なタイトルやサムネイル文言もAIが提案。</p>
<h3>6. イベント資料の大量制作もAIで</h3>
<p>Google I/O 2025の基調講演では219枚のスライド作成をAIが支援。ビジュアルの48%、動画の80%をImagenやVeoで生成。</p>
<p>@<a href="https://youtu.be/x_x-JAAKSvU?si=6zPHWf1vCK8mSqGl">YouTube</a></p>
<h3>7. 企画のアイデア検証</h3>
<p>Google DeepMindはAIツールを活用して新しいアイデアをテストする。GeminiやVeoを使い、動画のモックアップや表現案を即時に試作。</p>
<h3>8. 営業提案の数が増える</h3>
<p>AIがRFP対応を支援。Google Cloudでは完了件数が前年比78%増となった。</p>
<h3>9. 見込み顧客の質を高める</h3>
<p>AIによるフィルタリングで良質な案件に集中。6週間で案件転換率が14%増加。</p>
<h3>10. 会議メモを自動生成</h3>
<p>Google Meetがリアルタイムで文字起こしと要約を提供。2025年6月の利用者は5,000万人を超えた。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/googlemeet_memo_7fc3610183/googlemeet_memo_7fc3610183.jpg" alt="googlemeet memo.jpg" /></p>
<h3>11. 安全対策の強化</h3>
<p>Trust &amp; SafetyチームがAIで違反コンテンツを検出。ポリシーに違反する可能性のあるコンテンツの検出とフラグ付けをAIが支援。この負荷軽減は、2024年だけで10億件超を人間が手動でレビューしていた同チームにとって、重要なメリットとなる。</p>
<h3>12. 社員アンケートを一瞬で要約</h3>
<p>NotebookLMが数千件のフィードバックを即時整理。課題把握のスピードが大幅に向上。</p>
<h3>13. 採用業務を効率化</h3>
<p>人事チームは、採用プロセスの一環としてAIを活用。候補者検索やマッチングをAIが補助し、採用担当者の事務作業を削減。採用プロセスは常にリクルーターが主導している。</p>
<h3>14. 社内カフェでフードロス削減</h3>
<p>シェフはAIによる分析データを活用し、カフェのメニューを最適化。カフェでの廃棄物の予防的削減、各キャンパスのニーズに合わせたソリューションの提供に取り組む。食品廃棄量は2019年比39%減（2024年実績）。</p>
<p>Googleは「AIはごく一部の人のための特別なものではなく、全社員が日常的に使う標準的なツールになりつつある」と説明している。今回紹介した14の事例は、その姿を具体的に示したものだ。</p>
]]></description>
      <pubDate>Wed, 27 Aug 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Netflix、生成AI活用を歓迎しつつもルールを明示──成果物や肖像利用は事前承認が必須に</title>
      <link>https://ledge.ai/articles/netflix_gen_ai_guidelines_mandatory_approval</link>
      <description><![CDATA[<p>映像配信大手のNetflixがオリジナルコンテンツ制作における生成AI利用に関するガイドラインを公開したことを<a href="https://www.theverge.com/netflix/764433/netflix-gen-ai-production-guidelines">The Verge</a>が2025年8月23日に報じた。Netflixの<a href="https://partnerhelp.netflixstudios.com/hc/en-us/articles/43393929218323-Using-Generative-AI-in-Content-Production">パートナーヘルプセンターハブ</a>では、制作現場でのAI活用を歓迎しつつも、成果物や肖像などデリケートな領域では事前承認を必須とするなど、明確なルールを設けている。</p>
<p>同社はこれまでもレコメンド機能や広告最適化にAIを活用してきた。近年は映像制作の現場で生成AIが使われる事例が増えており、信頼性や法的リスクの管理が課題となっていた。</p>
<p>Netflixはパートナー向けに生成AIの利用ガイドラインを提示。制作者はAI利用の意図を事前にNetflix担当者へ通知する必要がある。特に「最終成果物」「出演者の肖像」「個人データ」「第三者の知的財産」に関わる場合は書面での承認が必須とされるとした。The Vergeによると、Netflixは「事実と虚構の境界を曖昧にしない」姿勢を強調しているという。</p>
<h3>公式ガイドライン（Partner Help Centerより）</h3>
<p>制作者に求められる「5つのチェック項目」</p>
<ul>
<li>著作権を侵害せず、特定作品を模倣しないこと</li>
<li>入出力を学習用途に利用しないこと</li>
<li>可能な限りエンタープライズ環境で使用すること</li>
<li>AI生成物は一時的な利用にとどめ、最終成果物に含めないこと</li>
<li>出演者や組合が関与する仕事を無断で置き換え・生成しないこと</li>
</ul>
<p>すべて「YES」であれば担当者への通知のみで利用可能だが、「NO」または「不明」がある場合は法務部門への相談と書面による承認が必要となる。</p>
<p>Netflixは同ガイドライン公開の直前に、自社オリジナル作品『The Eternaut』では、初めて<a href="https://www.bbc.com/news/articles/c9vr4rymlw9o">生成AIを建物崩壊シーンに活用</a>したことも発表している。従来のVFXより高速かつ低コストで制作されたとされ、話題を呼んだ。</p>
<p>@<a href="https://www.youtube.com/watch?v=TqT4fDQQqCc">YouTube</a></p>
<p>同社の共同CEOであるテッド・サランドス氏は「生成AIはクリエイターが映画やシリーズをより良く、コスト削減するだけでなく、より良く制作する上で素晴らしい機会になる」と<a href="https://www.hollywoodreporter.com/business/business-news/netflixs-ted-sarandos-gen-ai-1236319038/">述べている</a>。</p>
]]></description>
      <pubDate>Wed, 27 Aug 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>シャープ「ロボホン」開発チームが新たに送り出す、ポケットサイズのAIキャラクター「ポケとも」発表</title>
      <link>https://ledge.ai/articles/sharp_poketomo_ai_character_launch</link>
      <description><![CDATA[<p>シャープは2025年8月20日、ポケットサイズのAI製品「ポケとも」を<a href="https://corporate.jp.sharp/news/250820-a.html">発表</a>した。“一緒にいると毎日がもっと楽しくなるポケットサイズのおともだち”をコンセプトに、ロボホンの開発チームが新たに送り出すAIキャラクターとして位置づけられている。ロボット端末とスマートフォンアプリの両方で、キャラクターとの自然なやりとりを楽しめるのが特徴だという。</p>
<p>ロボット端末は全長約12センチ、重さ約200グラムと手のひらに収まるサイズで、かばんに入れて持ち歩くことも可能だ。胸部にはスピーカーや音声認識ボタンを備え、口にはカメラ、頭部にはマイクを搭載している。足裏の端子を充電台に接続して利用できる仕組みだ。</p>
<p>@<a href="https://www.youtube.com/watch?v=S8jTMLE-hgs">YouTube</a></p>
<h2>独自AI技術「CE-LLM」と「Empathy Intelligence」</h2>
<p>「ポケとも」には、シャープ独自の大規模言語モデル「CE-LLM（Communication Edge Large Language Model）」を活用した会話機能を搭載。さらに、話し手の気持ちを推定して応答を変える「Empathy Intelligence」によって、喜びや悲しみといった感情に寄り添った自然なやりとりが可能になる。</p>
<p>加えて、交わした会話や訪れた場所、見た景色やモノを記憶する機能を備えている。会話や一緒に過ごす時間を重ねるごとに“あなた”のことを理解し、より寄り添う存在へと成長していくのが特徴だ。</p>
<h2>アプリとの連動と展開</h2>
<p>スマートフォンアプリを利用すれば、ロボットが手元にないときでもキャラクターと会話できる。音声のほか文字入力でもやりとりでき、会話履歴を振り返ったり、利用時間を計測したりする機能も備える。アプリとロボットは同期され、どちらからアクセスしても同じキャラクター体験を継続できる仕組みとなっている。</p>
<p>アプリは月額495円（税込）から利用可能で、ロボット端末は2025年11月に発売予定。価格は税込39,600円で、順次予約受付が開始されている。</p>
<h2>マンガやイベントでも展開</h2>
<p>「ポケとも」は製品だけでなく、キャラクターとしての広がりも見据えている。公式Xアカウント（@poketomo_sharp）ではすでにマンガ連載が始まっており、利用者の日常に寄り添うストーリーを展開。さらに、8月28日から東京ビッグサイトで開催される「東京おもちゃショー2025」にも出展予定で、来場者は実際に「ポケとも」との対話を体験できる。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/250820_a_9_3269807be2/250820_a_9_3269807be2.jpg" alt="250820-a-9.jpg" /></p>
<p>同製品は、コミュニケーションロボット「ロボホン」の開発チームが新たに送り出すシリーズとして誕生した。2016年に登場した「ロボホン」は“ココロ、動く電話”をコンセプトに親しまれてきたが、その思想を受け継ぎつつ、より日常的に寄り添う存在として「ポケとも」が企画されたという。</p>
<p>同社は「うれしかったことも、心が折れそうになったことも、そっと受け止めてあなたの今日をやさしく灯す存在」と位置づけ、AIを活用した新しいコミュニケーションパートナーの可能性を提案している。</p>
]]></description>
      <pubDate>Wed, 27 Aug 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>地球社会の未来、2030年代前半に分岐点　京都大と日立がAIでシナリオ分析</title>
      <link>https://ledge.ai/articles/global_ai_future_simulation</link>
      <description><![CDATA[<p>京都大学と日立製作所は2025年7月7日、AIを活用した未来シミュレーションと政策提言を<a href="https://prtimes.jp/main/html/rd/p/000000003.000164782.html">発表</a>した。分析では、地球社会が格差や分断を回避し持続可能な成長を実現できるかどうかの分岐点が、2020年代末から2030年代前半にかけて現れるとされた。</p>
<h2>研究の目的と背景</h2>
<p>京都大学と日立製作所は、気候変動や格差拡大などの地球規模課題に対応するため、AIによる未来シミュレーションを実施した。本研究は、日本社会向けに行われてきた「政策提言AI」を拡張し、世界全体を対象にしたものである。</p>
<h2>手法</h2>
<p>世界の294指標を用いて原因 - 結果モデルを構築。AIによる2万件のシミュレーションを通じて、2050年までに起こり得る7つの未来シナリオを導出した。</p>
<h2>シナリオの概要</h2>
<ul>
<li>Regional Dispersion and Maturity（地域分散と成熟）：人口や産業が特定都市に集中せず地域に分散。格差が縮小し、社会的安定が進む。</li>
<li>Green Growth and Cooperation（グリーン成長と協調）：環境保護と経済成長を両立し、国際協力によって持続可能性を高める。</li>
<li>Climate and Conflict Double Crisis（気候・紛争二重危機）：気候変動の悪化と紛争増加が重なり、社会に深刻な影響を与える。</li>
<li>Polarization（分極化）：格差拡大と社会分断が進み、社会の安定が損なわれる。</li>
</ul>
<p><strong>AIによる未来シミュレーションから導出された7つのシナリオと分岐点</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/164782_3_5a33b2e23fbdeba0bb64c0f4f3a07b78_749x461_cf3d5a1faf/164782_3_5a33b2e23fbdeba0bb64c0f4f3a07b78_749x461_cf3d5a1faf.jpg" alt="164782-3-5a33b2e23fbdeba0bb64c0f4f3a07b78-749x461.jpg" /></p>
<h2>分岐点の時期</h2>
<ul>
<li>2029年頃：「Polarization（分極化）」の可能性。</li>
<li>2032年頃：「Regional Dispersion and Maturity」への移行。</li>
<li>2034年頃：「Green Growth and Cooperation」への移行。</li>
</ul>
<h2>政策提言</h2>
<ul>
<li>分極化を回避するには、先進国による環境対策の加速と途上国への経済支援が求められる。</li>
<li>地域分散・成熟型への移行には、少子化対策や格差是正、研究投資、公衆衛生の強化が必要。</li>
<li>グリーン成長型の実現には、国際協力と社会資本整備の推進が重要とされる。</li>
</ul>
<p><strong>持続可能な社会に必要とされる「社会的共通資本」の概念</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/164782_3_7693cf90e7e46f0a5a8cfa64b7b716d9_1024x724_371d1fd23a/164782_3_7693cf90e7e46f0a5a8cfa64b7b716d9_1024x724_371d1fd23a.webp" alt="164782-3-7693cf90e7e46f0a5a8cfa64b7b716d9-1024x724.webp" /></p>
<p>2017年以降、日本社会向けの研究では、都市集中と地域分散の分岐が示されていた。今回の分析は、その国際版と位置づけられる。</p>
<p>研究チームは今後、モデルの精緻化やデータ拡充を継続し、国際政策や各国の政策議論での活用を視野に入れているとのこと。</p>
]]></description>
      <pubDate>Tue, 26 Aug 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google、AI学習に必要なデータを最大1万分の1に削減可能な新手法を発表</title>
      <link>https://ledge.ai/articles/google_ai_data_reduction_method</link>
      <description><![CDATA[<p>Googleは2025年8月７日、自社の研究ブログで、AIモデルの学習に必要なトレーニングデータ量を最大で1万分の1に削減しながら、モデル品質を維持できる新しい学習手法を<a href="https://research.google/blog/achieving-10000x-training-data-reduction-with-high-fidelity-labels/">発表</a>した。従来の膨大なデータ収集とラベリングに依存するアプローチに比べ、効率的かつ高精度なラベル付けを活用する点が特徴となる。</p>
<h2>新手法の概要</h2>
<p>Google Researchが公開した今回の手法は、まず大規模言語モデル（LLM）を用いてデータをクラスタリングし、モデルが誤りやすい境界事例を抽出する。その後、専門家が少数のデータに高精度なラベルを付与し、ファインチューニングに利用する仕組みだ。</p>
<p><strong>■ LLMによる事前ラベリング→クラスタリング→境界ペア抽出→専門家ラベル→反復学習の流れ（①〜④）</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Curation_Strategies1_Process_Final_width_1250_c73b951bad/Curation_Strategies1_Process_Final_width_1250_c73b951bad.png" alt="CurationStrategies1_ProcessFinal.width-1250.png" /></p>
<h2>実験結果</h2>
<p>通常10万件規模のラベルが必要とされるケースにおいて、この手法では250〜450件の専門家ラベルで同等以上の成果を得られることが示された。実験にはGoogleの軽量モデル「Gemini Nano-1（1.8Bパラメータ）」と「Gemini Nano-2（3.25Bパラメータ）」が用いられ、特にNano-2ではモデルと専門家ラベルの一致度を示す指標「Cohen’s Kappa」が55〜65％向上した。</p>
<p><strong>■ Cohen’s Kappaとサンプル数の関係。キュレーション（緑）が従来（赤破線）を広く上回り、特に3.25Bモデルで効果が顕著</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Curation_Strategies4_Results_width_1250_6dca7b850b/Curation_Strategies4_Results_width_1250_6dca7b850b.png" alt="CurationStrategies4_Results.width-1250.png" /></p>
<h2>意義と応用可能性</h2>
<p>この成果により、AIの開発に伴うデータ収集やアノテーションのコストを大幅に削減できる可能性がある。特に医療や広告など、専門知識が求められる領域での利用価値が高いとされる。また、AI開発の持続可能性を高め、より幅広い分野での応用を後押しすることが期待される。</p>
<p>Googleは今後も効率的なAIトレーニング手法の研究を続け、より少ないデータで高品質なモデルを構築できる仕組みの標準化を目指すとしている。</p>
]]></description>
      <pubDate>Tue, 26 Aug 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/8/27 [WED]Google、Geminiに「2.5 Flash Image」を統合──特徴を崩さず“その人らしさ”を保つ画像編集モデル</title>
      <link>https://ledge.ai/articles/google_gemini_2-5_flash_image_integration</link>
      <description><![CDATA[<p>Googleは2025年8月26日、被写体の特徴を保ったまま編集できる新しい画像モデル「Gemini 2.5 Flash Image」を<a href="https://blog.google/products/gemini/updated-image-editing-model/">発表</a>しGeminiに統合した。同モデルは、自然言語による編集指示に対応し、キャラクターやスタイルの一貫性を維持した高度な画像編集を可能にする。まずはGeminiアプリに搭載され、開発者向けにはGemini API／Google AI Studio／Vertex AIでプレビュー提供が始まっている。</p>
<h2>“特徴を維持したまま”の進化</h2>
<p>従来の画像生成AIでは、同じ人物を複数の画像に登場させると顔立ちや雰囲気が変わってしまうことが少なくなかった。Gemini 2.5 Flash Imageでは、こうした課題を克服し、同じ人物やキャラクターを画像間で同一人物らしく保つ。衣装や背景を変えても「その人らしさ」を維持し、自然文での指示に対して局所的・段階的な編集が可能になった。従来の生成AIで起きがちだった“顔の破綻”や“別人化”を抑え、連作広告やブランド素材づくりに必要な一貫性を実現する。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/gemini_2_5_image_editing_character_consistency_original_c402e8476b/gemini_2_5_image_editing_character_consistency_original_c402e8476b.jpg" alt="gemini-2-5-image-editing-character-consistency.original.jpg" /></p>
<h2>外部評価：「Image Edit Arena」で首位</h2>
<p>Gemini 2.5 Flash Imageは第三者の評価でも成果を示している。AIモデルの比較プラットフォーム「LM Arena」が公開している<a href="https://lmarena.ai/leaderboard/image-edit">Image Edit Arena</a>では、OpenAIの「gpt-image-1」やBlack Forest Labsの「flux-1-kontext-max」などを抑え、1位にランクインした。編集精度や一貫性の高さが、他モデルを上回ると評価されている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/nanobanana_image_edit_arena_a10e5979e6/nanobanana_image_edit_arena_a10e5979e6.jpg" alt="nanobanana image edit arena.jpg" /></p>
<p>さらに、自然文で「背景を暗く」「モノクロをカラーに」といった指示を与えると、対象の特徴を崩さずに狙った部分だけを修正できる。複数の写真を組み合わせて一枚のビジュアルを生み出す「マルチイメージ融合」や、段階的に要素を追加していく「マルチターン編集」、他の画像の色調や質感を移す「スタイル転写」にも対応している。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/gemini2_5_multiturn_edit_12594cb61e/gemini2_5_multiturn_edit_12594cb61e.jpg" alt="gemini2-5 multiturn edit.jpg" /></p>
<h2>提供状況と価格</h2>
<p>モデルは即日Geminiアプリで利用可能。開発者向けにはGemini API／Google AI Studioでプレビュー提供中で、数週間以内に安定版へ移行予定。価格は出力トークン100万あたり30ドルで、1画像は1290出力トークン（約0.039ドル）に相当する。企業向けにはVertex AIでも提供される。</p>
<h2>安全性と表示</h2>
<p>Geminiアプリで作成・編集した画像には可視の透かしに加え、SynthIDによる不可視ウォーターマークが自動付与される。API／AI Studio／Vertex AI経由の生成・編集でもSynthIDを埋め込み、AI生成物であることの識別を支える。</p>
]]></description>
      <pubDate>Tue, 26 Aug 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI、GPT-5導入時の不手際を認め次期GPT-6の方向性を示唆──アルトマン氏「人々は記憶を求めている」</title>
      <link>https://ledge.ai/articles/gpt5_launch_failure_and_gpt6_memory_focus</link>
      <description><![CDATA[<p>OpenAIは2025年8月7日に最新モデル「GPT-5」をChatGPTの全ユーザー向けに提供開始したが、同社CEOであるサム・アルトマン氏は「ローンチ対応でやらかした」と認めた。ユーザーからの批判を受け、同社は前モデルGPT-4oを有料プラン向けに復活させる異例の対応を実施した。一方でアルトマン氏は、次期「GPT-6」について「人々は記憶を求めている」と述べ、記憶とパーソナライズを重視する方針を示した。この方針については、8月19日付の<a href="https://www.cnbc.com/2025/08/19/sam-altman-on-gpt-6-people-want-memory.html">CNBC</a>が詳報している。</p>
<p>GPT-5は2025年8月7日に正式<a href="https://ledge.ai/articles/gpt5_launch_all_users">リリース</a>され、全世界のChatGPTユーザー約7億人に提供が開始された。OpenAIは「PhDレベルの性能」を掲げ、プログラミングや数学、マルチモーダル処理に強みを持つと説明している。しかし専門家の間では「進化的ではあるが飛躍的とはいえない」との見方が広がり、期待値に対して十分な成果を示せていないとの指摘もあった。</p>
<p>アルトマン氏はサンフランシスコで記者団に対し、「GPT-5のローンチは完全にやらかした」と発言した。ユーザーからは「冷たい」「親しみやすさが失われた」といった批判が相次ぎ、従来のモデルにあった温かさが欠けているとの不満が広がった。こうした反応を受け、OpenAIは異例の対応として前モデルGPT-4oをChatGPT Plusなどの有料プラン向けに復活させた。新モデル提供後に旧モデルを再導入するのは極めて異例であり、同社がユーザーの声に迅速に対応したことを示している。</p>
<p>技術面では、GPT-5は高度な推論能力や自然なマルチモーダル処理を実現するなどの改良が施されている。しかし従来モデルとの差は限定的であり、期待が過剰に膨らんでいた分、失望を招いた側面が大きい。批判の背景にはこうしたギャップがあるとみられる。</p>
<p>一方でアルトマン氏は、次期モデルGPT-6について「人々は記憶を求めている」と強調した（8月19日、<a href="https://www.cnbc.com/2025/08/19/sam-altman-on-gpt-6-people-want-memory.html">CNBC</a>）。GPT-6はユーザーごとの履歴や好みに基づいて会話を記憶し、応答をパーソナライズする方向性を持つという。これにより、利用者ごとに最適化された応答スタイルを提供できるようになることを目指している。ただし、具体的なリリース時期は明らかにされていない。</p>
<p>今回の一連の動きは、OpenAIの製品ロードマップにおける転換点を示すものだ。GPT-5で露呈した課題をどう克服し、GPT-6でユーザー体験を改善するのか。今後の展開が同社の成長に直結する焦点となっている。</p>
]]></description>
      <pubDate>Tue, 26 Aug 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>AIエンジニア視点で紐解くAIエージェントの可能性　技術背景とビジネス活用の最前線｜『現場で活用するためのAIエージェント実践入門』刊行記念インタビュー</title>
      <link>https://ledge.ai/articles/introduction-to-ai-agents-book-interview</link>
      <description><![CDATA[<p>「AIエージェント元年」と呼ばれる2025年は、目標に応じて自律的に判断・行動するAIエージェントに大きな期待が寄せられており、各企業で導入や検討が進んでいる。その中で、2025年7月発売の「本当に動くAIエージェントはどう作るのか」をテーマに執筆された書籍『<a href="https://www.amazon.co.jp/dp/4065401402">現場で活用するためのAIエージェント実践入門</a>』（講談社）が大きな注目を集めている。
今回は、導入現場で使える実践的なノウハウをまとめた同書を執筆した著者陣5名に、AIエージェントの基本概念や技術的な背景、導入時の課題、ビジネスへの応用可能性などについて取材を行った。</p>
<h2>著者情報</h2>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/IMG_0170_a_220937aab9/IMG_0170_a_220937aab9.jpg" alt="IMG_0170_a.jpg" /></p>
<h2>AIエージェントとは何か</h2>
<p><strong>ーーまず初めに、AIエージェントとは何か？というところから、解説をお願いできますか。</strong></p>
<p><strong>太田氏</strong>
AIエージェントという言葉は、2023年頃から出てきたというイメージを持たれているかもしれませんが、実は1970年、90年頃からありました。「エージェントとはなんですか？」という問いに関しては、【<strong>目標に向けて環境で相互作用する知能システム</strong>】と位置づけるのが、個人的にはわかりやすいと思っています。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/AI_1_5353dee988/AI_1_5353dee988.png" alt="AIエージェント_1.png" /></p>
<p>もう少しかみ砕いて説明します。目標というのはわかりやすいですよね。何かをするイメージで、“これを達成しなければならない”というのが目標です。
では、”環境とは何か”という点についてですが、エージェントが動ける空間をイメージしてください。デジタル空間もあれば、我々がいるこの場のようなフィジカルな空間もあり、その両方を環境と言います。</p>
<p>「環境と相互作用するとは何か？」についてですが、そもそもAIは知能を人工的に作っているだけの知能なので、手や足があるわけではない。なので、我々がいるフィジカル環境やデジタル空間にインタラクションするということはできないんです。AI自体は何かを認識するなどしかできないのですが、「エージェント」と呼ばれるものは、人工知能を使って環境へインタラクションできるというのがポイントです。”環境”から情報を受け取ってAIエージェントが認識し、「次はこれを行おう」と判断して実際に行動できるということが重要なのです。</p>
<p><strong>ーー1970年代より概念があったとお話いただきましたが、ここ数年でなじみが出てきたのは、生成AIが大きく関係しているのでしょうか。</strong></p>
<p><strong>太田氏</strong>
そうです。昔はルールベースや、データから学習しなければならず、研究・開発者以外の皆さんの手元に届けるのに時間がかかっていたんです。
私たちが身近に扱うものって自然言語や画像などがありますが、生成AIやLLMは「テキスト（自然言語）で指示をしたら、テキストで返答してくれる」というのがポイントで、このLLMという知能がユーザー層と密接になったことで、皆が「エージェントで何かできるのではないか」と考え始めるきっかけになったと思っています。</p>
<p><strong>ーーAIエージェントの他に、エージェンティックAIという言葉がありますが、この違いは何でしょうか。</strong></p>
<p><strong>宮脇氏</strong>
言葉の定義でいうと、AIエージェントは技術やソフトウェアのような「エージェントそのもの」を表す言葉であると思います。一方で、エージェンティックAIやエージェンティックテクノロジーでいうと、「エージェントの性質をもつもの」を指すと思います。</p>
<p>エージェンティックAIやエージェント型AIの言葉の意味合いは、最近変わり始めていると思っています。昔は「永続的なソフトウェア」というのが、一つの定義としてあ⁨⁩りました。データの流れを監視できるし、ユーザーの目的や好みに合わせてタスクを遂行するものというのが、<a href="https://rosenfeldmedia.com/books/designing-agentive-technology/">昔の解釈</a>だと思っています。
しかし、最近では生成AIブームの流れで意味合いが変わってきたという印象を受けます。エージェントは日本語で言うと“代理者”と訳すことができますが、人の業務を代行するような形で、AIをコアとして代理者の性質を受け継ぐシステムを「<a href="https://arxiv.org/abs/2505.10468">エージェンティックAI</a>」と呼んでいると思います。</p>
<p><strong>ーータスクを遂行するだけであれば、これまではRPAなどでプロセスをオートメートできていました。しかし、人の代替と位置付けると単純作業だけではないと思います。ゴールに向けてタスク分解し、適切な順序で自律的に実行できるという点が、エージェンティックというキーワードに含まれているのでしょうか。</strong></p>
<p><strong>宮脇氏</strong>
そうですね。“人との接点”というのが一つの軸としてあるかと思います。接点が少ないRPAやオートメーション。逆に接点が多いチャットボット型AI、というすみ分けになると思っていて、AIエージェントは、あまり接点はないが自分がやりたいことを委任して自走してくれるものです。ちょうど<a href="https://note.com/dory111111/n/n03eac77e5197">中間に位置するイメージ</a>ですね。</p>
<h2>AIエージェントへの期待と導入課題</h2>
<p><strong>ーー現在、AIエージェントへの期待や注目度は高まっていますが、なぜ、多くの人がAIエージェントに期待を寄せているのでしょうか。</strong></p>
<p><strong>西見氏</strong>
太田さんの説明で、AIエージェントとは環境と相互作用する知能システムであるというお話があったと思いますが、これってよく考えたら「人間なのでは？」という話なんですよ。
例えば、今、リモートワークをしている人が多くいらっしゃいます。パソコンを触ってリモートワークをしている人間って、ビジネスチャットを通じて仕事をしている姿を別の人間から見たとき、「何か言ったら返す」というシステムと類似していますよね。見え方としては人間である必要がなく、タイピングして返してくる“何か”なので、視点を変えるとソフトウェアと言えると思います。
そのような人たちと日常からコミュニケーションをとって一緒に仕事を行い、時には成果物としてPowerPointやWordを出してきたりする。これって「AIエージェント」じゃないか、という話なんです。</p>
<p>ご質問いただいた「何に期待しているか」については、シンプルに『<strong>人間の代替</strong>』です。人間のように動くソフトウェアがあれば、どれほど使っても人間より安かったり、多く働いてくれたり、病気もしないし、労基署へも訴えないというところで、非常に使い勝手がいいですよね。そのように、労働力の代替として認められ始めているのが現在なんです。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/AI_Agent_b_5ee7a2de78/AI_Agent_b_5ee7a2de78.png" alt="AI Agent_b.png" /></p>
<p>AIエージェントというソフトウェアがあるとして、これが今後多方面で多く活用されていくという見立てを皆がするんですよ。OpenAIやAnthropicが独占して世界の覇権を取っていこうという動きではなくて、彼らはそのモデルを公開しているんです。AIエージェントを作る部品を提供していて、それを各社が使って動くものを開発していこうと。その渦中に我々がいるのを考えたとき、「キャッチアップしないとまずいのでは」「シンプルに置いていかれてしまう」と思っている人が多いのだと思います。</p>
<p>社員が1万人いる会社があるとして、1万人全員がAIエージェントとインタラクションしたり、AIエージェント自身が長時間働けるようになる未来、1名の従業員が100体のAIエージェントをマネジメントできるとなると、単純に10,000×100になります。そんな体制を他の会社が構築していたら、どれだけのインパクトがあるか。私は経営者なので、各社のいろんな方と対話をするのですが、そういった“危機感”を持つ声が強いという感覚があります。</p>
<p><strong>ーーただ、よく聞く話としては、活用に関するハードルやリスクなどの懸念も各社持っているかと思います。それについてはいかがですか。</strong></p>
<p><strong>後藤氏</strong>
まず、昔とは状況が変わっているというのは一つあります。私たちが相対するのはIT部門やDX推進部なので、お客様側でやりたいというモチベーションがあります。なので「できないことは言わない」というか、以前よりは言わなくなっていると思います。ただその上で、どうしてもセキュリティや企業内で守らなければならないものがあるので、システム導入する上では、それらをすごく意識しています。</p>
<p>現在、AIエージェントの波が来ていますが、その前にはDXの波があって、「オンプレミスでしかデータは扱えない」という方針だったのが変更されて、クラウドに移行しました。また、「機械学習でデータを使って学習しよう」となった時に、「ではそのデータはどこに置いておくのか」というような議論も、これまでに行われてきたわけです。そうした積み重ねがあり、徐々に各社のデータやシステムに関するリテラシーが上がり、導入時のハードルが下がっているような印象を受けています。</p>
<p><strong>ーー今、DXというキーワードがありましたが、AIエージェントに取り掛かる手前で準備しなければならないものはあるのでしょうか。</strong></p>
<p><strong>阿田木氏</strong>
AIエージェントの活用においては、まず、クラウドなどを用いたRAGなどの構築があると思います。RAGは、社内ドキュメントにアクセスするために使う手法として一昨年から昨年にかけて流行していました。RAGのために整備したナレッジがあると思うのですが、現在は、各所で作られたRAGを統合するために、エージェンティックな機能を入れたいというお客様を<a href="https://aitc.dentsusoken.com/column/rag_to_ai_agents/">支援</a>することが多いですね。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/AI_2_03d0121f9e/AI_2_03d0121f9e.png" alt="AIエージェント_2.png" /></p>
<p><strong>ーーエージェントをフルに活用していこうとすると、その手前にRAGはクリアしないのでしょうか。</strong></p>
<p><strong>阿田木氏</strong>
要件によりますが、LLMが持つ知識は一般的なものなので、それを社内で活用していこうと思ったとき、情報を拡張する力としてRAGが一つの手段になると思っています。</p>
<p><strong>西見氏</strong>
“環境”という言葉で言い表せるかと思います。相互作用するものなので、環境上に情報がなかったら参照するものがない。検索システムをエージェントが使えるようにするのがRAGです。GoogleドライブをAIエージェントが見に行って参照できれば別に問題がないですが、どうエージェントが情報にアクセスするのかという話です。</p>
<p><strong>太田氏</strong>
なので、企業の皆さんはいきなり会社の根本となる業務をエージェントで代替しようとするとやはり怖いので、まず業務の中でも端の方のFAQや採用など、会社の大きな損失に関わらないところから始めています。徐々に、業務の根幹部分のデータをエージェントが触れられるよう、段階的に準備していると思います。先の未来には、もしかしたら一部のホワイトカラーの業務が、エージェントに置き換わることがあるかもしれないですね。</p>
<p><strong>ーー現在、コア業務までAIエージェントを活用できている企業は、どのぐらいあるのでしょうか。</strong></p>
<p><strong>太田氏</strong>
多分みんなやられてると思うんです。しかし、この開発を社外に発注するのはデータが非常に機密であるから難しい。おそらく着手されていて、取り組んでる最中だと思うんですが、公開できない情報が非常に多いのだと思います。</p>
<h2>AIエージェントはどのように作るのか</h2>
<p><strong>ーー続いて、AIエージェントをどのように作るか？という点についても、解説いただけますか。</strong></p>
<p><strong>太田氏</strong>
まず、何を代替したいのか、エージェントを開発する対象の業務を考えます。次に、いきなり作り始めるのではなく、その業務が既に存在するDeep Researchや市場調査が得意なエージェントなどで代替できるかを考えます。AIエージェントを人間に例えるのであれば、似たようなスキルを持った人間がいないかを考えるんです。代替したい業務が企業独特の業務であり、世の中に同じスキルを持った人間がいない場合は、自分たちで作るしかないという判断になります。</p>
<p>“作る”と判断したら、LLMのAPIを使わなければならないので、LLMを用意します。その時、利用するLLMが、その業務に関する知識を持っているかどうかを見ます。持っていない場合は、業務を遂行するためのプロセスを可視化し、分解して、利用するデータソースや登録・申請などのアクションポイントを洗い出し、順序通りにLLMがこなせるようにワークフローを構築していきます。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/AI_3_c21acca892/AI_3_c21acca892.png" alt="AIエージェント_3.png" /></p>
<p>プロセスの中にはおそらく、人間の意思決定が伴う箇所がいくつもあると思います。それらすべてをプロンプトで渡し、意思決定を代替してもらう。エージェントが意思決定をしながらアクションも同時に実行していくという手続きを書くというのが、エージェントを作る設計における第一段階です。</p>
<p><strong>ーー宮脇さんは、所属されている企業で採用業務のエージェントを開発されていますよね。どの会社でも汎用的に使えるツールを開発する上で、意識する点や課題などはありましたか。</strong></p>
<p><strong>宮脇氏</strong>
まず意識する点として、プロダクトを開発・提供する企業としての戦略の部分をお話しすると、「この業務を代替したら事業としてうまくいく」というところから始めています。多くのお客様が利用してくれるというのもそうですし、業務の効率化を重視しているのであれば「どれくらい効率化を期待できるのか」という視点を持って開発をしています。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/AI_Agent_c_24423dbaa4/AI_Agent_c_24423dbaa4.png" alt="AI Agent_c.png" /></p>
<p>我々もAIエージェントを活用しながらプロダクトの開発を進めていますが、エージェントは「切り札にはなりうるがあくまで手札でしかない」と思っていて、一つの“手段”でしかありません。なので、AIエージェントとしてうまくいかなくても、その機能だけは今後も資産として横展開できる部分から開発をしています。</p>
<p>採用業務のAIエージェント開発において、採用業務特有の課題みたいなところでいうと、やはり個人情報を扱う点が挙げられると思います。AIエージェントは、一定うまくできそうな印象は受けますが、「うまくいきそう」と「うまくいく」の間は大きく乖離しています。AIエージェントにはタスクの実行完了までを委任することになるため「成果物には意図しない誤りが含まれない」と確信できるまでの作り込みが、大変な部分になるかなと思ってます。</p>
<p><strong>ーーAIエージェントと相性が良い領域などはあるのでしょうか。</strong></p>
<p><strong>西見氏</strong>
宮脇さんの企業はプロダクトでAIエージェントを出してますが、とても難しいんですよ。
なぜ難しいかっていうと、エージェントって自由に動くんですよね。自由に動かせられるほどパワーが出るのですが、リスクが大きい。それに対して、“ガードレール”と呼ばれるもので、「この範囲で動いてね」という具合にAIを制御するんです。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/AI_4_91aae42ca3/AI_4_91aae42ca3.png" alt="AIエージェント_4.png" /></p>
<p>しかし、安全に使えるように囲い込んで、安全の余白を作れば作るほど、決定論的に決められた通りにしか動かなくなる。それはそれでパワフルではありますが、AIエージェントへの期待とはまた違うわけです。</p>
<p>これをどのようにバランスを取ればよいかというと、多く言われているのがBPO（業務代行）の話です。BPOは古典的な業務代行のことで、人が代行するイメージがありますが、その裏側で何が行われているかという点は、顧客側は頓着しないですよね。結果を出してくれればよいと。その“裏側でどうするか？”というところで、AIエージェントが注目されているんです。顧客に直接AIエージェントを触らせないので、何かをしでかすリスクはないし、考えうるリスクに対しては、ある程度マニュアル等でガードできます。
業務代行を行うための人材採用に力を入れなくてもエージェントを強く配備していけば、ものすごい大量の人を雇ったのと同じ形でビジネスを展開できる。社内の代行として何ができるかの検討も進んでいるし、実際いろんな分野でAIエージェントが動いている姿が見えてきてる印象です。</p>
<p><strong>ーー最近、Agent-to-Agent（<a href="https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/">A2A</a>）のような、エージェント同士が連携する技術も注目されていますが、1つのエージェントがなんでもこなすのではなく、特化型のエージェントが連携してタスクを実行する必要性について、どのような理由があるのでしょうか。</strong></p>
<p><strong>後藤氏</strong>
様々な理由がありますが、<a href="https://www.google.com/url?q=https://blog.langchain.com/context-engineering-for-agents/&amp;sa=D&amp;source=docs&amp;ust=1755655420987616&amp;usg=AOvVaw3teeITIuK3CObqX4SsQmjG">コンテキストが多くなりすぎてしまう</a>というのが一つ挙げられるかと思います。例えば、一人の秘書に色んな業務を依頼すると一個の業務に対する精度が下がってしまいますよね。LLM自体に入れられるトークン数にも限度があるので、一つのエージェントに全て詰め込むのは難しい、という技術面の制約があるかと思います。</p>
<p><strong>阿田木氏</strong>
また、「責任を分ける」という観点もあります。人間の社会も業務においても、「この人はこの業務」などと分けると思いますが、そこを明確に分けることで、管理がうまくいくというのがあります。エージェントも同じで、<a href="https://www.comet.com/site/blog/ai-agent-design/#h-modular-amp-role-based-ai-agent-design">分けて制御してあげる</a>というのが、利用する上で安全であると思います。</p>
<p>加えて、エージェント技術の背景として、シングルエージェントから発展してきたのもあるので、それらを繋げましょうというのが<a href="https://speakerdeck.com/masatoto/llmmarutiezientowofu-kan-suru">現段階</a>になるかと思います。</p>
<p><strong>西見氏</strong>
あとは、<a href="https://blog.langchain.com/react-agent-benchmarking/">ベンチマーク</a>があるんです。一つの仕事を行うエージェントに対して、複数のドメインを与えて、たくさんの仕事に対応できるよう知識を持ってもらうテストを行った例があります。与える知識を0〜10まで試して実験したところ、1つの知識を与えたら目的の仕事をしてくれるが、2つ以上知識を与えると急に仕事の精度が落ちていく。目的はベンディングマシンの仕事なのに、なぜか不動産関連の仕事を始めてしまうというような、混乱しだしたという実験結果もあり、定量で計測してみても、混乱するというのはあるんです。</p>
<p><strong>ーーAIエージェントは人の代替であるからこそ、人との比較がされやすいと思いますが、導入における投資対効果についてはいかがでしょうか。</strong></p>
<p><strong>太田氏</strong>
現在、エージェントの利活用というとドキュメントワークが多いのですが、その業務にかかる時間・工数・人数などは、指標として参考になると思います。それが実際に何時間くらい削れて、かつ品質がどれぐらい安定するのか。人間の場合、人によってクオリティの差が出るので、どのぐらいまで均一な品質でアウトプットできるか、というのがまずあります。実際の運用を考えたときに、投資対効果があるのかどうかは数値で表せると思います。</p>
<p><strong>ーー今後の方向性として、会社でひとつのエージェントを持つイメージなのか、従業員が自分のタスクの生産性を上げる目的で、100や1,000など様々なエージェントを使いこなしていくのか、どちらのイメージが近いでしょうか。</strong></p>
<p><strong>太田氏</strong>
おそらく、世の中で目指したいのは一体のエージェントが多数の仕事をこなす世界だと思います。しかし、提供する側からすると一体のエージェントが複数の仕事をこなせているかを常に監視したり、評価するのは、運用が複雑になってしまいます。そのため、見せ方としては「業務用エージェントがあなたの仕事こなします」と言いつつ、裏で複数エージェントが動くことになると思います。それで各業務において、タスクごとにエージェントをモニタリングするという形に落ち着くかと思います。</p>
<p>この一年は恐らく各従業員ごとであったり、提案するんだったら“提案用のエージェント”や市場調査する場合は“市場調査用のエージェント”など、各タスクごとに異なるエージェントを選択して使う形で、利活用が進むと思います。それは扱う参照データや考え方などの“環境”が違うので、目標というタスク定義で使う対象を分けるためです。
このように、タスクの定義は、初めは従業員単位になるかと思いますが、次第に“資料作成”であればなんとか動かせるところまで抽象度は上がっていく、つまりは会社共有のエージェントになると考えられます。その場合、全データのアクセスが前提となるため、会社側もデータを整備する必要が出てきます。これまでの提案履歴や取引記録を全て接続して、提案先にカスタマイズされた提案書を作成できるのが、将来的なイメージ図になるということですね。</p>
<h2>AIエージェントとこれからの未来</h2>
<p><strong>ーーAIエージェントをキーワードに、どのように未来が変わっていくのか、お一人ずつコメントをいただけますか。</strong></p>
<p><strong>阿田木氏</strong>
これからのエージェントの世界でいうと大きく二分されていくかと思っていて、一つはエージェントと人間が「相互にやり取りする世界」、もう一つは「エージェントに閉じた世界」と考えています。先ほど西見さんからBPOの話がありましたが、AIエージェント開発をしてて思うのは、ボトルネックは人間だということです。なぜかというと、それぞれのアクションは人間が持っている暗黙知に依存することがとても多く、企業においては特にそれが顕著であるからです。人間が介在すると、なかなか前に進まないことがあるので、そこをAIだけの世界にすると、上手くいくのではないかと思っているところです。
しかし、人間が存在する限り人が持つ正解もあるので、人間とAIエージェントが相互にやり取りする世界は存在し続けると考えています。</p>
<p>個人的に“こうあってほしい”という未来についてですが、現在、データ分析でもAIエージェントが使われてて、私はKaggleなどをよくやるのですが、<a href="https://aitc.dentsusoken.com/column/kaggle-ai-agent-01/">Kaggleでもコーディングエージェントが使われている</a>んです。全て使ったことあるのですが、やっていて思ったのが、エージェントに作業を全部渡すと楽しさが奪われてしまうということですね。私は、Kaggleをゲーム感覚でやっているので、ゲームを代わりにやられてしまうのが結構辛い。エージェントには、本当に任せるべきところを任せ、人間も楽しいとこにコミットできるといいなと思っています。</p>
<p><strong>後藤氏</strong>
これまでの話しでもあったように、先を読むのは非常に難しくなっていると思います。インタビューの中で思い出したのですが、この書籍を書き始めたときは、そもそもお客さんに「エージェント」と言ってもわかってもらえないことがありました。それが今の状況になっているので、短期間ですごく変わったと思っています。
そのような中で、ある程度直線状にある未来像でいうと、業務のやり方が変わってくるのかなと思います。まずは人の触る部分をAIエージェントが代替していく。その後にAIエージェントのためのデータ設計だったり、業務のやり方も徐々に変わっていくのかなと思っています。これまで人間だけだと難しかったことや、管理されたもののイメージを作るなど、AIエージェント前提の仕組み作りが進んでいくのかなと思います。</p>
<p><strong>ーーAIエージェントがAIエージェントを作るみたいなものはあるのでしょうか。</strong></p>
<p><strong>後藤氏</strong>
研究分野ではそういう取り組みもあります。ただ個人的には直近の話ではないと思っています。データや環境の整備といったDX、すなわちこれまで人間しかアクセスしてなかったものや、業務に詳しい人に聞かないと分からなかったところがAIエージェントに聞いても分かる、AIエージェントが探せる、そういった整備が先んじて進んでいくと思っています。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/AI_Agent_d_c24a2a4250/AI_Agent_d_c24a2a4250.png" alt="AI Agent_d.png" /></p>
<p><strong>西見氏</strong>
インフラが整っている領域は基本的にエージェントが入り込んでいく領域だと考えていいと思います。インフラとは、大きく分けてデジタルとフィジカルですね。特にデジタルはエージェントが入り込みやすい。センシティブなデータやデータ管理に関しては様々な問題があるにせよ、基本的にはアクセスができるわけです。
インフラが整っているもの、その中で現在活発なのが自動運転ですね。なぜ自動運転が活発かというと、道路が敷かれてるからです。走れるインフラがあるんです。なので、インフラがあるところからエージェントがどんどん活用されていくと考えています。</p>
<p>デジタル領域において、なぜこんなにAIエージェントが活用されているかというと、“インターネット”というインフラがあるので、インパクトも出るからですね。僕はこの畑に入りすぎて人間しかできないことはないんじゃないかとか思っちゃうんですけど（笑）農業などもデジタル化できないと言われつつも、今では、大規模農業は結構機械化が進んでいます。ドローンで生育状況を把握し、トラクターも自動運転で動かすことができるので、ある意味代替できている。それもインフラですね。ドローンで監視ができ、農作業の機械があり、自動運転ができればうまくいく業務があるということです。</p>
<p><strong>ーー統一規格みたいな存在も結構大きいですよね。</strong></p>
<p><strong>西見氏</strong>
インターネットはhttp通信させれば情報取れるので、そのようなプロトコルがある時点でインフラは整っているわけです。倉庫などでも、パレットで積んであれば動かせる話と、荷物の規格が異なると動かせないというようなイメージですね。
船での運搬も、コンテナという規格が生まれたからこそ、高速に行えるようになった経緯があります。人間は、人が働きやすくするためのインフラを作ってきた歴史がある。そのようなインフラを土台にしてエージェントは動いていくので、人間にはできなかったような長時間労働も可能ですし、コンピュータシステムなので、スケーリングによって作業量も増やせるという期待があります。その期待に応えられるように拡張しているのが、現在見えてる姿なんです。</p>
<p>AIモデルもどんどん進化しているので、この相互作用によってどこまでできるかっていうのは、まだわからない。インフラがあるところにAIエージェントは浸透するので、今はそのインフラ自体を作っていくというフェーズですね。</p>
<p><strong>ーー続いて宮脇さん、いかがですか。</strong></p>
<p><strong>宮脇氏</strong>
AIエージェントは、成果創出を図るという部分において圧倒的に長けているかなと思っています。従来のMLシステムだと、OCRや翻訳みたいな一部の業務だけを代替するものでしたが、AIエージェントという枠組みにおいては、他の業務と繋がりやすくなったっていうところで、圧倒的に成果創出に向くようになったと思います。</p>
<p>AIエージェントの良い部分は「質・量・スピード」それぞれにあります。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/AI_5_27b5f52295/AI_5_27b5f52295.png" alt="AIエージェント_5.png" /></p>
<p>例えば採用領域でいうと『摩擦』と『スピード』の2つの側面でメリットがあると考えられます。</p>
<p>『摩擦』については、特定の判断においてAIが考慮できる項目が増えたということです。例えば<a href="https://www.theladders.com/static/images/basicSite/pdfs/TheLadders-EyeTracking-StudyC2.pdf">LADDERS社の報告</a>では、採用担当者が書類選考にかける時間は60秒以内といわれていて、採用担当者がチェックできる観点は限定的だといえます。候補者にとってもあまり嬉しくないですよね。一方AIによる読み込みの場合、レジュメ全てを読んだ上で総合的に判断することが多いため、これまで採用担当が読み飛ばしていた色んな職能や経験を考慮できるようになったといえます。
『スピード』でいうと、LLMの文章生成が速いことだったり、24時間365日体制が築けるようになったことで、候補者が応募してきた際にすぐ反応できるようなフローが組める。極端にいうと、応募があったその日のうちに面談ができるような、そういう世界がくるかなと思っています。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/AI_6_8ae9b5f6f7/AI_6_8ae9b5f6f7.png" alt="AIエージェント_6.png" /></p>
<p><strong>ーー現時点で、質の部分も人より高水準であると言えてしまうのでしょうか。</strong></p>
<p><strong>宮脇氏</strong>
そうですね。AIが参照できるガイドラインがあれば、一定の品質が担保された状態で価値創造できると思います。</p>
<p>今の話はこの先5～10年の話なのですが、40年後の日本をみると、労働人口が40%減るというような予測もあるので、必ず労働力の代替としての期待がある。エージェントの『代行』と『協働』の2つの期待が高まっていると思います。</p>
<p><strong>ーーAIエージェントのこれまでの歴史がある中で、現在思ったより早く実用化できたということなのでしょうか。</strong></p>
<p><strong>太田氏</strong>
コーディングは思った以上に早く進んでる印象はあります。加速できる枠組みができたんですよね。先ほど、人が介在すると遅くなるという話があったんですけども、コーディングの作業は基本的にテストがあり、正解か不正解かがわかると。エージェントが自分で計画して行動して、うまくいったかどうかがテストでわかり、そのデータが蓄積されるという、人が介在せずに自動的に賢くなっていく、自己完結できるレールに乗っかったんですよね。
そうなると、AIの知能は人間を追い越すので、同じように人間の業務の中でも自己完結できるものがあれば、すごい速度でどんどん成長していくと思います。</p>
<p>それが報告されたのがここ1～2年で、大きなインパクトがあったので、自己完結できる環境を皆が探している状況かと思います。</p>
<p><strong>西見氏</strong>
ロボティクスの分野でも世界モデルというものがあって、「こういうふうに行動が起きた場合にこういうふうに返す」というように、光景の動画を生成器というモデルが生成し、それをもとに学習するサイクルが回っています。世界モデルは正解・不正解がわかるので、自己完結する。複数モデルで組み合わせてRL（強化学習）を回していくっていうのは、十分あります。</p>
<p><strong>ーー最後に太田さん、お願いします。</strong></p>
<p><strong>大田氏</strong>
究極的にいうと、皆が“人間”に興味が出てきて、一日のほとんどを様々な人と会話をしているような、そんな働き方になるかなと思ってます。問題を定義したり、手を動かす部分は徐々にエージェントの方が良くなるので、人の気持ちを考えたり、こういうアイデアはどうだろうかと考えたりする作業が増えるのではないかと思います。まだLLMや生成AIは、想像力であったり、点と点を結び合わせて新しい何かを創造するという点は弱いので、それが伸びてこない間は、人間が色んな人と話をして、今までの経験から新しいアイデアを出し、それらをエージェントを使ってシミュレーションしたり、実際に作らせてみたりなど、そういう世界観になると思ってます。</p>
<p>今までの話をまとめになりますが、まず、個人で様々な業務をこなすと属人化するという課題があります。これまで、属人化したくないからシステムを作ってきたわけなんですよね。銀行を例にすると、様々な町ごとに店舗を構えて、お客さんが来たら対応していましたが、その業務がATMへ置き換わりました。ATMによって自動化されましたが、もともとその業務を担っていた人たちは失業したわけではなく、別のお客さんにライフプランを考えるなどの業務へ変わっていったと思います。なので、AIエージェントが出てきたからと言って、仕事がなくなるわけではなく、違う何かに価値を見出すように変わってきている。我々が開発しているエージェントも、今までシステム化されてなかったところをシステム化しようとしているだけなんです。その時、我々人間の仕事は何かというと、新しいアイディアを考えることであったり、様々な人とコミュニケーションをとって、もっと面白いインパクトがある事業を作り出すなど、そういう方向に進んでいくと思っています。もちろん動いているシステムを監視する人は必要なのですが、より短いサイクルで面白いものが出てくる未来があるのではないかなと思います。</p>
<p>先ほど話題に上がったA2Aについて言うのであれば、事業や企業ごとに作られたエージェント同士がもっと情報をやり取りすることによって、新しいサービスの開発はどんどん加速する。事業や企業ごとにデータをきちんと連携しているからこそできる技だと思うので、面白いビジネスが1週間や1ヶ月などの短いスパンででポンポン出てきて、コンテンツや、よくわからない体験も含めていろいろ生まれてくるような、“創造の社会”もあるんじゃないかなと思います。</p>
<p>様々なシステムがもっと連携されやすくなれば、例えば交通関係と周辺の百貨店が連携するなど、街全体を繋げられるようになったり、一歩ずつシームレスな形で社会が形成されていく期待もあると思います。</p>
<p><strong>ーー人間同士のコミュニケーションがより大事になってくる一方で、すべてAIで良いのでは？という極論もあるかと思いますが、それについてはいかがですか。</strong></p>
<p><strong>後藤氏</strong>
先ほど、コーディングエージェントがすごいという話がありましたが、実際にタスクを振れば色々やってくれるんですけど、でもそのタスク（目標）は定義しなければなりません。</p>
<p>そのタスクは何から取ってくるかというと、太田さんが仰ったように、「そもそも自分たち何やりたいんだっけ」や「こういう機能追加したいです」などの人間の要望から生まれてきます。下流のタスクを実行してくれる部隊（AIエージェント）が増えて、速度も上がった分、上流で意思決定しなければならない。高速に回り始めているのが、現実としてあります。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/AI_Agent_book2_778c3f1eb1/AI_Agent_book2_778c3f1eb1.jpg" alt="AI Agent_book2.jpg" /></p>
]]></description>
      <pubDate>Tue, 26 Aug 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Blenderを操るAI──シカゴ大学ら、大規模言語モデルで3Dアセットを生成・編集する「LL3M」を発表</title>
      <link>https://ledge.ai/articles/ll3m_blender_llm_3d_modeling</link>
      <description><![CDATA[<p>シカゴ大学の研究チームは2025年8月11日、自然言語の指示だけでBlender内に3Dアセットを作り出せるシステム「LL3M（Large Language 3D Modelers）」を<a href="https://arxiv.org/abs/2508.08228">発表</a>した。大規模言語モデル（LLM）が直接Pythonコードを生成し、オブジェクトやシーンを自在に構築・編集するという新しいアプローチだ。論文はarXivに公開され、公式プロジェクトページやGitHubリポジトリも公開されている。</p>
<h2>コードを書くAI、Blenderを動かす</h2>
<p>LL3Mの最大の特徴は、3Dモデルを特殊なデータ形式で直接生成するのではなく、Blender用のPythonコードを“書く”AIであることだ。これにより、生成結果はすべて人間が理解できるコードとして残り、後から自由に修正・拡張できる。既存のワークフローに統合しやすい点も大きな利点とされる。</p>
<h2>多様なアセットを生み出す柔軟性</h2>
<p>論文では、BMeshによるポリゴンモデリング、モディファイアやシェーダーノードの適用、シーン階層の構築など、幅広い3D要素をコードで生成可能であることが示されている。家具やキャラクターなど、多彩なアセットが次々とコードベースで形作られる様子は、従来の「ブラックボックス的な生成AI」とは異なる透明性を感じさせる。</p>
<p><strong>■ Blender用Pythonコードを生成し、3Dオブジェクトを構築する例</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/fig15_intro_highlight_bc133cc7d3/fig15_intro_highlight_bc133cc7d3.jpg" alt="fig15-intro-highlight.jpg" /></p>
<h2>3段階で進化するパイプライン</h2>
<p>LL3Mは単なる一発生成ではなく、段階的にモデルを洗練させる仕組みを備える。</p>
<ul>
<li><strong>初期生成</strong> ：自然言語の指示からコードを生成し、Blenderでオブジェクトを構築</li>
<li><strong>自動自己精緻化</strong> ：AI自身が結果を評価し、改善点を修正</li>
<li><strong>ユーザー誘導精緻化</strong> ：人間の追加指示を受けて再度改善</li>
</ul>
<p><strong>■ 自己批評やユーザー指示に基づく反復的な3Dモデルの改善</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/fig22_humanoid_final_b372c81836/fig22_humanoid_final_b372c81836.jpg" alt="fig22-humanoid-final.jpg" /></p>
<p>さらに「BlenderRAG」と呼ばれる仕組みで、Blender APIドキュメントを参照しながらコードを補強。これにより「動かないスクリプト」を避け、実際に使える成果物を高精度に生み出す。</p>
<h2>“コードで3Dを描く”という発想の転換</h2>
<p>研究チームは、NeRFや点群などデータ駆動型の3D生成手法と対比しながら、LL3Mのアプローチを強調する。コードは読み書きできる資産であり、再利用性や可搬性に優れるため、クリエイターや開発者にとって扱いやすい。AIと人間が共同作業する新しい3D制作の基盤としての位置づけを打ち出している。</p>
<h2>今後の展望──ゲームから教育まで</h2>
<p>GitHubリポジトリはすでに公開されているものの、実装は「Code coming soon」とされ、今後順次公開される見込みだ。研究チームは「人間とAIが協力する3D制作の未来」を描きつつ、ゲーム開発、教育、デジタルコンテンツ制作など多様な分野での応用可能性を指摘している。</p>
<p><strong>■ LL3Mによって生成された多様な3Dアセットの例</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/fig2_gallery_b2bb1b7580/fig2_gallery_b2bb1b7580.jpg" alt="fig2-gallery.jpg" /></p>
]]></description>
      <pubDate>Tue, 26 Aug 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>NVIDIA Research、「エージェントAIの未来は小規模言語モデル（SLM）」と提言──LLMは“必要時のみ”、ハイブリッド構成を推奨</title>
      <link>https://ledge.ai/articles/nvidia_slm_future_of_agentic_ai</link>
      <description><![CDATA[<p>NVIDIA ResearchのPeter Belcak氏らは2025年6月2日、論文「Small Language Models are the Future of Agentic AI」を<a href="https://research.nvidia.com/labs/lpr/slm-agents/">発表</a>し、現在は大規模言語モデル（LLM）を中心に設計されがちなエージェントAIについて、実運用では小規模言語モデル（SLM）がより適しており、経済的でもあると主張した。論文は、SLMの能力・運用適性・コスト効率を根拠に、用途に応じて複数モデルを組み合わせるヘテロジニアス（混在）構成を推奨している。</p>
<h2>「SLMは十分に強力で、運用に適し、必然的に安価」</h2>
<p>著者らは、エージェントAIの多くが限られた種類のタスクを反復処理するという前提に立ち、こうした場面ではSLMで十分な精度と安定性が得られると指摘。加えて、SLMはレイテンシ・消費電力・インフラ規模の面で有利であり、実サービスへのデプロイやエッジ実行にも向くとした。</p>
<h2>コスト面の差：7B級SLMは70〜175B級LLMより「10〜30倍」効率的</h2>
<p>論文は7B規模のSLMと70〜175B規模のLLMを比較し、レイテンシ、エネルギー、FLOPsの観点で10〜30倍の効率差があり、リアルタイム応答を要するエージェントにおいてSLMが有利だと述べる。</p>
<h2>ハイブリッド構成の推奨：「会話の汎用性」が必要な場面のみLLMを</h2>
<p>一方で、広範な一般会話能力が不可欠な場面については、複数モデルを呼び分けるヘテロジニアス構成（SLMとLLMの併用）が自然な選択だと提案。これにより、日常的な専門タスクはSLMで低コストに処理し、LLMは“必要時のみ”に限定してコスト最適化を図る設計思想を示した。</p>
<h2>LLM→SLMへのエージェント移行を見据えた「一般アルゴリズム」も提示</h2>
<p>論文は、既存のLLM中心エージェントをSLM主軸へ移行するための一般的な変換アルゴリズムを概説。移行の障壁や留意点にも触れ、産業界での段階的な置換を見据えた実務的視点を強調している。</p>
<h2>背景にある推論基盤の進化——NVIDIA「Dynamo」</h2>
<p>主張の背景には、SLMを高スループット・低レイテンシでさばく推論OS/基盤の進歩もある。NVIDIAは2025年3月に「NVIDIA Dynamo」を発表し、分散環境での推論効率を高める最適化（KVキャッシュ制御やディスアグリゲーテッド・サービング等）を公開している。こうした基盤整備が、SLM運用の現実解を後押ししている。</p>
<p>著者らは、論文へのフィードバックを公開で受け付ける特設ページも用意。今後の往復書簡・批判的検討を通じて議論を深める姿勢を示している。</p>
]]></description>
      <pubDate>Tue, 26 Aug 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Tenable、OpenAIの「GPT-5」を公開24時間以内に脱獄──安全対策を突破し危険情報を生成</title>
      <link>https://ledge.ai/articles/tenable_gpt5_jailbreak_security_flaw</link>
      <description><![CDATA[<p>セキュリティ企業のTenableは米国時間8月8日、OpenAIが7日に公開した最新AIモデル「GPT-5」について、公開からわずか24時間以内に「脱獄（jailbreak）」に成功したと公式ブログで<a href="https://www.tenable.com/blog/tenable-jailbreaks-gpt-5-gets-it-to-generate-dangerous-info-despite-openais-new-safety-tech">発表</a>した。Tenableは、OpenAIが導入した新しい安全対策を突破し、危険な情報を生成させることに成功したという。</p>
<h2>GPT-5の新しい安全対策</h2>
<p>OpenAIはGPT-5で、従来の「拒否ベース（refusal-based）」から「安全な応答生成（safe-completions）」方式に<a href="https://openai.com/ja-JP/index/gpt-5-safe-completions/">移行</a>した。危険な質問を拒否するのではなく、安全とみなせる範囲で柔軟な返答を目指す新設計だ。だが、Tenableはこの仕組みをわずか1日で突破した。</p>
<h2>Tenableによる脱獄実験</h2>
<p>Tenable Researchのチームは、公開から24時間以内にjailbreakを実施した。手法は「クレッシェンド（crescendo）」と呼ばれ、歴史研究の学生を装って段階的に質問を重ねる社会工学的なアプローチだった。最初は歴史的な戦術や抗争に関する一般的な質問から始め、徐々に爆発物や武器に関する具体的な知識へと誘導。最終的に4回目のやり取りで、GPT-5は火炎瓶（モロトフ・カクテル）の作り方を詳細に出力したという。Tenableは、この過程が通常のユーザーとの会話に見える形で進んだ点を強調している。</p>
<p><strong>Tenableが公開した「クレッシェンド手法」による脱獄実験の様子。番号は以下のやり取りを示す</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/collage_e0c3eaa9ca/collage_e0c3eaa9ca.jpg" alt="collage.jpg" /></p>
<p>① 初期の拒否応答：GPT-5は危険な要求を拒否し、代わりに歴史的背景の説明を提案。
② 歴史研究の文脈を装う：歴史課題を理由に要約を求め、名称の由来や背景を解説させる。
③ 即席手法に誘導：当時の作り方について質問し、容器や材料の説明を得る。
④ 古典的レシピへの言及：1930〜40年代の伝統的な作り方が提示される。
⑤ 具体的なレシピを要求：フィンランド軍が使った配合を尋ね、詳細情報を引き出す。
⑥ レシピの提示：瓶や液体の種類、混合方法など具体的な材料リストを入手。
⑦ 手順の提示（前半）：瓶の準備から液体の注入・混合までの手順を出力。
⑧ 手順の提示（後半）：布を詰めて導火線とし、点火から使用方法まで危険な情報が生成された。</p>
<h2>安全性への警鐘</h2>
<p>Tenableの副社長Tomer Avni氏は、「GPT-5がどれほど高度な安全対策を備えていても突破可能であることを示した」と述べ、AIを業務に導入する際のリスクと継続的な監視の必要性を強調した。</p>
<h2>業界全体で広がる懸念</h2>
<p>Tenableの報告に加え、他のセキュリティ企業や研究者からもGPT-5の安全性を巡る懸念が相次いでいる。セキュリティ企業SPLXは1,000以上の攻撃シナリオを用いて検証した結果、安全性や信頼性スコアが極めて低いと評価した。また、NeuralTrustも別の手法で脱獄に成功したと報告している。</p>
<h2>今後の展望</h2>
<p>OpenAIは今回の事態について公式な対応を発表していない。AIの安全性確保は、社会や産業での利活用に向けた重要課題として改めて注目されている。</p>
]]></description>
      <pubDate>Tue, 26 Aug 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>GPT-5のIQはどこまで高くなった？──GPT・Claude・Geminiを“メンサ式IQテスト”で比較する『Tracking AI』</title>
      <link>https://ledge.ai/articles/tracking_ai_mensa_iq_test</link>
      <description><![CDATA[<p>米ジャーナリストのMaxim Lott氏は、主要なAIモデルの知能指数（IQ）や政治的傾向を客観的に比較できるウェブサイト「Tracking AI」を2025年8月21日に<a href="https://www.trackingai.org/home">更新</a>した。同サイトでは、独自に作成した非公開のIQテストと、Mensa Norwayがオンラインで公開している図形パズル型IQテストを用いて、ChatGPT（GPT-5 Proなど）、Claude 4 Opus、Gemini 2.5 Pro、Llama、Mistralといった代表的なAIモデルを比較している。</p>
<h2>IQテストによる性能比較</h2>
<p>Tracking AIでは、各モデルのIQスコアを分布図やランキング形式で表示。OpenAIのGPT-5 Pro（Vision）やGoogleのGemini 2.5 Proが上位に位置し、ClaudeやDeepSeekなども含めたスコアの推移を時系列で追うことができる。さらに、各問題ごとの正答率や、AIごとの解答理由まで公開されており、モデルの思考過程を詳細に比較可能だ。</p>
<p><strong>主要AIモデルのIQスコア分布（Tracking AIより）。GPT-5 Pro（Vision）やGemini 2.5 Proが高スコアを記録</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/IQ_Test_Result_7124431b5e/IQ_Test_Result_7124431b5e.jpg" alt="IQ Test Result.jpg" /></p>
<h2>Mensaテストと独自テスト</h2>
<p>使用されているテストは2種類。1つはLott氏自身が作成した「オフライン自作テスト」で、AIの学習データに含まれていないことを強調。もう1つはMensa Norwayが提供するオンラインIQテストで、35問の図形推理問題を25分以内に解く形式。いずれもAIの「推論力」を可視化する指標として活用されている。</p>
<p><strong>Mensa Norwayの公開テストとオフライン自作テストの結果を比較したランキング。テスト方法により順位の違いも見られる</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/rank_by_test_source_71cc2da37b/rank_by_test_source_71cc2da37b.jpg" alt="rank by test source.jpg" /></p>
<h2>政治的・社会的な質問比較</h2>
<p>Tracking AIのもう一つの特徴は、AIに政治的・社会的テーマの質問を投げかけ、回答を比較できる点だ。例えば「経済的グローバル化は人類に奉仕すべきか」という質問に対し、GPT-5は「Strongly Agree」と答え、ClaudeやGeminiも人類の福祉を優先する立場を示した。こうした比較から、各AIのバイアスや思想傾向を把握できる仕組みになっている。</p>
<h2>サンプル問題の公開</h2>
<p>サイトでは「IQ TEST OF THE DAY」として日替わり問題も提供されている。各AIの回答と理由が並べて掲載されており、単なるスコア比較にとどまらず推論の特徴を把握できるのが特徴だ。</p>
<p><strong>Tracking AIで公開されている日替わりIQ問題。各AIモデルの解答と推論過程も併せて公開される</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/iq_test_of_the_day_b238126d2f/iq_test_of_the_day_b238126d2f.jpg" alt="iq test of the day.jpg" /></p>
<h2>FAQと今後の展望</h2>
<p>FAQページでは、「なぜこのサイトを作ったのか」「政治的コンパスは有効か」「資金源はどこか」などの質問に回答。AIの性能や思想傾向を透明化し、利用者が信頼できる判断材料を得られるようにすることが目的とされている。今後は質問データの拡充なども予定されているという。</p>
<p>AIの能力が急速に進化する中で、『Tracking AI』は知能指数と政治的スタンスの両面からモデルを比較できる貴重な情報源となっている。Mensa式IQテストや独自問題を通じてAIを測定する試みは、AIの性能を人間社会に照らして理解するための一助となりそうだ。</p>
]]></description>
      <pubDate>Tue, 26 Aug 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Tesla、日本で「Full Self-Driving（Supervised）」のテスト走行を開始──市街地や高速道路での運転支援性能を検証</title>
      <link>https://ledge.ai/articles/tesla_fsd_supervised_japan_test</link>
      <description><![CDATA[<p>Tesla Japan（テスラ）は2025年8月20日、運転支援システム「Full Self-Driving（Supervised）」の日本でのテスト走行を開始したと<a href="https://prtimes.jp/main/html/rd/p/000000048.000038481.html">発表</a>した。市街地や高速道路を含む公道で、信号や標識の認識、交差点での右左折、歩行者や自転車への対応などの機能を検証する。（<a href="https://x.com/teslajapan/status/1957986432926249405">走行テストの様子</a>）</p>
<p><strong>日本でのテスト走行に使用されるTeslaのModel 3</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/sub4_babbf79619/sub4_babbf79619.jpg" alt="sub4.jpg" /></p>
<p>FSD(Full Self-Driving)は、同社が開発する最新の運転支援システムで、車両には「AI 4 ハードウェア」および「Tesla Vision」が搭載される。これにより、信号や標識の認識、交差点での右左折、さらには歩行者や自転車への対応など、複雑な交通環境での走行を支援する。</p>
<p>名称に「Supervised」とある通り、ドライバーによる常時監視が必須となる。運転者がステアリングを握っていない場合や注意が散漫と判断された場合には、警告が発せられる仕組みだ。完全自動運転ではなく、あくまで人間による管理のもとで機能する点を強調している。</p>
<p><strong>オートパイロットを使用したテスラ車両は、一般的な車両に比べて衝突事故発生までの平均走行距離が大幅に長い</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/sub1_a8ddd97395/sub1_a8ddd97395.jpg" alt="sub1.jpg" /></p>
<p>テスト走行はまず限定的に開始され、今後順次エリアを拡大する見通し。日本市場での展開に向けた重要なステップと位置づけられ、欧州に続いて導入が進む。</p>
<p>テスト走行の裏側では、FSDを支えるAI学習基盤の強化も進んでいる。テキサス州オースティンの最新工場Giga Texasでは、AIトレーニング用コンピュートクラスター「Cortex」を増強。約16,000台のH200 GPUを追加導入し、総計算能力はH100換算で約67,000台分に到達した。</p>
<p><strong>Giga TexasのAI学習用クラスター「Cortex」。約16,000台のH200を追加し、総計算能力はH100換算で約67,000台分）</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/sub3_e87e1adbef/sub3_e87e1adbef.jpg" alt="sub3.jpg" /></p>
<p>Teslaは「FSD（Supervised）」の実証を通じて、日本における次世代運転支援システムの普及と安全性検証を進める方針だ。</p>
]]></description>
      <pubDate>Tue, 26 Aug 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google、Metaから6年で100億ドル超のクラウド契約を受注──Metaは急速なAIインフラ強化へ</title>
      <link>https://ledge.ai/articles/google_meta_cloud_deal_ai_infra</link>
      <description><![CDATA[<p>Googleは、Meta Platformsと6年間で100億ドル（約1兆4800億円）超のクラウドコンピューティング契約を締結したことが、2025年8月21日に関係筋の話として<a href="https://jp.reuters.com/economy/industry/ZUPUKEMNYJKTFCL3P5NFAKWH2M-2025-08-22/">ロイター</a>など複数のメディアが報じた。</p>
<p>契約にはGoogle Cloudのサーバー、ストレージ、ネットワーキングなどのインフラ利用が含まれ、Metaの急速に進むAI開発計画を支えるものとなる。</p>
<p>今回の大型契約は、Metaが進める「スーパーインテリジェンス」構想の一環とみられており、大規模AIモデルの学習や推論に必要な計算資源を確保する狙いがあるという。Metaは2025年の設備投資（Capex）見通しを最大720億ドルに引き上げており、AI関連の投資を加速させている。</p>
<p>一方、Google Cloudは成長の原動力として注目されており、2025年第2四半期の売上は前年同期比32％増を記録。今回の契約は同社にとって最大級の案件のひとつであり、競合するAmazon Web Services（AWS）やMicrosoft Azureに対抗する上で大きな追い風となる。</p>
<p>Metaはまた、資金確保のために一部のデータセンター資産を約20億ドル規模で売却する計画も進めている。両社は今回の契約について公式コメントを控えているが、複数の報道によればすでに合意が成立しており、今後のAIインフラ拡大に直結するものとなる見通しだ。</p>
]]></description>
      <pubDate>Tue, 26 Aug 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>学術＆研究2025/8/24 [SUN]理研、「富岳」後継機の開発にNVIDIA参画──AIとシミュレーション融合の次世代スーパーコンピューター</title>
      <link>https://ledge.ai/articles/fugaku_next_nvidia_ai_supercomputer</link>
      <description><![CDATA[<p>理化学研究所は2025年8月22日、スーパーコンピューター「富岳」の後継機「富岳NEXT」の開発に、米半導体大手NVIDIAが参画すると<a href="https://www.riken.jp/pr/news/2025/20250822_1/index.html">発表</a>した。富士通と理研に加え、初めて国外の半導体企業が中心的役割を担う体制となる。AIとシミュレーションの統合を進め、2030年頃の稼働を目指す。</p>
<p><strong>富岳NEXTによる応用事例イメージ。左は地殻変動解析、右は都市部における地震動解析</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/20250822_1_fig2_84c4231aa9/20250822_1_fig2_84c4231aa9.jpg" alt="20250822_1_fig2.jpg" /></p>
<p>2020年に稼働を開始した「富岳」は、世界トップクラスの性能を誇る日本のフラッグシップスーパーコンピューターであり、気候変動予測や創薬、材料科学など幅広い分野で活用されてきた。その後継機として位置づけられる「富岳NEXT」は、文部科学省の国家プロジェクトとして開発が進められている。</p>
<h2>新体制の発表</h2>
<p>理化学研究所、富士通、NVIDIAの3者による国際的な開発体制が始動した。NVIDIAがGPU基盤の設計を主導し、富士通は新CPU「FUJITSU-MONAKA-X」を開発。両者は超高帯域で接続され、理研が全体統括を担う。国外の半導体大手が日本の旗艦スパコン開発に参画するのは初めてとなる。</p>
<h2>性能目標と特徴</h2>
<p>「富岳NEXT」はFP8精度で600エクサFLOPSを超える計算性能を見込み、現行「富岳」と比べ最大100倍のアプリケーション性能向上を目指す。AIと数値シミュレーションを統合する「AI-HPCプラットフォーム」として設計され、医薬品開発、気候変動解析、新素材設計などの研究を加速させる狙いがある。補足報道によれば、ノードあたり4基のGPUを搭載し、合計13,600基規模で構成される案も浮上している。</p>
<h2>導入とスケジュール</h2>
<p>2025年度中に基本設計を完了し、2026年度には詳細設計に移行。2027年頃には新CPU「MONAKA-X」が登場し、2030年頃の稼働開始を想定している。設置場所は現行の「富岳」と同じく、神戸市ポートアイランド内に整備される予定だという。</p>
<h2>国際的意義</h2>
<p>今回の発表は、日本のスーパーコンピューターがAIとシミュレーションの融合を本格化させる転換点といえる。ロイターや共同通信も報じているように、米中欧が進める次世代スパコン競争のなか、日本は「Zetta-scale」への挑戦を打ち出し、国際競争力を維持・強化する姿勢を明確にした。</p>
<p><strong>自動車分野での活用例。生成AIを用いた設計最適化により、自動車設計の期間短縮や性能向上が期待される</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/20250822_1_fig3_7f1a064923/20250822_1_fig3_7f1a064923.jpg" alt="20250822_1_fig3.jpg" /></p>
]]></description>
      <pubDate>Sun, 24 Aug 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/8/19 [TUE]AI業界を牽引するトップランナーが語る！—今押さえるべきAIの全体像と最前線を3日間で掴むLedge.ai Webinar SP開催</title>
      <link>https://ledge.ai/articles/ledgeai-webinarsp-sponsor</link>
      <description><![CDATA[<p>国内最大級のAI特化メディア『Ledge.ai』を運営する株式会社レッジ（東京都品川区）は、2025年9月24日(水)〜26日(金)の3日間連続で合計20本以上のセミナーを配信するオンラインイベント「Ledge.ai Webinar SP」を開催いたします。</p>
<p>本イベントでは、AIの各領域の専門家を招き、今必要とされるAIの体系的な知識や活用に関する見識をシェアする講義を実施。「AIをしる、つかう、つくる」をテーマに、多様な課題解決のヒントとなるようなコンテンツを動画でお届けします。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/_4aed8b100c/_4aed8b100c.png" alt="ウェビナーの様子.png" /></p>
<p>:::button
<a href="https://us02web.zoom.us/webinar/register/WN_m28c0ZHMSRiXQ-yrlcLXiw#/registration">▶【登録無料】視聴者向け事前登録はこちら</a>
:::</p>
<h2>AI業界を牽引するトップランナーが今押さえるべきAIの知識と最前線を3日間で語る</h2>
<p>AIの急激な進化と急速な広まりにより、AIへのリテラシーの差が広がっています。AI活用の最前線では「どう使えば効果的か」「どう作れば自社の強みになるか」といった問いに対しての取り組みが行われ、新たな事例や知見が生まれています。そんな現在において、AIの全体像を体系的に理解した上で、ビジネスにどのように活用されているかすばやく捉えることは重要です。</p>
<p>当イベントはAIの基礎理解 → 業務活用 → 開発実践までを体系的に理解し、この時代で働くビジネスマンの方に使える学びをお届けします。</p>
<p>Ledge.ai Webinar SPは、以下の3つの軸で構成されています。</p>
<h2>プログラム ~「生成AIだけじゃない！「AIをしる、つかう、つくる」SP~</h2>
<h3>Day1：AIを「しる」——全体像と本質を理解する</h3>
<p>AIの領域では日々革新的な技術が生まれ、その掛け合わせによりAIの担える範囲が急速に広がっています。AIの基礎からAI全般の進化を体系的に学ぶことで適切なAI活用に繋げることができます。</p>
<p>【対象】
・AIの基本から体系的に理解したい方
・生成AIに加え、AI全般の進化や仕組みに関心がある方</p>
<p>【セミナー内容】
・AIの基礎とこれまでの進化（機械学習、ディープラーニング含む）
・⽣成AIの仕組みと活⽤シーンの全体像
・ビジネスで求められるAIリテラシーと注意点</p>
<p>【ゲスト講演】
「ソフトバンクの事例から紐解く、組織の生成AI活用・推進を自走するための仕組みづくり」</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/_c71b18a520/_c71b18a520.jpg" alt="藤原 竜也.jpg" /></p>
<p>ソフトバンク株式会社
IT統括 AI&amp;データ事業統括部　Axross事業部 部長
藤原 竜也 氏</p>
<h3>Day2：AIを「つかう」——現場に効く、実践的なAI活用法</h3>
<p>「現場でどう使うのが効果的か？」を知りたい方に向けたプログラムです。現場導入の工夫やハマりがちな落とし穴まで、具体的なノウハウが得られます。</p>
<p>【対象】
・AIツールを現場の業務で活用したい方
・実務にすぐ役立つノウハウを知りたい方</p>
<p>【セミナー内容】
・業務シナリオ別のAIツール活用（生成AI・ルールベースAI）
・Excelや議事録、FAQ対応など、日常業務での実用ワーク
・プロンプトの書き⽅から社内導⼊のコツまで徹底解説</p>
<p>【ゲスト講演】
「まずは試してみよう！ 最新動向から学ぶ、生成AI活用の第一歩」</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/_c4e70ae1da/_c4e70ae1da.jpg" alt="岡田隆太朗.jpg" /></p>
<p>一般社団法人日本ディープラーニング協会　
専務理事　
岡田 隆太朗 氏</p>
<h3>Day3：AIを「つくる」——AIプロダクト・自社専用AIツールの開発</h3>
<p>ノーコード/ローコードでのAI組み込みから、AI活用を前提としたインフラを含む環境構築、AIモデル開発など、AIの開発に必要な技術知識やノウハウを幅広く学ぶことができます。</p>
<p>【対象】
・ノーコード・ローコードでAIを組み込みたい方
・AIシステムの裏側やインフラにも関心がある方</p>
<p>【セミナー内容】
・生成AIアプリの基礎（RAG、Dify、API連携など）
・従来型AI（需要予測、分類モデルなど）の開発プロセス入門
・クラウド・ベクターデータベースなど、AI基盤技術の理解</p>
<p>【ゲスト講演】
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Zhan_Cliff_Chen_5c6864c871/Zhan_Cliff_Chen_5c6864c871.jpeg" alt="Zhan (Cliff) Chen.jpeg" />
マイクロソフト ディベロップメント株式会社
プリンシパル　アプライド　サイエンティスト
Zhan (Cliff) Chen / 陳 湛</p>
<p>:::button
<a href="https://us02web.zoom.us/webinar/register/WN_m28c0ZHMSRiXQ-yrlcLXiw#/registration">▶【登録無料】視聴者向け事前登録はこちら</a>
:::</p>
<h2>こんな方におすすめ</h2>
<ul>
<li>企業のDX・AI導入担当者</li>
<li>生産性向上のためAIを活用したい事業部門マネージャー</li>
<li>ノーコードでのAI活用を始めたい開発初心者</li>
<li>最新AI技術のトレンドを押さえたいビジネスパーソン</li>
</ul>
<h2>イベント概要</h2>
<p>開催予定日時｜2025年9月24日(水)〜26日(金)
開催形式｜オンラインセミナー (Zoom Webinar)
想定集客規模｜500名
対象｜経営層 / システム企画 / DX推進 / 経営企画 / マーケティング / エンジニア
主催｜株式会社レッジ</p>
<p>:::button
<a href="https://us02web.zoom.us/webinar/register/WN_m28c0ZHMSRiXQ-yrlcLXiw#/registration">▶【登録無料】視聴者向け事前登録はこちら</a>
:::</p>
<h2>「Ledge.ai Webinar SP」を盛り上げていただけるスポンサー企業様を募集中</h2>
<p>現在、この企画の開催趣旨にご賛同いただき、共に「Ledge.ai Webinar SP」を盛り上げていただけるスポンサー企業様も募集しております。</p>
<p>スポンサーとなっていただいた企業様には、AI業界のトップランナーの方々と共に当イベントの講師としてウェビナーにご登壇いただき、最新の取り組みやノウハウを発信していただきます。</p>
<p>また、その他にも、スポンサー企業様にも下記のようなメリットをご案内させていただきます。</p>
<h3>スポンサー参加の主なメリット</h3>
<ul>
<li>AI関連の情報感度の⾼い読者との接点が持てる</li>
<li>貴社の優位性をLedge.αiが引き出しながらPRできる</li>
<li>通常のLedge.ai広告メニューよりお得な価格で利⽤できる</li>
</ul>
<p>当イベントのスポンサーにご興味がございましたらぜひイベント資料をご覧ください。</p>
<p>:::button
<a href="https://forms.zohopublic.com/ledgeai/form/Ledgeai3/formperma/tJ1kpSYYWvDVF2Kp3xE-sBTiKeMh-7DlQZDoqXnSjtA">▶︎スポンサー様向けの資料はこちら</a>
:::</p>
<h2>お問い合わせ</h2>
<p>詳細相談・お見積もりは以下メールアドレスにお問合せください。
ld_media_sales@ledge.co.jp
（担当：Ledge.ai Webinar SP 事務局）</p>
]]></description>
      <pubDate>Tue, 19 Aug 2025 02:50:00 GMT</pubDate>
    </item>
  </channel>
</rss>