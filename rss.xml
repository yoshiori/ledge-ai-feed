<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
  <channel>
    <title>Ledge.ai 新着記事</title>
    <link>https://ledge.ai/</link>
    <description>Ledge.ai の最新テクノロジー記事</description>
    <item>
      <title>ビジネス2025/12/12 [FRI]OpenAI、「エンタープライズAIの現状 2025」公開──ChatGPT Enterpriseで1日40〜60分短縮、導入企業は100万社超に</title>
      <link>https://ledge.ai/articles/enterprise_ai_2025_openai_report</link>
      <description><![CDATA[<p>OpenAIは2025年12月8日、企業におけるAI活用の最新動向をまとめたレポート「The state of enterprise AI 2025」を<a href="https://openai.com/index/the-state-of-enterprise-ai-2025-report/">発表</a>した。調査では、ChatGPT Enterpriseの利用が業務の中心に組み込まれつつあり、従業員が1日あたり平均40〜60分の作業時間を節約していることが確認された。</p>
<p>同レポートは企業向け利用データと、約100社・9,000人を対象にした調査を基に作成されており、AI導入が「実験段階」から「本格運用」へと移行している現状を示す内容となっている。（詳細なレポートは<a href="https://cdn.openai.com/pdf/7ef17d82-96bf-4dd1-9df2-228f7f377a29/the-state-of-enterprise-ai_2025-report.pdf">こちら</a>のPDFから閲覧可能）</p>
<p>レポートによれば、OpenAIはすでに100万以上のビジネス顧客を抱え、ChatGPT Enterpriseの席数は700万を超えた。メッセージ数やカスタムGPTの利用も急増しており、特に独自ワークフローの自動化やナレッジ検索など、企業固有の業務に深くAIが統合されているという。</p>
<p>さらに、AI活用の“深さ”と生産性の向上には明確な相関があることも示された。AIを集中的に利用する従業員ほど、週あたりの時間節約量が大きい傾向が見られた。</p>
<p><strong>■ AIの利用量が多いほど、週あたりの節約時間が増加する：</strong> 週10時間以上を節約しているグループは、ほとんど節約していない層の約8倍のクレジットを利用
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Productivity_gains_increase_with_intensity_of_AI_use_22bd78f29d/Productivity_gains_increase_with_intensity_of_AI_use_22bd78f29d.jpg" alt="Productivity gains increase with intensity of AI use.jpg" /></p>
<p>業種別に見ると、AI導入の速度と規模にも大きな差が浮き彫りになった。特にテクノロジー業界が突出しており、利用規模も成長率も最大だった。一方で、ヘルスケアや製造、建設など、従来デジタル化が進みにくいとされていた業界でも急速に導入が進んでいる。</p>
<p><strong>■ 業種別：AI利用規模と前年比成長率</strong> テクノロジーが規模・成長率とも最大。ヘルスケア・製造が急伸し、金融・プロフェッショナルサービスは高い安定利用を示す
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/AI_adoption_by_industry_b7cdebd753/AI_adoption_by_industry_b7cdebd753.jpg" alt="AI adoption by industry.jpg" /></p>
<p>企業間でも、AI活用の深度には明確な“格差”が存在する。特に「フロンティアワーカー」と呼ばれる AI 活用上位5％の従業員は、AIへの依存度が全く異なる。フロンティア層は高度な分析や推論タスクを日常的にこなし、中位層との間に大きな使用量の差が生じていることが確認された。</p>
<p><strong>■ フロンティアワーカーと中央値ユーザーの利用格差</strong> メッセージ量は6倍、データ分析関連のメッセージは16倍と、格差は高度なタスクほど拡大
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Worker_usage_gaps_widen_with_more_advanced_tools_95836d5bc8/Worker_usage_gaps_widen_with_more_advanced_tools_95836d5bc8.jpg" alt="Worker usage gaps widen with more advanced tools.jpg" /></p>
<p>OpenAIは総括として、AIを幅広い業務で深く活用できる企業ほど、時間短縮だけでなく新しい業務遂行能力を獲得していると指摘する。AIが単なる生産性向上ツールから、企業の競争力を左右する「中心インフラ」へと位置づけが変わりつつあることが、調査により浮き彫りとなった。</p>
<p>単なる効率化ツールから、企業基盤となるインフラへ──AI導入のフェーズは次の段階へ移り始めている。</p>
]]></description>
      <pubDate>Fri, 12 Dec 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ゆうちょ銀行、ATM前の「通話」をAIで検知し取引中止も──特殊詐欺被害防止で2026年1月から新措置</title>
      <link>https://ledge.ai/articles/jpbank_atm_ai_call_detection_fraud_prevention_2026</link>
      <description><![CDATA[<p>ゆうちょ銀行は2025年12月8日、ATM利用時の特殊詐欺被害を防止する新たな取り組みとして、ATM前での携帯電話などによる通話動作をAIで検知し、必要に応じて取引を中止する措置を導入すると<a href="https://www.jp-bank.japanpost.jp/news/2025/news_id002346.html">発表</a>した。新措置は2026年1月から、全国のゆうちょATMで順次適用される。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/_309f19b48d/_309f19b48d.jpg" alt="ゆうちょおしらせ.jpg" /></p>
<p>同社はこれまでも、ATMコーナーに設置された防犯カメラ画像をAIで分析し、通話している可能性のある動作を検知した場合、デジタルサイネージによる警告表示や警告音で注意喚起を行ってきた。今回の取り組みはこの対策を一段強化したもので、通話動作を検知した際に「取引そのものを中止する場合がある」という点が大きな変更点となる。</p>
<p>リリースによると、AIが検知するのは「携帯電話等による通話のような動作」。詐欺の指示役が電話越しに被害者へ振り込み操作を促す手口が全国的に増加していることから、ATM操作中の通話はリスク要因として位置付けられている。ゆうちょ銀行は「お客さまの大切な財産を守るための取り組み」であり、利用者に理解と協力を求めている。</p>
<p>特殊詐欺は国内で依然として深刻な状況が続く。警察庁のまとめでは、2024年の特殊詐欺被害額は約718億円にのぼり、前年から増加した。特に高齢者の預貯金を狙う手口が多く、金融機関による対策強化が求められている。</p>
<p>ゆうちょ銀行は、AI画像分析による不審動作の検知やATM操作時の注意喚起など、既存の取り組みを継続しながら、今回の取引中止措置を組み合わせることで、さらなる被害防止を図るとしている。今後も特殊詐欺対策を継続的に強化していく方針だ。</p>
]]></description>
      <pubDate>Fri, 12 Dec 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>トランプ大統領、NVIDIA「H200」の対中輸出を条件付き解禁──25％を米国に支払い、承認顧客に限定、最新鋭Blackwellは対象外</title>
      <link>https://ledge.ai/articles/trump_h200_conditional_export_approval_blackwell_excluded</link>
      <description><![CDATA[<p>トランプ米大統領は2025年12月9日（現地時間）、自身のSNS「Truth Social」への投稿で、NVIDIAのAI半導体「H200」について、中国を含む「承認された顧客（approved customers）」への輸出を条件付きで認める方針を<a href="https://truthsocial.com/@realDonaldTrump/posts/115686072737425841">明らかにした</a>。中国向けを含む先端AIチップの輸出を厳格に制限してきたバイデン政権の規制を一部転換する内容となる。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/donald_trump_truth_74a9d1f98d/donald_trump_truth_74a9d1f98d.jpg" alt="donald trump truth.jpg" /></p>
<p>投稿によると、H200の輸出解禁にはいくつかの条件が設けられる。最も特徴的なのが、取引額の「25％を米国に支払う」という条項だ。トランプ氏は、この仕組みが「米国の納税者に利益をもたらす」とし、米国の製造業や雇用にプラスになると主張している。ホワイトハウス高官によると、この25％はチップを米国に一度輸入する際に課される税として扱い、その後、安全保障審査を経て中国への再輸出を許可するスキームが検討されている。</p>
<p>対象となるのは、あくまで米政府が審査した「承認顧客」に限定される。投稿では「国家安全保障を維持する条件下でのみ認める」とも明記されており、商務省（Department of Commerce）が最終的な運用の詳細を詰めている段階だ。</p>
<p>一方で、NVIDIAが現在米国内で展開している最新世代のGPU「Blackwell」シリーズや、次世代に位置づけられる「Rubin」については、今回のディールの対象外とした。トランプ氏はこれらを「高度で先進的な製品」と位置づけ、米国のAI競争力を維持する観点から、中国向けには提供しない方針を示した。</p>
<p>今回の政策転換はNVIDIAだけにとどまらず、AMD、Intelなど他の米国半導体企業にも同様の枠組みを適用する方針だという。H200はH100の後継となるHopper世代のAIチップで、大容量メモリを備え、生成AIや大規模言語モデル（LLM）向けの高い性能を持つ。一方で、米国内ではさらに性能の高いBlackwell世代への移行が進んでおり、米政府は「旧世代チップを輸出し、最先端は国内に温存する」という線引きを明確にしている。</p>
<p>この方針は、米メディア Semafor が前日に「商務省がH200の中国向け輸出を条件付きで認める案を検討している」と<a href="https://www.semafor.com/article/12/09/2025/trump-says-nvidia-can-sell-h200-ai-chips-to-china">独自報道</a>していた。先端チップの供給制限が続いていた中国側が、実際にどの程度H200を受け入れるかは依然不透明であり、25％の支払い方式や承認制の運用が市場に与える影響も注目される。</p>
]]></description>
      <pubDate>Thu, 11 Dec 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Amazon、次世代LLM「Amazon Nova 2」発表　「思考」するProやマルチモーダルなど4モデル、独自フロンティアモデル構築「Nova Forge」Bedrockで提供開始</title>
      <link>https://ledge.ai/articles/amazon_nova_2_launch_four_models</link>
      <description><![CDATA[<p>Amazon Web Services（AWS）は2025年12月3日（現地時間）、米ラスベガスで開催中の年次開発者会議「AWS re:Invent 2025」において、次世代の大規模言語モデル（LLM）ファミリー「Amazon Nova 2」を<a href="https://www.aboutamazon.com/news/aws/aws-agentic-ai-amazon-bedrock-nova-models">発表</a>した。企業が自社専用のフロンティアモデルを構築できる新サービス「Nova Forge」や、ブラウザ操作を自動化する「Nova Act」もあわせて公開された。</p>
<p>同社が展開する生成AIサービス「Amazon Bedrock」を通じて、即日提供が開始されている。</p>
<h2>推論能力とコスト効率を大幅に強化</h2>
<p>「Amazon Nova 2」は、前世代モデルと比較して推論能力、処理速度、コストパフォーマンスのすべてにおいて大幅な進化を遂げている。特に、複雑なタスクを自律的に遂行する「AIエージェント」としての利用を想定し、文脈理解能力や論理的思考能力が強化された点が特徴だ。</p>
<p>今回発表されたラインナップは、用途に合わせて最適化された以下の4つのモデルで構成されている。</p>
<ul>
<li><strong>Amazon Nova 2 Pro</strong>  高度な推論（Reasoning）能力に特化した主力モデル。数学、コーディング、複雑な論理的推論において高い性能を発揮する。「思考プロセス」を強化しており、難解な指示に対しても正確な回答を生成可能だ。</li>
<li><strong>Amazon Nova 2 Lite</strong>  応答速度とコスト効率を最優先した軽量モデル。リアルタイム性が求められるチャットボットや、大量の文書要約・データ処理タスクに適しており、軽快な動作が特徴である。</li>
<li><strong>Amazon Nova 2 Omni</strong>  テキスト、画像、音声、ビデオをネイティブに理解・生成できるマルチモーダルモデル。従来のモデルのようにデータをテキストに変換する工程を経ず、複数の異なる入力情報を同時に、かつシームレスに処理することができる。</li>
<li><strong>Amazon Nova 2 Sonic</strong>  音声対話に特化したモデル。極めて低い遅延（ローレイテンシー）での音声入力・出力が可能で、人間と話しているかのような自然なリアルタイム対話を実現する。</li>
</ul>
<h2>ベンチマークでは「同等またはそれ以上」と説明</h2>
<p>Amazon は、Nova 2 Pro とNova 2 Lite について、Claude、GPT、Gemini 系列の競合モデルと公開ベンチマークで比較し、多くの指標で「equal or better（同等またはそれ以上）」と説明した。対象となったのは、マルチドキュメント分析、動画推論、複雑な指示のフォロー、高度な数学的推論、ソフトウェアエージェントタスクなど。また、Pro はより小型モデルへの知識蒸留にも利用できるとしている。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/nova2_pro_lite_bench_3863cb81b0/nova2_pro_lite_bench_3863cb81b0.jpg" alt="nova2 pro lite bench.jpg" /></p>
<h2>企業が自社専用モデルを構築できる「Nova Forge」</h2>
<p>同日発表された Nova Forge は、企業が自社データを使って Nova ベースの独自フロンティアモデル「Novellas」を構築できる「open training」アプローチを採用したサービス。事前学習・中間学習・事後学習の各段階における Nova のチェックポイントを使い、企業固有のドメイン知識を深く反映したモデルを構築できるとしている。</p>
<p>Reinforcement Learning（RL）による強化学習環境、責任ある AI のためのツールキット、小型高速モデル生成のための蒸留機能なども提供される。Booking.com、Reddit、Sony などが活用を進めている事例として紹介された。</p>
<h2>ブラウザ操作を自動化する「Nova Act」</h2>
<p>もう一つの新サービス Nova Act は、ブラウザ UI 上の操作をエージェントが代行するためのサービスだ。数百のシミュレーション環境で強化学習を行い、CRM 更新や E2E テスト、保険請求フォームの送信などの UI ワークフローを自動化する。早期導入企業では約90％の成功率を達成したという。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/1_c39392cc36/1_c39392cc36.gif" alt="ダウンロード (1).gif" /></p>
<h2>Bedrock で提供、開発者向けポータルも公開</h2>
<p>Nova 2 の各モデルは Amazon Bedrock で利用可能となり、API 経由でアプリケーションへの統合が行える。Nova Forge で構築した企業専用モデル「Novellas」も同様に Bedrock 上で運用できる。</p>
]]></description>
      <pubDate>Thu, 11 Dec 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Helical Fusionとアオキスーパー、日本初のフュージョンエネルギー電力売買契約を締結──ヘリカル型核融合炉「Helix Program」で究極の脱炭素社会を目指す</title>
      <link>https://ledge.ai/articles/helical_fusion_aoki_fusion_power_purchase_agreement</link>
      <description><![CDATA[<p>Helical Fusion（東京都中央区）と、愛知県を中心に食品スーパーを展開するアオキスーパーは2025年12月8日、フュージョン（核融合）エネルギーによる電力の売買契約を締結したと<a href="https://prtimes.jp/main/html/rd/p/000000050.000089262.html">発表</a>した。両社によると、フュージョンエネルギーによる電力売買契約としては、国内初（Helical Fusion調べ）だという。</p>
<p>Helical Fusionが推進するヘリカル型核融合炉の開発計画「Helix Program」に基づき、将来建設される核融合発電所からの電力供給を前提とする取り組みで、次世代クリーンエネルギーの社会実装を見据えた重要な一歩と位置づけられている。</p>
<p>アオキスーパーは、冷蔵・冷凍設備など多くの電力を必要とする業態特性からエネルギー消費量の削減が課題となっていたほか、温暖化による農産物の栽培適地の変化や漁業資源への影響など、気候変動が事業に与えるリスクを問題視していた。同社は2025年7月、持続可能な社会への貢献を目的にHelical Fusionへ出資しており、今回の契約はその関係を踏まえた「需要家としての参画」となる。</p>
<p>Helical Fusionは、大学共同利用機関法人 自然科学研究機構 核融合科学研究所の研究成果を基盤に、2021年に設立された日本発の核融合スタートアップ。日本独自の「ヘリカル方式」に基づく核融合炉は、約70年にわたる研究開発の結果、商用炉として必要とされる定常運転・正味発電・メンテナンス性の確保に適した方式として位置づけられている。</p>
<p>同社が推進する「Helix Program」では、2020年代中に高温超伝導マグネットやブランケット兼ダイバータといった要素技術の個別実証を完了し、2030年代中に最終実証装置「Helix HARUKA」で統合実証を行い、その後、発電初号機「Helix KANATA」による世界初の実用発電の達成を目指すとしている。</p>
<p><strong>Helical Fusionが2030年代に「実用発電」を計画する発電初号機「Helix KANATA」のイメージ</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/89262_50_4ecca09204adfd851cfb9db7db0e217d_2940x1650_5c7ddf1551/89262_50_4ecca09204adfd851cfb9db7db0e217d_2940x1650_5c7ddf1551.webp" alt="89262-50-4ecca09204adfd851cfb9db7db0e217d-2940x1650.webp" /></p>
]]></description>
      <pubDate>Thu, 11 Dec 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>NEC、警察・消防の指令室を支援する次世代AI技術を開発──独自のAgentic AIで状況把握から行動提案まで自動化</title>
      <link>https://ledge.ai/articles/necs_agentic_ai_emergency_command_support</link>
      <description><![CDATA[<p>NECは2025年12月2日、警察・消防などの緊急通報を受ける指令室業務を高度化する次世代支援技術を開発したと<a href="https://jpn.nec.com/press/202512/20251202_03.html">発表</a>した。独自のAgentic AIを核とし、通話内容の理解、状況分析、行動提案までを一連で支援することで、オペレーターの負担軽減と業務精度の向上を図る。同技術はマルチ言語に対応し、2026年度中の国内外での活用を見込む。</p>
<p>発表によると、指令室での緊急通報対応は高い専門性が求められ、通報者の説明が曖昧になりやすいなど状況把握が難しいケースも多い。NECはこうした課題に対し、リアルタイム解析で重要情報を抽出し、次にとるべき行動を提案する仕組みを導入することで、迅速かつ正確な判断を支援する。</p>
<p>新技術では、複雑な会話でも誤情報の混入や途中の状況変化を踏まえて正確に把握できる点が特徴だという。NEC独自のAgentic AIが「会話分析」「情報管理」「行動提案」など複数の専門AIを連携させ、多角的に文脈を解釈。熟練オペレーターに近い安定した精度での応対を実現する。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/0203_03_d844de8bb6/0203_03_d844de8bb6.jpg" alt="0203-03.jpg" /></p>
<p>また、説明可能なAI（Explainable AI）を搭載し、AIが提示した判断根拠を確認できる点も特徴とされる。人命に関わる現場での信頼性を確保し、オペレーターが適切な判断を行うための支援を行う。さらに、EUのAI規制法を含む国際基準にも適応できるアーキテクチャを採用し、英語を含むマルチ言語に対応するなど、グローバル展開も可能としている。</p>
<p>高い汎用性も強みで、Agentic AIを役割ごとに分割した構造により、業務要件に応じたチューニングを低コストかつ短期間で実施可能。警察・消防の指令室業務にとどまらず、企業のコンタクトセンターなど幅広い業務への適用を想定している。</p>
]]></description>
      <pubDate>Wed, 10 Dec 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>【AI歴史年表】AIはダートマス会議から数えて来年で70周年！起源から生成AI革命までのAI全史を振り返る──Ledge.ai年末年始特集「&apos;25 to &apos;26」から注目コンテンツを特別公開！</title>
      <link>https://ledge.ai/articles/70year_history_of_ai_from_the_dartmouth_conference</link>
      <description><![CDATA[<p>:::button
<a href="https://25to26.ledge.ai/">Ledge.ai年末年始特集サイトはこちら</a>
:::</p>
<p>2026年は、人工知能（AI）研究が正式に始動した歴史的な瞬間、1956年のダートマス会議から70周年という記念すべき節目を迎える。この70年間、AIは期待と幻滅の波を乗り越え、ついに人類の創造性を拡張する「生成AI」の時代へと到達した。この壮大な進化の軌跡を、各時代のエポックメイキングな出来事とともに紹介する。</p>
<h2>1. AIの誕生、最初の挫折と基礎構築 (1956–1979)</h2>
<p>人工知能（AI）は、1956年のダートマス会議でJ.マッカーシーらによって正式に分野として確立された。このダートマス会議で若手の中心となったJ.マッカーシー、M.ミンスキー、A.ニューウェルの3人はいずれも1927年生まれ、30歳を少し過ぎたところだ。ダートマス会議後、この3人はそれぞれスタンフォード大学、MIT、カーネギーメロン大学で活動し、AI研究の世界的な拠点が形成されていく。</p>
<p>初期の成功として、1958年にはF.ローゼンブラットが脳を模倣した初の学習可能モデルであるパーセプトロンを発表し、1966年にはJ.ワイゼンバウムが初の対話型システムであるELIZAを開発した。また、NNの学習においては、1967年に甘利俊一が確率的勾配降下法という後のディープラーニングの基礎となる最適化手法を発表するなど、技術的な基盤も築かれ始めていた。</p>
<p>しかし、この楽観的なブームは短期間で終焉を迎える。1969年、M.ミンスキーらがパーセプトロンの限界証明を行い、単層NNでは複雑な問題が解けないことを示唆した結果、AI研究への資金が大幅に削減され、最初の「冬の時代」が到来した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/01_70_AI_2ddf249970/01_70_AI_2ddf249970.jpg" alt="表01_70AI.jpg" /></p>
<p>この停滞期においても、後のAIの土台となる研究は継続された。日本では、1972年に甘利俊一が脳の記憶を模倣した連想記憶モデルを発表し、1979年には福島邦彦が、後の畳み込みニューラルネットワーク（CNN）の原型となるネオコグニトロンという階層型のNNモデルを開発した。この時期の日本の研究者の貢献は、AIの次の飛躍に向けた重要な種を蒔いたと言える。</p>
<p>:::button
<a href="https://25to26.ledge.ai/">Ledge.ai年末年始特集サイトはこちら</a>
:::</p>
<h2>2. AI研究の転換期における三つの潮流 (1980–1996)</h2>
<p>1980年代から1990年代前半にかけて、AI研究は、1970年代の停滞期を脱するため、異なる哲学に基づいた三つの潮流が並立した。</p>
<p>まず、記号主義（知識ベース）AIの流れを極限まで推し進めようとする試みがあった。その代表が1984年にD.レナートによって開始されたCycプロジェクトである。このプロジェクトは、人間が持つ膨大な常識をすべて手作業で知識ベースに構築し、究極のエキスパートシステムを実現することを目指した。これは、記号主義AIの可能性を探る壮大な挑戦であったが、同時に知識を形式化し獲得することの難しさを浮き彫りにした。</p>
<p>次に、長らく停滞していたニューラルネットワーク（NN）研究が息を吹き返した。この復活は、1982年にJ.ホップフィールドが、甘利俊一の先行研究と同系統の連想記憶モデルを、統計物理学の手法を用いて再発見したことに端を発する。これにより、NNが「記憶」のメカニズムを持つことが示唆された。決定的なブレイクスルーとなったのは、1986年にG.ヒントンらが多層NNを効率的に学習させる誤差逆伝播法（Backpropagation）を普及させたことである。この手法の登場は、NNが単層の限界を乗り越え、複雑なパターン認識を扱えるようになる第2次NNブームを牽引した。</p>
<p>そして第三の潮流として、従来の複雑な推論中心のAIに異を唱える行動ベースAIが登場した。1991年、iRobotの創業者でもあるR.ブルックスは、包摂アーキテクチャという新しい考え方を提唱し、その具体例として小型六本足ロボットのGenghisを開発した。これは、中央の知識ベースを持たず、環境からのセンサー情報に基づいて直接行動することで、現実世界でのタスク実行を重視するアプローチである。この研究は、AI研究の主流を、抽象的な推論から知覚と行動の統合へとシフトさせるきっかけとなり、その後のロボット工学に大きな影響を与えた。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/02_70_AI_204dc3e658/02_70_AI_204dc3e658.jpg" alt="表02_70AI.jpg" /></p>
<p>このように、1980年代から90年代前半は、知識ベースの限界と挑戦、NNの劇的な復活、そして現実世界指向の新しいパラダイムの誕生という、複数の試行錯誤を通じて、後のAI発展の基礎が築かれた重要な転換期であったと言える。</p>
<p>:::button
<a href="https://25to26.ledge.ai/">Ledge.ai年末年始特集サイトはこちら</a>
:::</p>
<h2>3. データと計算力によるAIの夜明け～ディープラーニングの衝撃 (1997–2016)</h2>
<p>1990年代後半から2010年代にかけてのAI研究は、インターネットの普及によるデータの爆発的な増加と、計算機能力の飛躍的な向上という二つの外部要因に強く支えられた。この時期、AIは「知識ベース」から「データ駆動」へとパラダイムを完全に転換し、特定のタスクで人間の能力を超える成果を上げ始めた。</p>
<p>まず、AIは特定の知的ゲームにおいて、人間を凌駕する能力を示した。1997年には、IBMのDeep Blueがチェスの世界チャンピオン、ガルリ・カスパロフに勝利し、「探索と計算」に特化したAIの能力を世界に示した。この頃、インターネットの本格的な普及は、AI研究の間接的な基盤を構築していた。Web上の膨大な情報（ビッグデータ）を分析するデータマイニングや統計的手法が発展し、AI研究も経験的なデータから知識を抽出する方向に傾倒していった。</p>
<p>AIに真のブレイクスルーをもたらしたのは、ニューラルネットワーク（NN）の進化であった。2006年、G.ヒントンらがディープラーニングを提唱し、多層NNを効率的に学習させる手法（深層化）に成功した。これは、増大するインターネット上のビッグデータを扱うために、極めて重要な進歩であった。この技術の有効性は、様々な分野で証明され始めた。2011年には、IBMのWatsonが、膨大な非構造化データ（書籍、記事など）から答えを導き出す能力により、米国の人気クイズ番組『ジェパディ！』で歴代チャンピオンに勝利した。そして2012年、ヒントンらが開発したAlexNetが、大規模な画像認識コンテスト（ILSVRC 2012）で圧倒的な性能を見せつけ、ディープラーニングが画像認識の主流技術となることを決定づけた。</p>
<p>この時期のAI研究の集大成となったのが、Google DeepMindによるAlphaGoの成功である。2016年、AlphaGoは、人間の直感と深い洞察力が求められる囲碁において、世界トップ棋士であるイ・セドル九段に勝利した。この勝利は、チェスのような「探索」だけでなく、「直感的な判断」が必要とされる領域でもAIが人間を超越したことを意味し、ディープラーニングと強化学習を組み合わせたAIが、人類の「知性の最後の砦」の一つを突破した歴史的な瞬間であった。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/03_70_AI_00677c2be5/03_70_AI_00677c2be5.jpg" alt="表03_70AI.jpg" /></p>
<p>この1997年から2016年にかけて、AIはインターネットによって供給されるデータと、高性能なGPUによって可能になった計算力を武器に、ディープラーニングという核技術を獲得し、次の「生成AI」時代への道筋を明確に作ったのである。</p>
<p>:::button
<a href="https://25to26.ledge.ai/">Ledge.ai年末年始特集サイトはこちら</a>
:::</p>
<h2>4. 生成AI革命の勃発 (2017–2022)</h2>
<p>2017年から2022年の期間は、AI研究史上最も劇的な変革期であり、技術的なブレイクスルーと、それによる生成AI革命の勃発が特徴である。</p>
<p>この変革の起点は、2017年にGoogleが発表したTransformerモデルにあった。このモデルは、入力データ内の重要度を把握する「注意機構（Attention）」を採用し、計算を並列処理できるようになったため、大規模で深いネットワークの学習を可能にし、大規模言語モデル（LLM）の時代の扉を開いた。</p>
<p>このアーキテクチャを基盤に、2018年にOpenAIがGenerative Pre-trained Transformer（GPT）を開発し、LLMの基礎を築いた。さらに2020年には、モデルを大きくするほど性能が向上するというスケーリング則が確立され、LLMの巨大化戦略が主流となった。また、同年には拡散モデルが実用化され、高精度な画像生成AIの道も開かれた。</p>
<p>そして2022年、AIは一気に社会へ浸透した。LLMの推論能力を飛躍的に高める思考の連鎖（CoT）などの手法が開発される一方、Midjourneyなどの対話型画像生成AIが普及した。極めつけは、同年後半にOpenAIからリリースされたChatGPTである。人間と遜色ない自然な対話能力を持つChatGPTは、リリース後わずか約2か月で月間アクティブユーザー数1億人を突破し、生成AIブームを世界中に巻き起こした。</p>
<p>一方で、技術の急速な発展に伴い、AIの倫理と安全性に関する議論も本格化した。2017年にはアシロマ会議が開かれ、AIの安全な開発と利用に向けた「アシロマAI 23原則」が策定されたことも、この時期の重要な出来事である。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/04_70_AI_28c3fdc49a/04_70_AI_28c3fdc49a.jpg" alt="表04_70AI.jpg" /></p>
<p>この5年間で、AIは「認識」から「創造」の領域へと能力を拡張し、人類の生活を一変させる新たなステージへと進んだのである。</p>
<p>:::button
<a href="https://25to26.ledge.ai/">Ledge.ai年末年始特集サイトはこちら</a>
:::</p>
<h2>5. 70周年の展望：AIと人類の未来 (2023年～)</h2>
<h3>5-1. 2023～2025年におけるAIと人類の状況</h3>
<p>2023年から2025年は、生成AI（Generative AI）技術が社会全体に急速に浸透した「AIの実用化元年」とも呼ぶべき変革期であった。特に大規模言語モデル（LLM）の進化により、AIは単なる自動化ツールから、人間の協働者や代行者へとその役割を急速に拡大した時期である。</p>
<p>日常生活においては、AIはスマートフォンや家電に深く組み込まれ、ルーティン作業の自動化、情報検索の高度化、そして個別化された健康管理の提供を通じて、人々の生活効率を向上させた。教育分野では、AIチューターや個別学習プログラムの利用が一般化し、生徒一人ひとりの進捗に合わせたカスタマイズ教育が主流となった一方で、教師は教材作成や評価の負担が軽減され、より対話的な指導に注力できるようになった。エンターテイメント領域では、AIによる画像、音楽、動画の生成が爆発的に増加し、コンテンツ制作の民主化が進んだ。また、AIを活用したパーソナライズされたゲーム体験や、没入型のMR/VRコンテンツも普及した。</p>
<p>ビジネスにおいては、AI導入が業務効率を大幅に向上させ、産業構造の再編を促した。ここでは利用形態に明確な対比が見られた。一つは、コーディングや文書作成などの専門作業において人間の作業を支援・加速するコパイロット型AIであり、これは人間の意思決定が最終的に介在する協調的な形態である。もう一つは、人間からの指示を基に複数のタスクを自律的に計画・実行し、ビジネスプロセスや顧客対応を代行・自動化するAIエージェントの進化である。この対比は、業務におけるAIの自律性の度合いを示す重要な指標となった。</p>
<p>政治においては、AIによる情報分析と政策立案支援が進み、行政の効率化が図られた。しかし同時に、AIが生成するディープフェイクや誤情報が選挙や世論形成に与える影響が重大な社会問題となり、各国でAIの倫理的利用と規制に関する議論が加速した。この時期、人類はAIの利便性を享受しつつも、その倫理性、安全性、社会への影響に対する向き合い方を確立する過渡期にあるのが現状である。</p>
<h3>5-2. 未来のAI：相乗効果と応用のグランドビジョン</h3>
<p>未来のAI技術 (AI_future) の進化は、現在の技術の単なる延長ではない。それは、基礎技術の「積」による指数関数的な相乗効果と、応用領域の「和」による社会的な価値の最大化によって実現されるビジョンである。
Ledge.aiでは、このビジョンを、以下のように定式化して考えている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/ai70th_formula_ff6d7ead4c/ai70th_formula_ff6d7ead4c.png" alt="ai70th_formula.png" /></p>
<h3>■SYNERGY（相乗効果：積の力）による性能の飛躍</h3>
<p>現在のAI技術 (AI_current) は、四つの主要な基礎技術が掛け合わされる（積Π）ことで、その性能を劇的に高める。これらの技術は、それぞれがAIの抱える限界を突破する鍵となる。</p>
<ul>
<li><strong>量子コンピューター (Quantum) :</strong> AIの処理速度と複雑な問題解決能力に演算能力のブレイクスルーをもたらし、現行のスーパーコンピューターでは不可能な領域の学習と計算を可能にする。</li>
<li><strong>Web3 技術 (Web3):</strong> ブロックチェーンや分散型台帳技術により、AIが扱うデータと意思決定プロセスに透明性と信頼性を与え、分散化された環境での安全なAI連携を実現する。</li>
<li><strong>核融合エネルギー (Fusion):</strong> ほぼ無限かつクリーンなエネルギー源を提供することで、大規模な計算資源の制約を完全に緩和し、膨大なデータを用いた学習（超大規模モデル）を経済的かつ環境負荷なく実行可能にする。</li>
<li><strong>データインフラ (DataInfra):</strong> 5G/6Gや次世代ストレージ技術が実現する高速・大容量データ処理基盤が、AIのリアルタイムな学習と推論を支える。</li>
</ul>
<p>これらの技術が個別に進化するのではなく、相互に作用し合う（積）ことで、AIはこれまでにないレベルの知性を獲得する。</p>
<h3>■APPLICATION（応用価値：和の力）による社会実装</h3>
<p>性能が飛躍的に向上したAIは、様々な応用領域へ展開され、その価値を社会へ還元する。これらの応用領域は、AIの価値を社会的効用として積み重ねていく（和Σ）役割を担う。</p>
<ul>
<li><strong>Robotics (ロボティクス):</strong> 高度な知性を持つAIが、物理的な世界で活動するロボットと統合され、自動化・遠隔操作・協調作業を飛躍的に進化させる。</li>
<li><strong>MR (複合現実):</strong> AIがMR環境を分析・最適化し、人間とAIが直感的かつシームレスに連携する新たなインターフェースと作業空間を提供する。</li>
<li><strong>Autonomous Driving (自動運転):</strong> 複雑で予測不可能な環境においても、AIがリアルタイムに安全な判断を下し、交通システム全体を最適化することで社会インフラを革新する。</li>
<li><strong>Social Engineering (社会システムへの適用):</strong> 都市計画、医療、教育などの大規模な社会システムにAIが組み込まれ、データの分析と最適化を通じて社会全体の効率と公平性を向上させる。</li>
</ul>
<p>結論として、未来のAIは、基礎技術の「積」によって知性の限界を超え、応用領域の「和」を通じて私たちの生活、産業、そして社会構造そのものを根本から変革するグランドビジョンである。</p>
<p>金融分野でのいわゆる”AIバブル”は、早晩弾ける可能性がある。しかし、AI技術は着実な進歩が予想される。このようなグランドビジョンのもと、人類とAIの未来を創造していただければ幸いである。</p>
<p>:::button
<a href="https://25to26.ledge.ai/">Ledge.ai年末年始特集サイトはこちら</a>
:::</p>
<h2>おわりに</h2>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/25to26_thumb_ced6fcf9a3/25to26_thumb_ced6fcf9a3.png" alt="25to26_thumb.png" /></p>
<p>Ledge.ai年末年始特集では、読者に向けて、2025年のAI関連の重要トレンドを振り返り、また2026年以降のAIの展望について発信している。新しい年へ動き出すための情報が詰まっているので、ぜひ以下ボタンより特集サイトをご覧いただきたい。</p>
<p><strong>【コンテンツ情報】</strong>
無料登録を行うと、これらすべての記事を閲覧できるようになる。
■ 特別インタビュー
京都賞受賞のAIのパイオニア甘利俊一先生をはじめ、量子コンピュータ/量子機械学習からロボット基盤モデル、話題の”PLURALITY”、NVIDIA、AMDなど、”いま読んでおくべき”インタビューが満載
■ 2025年のAI動向総ざらい
■ 厳選注目記事49本</p>
<p>:::button
<a href="https://25to26.ledge.ai/">Ledge.ai年末年始特集サイトはこちら</a>
:::</p>
]]></description>
      <pubDate>Sun, 07 Dec 2025 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Adobe、ChatGPTでPhotoshop・Express・Acrobatを提供開始──対話だけで画像編集やPDF作業が可能に</title>
      <link>https://ledge.ai/articles/adobe_apps_for_chatgpt_photoshop_express_acrobat</link>
      <description><![CDATA[<p>Adobe、ChatGPTでPhotoshop・Express・Acrobatを提供開始──対話だけで画像編集やPDF作業が可能に</p>
<p>Adobeは2025年12月10日（米国時間）、ChatGPT上で同社の主要アプリを利用できる「Adobe Apps for ChatGPT」の提供を開始したと<a href="https://news.adobe.com/ja/news/2025/12/20251211-adobe-photosop-express-acrobat-chatgpt">発表</a>した。対象となるのは「Adobe Photoshop」「Adobe Express」「Adobe Acrobat」の3製品で、ChatGPTとの会話を通じて画像編集やデザイン作成、PDF編集などを直接実行できる。</p>
<p>@<a href="https://www.youtube.com/watch?v=uouNjFuJ3QU">YouTube</a></p>
<p>同社によると、ChatGPT上で「この画像の背景をぼかしてほしい」「このPDFの文章を修正してほしい」といった自然文で指示するだけで、各アプリの機能が呼び出され、編集が行われる。従来のようにアプリを起動して操作する必要はなく、会話の文脈を理解したうえでタスクを実行する点が特徴だ。</p>
<p>Photoshopでは、明るさやコントラストの調整、オブジェクトの編集、グリッチやグローといったクリエイティブエフェクトの適用などが可能となる。画質を保ったまま編集でき、スライダーなどの直感的なUIを用いた調整にも対応する。</p>
<p><strong>■「背景にクリエイティブな効果を追加して」と指示するだけで、Photoshopが編集を実行する</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/media_157c8bf8df2f28f50a647a42c5d3d9ef436a1faa5_168c1e2ea5/media_157c8bf8df2f28f50a647a42c5d3d9ef436a1faa5_168c1e2ea5.jpg" alt="media_157c8bf8df2f28f50a647a42c5d3d9ef436a1faa5.jpg" /></p>
<p>Adobe Expressでは、テンプレートを活用したデザイン作成やテキスト・画像の差し替え、アニメーションの追加などをChatGPT上で行える。編集内容を対話的に修正しながら仕上げていくことができ、デザイン経験の少ないユーザーでも扱いやすい構成となっている。</p>
<p><strong>■ Adobe Expressでは、ChatGPTとの対話を通じてテンプレート選択やデザイン作成が可能になる</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/media_15d631a2f2b908e1ebffd15a9cd75d6d591838062_9927f6fc0a/media_15d631a2f2b908e1ebffd15a9cd75d6d591838062_9927f6fc0a.jpg" alt="media_15d631a2f2b908e1ebffd15a9cd75d6d591838062.jpg" /></p>
<p>Acrobatでは、PDFの編集やテキスト・表の抽出、複数ファイルの統合、圧縮、形式変換などに対応する。レイアウトや体裁を維持したまま編集できるほか、機密情報の修正といった用途にも利用できるという。</p>
<p><strong>■ ChatGPT上でPDFを直接編集。Adobe Acrobatの編集機能を対話形式で利用できる</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/media_1fe39d2bd0158dc797544d0d694b43d51bc6bea35_5644bc6742/media_1fe39d2bd0158dc797544d0d694b43d51bc6bea35_5644bc6742.jpg" alt="media_1fe39d2bd0158dc797544d0d694b43d51bc6bea35.jpg" /></p>
<p>利用環境はChatGPTのWeb版、デスクトップ版、iOS版で、料金は無料。利用にあたってはAdobeアカウントとの連携が必要となるが、無料のAdobe IDで利用できる。Adobe ExpressはAndroidでもすでに利用可能で、PhotoshopとAcrobatのAndroid対応は近日中を予定している。</p>
<p>なお、ChatGPT上で利用できるのは基本的な編集機能に限られ、高度な編集や最終的な仕上げが必要な場合は、ChatGPTから各Adobeアプリにシームレスに移行して作業を継続できる。</p>
<p>Adobeは本取り組みを、同社が進めるエージェント型AI戦略の一環と位置付けている。Model Context Protocol（MCP）を活用し、ユーザーの意図を理解した上で適切な操作を実行する対話型体験の拡張を進めてきた。2025年には「Acrobat AI Assistant」や「Photoshop AI Assistant」なども発表しており、今回のChatGPT連携はその延長線上にある。</p>
<p>Adobeのデジタルメディア事業部門プレジデントを務めるデイビッド・ワドワーニ氏は、「世界中のChatGPTユーザーに、PhotoshopやAcrobat、Expressの機能を直接届けられることをうれしく思う。人々が自分の言葉だけで、簡単にアイデアを形にできるようになる」とコメントしている。</p>
]]></description>
      <pubDate>Sun, 07 Dec 2025 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>学術＆研究2025/12/12 [FRI]諸説あるAGIの定義をスタンフォード大などが明確化、「教養ある成人」がモデルケース──GPT-4は27%、GPT-5は約6割に到達と試算</title>
      <link>https://ledge.ai/articles/agi_definition_stanford_gpt4_27_gpt5_60</link>
      <description><![CDATA[<p>スタンフォード大学やUCバークレー、MITなどの研究者を含む国際チームは、AIの到達度を測る新しい枠組みを提示した論文「A Definition of AGI」を<a href="https://arxiv.org/abs/2510.18212v3">公開</a>した。これまで曖昧だったAGI（汎用人工知能）の定義について、同チームは「教養ある成人（well-educated adult）の認知の幅と熟達度にマッチするAI」と明確化。加えて、人間の知能研究で広く使われるCHC（Cattell-Horn-Carroll）理論に基づき、10種類の認知ドメインでAIモデルを評価する「AGIスコア」を提案した。</p>
<p>今回の枠組みに基づく試算では、GPT-4は27%、GPT-5は約57〜58%に到達しているとされる（100%が「教養ある成人の平均レベル」に相当）。論文のPDF版では57%、公式サイトや解説資料では58%と表記されており、約6割前後の水準と整理できる。</p>
<h2>AGIをめぐる“動くゴールポスト”問題を解消へ</h2>
<p>研究チームが強調するのは、AGIという用語の曖昧さだ。企業・研究者・メディアで異なる意味合いで使われてきた結果、実際にどの能力を満たせばAGIと呼べるのかが明確でなく、「モデルが進化するたびにゴールが後ろにずれる」として批判もあった。</p>
<p>Center for AI Safety（CAIS）はニュースレターの中で、この曖昧さが研究目標、社会的リスク評価、政策議論を複雑化してきたと指摘している。今回の提案は、こうした状況に対し「包括的でテスト可能な定義」を与える試みと位置づけられる。</p>
<h2>10の認知ドメインを10%ずつ評価する「AGIスコア」</h2>
<p>論文が提示する枠組みでは、人間の認知能力を10の中核ドメインに分解し、それぞれを「10%」の重みで評価する。ドメインは以下の通り。</p>
<ul>
<li>一般知識</li>
<li>読解・文章</li>
<li>数学</li>
<li>推論（問題解決・抽象化）</li>
<li>作業記憶・注意制御</li>
<li>長期記憶の保存と検索</li>
<li>視覚処理</li>
<li>聴覚処理</li>
<li>処理速度</li>
<li>中央実行系（複数能力の統合）</li>
</ul>
<p>研究チームはこれらの領域を、人間の心理測定バッテリー（知能検査など）の形式に合わせて評価タスク化。AIモデルを人間の標準スコアにマッピングしたうえで、各分野を合成して「AGIスコア（0〜100%）」として算出する。</p>
<p><strong>■ AGIを構成する10の認知ドメイン（CHC理論に基づく）：</strong> 一般知識・読解・数学・推論・記憶・知覚・速度など、人間の認知を広範にカバーし、各領域を10%ずつ評価してAGIスコアを算出する。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x2_8_1eb9f5c414/x2_8_1eb9f5c414.png" alt="x2 (8).png" /></p>
<p>この枠組みの特徴は、一部の領域だけ優れていてもAGIとは見なされない点にある。たとえば「読解や知識」では人間レベルであっても、「長期記憶」や「視覚処理」が極端に低ければ、スコアは上がらない。</p>
<h2>GPT-5は約6割──強い領域と“ほぼゼロ”の領域が共存</h2>
<p>論文および公式サイトのデータによると、GPT-4とGPT-5には以下のような特徴がある。</p>
<ul>
<li>GPT-4：AGIスコア27%</li>
<li>GPT-5：AGIスコア約57〜58%（約6割）</li>
</ul>
<p><strong>■ GPT-4 と GPT-5 の10ドメイン能力比較：</strong> GPT-5 は読解・数学・推論などで高スコアを示す一方、長期記憶や視覚・聴覚処理などの基盤能力は依然として低く、“ギザギザ”なプロファイルが特徴となっている。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x1_5c3108a876/x1_5c3108a876.png" alt="x1.png" /></p>
<p>高いスコアを示すのは「一般知識」「読解・文章」「数学」「推論」といったデータ駆動型の領域で、人間を上回るケースもある。一方で、</p>
<ul>
<li>長期記憶の保存・検索</li>
<li>視覚的推論</li>
<li>聴覚処理</li>
<li>処理速度（リアルタイム性）</li>
</ul>
<p>といった基盤的能力では、多くのモデルが極めて低いスコアにとどまっており、ほぼゼロに近い領域も存在する。</p>
<p>研究チームは、現行の大規模言語モデルは「部分的には人間を超えるが、能力の分布がギザギザで、総合的な認知としては成人レベルに達していない」と整理している。</p>
<h2>政策・規制議論への影響も──共通の“物差し”が登場</h2>
<p>AGIの定義が明確化されたことで、今後は次のような領域で議論が進む可能性がある。</p>
<ul>
<li><strong>米上院で進むAI安全法案の議論：</strong> 規制対象となるAIモデルをどの基準で選定するか、共通の指標が求められていた。</li>
<li><strong>企業のロードマップ策定：</strong> 「AGIにどれだけ近いか」を定量的に示せることで、開発の透明性や説明責任が高まる可能性。</li>
<li><strong>研究コミュニティでの基準化：</strong> 進捗を測る複数のベンチマークが乱立してきた中で、総合的な認知評価として採用される可能性。</li>
</ul>
<p>ただし研究チームは、今回の枠組みが「最終版の定義」ではなく、今後の議論の叩き台であることも強調している。</p>
<h2>今後の焦点：長期記憶・知覚・実世界タスク</h2>
<p>GPT-5が約6割に到達した一方、AGI達成には大きなギャップが残されている。特に論文が指摘するのは、次のような領域だ。</p>
<ul>
<li>長期記憶の安定した形成と参照</li>
<li>視覚・聴覚などマルチモーダル知覚の統合</li>
<li>現実世界での連続的タスク遂行（エージェント的行動）</li>
</ul>
<p>研究チームは、これらの領域が改善すれば、AGIスコアが急速に伸びる可能性があるとしている。</p>
]]></description>
      <pubDate>Sun, 07 Dec 2025 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Anthropic、AIエージェント標準「MCP」をLinux Foundation傘下へ移管──Agentic AIの共通基盤化へ</title>
      <link>https://ledge.ai/articles/anthropic_mcp_linux_foundation_aaif_transfer</link>
      <description><![CDATA[<p>Anthropicは2025年12月10日、同社が開発してきたAIエージェント向けの標準仕様「Model Context Protocol（MCP）」を、Linux Foundation傘下に新設された「Agentic AI Foundation（AAIF）」へ移管すると<a href="https://www.anthropic.com/news/donating-the-model-context-protocol-and-establishing-of-the-agentic-ai-foundation">発表</a>した。</p>
<p>Anthropicはこれまで、対話型AI「Claude」を中心としたエコシステムの中でMCPを提唱・整備してきた。エージェントが自律的にタスクを実行する「Agentic AI」への関心が高まる中、MCPはその基盤技術の一つとして注目されている。</p>
<p>今回の移管により、MCPの仕様管理や将来の拡張は、Linux Foundation傘下のAAIFが担う。AAIFは、AIエージェント分野における共通基盤の整備と標準化を目的として新設された組織で、複数の企業や開発者コミュニティが参加する中立的な運営体制を採る。</p>
<p>Linux Foundationによると、AAIFはMCPのほか、Blockが開発したエージェント関連プロジェクト「goose」や、OpenAIが提唱するエージェント仕様「AGENTS.md」などの初期プロジェクトを受け入れ、AIエージェント分野における共通基盤の整備を進めるとしている<a href="https://aaif.io/press/linux-foundation-announces-the-formation-of-the-agentic-ai-foundation-aaif-anchored-by-new-project-contributions-including-model-context-protocol-mcp-goose-and-agents-md/">Linux Foundationの発表</a>。</p>
<p>Anthropicは、MCPの創始者として引き続き技術的な貢献を行う一方、主導権はコミュニティ側に委ねられる。</p>
<p>Linux Foundationは、LinuxやKubernetesなど、業界横断的なソフトウェア基盤を中立的に運営してきた実績を持つ。AI分野ではすでに「LF AI &amp; Data」を通じてオープンな研究・開発を支援しており、今回のAAIF設立とMCP受け入れは、Agentic AIを巡る標準化の動きを一段進めるものとなる。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/aaif_social_media_image_e440cd3697/aaif_social_media_image_e440cd3697.webp" alt="aaif_social media image.webp" /></p>
<p>AIエージェントを巡っては、各社が独自の実装やフレームワークを競う一方で、相互運用性やベンダーロックインへの懸念も指摘されてきた。Anthropicが自社主導で開発してきたMCPをLinux Foundation傘下へ移管したことは、競争領域と共通基盤を切り分け、エコシステム全体の拡大を優先する姿勢を示す動きといえる。</p>
]]></description>
      <pubDate>Sun, 07 Dec 2025 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>「心の中の声」をAIが文字に変換　米研究チーム、脳活動から直接文章を生成する新技術「BIT (BraIn-to-Text)」発表</title>
      <link>https://ledge.ai/articles/brain_to_text_inner_speech_decoder</link>
      <description><![CDATA[<p>コロンビア大学やスタンフォード大学などの研究者らによるチームは2025年11月21日、脳の活動データを直接テキストに変換する新しいAIフレームワーク「BIT (BraIn-to-Text)」を<a href="https://arxiv.org/abs/2511.21740v1">発表</a>した。</p>
<p>このシステムは、発声しようとした言葉だけでなく、声を出さずに心の中で唱えた「内なる声（inner speech）」も一定の精度で文章化できることを示した。従来の類似手法と比べ、単語誤り率（WER）を24.69％から10.22％へと半分以下に抑えた点が特徴で、麻痺などによって話すことが難しい人のコミュニケーション支援技術としての応用も期待される。</p>
<p>なお、研究で使用されたのはUtahアレイと呼ばれる脳表面に埋め込む侵襲型電極であり、非侵襲の脳波（EEG）とは異なる。</p>
<h2>脳波を「音声」と見なしてLLMが解読</h2>
<p>従来の脳内音声解読システム（BCI）では、多くが「脳活動 → 音素 → 単語 → 言語モデル」という複数段階の“カスケード型”処理を採用していた。この構造では、各工程が独立しているため、システム全体をまとめて最適化できない点が課題とされていた。</p>
<p>今回発表された「BIT」は、脳活動の特徴を捉えるTransformerエンコーダーと、大規模言語モデル（LLM）を結合させたEnd-to-End型（統合的最適化）フレームワークである。</p>
<p>ヒトやサルの脳活動データ約367時間分を用いて事前学習を行い、神経活動のパターンから直接テキストを生成する。この仕組みにより、従来必要だった音素への変換ステップを省き、脳活動から直接文章を出力できるようになった。</p>
<p><strong>図1：BIT（BraIn-to-Text）フレームワークの全体構成</strong>
脳に埋め込んだ電極から取得した神経活動をAIが処理し、最終的に文章として出力するまでの流れを示す図。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x1_11_017e439017/x1_11_017e439017.png" alt="x1 (11).png" /></p>
<h2>単語誤り率を大幅に低減</h2>
<p>このEnd-to-Endのアプローチによる精度の変化について、論文では以下の数値が報告されている。従来の同様のシステムと比較し、単語の読み取りミスを示す「単語誤り率（WER）」は低下した 。</p>
<ul>
<li><strong>従来のEnd-to-End手法</strong> ： 単語誤り率 24.69%</li>
<li><strong>新技術「BIT」（音声LLM統合版）</strong> ： 単語誤り率 10.22%</li>
</ul>
<p>さらに、脳活動エンコーダーを用いて「Brain-to-Text Benchmark」に参加したところ、カスケード型設定を含む全カテゴリで最も低い誤り率を記録した。</p>
<p>アンサンブル（複数モデル併用）設定でのWERは以下の通りである。</p>
<ul>
<li><strong>Brain-to-Text '24 ベンチマーク</strong> ： 単語誤り率 5.10%</li>
<li><strong>Brain-to-Text '25 ベンチマーク</strong> ： 単語誤り率 2.21%</li>
</ul>
<p><strong>図2：BITの性能比較と解読例</strong>
従来手法と比較した誤り率の低減、および実際に生成された文章の例を示す図。内言においても意味的に近い文章を生成できている点が示されている。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x2_8f9b3b00ac/x2_8f9b3b00ac.png" alt="x2.png" /></p>
<h2>「想像発話」の読み取りにも成功</h2>
<p>研究では、筋肉を動かして発話しようとする「試行発話（attempted speech）」だけでなく、声を出す動作を伴わずに頭の中で言葉を思い浮かべる「想像発話（imagined speech）」のデータセットを用いた検証も行われた。</p>
<p>解析の結果、実際に話そうとする時と心の中で話す時の脳内活動には、共通する意味的構造が存在することが示唆された。AIはこの共通性を利用することで、データの少ない想像発話においても文章変換を可能にしている。</p>
<p><strong>図3：音声LLMとの比較──脳活動の文章化に適したモデル</strong>
複数の音声LLM・テキストLLMを比較し、どのモデルが脳活動からの文章生成に適しているかをまとめた図。音声を扱う小規模モデルが高い適性を示した。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x3_4_7cc8f5e441/x3_4_7cc8f5e441.png" alt="x3 (4).png" /></p>
<h2>話せない人のコミュニケーション支援へ</h2>
<p>発話が困難なALS患者や脳損傷患者にとって、頭の中で言葉を思い浮かべるだけで意思を伝えられる技術は大きな可能性を持つ。</p>
<p>論文では、将来的な臨床応用を見据えつつ、次のような課題を整理している。</p>
<ul>
<li>神経信号の非定常性への適応</li>
<li>電極の長期安定性</li>
<li>オンデバイス実行に向けた効率化</li>
</ul>
<p>また、特に内言の解読は倫理的に慎重な扱いが求められ、研究チームも「利用者の明確な同意を欠いた読心的用途は許されない」と明記している。</p>
<h2>今後の展望──神経データ版“基盤モデル”へ</h2>
<p>BITで用いられたNeural Encoderは、サルを含む多様な神経活動から学習されている。論文では、近年さまざまな分野で「foundation models」と呼ばれる大規模事前学習モデルが提案されていることを紹介し、こうした方向性がBCIの性能向上にも有効である可能性を挙げている。</p>
<p>研究チームは、今後の課題として、電極の非定常性への適応、長期的な記録の安定性、インターフェースの効率化などを明確にし、改善を重ねることで「ユーザーとシステムが互いに適応しながら利用可能な、より柔軟なBCIの開発につながる」とまとめている。</p>
]]></description>
      <pubDate>Sun, 07 Dec 2025 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ChatGPTが選んだ38銘柄ファンド、2年半で+66.1%に──英国人気投信10本の平均+41.36%を引き離す</title>
      <link>https://ledge.ai/articles/chatgpt_fund_performance_2025_update</link>
      <description><![CDATA[<p>英国の金融比較サイト Finder が実施している「<a href="https://www.finder.com/uk/share-trading/share-trading-research/ai-investing">ChatGPTファンド</a>」の概念実験で、2025年10月28日時点の最新パフォーマンスが明らかになった。同ファンドは、生成AI「ChatGPT」が選んだ38銘柄で構成される仮想ポートフォリオで、運用開始から約2年半の累計リターンは +66.1%。ベンチマークとしている英国の人気投資信託トップ10本の平均リターン +41.36% を大きく上回り、開始以来 99% の営業日でアウトパフォーム したという。</p>
<p>Finderは2023年3月、ChatGPTに「借入が少なく、持続的成長と競争優位性を持つ企業」を条件として提示し、株式ポートフォリオの作成を依頼した。ChatGPTは「概念上の演習」であることを前提に38銘柄を選定。このリストをもとに、同社は等金額の仮想ファンドを組成し、Interactive Investor 上で最も人気のある投資信託10本とパフォーマンスを比較する継続実験を開始した。</p>
<p>同年5月にFinderが公表したプレスリリースでは、開始8週間時点でChatGPTファンドが +0.48%、人気投信10本の平均が -0.78% となり、初期段階から優位に立っていたことが示されている。</p>
<p>今回の最新アップデートでは、2年半にわたる追跡期間のうち、ChatGPTファンドが人気投信10本の平均を下回った日はわずか1%。選定銘柄には、Microsoft、Alphabet、Meta、NVIDIA、TSMC、ASML、Johnson &amp; Johnson、PepsiCo など、大型株を中心とした多国籍企業が並ぶ。</p>
<p>Finderは同実験について「ChatGPTは2021年までの訓練データを持つ言語モデルであり、リアルタイム市場データに基づく投資判断を行っているわけではない」と明記。ChatGPTの回答が投資助言に該当するものではないこと、またこのファンド自体は実在せず、あくまでAIの応答を可視化する概念実験である点を繰り返し注意喚起している。</p>
<p>同社は、実験の目的を「AIが人間のファンドマネージャーと比較してどのようなポートフォリオを組むかを検証すること」と説明。今回の結果は、2年半の長期観測においても、ChatGPTが選んだポートフォリオが英国の人気投信平均を継続して上回り続けていることを示すものとなった。</p>
]]></description>
      <pubDate>Sun, 07 Dec 2025 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ディズニーとOpenAI、生成AIに公式キャラクターを解禁──「Sora」でミッキーやスター・ウォーズの動画生成が可能に</title>
      <link>https://ledge.ai/articles/disney_openai_sora_character_license</link>
      <description><![CDATA[<p>2025年12月11日、ウォルト・ディズニー・カンパニーとOpenAIは3年間のライセンス契約を締結したと<a href="https://thewaltdisneycompany.com/disney-openai-sora-agreement/">発表</a>した。</p>
<p>これにより、OpenAIの動画生成AI「Sora」において、ディズニー、マーベル、ピクサー、スター・ウォーズなど、200以上の公式キャラクターを用いた短編動画を生成できるようになるという。対象にはキャラクター本体に加え、衣装、小道具、乗り物、象徴的な環境なども含まれる。</p>
<p>本契約は、生成AIが世界的エンターテインメント企業のキャラクターIPを正式に扱う大規模な事例の一つとなる。両社は、著作権とブランド保護を前提とした「責任あるAI利用」を掲げ、キャラクターの不適切な利用を防ぐための安全策を講じるとしている。</p>
<h2>ディズニー、生成AIに公式キャラクターを解禁</h2>
<p>今回の契約により、ディズニーが保有する主要フランチャイズのキャラクターが、OpenAIの動画生成AI「Sora」に正式にライセンス提供される。対象には、ディズニーのオリジナルキャラクターをはじめ、マーベル、ピクサー、スター・ウォーズといった世界的に認知度の高いブランドが含まれる。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/disney_and_sora_82d3de54a5/disney_and_sora_82d3de54a5.jpg" alt="disney and sora.jpg" /></p>
<p>生成AIによるキャラクター表現をめぐっては、これまで権利侵害の懸念から非公式な利用が問題視される場面も多かった。今回の契約は、公式ライセンスのもとで生成AIを活用する枠組みを明示したものとなる。</p>
<h2>「Sora」でミッキーやスター・ウォーズの動画生成が可能に</h2>
<p>Soraでは、ユーザーが入力したプロンプトに基づき、公式キャラクターを用いた短い動画を生成できるようになる。OpenAIとディズニーは、キャラクターの扱いについて不適切な利用を防ぐための安全策を講じると説明している。</p>
<p>あわせて、同様のキャラクターIPは、静止画生成機能である「ChatGPT Images」でも利用可能になるとしている。</p>
<h2>俳優の顔や声は含めず、キャラクターに限定</h2>
<p>今回のライセンス契約では、生成対象をキャラクターIPに限定している点も明確にされている。実写作品に出演する俳優本人の容姿や声、実在の人物を想起させる表現は含まれない。</p>
<p>両社は、こうした制限を通じて、俳優や声優の肖像権・パブリシティ権への配慮を含む、権利保護と安全性を重視した設計であることを強調している。</p>
<h2>生成された動画はDisney+にも並ぶ予定</h2>
<p>生成された動画の一部については、動画配信サービス「Disney+」での配信も予定されている。具体的な配信形態や選定基準は明らかにされていないが、ユーザーが生成したコンテンツが公式配信プラットフォームに掲載される可能性が示された形だ。</p>
<p>一般向けの展開は2026年初頭を見込んでおり、Soraで制作されたコンテンツがどのように視聴体験へ組み込まれるのかが注目される。</p>
<h2>ライセンスにとどまらず、ディズニーはOpenAIに出資</h2>
<p>今回の提携は、ライセンス契約にとどまらない。ディズニーはOpenAIに対し10億ドルを出資し、あわせて追加株式の購入権も取得した。</p>
<p>ディズニーは、OpenAIのAPIを活用し、映画、テレビ、テーマパーク、消費者向けプロダクト、そしてDisney+を含む体験の構築を進めるとしており、OpenAIにとっても、世界最大級のエンターテインメント企業が主要顧客となる。</p>
<h2>エンタメ大手が踏み出した「公式×生成AI」の一歩</h2>
<p>ディズニーとOpenAIの契約は、生成AIによるキャラクター表現をめぐり、公式ライセンスの枠組みを示した事例となった。非公式利用や訴訟リスクが先行してきた分野において、権利者とAI開発企業が協調する形での活用モデルが提示された格好だ。</p>
<p>両社は今後も、著作権やブランド保護を前提としたAI活用を進めるとしており、生成AIがエンターテインメント制作や体験の在り方にどのような変化をもたらすのか、引き続き注目される。</p>
]]></description>
      <pubDate>Sun, 07 Dec 2025 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>アシモフもびっくり？ 中国EngineAI、人型ロボット「T800」がCEOを蹴り倒す映像公開──CG疑惑を巡り“体を張った実演”</title>
      <link>https://ledge.ai/articles/engineai_t800_ceo_kick_video_cg_suspicions</link>
      <description><![CDATA[<p>中国のロボット企業 EngineAI（众擎机器人） は2025年12月6日、人型ロボット「T800」が同社の創業者で最高経営責任者（CEO）の趙同陽（Zhao Tongyang）氏をキックで倒す様子を収めた映像を<a href="https://www.instagram.com/reel/DR7C3kMEz4b/">公開</a>した。映像はInstagramやXなどの投稿サイトを通じて公開され、SNS上で広がっていた「CGではないか」という疑念に対応するため、CEO自らが実演に臨んだ形だ。</p>
<p>公開された動画では、防具を着用した趙氏がT800の前に立ち、ロボットが繰り出した前方キックを受けて地面に倒れる様子が確認できる。映像内ではキックの威力の強さが強調されており、公開直後から中国国内外のSNSで急速に拡散した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/enginai_ceo_651eec7e25/enginai_ceo_651eec7e25.jpg" alt="enginai ceo.jpg" /></p>
<p>この映像が公開される前、EngineAIはT800がキックや宙返りのような動作を見せるデモ動画を複数公開していた。動きの滑らかさやダイナミックさから、SNS上では「実写ではなくCGIではないか」といった指摘や懐疑的な声が相次いでいた。こうした疑念に反論する意図で、CEOが自らロボットのキックを受ける実演映像を公開したと米Business Insider等が報じている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/enginai_no_aigc_nocgi_f4537fb037/enginai_no_aigc_nocgi_f4537fb037.jpg" alt="enginai no aigc nocgi.jpg" /></p>
<p>映像はInstagramをはじめとする複数の投稿サイトで転載・拡散され、海外メディアが相次いで取り上げた。多くの報道では、ロボット技術の進展そのものに加え、CGや高度な映像編集が一般化した時代において、実機であることをどのように証明するかという点に注目が集まっている。</p>
<p>近年、人型ロボットの開発を巡っては、各国企業が実機デモ映像を積極的に公開する一方で、その真偽や編集の有無が議論になるケースも増えている。EngineAIの今回の対応は、第三者による検証ではなく、CEO本人が体を張る形で“実在性”を示そうとした点で、興味深い事例といえる。</p>
<p>人型ロボットが人間に危害を加える映像は、SF作家アイザック・アシモフが提唱した「ロボット工学三原則」で描かれてきた理想像とは対照的だ。三原則では、ロボットは人間に危害を加えてはならないとされてきた。一方、現実のロボット開発では、技術の実在性や性能を示すために、こうした象徴的な実演が行われるケースが現れた。</p>
<p>技術的な進歩と同時に、安全性や検証方法、情報公開のあり方が問われる人型ロボット。一連の映像は、ロボット技術の進化とともに、社会がどのようにその信頼性を評価していくのかという課題を浮き彫りにしている。</p>
]]></description>
      <pubDate>Sun, 07 Dec 2025 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>AIで“裸眼3D”が進化──上海AI研究所らが超広視野角ディスプレイ「EyeReal」を開発、Natureに発表</title>
      <link>https://ledge.ai/articles/eyereal_ai_glasses_free_3d_ultrawide_display_nature</link>
      <description><![CDATA[<p>上海人工智能実験室と復旦大学の研究チームは、専用メガネなしで広視野角の3D映像を表示できる新方式「EyeReal」を開発し、科学誌 Nature に<a href="https://www.nature.com/articles/s41586-025-09752-y">発表</a>した。従来のホログラフィック方式や自動立体（autostereoscopic）ディスプレイが抱えていた物理的制約を、AIと多層LCD構造を組み合わせた新たな光場生成手法で克服した点が特徴だ。</p>
<h2>従来の裸眼3D方式の限界を超えるアプローチ</h2>
<p>過去の裸眼3D技術は、視野角や画面サイズなどのトレードオフが大きな課題だった。ホログラフィック方式は高密度の光学情報を扱える一方、視域が極めて小さく、実用性が限定される。一方、ビューセグメント型・ビューデンス型の自動立体ディスプレイは、視点数や視域が固定され、観察者が自由に動いた際に自然な視差変化を再現することが難しかった。</p>
<p>EyeReal は、こうした従来方式を図示しつつ、観察者の目の周囲に最適化された光場をリアルタイム生成するというアプローチを採用する。</p>
<p><strong>従来方式（a〜c）が視野角や表示領域に制約を抱える中、EyeReal（d）は観察者の眼のまわりで光場を最適化し、超広視野角の裸眼3D表示を可能にする</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/41586_2025_9752_Fig1_7a43aeeee1/41586_2025_9752_Fig1_7a43aeeee1.jpg" alt="41586_2025_9752_Fig1.jpg" /></p>
<h2>EyeReal の構造：多層LCD × AIによる光場最適化</h2>
<p>研究チームは、市販の液晶パネルを複数枚重ね合わせた構造を採用。光源、偏光板（縦・横）、液晶層の組み合わせにより、各ピクセルからの位相（フェーズ）を精密に制御する。</p>
<p>RGB-D センサーが観察者の両眼位置と姿勢（6D pose）を取得し、ニューラルネットワークがその位置に最適な「位相パターン」を生成する。これにより、従来の光学系では物理的に困難だった複雑な光場を、液晶ディスプレイのみで再現できる。</p>
<p><strong>多層LCD、偏光板、RGB-D センサー、ニューラルネットワークで構成される EyeReal の光場生成システム。両眼位置の推定から位相パターン生成までをリアルタイムで行う</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/41586_2025_9752_Fig2_d941741a66/41586_2025_9752_Fig2_d941741a66.jpg" alt="41586_2025_9752_Fig2.jpg" /></p>
<h2>動きに応じて自然に変化する視差と焦点表現</h2>
<p>実験では、都市景観や屋内シーンなどを用いて、左右の眼に届く画像を検証した。EyeReal が生成した左右画像は、Ground truth（正解画像）と高い一致率を示し、水平・垂直・前後方向の動きに対して自然なモーションパララックスを維持した。</p>
<p>さらに、前景・後景の被写界深度（焦点表現）も再現可能で、奥行き感の自然さが向上している。</p>
<p><strong>EyeReal の再現画像（Prediction）は、Ground truth と高い一致を示し、上下左右・前後の動きに伴う視差変化を自然に再現。前景・後景の焦点表現も維持されている</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/41586_2025_9752_Fig3_c6449eee0f/41586_2025_9752_Fig3_c6449eee0f.jpg" alt="41586_2025_9752_Fig3.jpg" /></p>
<h2>超広視野角100度超、50Hzリアルタイム処理</h2>
<p>定量評価では以下の性能が確認された：</p>
<ul>
<li><strong>視野角：100度超</strong> ：観察者が大きく移動しても PSNR・SSIM が高値を維持した</li>
<li><strong>未知視点に対する一般化性能（Generalization）</strong> ：学習データに存在しない視点からでも安定した画質を確保</li>
<li><strong>動作速度：50.2 Hz</strong> ：多層LCDとAI処理によるシステムとしては高速で、リアルタイム裸眼3Dに求められる水準を満たす</li>
</ul>
<p>奥行き層ごとの PSNR 曲線や視野マップも示され、前景から背景まで一貫した再現品質を持つことが確認された。</p>
<p><strong>EyeReal の定量評価。超広視野角でも高い PSNR/SSIM を維持し、未知視点に対しても一般化。50Hzを超える処理速度によりリアルタイム裸眼3Dとして実用レベルに到達している</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/41586_2025_9752_Fig4_e554e7ef93/41586_2025_9752_Fig4_e554e7ef93.jpg" alt="41586_2025_9752_Fig4.jpg" /></p>
<p>論文では、エンターテインメント用途にとどまらず、医学画像の立体視、建築設計、教育、科学データの立体構造可視化など、多様な分野への応用可能性が示されている。</p>
<p>今後の課題としては、複数ユーザー同時視聴への拡張、消費電力の最適化、さらなる高速化などが挙げられる。
研究チームは、本方式が「裸眼3Dディスプレイの物理的限界をAIで超える新しい方向性を提示した」としている。</p>
]]></description>
      <pubDate>Sun, 07 Dec 2025 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>月額3万6400円の「Google AI Ultra」に最強推論モード──Geminiアプリに、思考するAI「Deep Think」モードの実装</title>
      <link>https://ledge.ai/articles/gemini_3_deep_think_google_ai_ultra_launch</link>
      <description><![CDATA[<p>Googleは2025年12月4日（現地時間）、Geminiアプリに新たな推論モード「Gemini 3 Deep Think」を追加したと<a href="https://blog.google/products/gemini/gemini-3-deep-think/">発表</a>した。</p>
<p>同社はブログで、「Today, we’re rolling out Gemini 3 Deep Think mode to Google AI Ultra subscribers in the Gemini app.」と述べており、最上位サブスクリプションプラン「Google AI Ultra」の加入者を対象に、Geminiアプリで順次ロールアウトしている。利用するには、モデル選択で「Gemini 3 Pro」を選び、プロンプト入力欄から「Deep Think」モードをオンにする。</p>
<h2>高難度課題向けに推論能力を強化</h2>
<p>公開された「Gemini 3 Deep Think」は、回答を出力する前に複数の推論ステップを挟むことで、従来よりも深い推論ができるよう設計されたモードだ。Googleはブログで、同モードが「meaningful improvement in reasoning capabilities」を提供し、複雑な数学・科学・論理推論に取り組む場面を想定していると説明している。</p>
<p>具体的には、高度な並列推論（advanced parallel reasoning）を用いて複数の仮説を同時に探索するアプローチを採用することで、問題解決の過程を強化しているという。これにより、単に次の単語を即座に予測するのではなく、複数の候補を比較しながら解答に至るプロセスを踏めるとしている。</p>
<p>Googleは、Gemini 3 Deep Thinkが高難度ベンチマークで次のようなスコアを記録したと紹介している。</p>
<ul>
<li>Humanity’s Last Exam：ツールなしで 41.0%</li>
<li>ARC-AGI-2：コード実行ありで 45.1%</li>
</ul>
<p>同社は、これらの結果について「industry leading」「unprecedented（前例のない）」と表現しており、深い推論が求められるベンチマークにおいて業界トップレベルの性能を示したと位置づけている。また、今回のGemini 3 Deep Thinkは、国際数学オリンピック（IMO）や国際大学対抗プログラミングコンテスト（ICPC）World Finalsで「gold-medal standard」とされる水準に達した「Gemini 2.5 Deep Think」の系譜にあると説明されている。</p>
<h2>利用対象は「Google AI Ultra」加入者</h2>
<p>日本向けの公式プランページでは、Google AI Ultraの料金は月額3万6400円（税込）と案内されている。現時点（2025年12月8日）では、GeminiアプリでのDeep ThinkおよびGemini Agentについては「米国のみ、英語のみで利用可能」とされており、機能レベルでは提供地域に制限が設けられている。</p>
]]></description>
      <pubDate>Sun, 07 Dec 2025 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>学術＆研究2025/12/14 [SUN]Google、AIに「長期記憶」を与える新アーキテクチャ「Titans」と設計理論のフレームワーク「MIRAS」を発表</title>
      <link>https://ledge.ai/articles/google_ai_long_term_memory_titans_miras</link>
      <description><![CDATA[<p>Google Researchは2025年12月4日、AIモデルに長期的な記憶能力を持たせるための新たなアーキテクチャ「Titans」と、その設計思想を統一的に説明するフレームワーク「MIRAS」を<a href="https://research.google/blog/titans-miras-helping-ai-have-long-term-memory/">発表</a>した。</p>
<p>Transformerを中心とする既存の大規模言語モデルが抱えてきた「長い文脈を扱うほど計算コストが増大し、重要な情報を保持しにくい」という課題に対し、推論時に記憶を学習・更新するという新しいアプローチを提示している。</p>
<h2>Transformerの限界と「記憶」の再定義</h2>
<p>TransformerはAttention機構によって高精度な依存関係のモデリングを可能にしてきた一方、計算量がコンテキスト長の二乗に比例するという制約を持つ。このため、極端に長い文書や時系列データを扱う際には効率面・性能面の両方で課題があった。</p>
<p>Google Researchは、Attentionを「短期記憶」、それを補完する仕組みとして「長期記憶」を明確に分離して設計する必要があると位置づけた。</p>
<h2>推論時に学習する「Titans」の中核</h2>
<p>Titansの中心となるのは「Neural Long-Term Memory Module」と呼ばれる長期記憶モジュールだ。このモジュールは、再学習を行うことなく、推論時に入力を受け取りながら記憶を更新する。単なるキー・バリューキャッシュとは異なり、過去の情報をモデル内部のパラメータとして蓄積できる点が特徴だ。</p>
<p><strong>■ Titansの全体構成：</strong> Attentionによる短期記憶に加え、推論時に更新されるNeural Memory（長期記憶）を統合。入力の重要度に応じて記憶を更新しつつ、固定パラメータとは独立して動作する。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Titans_1_Overview_width_1250_e2f30e7bc8/Titans_1_Overview_width_1250_e2f30e7bc8.png" alt="Titans-1-Overview.width-1250.png" /></p>
<p>Titansでは、長期記憶の統合方法として以下の3つの構成を提示している。</p>
<ul>
<li><strong>MAC（Memory as a Context）：</strong> 記憶を文脈としてAttentionに渡す方式</li>
<li><strong>MAG（Memory as a Gate）：</strong> 記憶によってAttention出力を制御する方式</li>
<li><strong>MAL（Memory as a Layer）：</strong> 記憶を独立したレイヤーとして組み込む方式</li>
</ul>
<p>特にMAC構成は、長距離依存関係を扱うタスクで高い性能を示したという。</p>
<h2>「驚き」に基づく選択的記憶と忘却</h2>
<p>Titansはすべての情報を無差別に記憶するのではなく、予測誤差が大きい、いわゆる「驚き（surprise）」の高いトークンを優先的に長期記憶へ反映する設計を採用している。
また、weight decayやモメンタムを用いた更新により、不要になった情報を忘却できる仕組みも備える。これにより、従来の線形RNNやゲート型モデルでは難しかった「完全な記憶の消去」も可能になるという。</p>
<h2>2Mトークン超でも性能を維持</h2>
<p>論文では、言語モデリング、常識推論、needle-in-a-haystackタスク、DNA解析、時系列予測など幅広いベンチマークで評価を実施。BABILongやRULERといった長文タスクでは、GPT-4やRAGを併用した大規模モデルを含む既存手法を上回る結果を示した。
有効なコンテキスト長は200万トークンを超えても性能が維持されることが確認されている。</p>
<h2>設計理論「MIRAS」が示す統一的枠組み</h2>
<p>MIRASは、Titansを含むさまざまなシーケンスモデルを「連想記憶システム」として捉え直すためのフレームワークだ。</p>
<p>設計要素を</p>
<ol>
<li>記憶構造</li>
<li>想起のための目的関数（attentional bias）</li>
<li>保持・忘却を制御するゲート</li>
<li>記憶の学習アルゴリズム</li>
</ol>
<p>の4点に整理し、Transformerや線形RNN、Titansを同一の理論枠組みで説明できるとしている。</p>
<p><strong>■ MIRASフレームワークの概念図：</strong> 記憶構造、想起基準（attentional bias）、保持・忘却（retention gate）、学習アルゴリズムの4要素で、TitansやTransformerを含むシーケンスモデルを統一的に説明する。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/miras_framework_47bb139255/miras_framework_47bb139255.jpg" alt="miras framework.jpg" /></p>
<p>MIRASを基に、Moneta、Yaad、Memoraといった新たなモデル群も提案されており、タスク特性に応じた記憶設計の可能性が示された。</p>
<h2>推論時学習という新たな方向性</h2>
<p>Google Researchは、TitansをMIRASフレームワークの具体例の一つと位置づけている。外部検索に依存するRAGや、巨大な固定コンテキストを用意する方法とは異なり、「推論時に学習する記憶」を内包したAI設計が、今後の長文・長期推論の重要な方向性となる可能性を示した形だ。</p>
]]></description>
      <pubDate>Sun, 07 Dec 2025 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/12/12 [FRI]ChatGPT、新モデル「GPT-5.2」公開──アルトマンCEO「最も賢い一般提供モデル」　推論・コード生成が大幅進化</title>
      <link>https://ledge.ai/articles/gpt_5_2_release_openai_chatgpt_update</link>
      <description><![CDATA[<p>OpenAIは2025年12月11日（現地時間）、ChatGPTおよびAPI向けの新モデル「GPT-5.2」を公開したと<a href="https://openai.com/ja-JP/index/introducing-gpt-5-2/">発表</a>した。同社の公式ブログ「Introducing GPT-5.2」では、推論能力や長文コンテキスト処理、コード生成、知識労働タスクなどの幅広い能力が前世代のGPT-5.1から大幅に向上したと説明している。</p>
<p>GPT-5.2は「 Instant」「Thinking」「Pro」 の3モデルで構成される。Instantは応答速度を重視した軽量モデル、Thinkingは複雑な数学・論理・科学タスクに特化した深い推論モード、Proは拡張コンテキストと高推論能力を備えた上位モデルとして位置付けられる。</p>
<p>ChatGPTでは、新たに「GPT-5.2 Auto」を導入し、InstantとThinkingをタスク内容に応じて自動で切り替える仕様となった。ユーザーは用途に応じてモデルを手動選択する必要がなく、質問の難易度に応じて適切な推論モードが選択される。</p>
<p>性能面では、GPT-5.2 ThinkingはGPT-5.1 Thinkingと比較して、匿名化されたChatGPTのクエリセットにおける「誤りを含む回答」が相対的に38%減少したとされる。加えて、UIコンポーネントの生成や複数ツールを組み合わせてコードを実行する、いわゆる「エージェント型コーディング」が安定し、問題設定から実装、検証までの一連の開発ワークフローを高精度に遂行できるようになったという。長文処理の安定性も向上した。</p>
<p><strong>GPT-5.2 Thinking の主要ベンチマーク結果：</strong> （SWE-Bench Pro、GPQA、ARC-AGI、GDPvalなど）。GPT-5.1比で複数指標が大幅に向上した。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/G76_Fh4_Wag_AAE_Ec_6b6e96de07/G76_Fh4_Wag_AAE_Ec_6b6e96de07.jpg" alt="G76Fh4WagAAE_Ec.jpg" /></p>
<h3>ARC-AGI-1でSOTAを更新、推論力の“質”を測る指標で高評価</h3>
<p>GPT-5.2の推論性能を示す指標のひとつが「ARC-AGI-1」だ。
ARC-AGI（Abstraction and Reasoning Corpus）は、人間が持つ抽象的な推論能力を測ることを目的に設計されたベンチマークで、単なる知識量やパターン暗記では解けない問題で構成されている。</p>
<p>具体的には
少数の例からルールを推測する
見慣れない問題に対して柔軟に考え方を切り替える</p>
<p>といった能力が求められ、「AIが本当に“考えているか”」を測るテストとして知られている。</p>
<p>このARC-AGI-1（Verified）において、GPT-5.2 Pro（X-High）は90.5%のスコアを記録した。この結果は第三者機関である <a href="https://x.com/arcprize/status/1999182732845547795">ARC Prize</a> によって検証されており、同団体は、1年前に検証された未公開モデルと比較して約390倍の効率改善が達成されたと評価している。</p>
<p>この結果は、GPT-5.2が単に正解率を積み上げたモデルではなく、未知の問題に対して抽象的に考え、解決策を導く能力が大きく向上していることを示している。</p>
<p><strong>ARC-AGI-1における各モデルのスコアとタスクあたりのコスト：</strong> GPT-5.2 Pro（X-High）は90.5%を記録し、推論能力とコスト効率の両面で最高水準に達した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/G76_GG_Zwag_A_Un7ub_b885ba305c/G76_GG_Zwag_A_Un7ub_b885ba305c.jpg" alt="G76GGZwagAUn7ub.jpg" /></p>
<p>知識労働タスクにおける性能も強化された。業界専門家の回答と比較評価する「GDPval」では、GPT-5.2 Thinkingが70.9%、GPT-5.2 Proが74.1%を記録し、前世代のGPT-5（38.8%）を大きく上回った。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/G76_Fi_Hka_AAE_Dmu_H_0ab7a9b29b/G76_Fi_Hka_AAE_Dmu_H_0ab7a9b29b.png" alt="G76FiHkaAAEDmuH.png" /></p>
<p>さらにGPT-5.2 Thinkingでは、スプレッドシートやプレゼンテーション資料など、「整形済みのファイル」を直接生成する機能も強化されている。OpenAIは、人員計画（ワークフォースプランニング）を例に、部門別のコスト計算や集計まで含めたスプレッドシートを自動生成する事例を公開した。</p>
<p><strong>左：GPT-5.2 Thinking が生成した部門別人員計画のスプレッドシート。集計・比較表まで自動生成されている。</strong>
<strong>右：複数部門のコスト、給与、採用費などが整形済み表として出力される。</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/G76_Fiaoag_AAH_i8_side_6039c883db/G76_Fiaoag_AAH_i8_side_6039c883db.jpg" alt="G76FiaoagAAH_i8-side.jpg" /></p>
<p>OpenAI CEOのサム・アルトマン氏は、自身のX（旧Twitter）でGPT-5.2について「現時点で一般提供されている中で最も賢いモデル」と述べ、特に知識労働タスクにおける性能向上を強調した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/GPT_5_2_is_here_by_sama_cbe335e446/GPT_5_2_is_here_by_sama_cbe335e446.jpg" alt="GPT-5-2 is here by sama.jpg" /></p>
<p>安全性面では、GPT-5系で用いられてきた既存の安全性フレームワークを引き継ぎつつ、GPT-5.2向けに調整した新版のシステムカードが公開されている。リスク緩和策や評価項目の更新が行われたとしている。</p>
<p>OpenAIはGPT-5.2を「GPT-5シリーズの中心モデル」と位置づけ、今後のChatGPTおよびAPIの基盤として、継続的な改善を進めていく方針だ。</p>
]]></description>
      <pubDate>Sun, 07 Dec 2025 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>NTT、次世代都市構想「光の街」始動──IOWNを都市に実装し、本社を2031年に日比谷へ移転</title>
      <link>https://ledge.ai/articles/ntt_iown_hikari_city_hibiya_hq_move_2031</link>
      <description><![CDATA[<p>NTT株式会社、NTTアーバンソリューションズ株式会社、NTT都市開発株式会社は2025年12月8日、次世代情報通信基盤「IOWN」を都市空間に実装し、街がテクノロジーとともに進化し続ける「光の街 powered by IOWN」構想を開始すると<a href="https://group.ntt/jp/newsrelease/2025/12/08/251208a.html">発表</a>した。本構想の第一弾は、2031年10月末に竣工予定の「NTT日比谷タワー」を中心に展開され、NTT本社も同タワーへの移転を予定している。</p>
<p>1961年に旧日本電信電話公社が本社を構えた日比谷の地に再び拠点を置くことで、NTTグループが蓄積してきた技術力を結集し、新たな価値創出と超・低消費電力化を実現することを目指すとしている。</p>
<h2>社会課題の深刻化とIOWNの役割</h2>
<p>日本では少子高齢化による人手不足、地球温暖化、自然災害の頻発に加え、AI・ロボティクスの普及による電力消費の増大が課題となっている。NTTは、膨大なデータを大容量・低遅延・低消費電力で処理できるIOWNを社会基盤として都市に実装することで、災害予測、インフラ制御、企業業務の効率化など、多領域での課題解決を進めるという。</p>
<h2>新しいビジネス・イノベーション</h2>
<p>IOWNが実装されるNTT日比谷タワーでは、世界中の企業とリアルタイムで協働できる環境を整備する。
通信とデータ処理を統合した基盤を活用することで、臨場感のあるコミュニケーションや高度な遠隔業務が可能になる。</p>
<p>また、NTTは業務支援の高度化に向けて、NTT版LLM「tsuzumi 2」や、大規模AI連携技術「AIコンステレーション」をIOWNと組み合わせることで情報検索や資料作成支援、国際的なコラボレーションなど、企業活動を支える多様なアプリケーションの展開を想定している。
将来的には、会議で生まれたアイデアをモデル化したり、必要な情報を即座に提示するなど、働く環境の高度化を支える技術として発展させる方針だ。</p>
<p>将来的な空間・時間を問わないビジネスシーンイメージ
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/251208ab_3cda72f446/251208ab_3cda72f446.jpg" alt="251208ab.jpg" /></p>
<h2>新たなライフスタイル・エンターテインメント</h2>
<p>日比谷タワー低層部に整備される大規模アトリウム「（仮称）Cross Gate」では、壁面・天井一体型の大型LEDビジョンを活用し、世界各地とリアルタイムでつながるイベント空間を形成する。</p>
<p>商業施設との連動イベントや多拠点同時発表会、ライブビューイングなど、多様な活用が可能。将来的には音響XR技術なども組み合わせ、より深い没入体験を提供する。</p>
<p>将来的な（仮称）Cross Gateでのエンターテイメントイメージ（左図：バーチャルアクアリウム、右図：他会場と融合したバスケットボール観戦）
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/251208ac_72eb58dead/251208ac_72eb58dead.jpg" alt="251208ac.jpg" /></p>
<h2>超・低消費電力化によるサステナビリティ</h2>
<p>都市全体の持続可能性に向け、NTTは以下の取り組みを明示している。</p>
<ul>
<li>オフィス部分で「ZEB Ready」を実現（従来比50％以上の省エネ）</li>
<li>光電融合デバイスによる消費電力削減</li>
<li>IOWN×AIの予測制御「Just Enough Energy」で10〜20％のCO₂削減</li>
<li>将来は光量子コンピュータや水素エネルギーの活用も検討</li>
</ul>
<h2>今後の展開</h2>
<p>内幸町一丁目街区は、官民連携や研究機関との共創が可能な次世代スマートシティへと整備が進む。NTT日比谷タワーで得られた知見は、周辺エリア、国内、さらには海外へと段階的に展開される予定だ。</p>
]]></description>
      <pubDate>Sun, 07 Dec 2025 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>アルトマン氏「エロティックばかり注目されたけど」──ChatGPT、成人ユーザーの自由拡大へ</title>
      <link>https://ledge.ai/articles/openai_chatgpt_adult_mode_update_oct2025</link>
      <description><![CDATA[<p>OpenAIのサム・アルトマンCEOは10月14日（現地時間）、X（旧Twitter）上で、ChatGPTの安全制限を一部緩和し、成人認証済みユーザーに対してエロティックな会話を許可する方針を<a href="https://x.com/sama/status/1978129344598827128">発表</a>
した。</p>
<p>投稿は瞬く間に注目を集め、「エロティック解禁」が大きな話題となったが、アルトマン氏は翌日に「その部分ばかり注目されてしまったが」と<a href="https://x.com/sama/status/1978539332215681076">補足</a>し、実際には“より人間らしいAI体験”を実現するための包括的な方針変更であることを強調した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/gpt5_erotica_7cea863241/gpt5_erotica_7cea863241.jpg" alt="gpt5 erotica.jpg" /></p>
<h2>安全を優先してきたChatGPTの制限</h2>
<p>ChatGPTはこれまで、性的表現や親密な会話を含むコンテンツを厳しく制限してきた。
アルトマン氏は「メンタルヘルス問題に慎重を期すためだった」と説明し、精神的に不安定なユーザーに配慮した措置であったと振り返った。
「深刻な危機状態にあるユーザーは別扱いとし、他者に害を与える行為は依然として許可しない」と述べ、ポリシーの根幹は維持されるとしたうえで、「リスクのない成人ユーザーにはより多くの自由を与える」と明言した。</p>
<h2>“4oらしさ”を再導入──人間的なAI体験へ</h2>
<p>アルトマン氏は同日、「数週間以内に“4oで好まれた振る舞い”に近い人格（パーソナリティ）を選べる新バージョンのChatGPTを提供する」と投稿した。
GPT-4oは会話の自然さや表情豊かな応答で人気を集めたモデルであり、今後はユーザーが望む場合に、フレンドリーな口調や絵文字を多用した“人間らしい”対話スタイルを選べるようになる。
アルトマン氏は「これは利用時間を増やすためではなく、ユーザーが自分の望む形でAIと関わる自由を得るための設計だ」と述べている。</p>
<h2>「成人は大人として扱う」──12月に年齢認証を本格導入</h2>
<p>アルトマン氏は、12月に年齢認証を本格導入し、認証済み成人ユーザーに対してはエロティック会話なども許可する方針を示した。
一方で、ティーンエイジャーに対しては「安全をプライバシーや自由より優先する」と述べ、メンタルヘルス関連ポリシーは緩めないと強調している。</p>
<p>アルトマン氏は、「社会がR指定映画で境界を設けるように、AIにも適切な年齢境界を設けたい」と例え、「我々は選挙で選ばれた道徳警察ではない」と付け加えた。</p>
<h2>倫理と自由の境界線</h2>
<p>アルトマン氏の発言は、AIにどこまで人間的な自由を与えるかという議論を再燃させた。</p>
<p>OpenAIは今後、成人向け表現やAIの人格設計に関するガイドラインをさらに明確化するとみられる。今回の方針転換は、「AIをどう設計し、どう育てるか」という人間社会全体のテーマに踏み込む第一歩となりそうだ。</p>
]]></description>
      <pubDate>Sun, 07 Dec 2025 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>リコー、「Gemma 3 27B」基盤の日本語LLMを開発──gpt-oss-20b同等性能を達成しつつ、PCサーバで動くオンプレ向けモデルとして企業提供を開始</title>
      <link>https://ledge.ai/articles/ricoh_gemma3_27b_japanese_llm_onprem_release</link>
      <description><![CDATA[<p>リコーは2025年12月8日、Google のオープンモデル「Gemma 3 27B」を基盤に、日本語向けに最適化した大規模言語モデル（LLM）を開発したと<a href="https://jp.ricoh.com/release/2025/1208_1">発表</a>した。同モデルは企業のオンプレミス環境での利用を想定しており、PCサーバ上で動作可能な規模に抑えつつ、日本語ベンチマークで OpenAI の「gpt-oss-20b」と同等スコアを記録したという。</p>
<h2>モデルマージと「Chat Vector」で性能を強化</h2>
<p>リコーは今回、独自のモデルマージ技術を活用し、ベースモデルである Gemma 3 27B に対して複数の「Chat Vector」を統合した。Chat Vectorは、約1万5,000件の指示チューニングデータから抽出した“指示追従能力”を表すベクトルで、これを組み合わせることで追加学習を行わずに対話性能を高める仕組みとなっている。</p>
<p>同社によると、非推論モデルでありながら初期応答性（TTFT）が短く、文書作成などの業務利用に適した特性を持つという。</p>
<p>@<a href="https://www.youtube.com/watch?v=mKftRMFEZYg">YouTube</a></p>
<h2>ベンチマークで「gpt-oss-20b」と同等水準</h2>
<p>評価には「Japanese MT-Bench」と「ELYZA-tasks-100」を使用。Google「gemma-3-27b-it」や Alibaba Cloud「Qwen3-32B」、OpenAI「gpt-oss-20b」などと比較し、リコーのモデルは平均スコアで OpenAI の gpt-oss-20bに近い水準を示した。</p>
<p>MT-Benchはコーディング、抽出、数学、推論、ライティングなど8分野の質問応答を、ELYZA-tasks-100は要約・意図汲み取り・複雑計算など100種類のタスクを評価対象とする。スコアはそれぞれ MT-Benchが10点満点、ELYZAが5点満点で、リコーは比較のため2倍換算した平均値で評価した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/LLM_cfe88d4fc3/LLM_cfe88d4fc3.jpg" alt="ベンチマークテストリコーLLM.jpg" /></p>
<h2>PCサーバで動作可能な27Bモデル、オンプレ向けに最適化</h2>
<p>モデル規模は270億パラメータと比較的コンパクトで、GPUリソースを大量に必要とする大規模モデルとは異なり、PCサーバなど既存のオンプレ環境で運用できる点が特徴となる。</p>
<p>リコーは同モデルを「低コストで導入できる日本語プライベートLLM」と位置づけており、データ主権やセキュリティ要件の高い企業での利用を想定している。</p>
<h2>PRIMERGYへプリインストール、Difyとともに提供</h2>
<p>2025年12月下旬から、エフサステクノロジーズの「Private AI Platform on PRIMERGY（Very Smallモデル）」に量子化済みモデルをプリインストールした形で提供を開始する。生成AIアプリをノーコードで構築できる「Dify」も組み込まれ、リコージャパンが環境構築済みの形で企業へ導入支援を行う。</p>
<p>Difyを利用すれば、FAQ対応、ナレッジ検索、文書要約などの業務アプリケーションを、専門知識なしに構築できるとしている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/1208_1_60d9384d72/1208_1_60d9384d72.webp" alt="1208_1.webp" /></p>
<p>リコーは2021年から自然言語処理を活用した文書分析サービスを提供し、2022年以降は独自LLM開発を本格化。700億パラメータの大規模モデル、指示追従性を高めたInstructモデル、モデルマージによる高速開発手法など、LLM関連の取り組みを継続してきた。</p>
<p>今回のGemma 3 27Bベースモデルは、その延長線上で「オンプレミスで利用できる高性能日本語LLM」というニーズを反映したものになる。</p>
]]></description>
      <pubDate>Sun, 07 Dec 2025 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>米TIME誌、「Person of the Year 2025」に「The Architects of AI」──世界を動かした8人の“AI設計者たち”</title>
      <link>https://ledge.ai/articles/time_person_of_the_year_2025_architects_of_ai</link>
      <description><![CDATA[<p>米TIME誌は2025年12月11日（現地時間）、その年に最も大きな影響を与えた存在として、「The Architects of AI（AIの設計者たち）」を「Person of the Year 2025」に選出したと<a href="https://time.com/7339685/person-of-the-year-2025-ai-architects/">発表</a>した。</p>
<p>選出は特定の1名ではなく、AIを構想し、設計し、社会へと実装してきた中核的な担い手を集合体として評価する選出となった。</p>
<h2>AIが“止められなくなった”年</h2>
<p>TIME誌は、2025年をAIの潜在力が全面的に可視化され、「後戻りも、参加しないという選択もできなくなった年」と位置づける。医療研究や科学分野での発見、生産性の向上、ソフトウェア開発や創作活動の加速など、AIは短期間で広範な領域に浸透した。AIの能力はかつてない速度で向上し、社会のあらゆる場面で存在感を示すようになった。</p>
<h2>就任式の裏で起きていたこと</h2>
<p>象徴的な出来事として、TIME誌は2025年1月のトランプ大統領就任式当日を挙げる。その日、中国の新興AI企業DeepSeekが新モデルを公開し、市場を動揺させた。翌日には、サム・アルトマン氏、ラリー・エリソン氏、孫正義氏らがホワイトハウスで、最大5000億ドル規模のAIデータセンター投資計画「Stargate」を発表した。これらの出来事は、AIを巡る国際競争、巨額投資、官民の利害が交錯する1年の幕開けを象徴していた。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/stargate_project_5b563d54c6/stargate_project_5b563d54c6.jpg" alt="stargate project.jpg" /></p>
<h2>8人が並ぶ理由</h2>
<p>今回の表紙は、特定の英雄を強調する構図ではない。生成AIのモデル開発、計算基盤、研究、社会実装といった異なる役割を担う人物が、同じ高さで横一列に並んで描かれている。TIME誌は、AIの進化が一人の天才や一社の成果ではなく、複数の層にわたる設計判断と意思決定の積み重ねであることを、この集合肖像で示しているという。</p>
<h3>表紙に描かれた顔ぶれ</h3>
<p>TIME誌の表紙には、「Architects of AI」を象徴する人物として、以下の8人が描かれている。</p>
<ul>
<li>Meta CEO：マーク・ザッカーバーグ氏</li>
<li>AMD CEO：リサ・スー氏</li>
<li>xAI創業者：イーロン・マスク氏</li>
<li>NVIDIA CEO：ジェンスン・フアン氏</li>
<li>OpenAI CEO：サム・アルトマン氏</li>
<li>Google DeepMind CEO：デミス・ハサビス氏</li>
<li>Anthropic CEO：ダリオ・アモデイ氏</li>
<li>スタンフォード大学教授／World Labs CEO：フェイフェイ・リー氏</li>
</ul>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/time_magazine_person_of_the_year_ai_2025_003_e2b05e7953/time_magazine_person_of_the_year_ai_2025_003_e2b05e7953.webp" alt="time-magazine-person-of-the-year-ai-2025-003.webp" /></p>
<h2>加速する現実</h2>
<p>2025年、AIの進展は技術領域にとどまらず、経済や政治とも強く結びついた。膨大な電力を消費するデータセンター建設が各地で進み、雇用構造の変化や誤情報の拡散、サイバーリスクの拡大も現実味を帯びた。権力と資本が少数の企業や経営者に集中する状況は、TIME誌が「金ぴか時代以来」と表現するほどの規模に達している。</p>
<h2>かつてPCが選ばれたように</h2>
<p>TIME誌の「Person of the Year」は、個人だけでなく、時代を画した概念を選んできた。1980年代のパーソナルコンピュータ、2006年の「You（あなた）」に続き、今回はAIという新たな時代の中核を担う存在が選ばれた。ソーシャルインターネットの時代から、AIが社会を形作る時代への移行点として、2025年が位置づけられている。</p>
<p>2025年はAIが実験段階や一部の先進的用途を超え、社会インフラの一部として機能し始めた年だった。生成AIは業務効率化やソフトウェア開発、研究活動だけでなく、映像・音楽・文章といった創作領域にも本格的に浸透した。</p>
<p>同時に、AIを巡る倫理、著作権、規制、雇用への影響といった課題も顕在化し、各国政府や国際機関による対応が進んだ。TIME誌は、こうした恩恵とリスクの双方を生み出した中心に「AIの設計者たち」が存在するとして、今年の<a href="https://time.com/7339621/person-of-the-year-2025-ai-architects-choice/">選出理由</a>に挙げている。</p>
<h2>設計者の時代</h2>
<p>同誌は、AIの未来は技術そのものではなく、それを設計し、運用し、社会に組み込む人間の選択に委ねられていると強調する。2025年は、その設計者たちが世界史の表舞台に立った年だった。称賛と不安の双方を伴いながら、AIはもはや一部の分野の話題ではなく、世界を動かす力となっている。</p>
<h2>AI時代を象徴する選出に</h2>
<p>「The Architects of AI」が選ばれたことで、2025年はAIが単なる技術トレンドではなく、社会の基盤を形作る存在として認識された年として刻まれることになった。</p>
<p>TIME誌は今回の選出を通じて、AIの進化が偶然ではなく、数多くの設計上の選択と意思決定の積み重ねによって生まれていることを強調している。AI時代の行方を左右するのは、技術そのものではなく、それを設計し、運用し、社会に組み込む人間であるというメッセージが込められている。</p>
<p>:::box
[関連記事：AIの\</p>
]]></description>
      <pubDate>Sun, 07 Dec 2025 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>チューリング、都内30分の完全自動運転「Tokyo30」を達成　テスラ猛追へ「富岳」級の計算基盤も公開</title>
      <link>https://ledge.ai/articles/turing_tokyo30_full_self_driving_achievement</link>
      <description><![CDATA[<p>完全自動運転を目指すスタートアップのチューリング株式会社は2025年12月1日、同社の技術カンファレンス「<a href="https://www.youtube.com/watch?v=HhQr0SAZs3Y">Turing AI Day 2025</a>」で、東京都内を人間の操作なしで30分以上走行するプロジェクト「Tokyo30」を達成したと発表した。当日は実際の走行映像も公開され、開発中のEnd-to-End（E2E）自動運転システムの挙動が披露された。</p>
<p>同社は創業当初より「We Overtake Tesla（テスラを超える）」をミッションとして掲げており、今回の成果を国産の完全自動運転に向けた重要なマイルストーンの一つと位置づけている。</p>
<h2>「カンブリア爆発」を経て実用域に近づくE2Eモデル</h2>
<p>AI Day の冒頭で公開された映像では、チューリングの実験車両が、市街地の複雑な交通環境下でハンドル・アクセル・ブレーキのすべてをAIが制御し、30分以上連続して走行する様子が確認できた。信号停止、右左折、歩行者や車両との交錯といった場面でも、安定した挙動を維持していた。</p>
<p>同社は創業時から「Day 1 から E2E」を掲げ、カメラ映像を入力とし、単一のニューラルネットワークが走行経路（トラジェクトリ）を直接出力する方式を採用している。データ収集から学習までのパイプラインを整備した段階でモデル性能が急激に向上した時期を、社内では「カンブリア爆発」と呼んでおり、Tokyo30 はその延長線にある成果だと説明した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/turing_ai_day20251_393e19e328/turing_ai_day20251_393e19e328.jpg" alt="turing ai day20251.jpg" /></p>
<h2>富岳の約40%相当の計算基盤が学習を支える</h2>
<p>E2Eモデルの性能向上を支えるのが、大規模GPUクラスタである。チューリングは自社専用の「Gaggle Cluster」を中心に、オンプレミスとクラウドを組み合わせて学習基盤を構築しており、2025年12月時点の総演算性能は、スーパーコンピュータ「富岳」のAI演算性能（FP16換算）の約40%に相当すると説明した。</p>
<p>同社はシリーズAラウンドの1st Closeとして152.7億円（約153億円）を調達しており、多くを計算基盤の拡充やデータ拡張、組織体制の強化に投じる計画だという。AI Day では、今後2年間で計算能力を5〜10倍（最大7 ExaFLOPS規模）へ拡張する方針も示された。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/turing_ai_day20252_608dfde61a/turing_ai_day20252_608dfde61a.jpg" alt="turing ai day20252.jpg" /></p>
<h2>VLAモデル・世界モデルへと発展する次世代アプローチ</h2>
<p>AI Day 2025 では次世代アプローチとして、視覚（Vision）・言語（Language）・行動（Action）を統合する「VLAモデル」や、未知の走行シーンを生成できる「世界モデル（World Model）」の研究状況が紹介された。</p>
<p>チューリングはこれまでに、日本語VLM「Heron」や運転QAデータセット「Stride-QA」「COBRA」を独自に構築しており、今後はこれらの技術を統合した70B級（700億パラメータ級）規模の「フロンティアモデル」を開発。そのうえで、車載向けに蒸留したE2Eモデルを量産レベルで展開する構想を掲げる。</p>
<h2>2030年の完全自動運転の商用化を見据える</h2>
<p>Q&amp;Aセッションでは、2030年前後にハンドルのない完全自動運転車が市場に登場する可能性が高いとの見通しが示された。チューリングは Tesla や Waymo など先行企業の技術進展を踏まえつつ、高速で追随する「セカンドムーバー・アドバンテージ」を戦略の中核に据えている。</p>
<p>短期的には、ドライバーが「安心して身を預けられるレベル」の挙動を実現することを目標とし、介入頻度の削減と危険シーンの排除に取り組むとした。安全性評価については、従来のISO規格だけではE2E型システムの特性を十分に捉えられないとして、3D Gaussian Splatting や世界モデルを活用した新たな検証アプローチの重要性が言及された。</p>
]]></description>
      <pubDate>Sun, 07 Dec 2025 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google傘下Waymoのロボタクシー、停車中のスクールバス追い越しに関する安全違反で規制当局（NHTSA）調査―—自動運転システムのソフトウェアを自主リコール</title>
      <link>https://ledge.ai/articles/waymo_robotaxi_school_bus_software_recall</link>
      <description><![CDATA[<p>Google傘下で自動運転事業を手がけるWaymoのロボタクシーが、停車中のスクールバスを追い越す際に交通安全法規を順守していなかったとして、米運輸省道路交通安全局（NHTSA：The National Highway Traffic Safety Administration）が<a href="https://static.nhtsa.gov/odi/inv/2025/INIM-PE25013-30896.pdf">調査</a>を実施している。これは、2025年12月6日の<a href="https://www.reuters.com/world/waymo-issue-recall-over-self-driving-vehicles-driving-past-stopped-school-buses-2025-12-05/">Reuters</a>などの報道から明らかになった。Waymoは問題となった挙動を認め、対象となる自動運転システムのソフトウェアを自主的にリコールするという。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/waymo_reuters_2aa02403a1/waymo_reuters_2aa02403a1.jpg" alt="waymo reuters.jpg" /></p>
<p>NHTSAの調査は、スクールバスの赤色灯や停止アームが作動している状況下で、Waymoのロボタクシーが本来停止すべき場面でも走行を継続したとされる事例を受けて行われている。調査を担当する同局の欠陥調査局（ODI）は、こうした挙動が各州の交通安全法規に抵触する可能性があるとして、Waymoに対して詳細な情報提供を求めている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/nhtsa_waymo_odi_ade67f60cb/nhtsa_waymo_odi_ade67f60cb.jpg" alt="nhtsa waymo odi.jpg" /></p>
<p>調査の過程でWaymoは、特定の条件下において自動運転ソフトウェアがスクールバス周辺の交通ルールを適切に反映できていなかったことを認めた。これを受け、同社は車両の機械的な欠陥ではなく、制御ソフトウェアの問題として是正措置を講じる方針を示し、ソフトウェア更新をリコールとして届け出る対応を取った。</p>
<p>NHTSAによると、今回の件は予備評価（Preliminary Evaluation）の対象となっており、Waymoは指定された期限までに、問題が発生した具体的な事例数、影響を受けた車両、ソフトウェア修正の内容や実施状況などについて説明する必要がある。調査は継続中で、当局は提出された情報をもとに、追加対応の必要性を判断する。</p>
<p>Waymoは、ロボタクシーの安全性を最優先事項として掲げており、規制当局と連携しながらソフトウェアの改善を進めるとしている。今回のリコールは強制措置ではなく、Waymo側の判断による自主的な対応だという。</p>
]]></description>
      <pubDate>Sun, 07 Dec 2025 04:50:00 GMT</pubDate>
    </item>
  </channel>
</rss>