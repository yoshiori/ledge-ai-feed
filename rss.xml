<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
  <channel>
    <title>Ledge.ai 新着記事</title>
    <link>https://ledge.ai/</link>
    <description>Ledge.ai の最新テクノロジー記事</description>
    <item>
      <title>Google DeepMind、「SIMA 2」研究プレビュー公開──Gemini搭載で推論・一般化・自己改善が進化、生成3D世界にも対応</title>
      <link>https://ledge.ai/articles/google_deepmind_sima_2_research_preview</link>
      <description><![CDATA[<p>米Google DeepMindは2025年11月13日（現地時間）、次世代汎用AIエージェント「SIMA 2：An Agent that Plays, Reasons, and Learns With You in Virtual 3D Worlds」の研究プレビューを<a href="https://deepmind.google/blog/sima-2-an-agent-that-plays-reasons-and-learns-with-you-in-virtual-3d-worlds/">公開</a>した。</p>
<p>2024年に発表されたSIMA1を基盤に、同社の大規模マルチモーダルモデル「Gemini」を中核へ統合することで、指示に従うだけのエージェントから、推論し、対話し、環境に適応し、自己改善する汎用エージェントへと大きく進化した。</p>
<p>DeepMindは本モデルを「Artificial General Intelligence（AGI）に向けた重要なステップ」と位置づけ、限定的な研究プレビューとして学術機関やゲーム開発者に先行提供する。</p>
<p>@<a href="https://www.youtube.com/watch?v=Zphax4f6Rls">YouTube</a></p>
<h2>未学習ゲームでもタスクを遂行：一般化性能が大幅に向上</h2>
<p>Geminiの統合により、SIMA 2の一般化性能は大きく向上した。公式ブログでは、訓練に使用していないゲーム環境での成功例が報告されている。</p>
<ul>
<li>ASKA（Vikingサバイバルゲーム）</li>
<li>MineDojo（Minecraft研究実装）</li>
</ul>
<p>これらの未学習ゲームでも、SIMA 2は長く複雑な指示に基づきタスクを遂行。評価タスクでは、SIMA1との性能差が大きく縮まり、人間プレイヤーに近づく結果が示された。</p>
<p><strong>MineDojoでのタスク比較：</strong> 訓練していないゲーム環境でも、SIMA 2（右）は抽象的指示を解釈し行動に移しタスク達成。一方、SIMA 1（左）はタスク達成に至らなかった。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/comparison_between_sima1_2_on_minedojo_6bec24bce2/comparison_between_sima1_2_on_minedojo_6bec24bce2.jpg" alt="comparison between sima1-2 on minedojo.jpg" /></p>
<p>また、以下のようなマルチモーダル指示にも対応する。</p>
<ul>
<li>スケッチ（ユーザーの手描き図）</li>
<li>多言語指示</li>
<li>絵文字のみの指示→ それぞれを正しく解釈し、行動に移すことが可能。</li>
</ul>
<p>さらに、ゲームAで学んだ“mining（採掘）”の概念を、ゲームBでの“harvesting（収穫）”に応用するなど、概念レベルでの転移学習も確認された。</p>
<h2>完全に新しい“生成された3D世界”でも行動可能：Genie 3との連携</h2>
<p>SIMA 2は、DeepMindの世界生成モデル「Genie 3」と組み合わせた実験でも高い適応力を示した。</p>
<p>Genie 3は、1枚の画像やテキストからリアルタイムで3D世界を生成するモデルで、SIMA 2は初めて見る環境にもかかわらず、</p>
<ul>
<li>方角を把握</li>
<li>ゴールに向けた行動</li>
<li>指示の理解とタスク遂行</li>
</ul>
<p>を実現した。</p>
<p>DeepMindはこの試験を「一般化能力の限界を試すもの」として位置づけ、従来の固定されたゲーム環境では確認できない柔軟性が明らかになったとしている。</p>
<h2>自己改善ループ：人間データに依存しない継続学習へ</h2>
<p>SIMA 2の大きな特徴の一つが、新たに導入された自己改善（self-improvement）サイクルだ。</p>
<p>Geminiが生成するタスクと推定報酬をもとにエージェントが試行を行い、その経験データを蓄積。それらのデータは次世代のSIMA 2の訓練に再利用される。</p>
<p><strong>SIMA 2 の自己改善サイクル：</strong> Gemini がタスクと推定報酬を生成し、エージェントの行動経験が「Self-Generated Experience」として蓄積され、次世代モデルの学習に再利用される。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/unnamed_2_fa5f0652e0/unnamed_2_fa5f0652e0.webp" alt="unnamed (2).webp" /></p>
<h3>自己改善サイクルの流れ</h3>
<ul>
<li>Geminiがタスクと推定報酬を生成</li>
<li>SIMA 2が試行（行動）</li>
<li>経験データを「Self-Generated Experience」として蓄積</li>
<li>次の世代のSIMA 2を再訓練（スキル向上）</li>
</ul>
<p>このループにより、人間のデモなしで未学習ゲームの能力が向上し、Genie 3で生成された新規世界でも“自己のみで上達する”ことが確認された。DeepMindはこれを「オープンエンドな学習へ向けたマイルストーン」としている。</p>
<h2>現時点の課題</h2>
<p>SIMA 2は研究段階であり、DeepMindは以下の課題を明示している。</p>
<ul>
<li>長時間・多段階の推論タスクは依然困難</li>
<li>コンテキスト保持（メモリウィンドウ）が短い</li>
<li>精確なキーボード／マウス操作が難しい</li>
<li>複雑な3Dシーンの視覚理解はまだ不十分</li>
</ul>
<p>これらはロボティクスや実世界の応用に向け、今後解決すべき点として挙げられている。</p>
<h2>ロボティクス・AGIへの展開</h2>
<p>DeepMindは、SIMA 2が取得する以下の能力を、物理世界のロボットに必要な基礎スキルと位置づける。</p>
<ul>
<li>ナビゲーション</li>
<li>道具操作</li>
<li>協働タスクの遂行</li>
</ul>
<p>3D仮想世界はロボット学習の“安全かつ多様な練習場”として使えることから、SIMA 2の進化はロボティクス領域にも直結する。</p>
<h2>責任ある開発と限定公開</h2>
<p>SIMA 2の自己改善能力は強力であるため、DeepMindは開発初期から同社のResponsible Development &amp; Innovation Teamと連携している。</p>
<ul>
<li>公開形態は 「限定研究プレビュー」</li>
<li>対象：少数の学者・ゲームスタジオ</li>
<li>目的：フィードバック収集とリスク理解の深化</li>
</ul>
<p>SIMA 2の技術レポート（SIMA Technical Report）は近日公開予定とされる。</p>
<p>:::box
[関連記事：AIの\\</p>
]]></description>
      <pubDate>Tue, 18 Nov 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Sakana AI、シリーズBで総額200億円の資金調達を発表──MUFGなど国内外投資家が出資、企業価値は約4,000億円に</title>
      <link>https://ledge.ai/articles/sakana_ai_series_b_200oku_fundraising</link>
      <description><![CDATA[<p>東京を拠点とするAI企業のSakana AIは2025年11月17日、シリーズBラウンドで総額約200億円（1億3,500万米ドル）を調達したと公式ブログで<a href="https://sakana.ai/series-b/">発表</a>した。</p>
<p>今回の資金調達により、ポストマネー評価額は約4,000億円（26億3,500万米ドル）、累計調達額は約520億円（3億4,700万米ドル）となる。</p>
<h2>国内外の投資家が参加、金融・エネルギー・政府系機関まで多様な顔ぶれ</h2>
<p>Sakana AIによれば、本ラウンドには既存・新規の投資家が幅広く参加した。既存投資家では、三菱UFJフィナンシャル・グループ（MUFG）、Khosla Ventures、New Enterprise Associates（NEA）、Lux Capitalなどが追加出資した。</p>
<p>新規投資家としては、米Factorial Funds、豪Macquarie Capital、スペインのSantander Group（VCファンドMouro Capitalを通じて出資）、米国政府の戦略投資機関In-Q-Tel（IQT）などが名を連ねる。このほか、Fundomo、Geodesic Capital、Ora Global、MPower Partners、日本の四国電力グループのSTNetなども参画し、国内外の多様な領域から支持を得た形だ。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/group_logo_e1f5e12cc1/group_logo_e1f5e12cc1.jpg" alt="group-logo.jpg" /></p>
<h2>「計算資源依存ではないAI開発」への問題意識</h2>
<p>同社はリリースで、世界的なAI開発競争が「大量の計算資源の投入」を前提にしている点に強い問題意識を示した。計算コストやエネルギー消費が増大する中、資源に限りがある日本にとって同じ競争路線を踏むのは現実的ではないと指摘。「自然界では、限られた資源の中で効率性を高めて進化してきた。AI開発も持続可能性を重視すべきだ」との立場を示した。</p>
<p>そのうえで、Sakana AIは「計算量を前提にした“大規模モデルの総当たり競争”」とは異なるアプローチとして、既存モデルを進化的に組み合わせる技法や、モデルが自ら改善する仕組みの研究に取り組んできたと説明している。</p>
<h2>R&amp;Dの進捗：自己進化モデルから新アーキテクチャまで</h2>
<p>設立から約2年で、同社はフロンティア研究と応用の両面で複数の成果を公開してきた。</p>
<ul>
<li><strong>Darwin Gödel Machine (DGM)</strong> ：モデルとコードが自己改善するアーキテクチャ</li>
<li><strong>ShinkaEvolve</strong> ：LLM生成プログラムを進化させるオープンソースシステム</li>
<li><strong>進化的モデルマージ</strong> ：複数のオープンソースモデルを融合</li>
<li><strong>AB-MCTS</strong> ：複数モデルが協調して推論するアルゴリズム</li>
<li><strong>Continuous Thought Machine (CTM)</strong> ：Transformer以降を見据えた新アーキテクチャ</li>
<li><strong>The AI Scientist</strong> ：仮説生成から論文執筆まで行う自律マルチエージェントシステム</li>
</ul>
<p>これらはいずれも「持続可能かつ効率的なAI」の実現を指向した研究群だと説明している。</p>
<h2>日本企業との社会実装──金融・防衛・製造へ拡大</h2>
<p>Sakana AIは2025年に、三菱UFJフィナンシャル・グループ（MUFG）、大和証券グループとの戦略的パートナーシップを発表し、金融領域でのカスタムAI開発を進めてきた。</p>
<p>一方で、実務には一般的な生成AIでは扱いにくい「暗黙知」が多く存在するとして、業務に深く入り込むエンジニアリングが不可欠と指摘。この課題認識のもと、同社は応用領域を金融から、防衛・製造業などの基幹産業へと広げている。</p>
<p>防衛・インテリジェンス分野では、総務省の「インターネット上の偽・誤情報対策技術の開発・実証事業」（2025年6月採択）などにも参加している。</p>
<h2>ソブリンAI──日本に特化した「事後学習（Post-training）」への集中</h2>
<p>同社は、AI開発の主戦場が「事前学習（Pre-training）」から「事後学習（Post-training）」へ移行していると整理。事後学習に注力することで、日本固有の文化・価値観・産業ニーズに即したモデルを構築でき、米中の巨大モデル開発競争とは異なるルートでソブリンAIを実現できると強調した。</p>
<p>今回調達した資金は、</p>
<ul>
<li>フロンティア研究の加速（集合知・自己進化・ソブリンAI向け最適化）</li>
<li>Applied Teamの体制強化</li>
<li>金融、防衛、製造領域での社会実装</li>
<li>技術・事業基盤拡大のための投資やM&amp;A</li>
</ul>
<p>などに充てる。</p>
<h2>「日本発の持続可能なAI」へ</h2>
<p>Sakana AIは「世界最先端のAI技術を日本で社会実装する」というミッションを掲げており、東京を拠点に研究・ビジネス・コーポレート部門の採用を強化している。</p>
<p>同社は今回のシリーズBを通じ、効率性・持続可能性・社会実装を軸とした「日本らしいAI開発」をさらに推し進めるとしている。</p>
]]></description>
      <pubDate>Mon, 17 Nov 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Anthropic、米AIインフラに500億ドル投資──テキサス＆ニューヨークで専用データセンター建設へ</title>
      <link>https://ledge.ai/articles/anthropic_us_ai_infrastructure_50b_investment</link>
      <description><![CDATA[<p>Anthropicは2025年11月12日（現地時間）、米国内のAI計算インフラに総額500億ドル（約7.7兆円）を投資し、テキサス州とニューヨーク州で次世代データセンターを建設すると<a href="https://www.anthropic.com/news/anthropic-invests-50-billion-in-american-ai-infrastructure">発表</a>した。</p>
<p>AIインフラ企業である英Fluidstackと共同で進める計画で、2026年を通じて順次稼働を開始する見通しだ。今回の計画は、同社の生成AI「Claude」シリーズとフロンティア研究を支える基盤を強化することを目的とする。</p>
<p>Anthropicは、データセンター建設により約800名の常勤職と2,400名の建設関連職を創出すると説明。テキサスとニューヨークでの新拠点に加え、今後さらに追加サイトを設ける予定も示した。</p>
<p>同社CEOのDario Amodei氏は、AIが科学・産業の領域で重要な発見をもたらす段階に近づいているとし、「こうした可能性を実現するためには、フロンティア研究を支える強力な計算インフラが必要だ」と述べた。急速に増加するClaude利用企業の需要に対応し、より高度なモデル開発を継続するための投資であるとしている。</p>
<p>提携先Fluidstackについては、ギガワット級の電力容量を迅速に提供できる俊敏性を評価したと説明。同社CEOのGary Wu氏は「私たちはこの瞬間のために準備してきた。AnthropicのようなフロンティアAI企業と協力し、大規模インフラを展開できることを誇りに思う」と述べている。</p>
<p>今回の発表では、米国の「AI Action Plan」にも言及されており、同社の投資が国内テクノロジーインフラの強化に資するとの見解を示した。Anthropicは今後も、費用対効果を重視した形で米国内インフラを拡張していく方針だ。</p>
]]></description>
      <pubDate>Mon, 17 Nov 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>三菱UFJ、ChatGPTに“銀行アプリ”を接続へ──「Apps in ChatGPT」連携で家計管理・資産運用を会話で完結</title>
      <link>https://ledge.ai/articles/mufg_chatgpt_apps_integration</link>
      <description><![CDATA[<p>三菱UFJフィナンシャル・グループ（MUFG）と三菱UFJ銀行は2025年11月12日、OpenAIと戦略的コラボレーション契約を締結したと<a href="https://www.mufg.jp/dam/pressrelease/2025/pdf/news-20251112-001_ja.pdf">発表</a>した。両社は、OpenAIが10月に公開した「Apps in ChatGPT」を活用し、MUFGのアプリをChatGPTに接続する取り組みを進める。対話を通じた家計管理や資産運用の相談など、金融関連の行動をChatGPT上で実行可能にする構想を示した。</p>
<h2>ChatGPT上で家計管理・資産運用の相談を可能に</h2>
<p>MUFGによると、Apps in ChatGPTとの連携は、リテール領域で推進する4つの施策の一つとして位置づけられる。ChatGPTの対話基盤にMUFG各社のアプリを接続し、ユーザーの状況に応じた家計関連情報の確認や、資産運用に関する相談を行えるようにする。
アプリを切り替えることなく、ChatGPT上で一連の操作が完結できる仕組みを目指す。</p>
<p>また、グループのアプリに蓄積されるデータを統合し、利用者ごとの状況に応じた情報提供の高度化も検討している。</p>
<h2>「エムット」ブランド内で複数施策を展開</h2>
<p>MUFGはリテールサービスブランド「エムット」の一環として、Apps in ChatGPT連携に加えて以下の施策も進める。</p>
<ul>
<li>AIコンシェルジュ：グループ各社のアプリにAIを組み込み、パーソナライズされたサポートを提供。デジタルバンクへの導入も予定。</li>
<li>エムットクイックスタート：口座開設やサービス申し込みの案内をAIチャットで行う仕組み。</li>
<li>Agentic Commerce対応：ChatGPT上で検索から決済まで行える仕組みに対応し、MUFGの決済サービスを連携可能とする取り組み。</li>
</ul>
<p>これらの施策を組み合わせ、リテール領域におけるAI活用の強化を図る。</p>
<h2>全行員3.5万人にChatGPT Enterpriseを展開</h2>
<p>MUFGは2026年1月以降、三菱UFJ銀行の全行員約3.5万人にChatGPT Enterpriseを順次展開する計画を示した。文書作成、調査、顧客対応、分析業務など、行内の幅広い業務で利用する。</p>
<p>さらに、両社は四半期ごとに戦略やプロダクトのレビューを実施し、OpenAIの最新モデルや機能を早期に導入できる体制を構築する。MUFGが社内外で推進するAI活用施策に、最新技術を反映しやすくする環境を整える。</p>
<h2>AI活用の全社的取り組みを継続</h2>
<p>MUFGは、全社員を対象としたAI浸透プログラム「Hello, AI @MUFG」を通じて教育や研修を行い、AIを活用できる人材（AIチャンピオン）の育成を進めている。今回の協業では、こうした社内施策に加え、四半期ごとの戦略レビューによりOpenAIの最新モデルや機能を随時取り込み、行内業務とリテール領域の双方でAI活用の取り組みを拡大していく計画を示した。</p>
]]></description>
      <pubDate>Mon, 17 Nov 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google、NotebookLMをリサーチ基盤に強化──Deep Research＋Drive/Sheets/Word対応でWorkspaceと統合加速</title>
      <link>https://ledge.ai/articles/notebooklm_deep_research_workspace_integration</link>
      <description><![CDATA[<p>Googleは2025年11月13日（米国時間）、AIリサーチツール「NotebookLM」にオンライン調査を自動化する新機能「Deep Research」を導入し、あわせてGoogle SheetsやGoogle Drive上のPDF、Microsoft Word（.docx）、画像など複数のファイル形式に対応したと<a href="https://blog.google/technology/google-labs/notebooklm-deep-research-file-types/">発表</a>した。NotebookLMはユーザーの手元資料とウェブ上の情報を横断して扱える“調査エージェント”へと進化する。</p>
<h2>NotebookLMが“調査を代行するAI”に進化</h2>
<p>Deep Researchは、NotebookLMがユーザーの代わりに複雑な調査タスクを実行するエージェント機能だ。質問を入力するとNotebookLMがリサーチプランを自動生成し、何百ものウェブサイトを巡回しながら検索内容を絞り込み、数分で構造化レポートを作成する。</p>
<p>特徴は、レポートを作って終わりではない点にある。
生成されたレポートと参照したソースは、そのままNotebookLMのノートブックに取り込める。さらに調査はバックグラウンドで進み続け、ユーザーは同時に別の資料を追加してナレッジベースを拡張できる。NotebookLMの音声要約（Audio Overview）や動画要約（Video Overview）機能とも組み合わせられ、調査テーマの理解を深めやすくなる。</p>
<p>また、Googleは「特定のサイトを指定して検索させる」ことも可能としており、調査対象の精度も調整できる。</p>
<h2>Fast Research と Deep Research、選べる2つの調査モード</h2>
<p>オンライン調査は、「Web」をソースとして選んだときに開始できる。
Googleは利用シーンに応じて以下の2スタイルを用意した。</p>
<ul>
<li><strong>Fast Research</strong> ：すぐに情報を把握したいときの高速モード。短時間で情報をスキャンし、必要なソースを素早く取り込める。</li>
<li><strong>Deep Research</strong> ：詳細なブリーフィングを生成したい場面に向く。高品質な記事・論文を見つけるために深く分析し、処理中もユーザーはNotebookLMで他の作業を続けられる。</li>
</ul>
<p>オンライン調査の“強度”を任意にコントロールできる点が従来のNotebookLMから大きく進化したポイントだ。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/notebooklm_deepresearch_509355bfb0/notebooklm_deepresearch_509355bfb0.jpg" alt="notebooklm deepresearch.jpg" /></p>
<h2>対応ファイル形式を拡大：Sheets、Drive URL、画像、PDF、.docx</h2>
<p>Googleは、ユーザーの調査ワークフローが複数の形式に分散している現状に合わせ、NotebookLMが扱えるソースタイプを大幅に拡張した。今回対応したのは以下の5種類。</p>
<ul>
<li><strong>Google Sheets</strong> ：スプレッドシートを直接取り込み、統計情報の抽出や表データの要約が可能</li>
<li><strong>DriveファイルのURL</strong> ：Drive上のファイルをURLとして追加でき、通常のウェブページ同様に複数リンクを一度に貼り付けられる</li>
<li><strong>画像（Images）</strong> ：手書きノートの写真やパンフレットの画像をアップロードし、内容を解析してテキスト化</li>
<li><strong>Google Drive上のPDF</strong> ：Driveに保存した論文やレポート、電子書籍などを直接NotebookLMで分析できる</li>
<li><strong>Microsoft Word（.docx）</strong> ：Word文書をアップロードして、ドラフトやメモをそのまま調査ソースとして扱える</li>
</ul>
<p>表データ、文書、PDF、画像メモといった“調査の現場で実際に使われる資料”を統合的に扱えるようになり、NotebookLMはGoogle Workspaceとの連携を強く意識した構造になってきた。</p>
<h2>来週すべてのユーザーが利用可能に、画像は数週間以内</h2>
<p>同社は、今回のアップデートは1週間以内に全ユーザーに展開される予定で、画像の取り込みに関しては数週間以内に順次対応するとしている。NotebookLMは、資料整理ツールから、ウェブ検索と手元資料を横断する“AIリサーチ基盤”へと大きく踏み出したかたちだ。</p>
]]></description>
      <pubDate>Sun, 16 Nov 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>arXiv、コンピュータサイエンス分野でレビュー論文の投稿ルールを厳格化──AI生成サーベイの“洪水”に対処</title>
      <link>https://ledge.ai/articles/arxiv_computer_science_review_policy_update_ai_generated_surveys</link>
      <description><![CDATA[<p>プレプリントサーバーarXivは、コンピュータサイエンス（Computer Science）カテゴリーでレビュー論文・ポジションペーパーの投稿ルールを厳格化すると2025年10月31日付の公式ブログで<a href="https://blog.arxiv.org/2025/10/31/attention-authors-updated-practice-for-review-articles-and-position-papers-in-arxiv-cs-category/">発表</a>した。背景には、AIによって大量に生成されたレビュー論文の急増があり、従来のモデレーション運用では処理が追いつかなくなっているという。</p>
<h2>arXivを覆った「レビュー論文の雪崩」</h2>
<p>近年のCSカテゴリーでは、レビュー論文・サーベイ論文が毎月数百件規模で流入している。arXivは、この多くが大規模言語モデル（LLM）によって短期間で作成されており、</p>
<ul>
<li>文献リストの再構成に近い</li>
<li>未解決課題や研究動向の深掘りがない</li>
<li>専門的整理や独自の洞察がほとんど示されていない
といった特徴が見られると指摘する。</li>
</ul>
<p>研究コミュニティでも状況の深刻さが共有されており、arXivにはAI生成サーベイ論文の急増を「研究コミュニティへのDDoS攻撃」と表現するポジションペーパーも投稿されている。量・質の両面で“雪崩”のような状態となり、従来の例外的な受理運用では対応が困難になっていた。</p>
<h2>「査読済み」の証明が必須に</h2>
<p>こうした状況を受け、arXivはCSカテゴリーでの投稿運用を明確にした。今後、レビュー論文やポジションペーパーをarXivに投稿するには、以下の条件を満たす必要がある。</p>
<ul>
<li>査読付きジャーナルまたは会議で採択されていること</li>
<li>査読が完了していること</li>
<li>投稿時に査読完了の証拠（DOI・掲載先・会議名など）をメタデータとして記載すること</li>
</ul>
<p>公式ブログでは、これらの情報が不足している投稿は「高い確率でリジェクトされる」と明記されている。また、軽査読が一般的なWorkshop採択は十分な要件とは見なされない。</p>
<h2>実は“新ポリシー”ではない──長年の例外運用を整理</h2>
<p>arXivは今回の対応について、「これはポリシー変更ではなく、例外扱いだった論文タイプに対する“運用の明確化”」と説明する。</p>
<p>arXivのコンテンツポリシーでは、主な受理対象は\</p>
]]></description>
      <pubDate>Sun, 16 Nov 2025 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/11/15 [SAT]AIの&quot;ゴッド・マザー&quot; Fei-Fei LiのWorld Labs、マルチモーダル世界モデル「Marble」を一般公開──テキスト・画像・動画から“永続3Dワールド”生成</title>
      <link>https://ledge.ai/articles/world_labs_marble_multimodal_world_model_release</link>
      <description><![CDATA[<p>米AIスタートアップのWorld Labsは2025年11月12日（現地時間）テキストや画像、動画、3Dレイアウトなど多様な入力から“永続的な3D世界”を生成できるマルチモーダル世界モデル 「Marble」 を<a href="https://www.worldlabs.ai/blog/marble-world-model">一般公開</a>した。</p>
<p>創業者には、スタンフォード人工知能研究所（SAIL）の元所長であり、コンピューターサイエンスの世界的権威である Fei-Fei Li氏 が名を連ねる。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/feifeili_e32159ea96/feifeili_e32159ea96.jpg" alt="feifeili.jpg" /></p>
<h2>Marbleの概要──マルチモーダル入力から3D世界を生成</h2>
<p>Marbleは、文章・画像・映像・簡易3Dレイアウトなどを手がかりに、整合性のある3D空間を構築する世界モデルだ。生成された3D世界はそのまま利用するだけでなく、ユーザーが自然言語で編集したり、別のワールドとつなぎ合わせたり、周囲へ拡張したりといった操作が可能。生成物は 画像・動画・Gaussian Splat・メッシュ といった形式でエクスポートでき、ゲームエンジンや3D制作ソフトとの連携も想定する。</p>
<p><strong>Marbleのワークフロー。文章・画像・動画・3Dシーンを入力し、生成した3Dワールドを編集・拡張し、画像・動画・Gaussian Splat・メッシュとして出力できる</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/The_Marble_World_Model_b07e2af0e7/The_Marble_World_Model_b07e2af0e7.jpg" alt="The Marble World Model.jpg" /></p>
<h2>空間知能を中核に据えた世界モデル</h2>
<p>World LabsはMarbleの位置づけについて「世界を再構築し、生成し、シミュレーションし、AIエージェントの理解の基盤となる“次世代ワールドモデル”」と説明する。人間が視覚・言語・空間認知を統合して“内的な世界モデル”を形成するように、AIにも複雑な空間情報を理解し推論する能力が必要になるという考え方だ。Li氏は過去の論考でも、次のAIの主要テーマを「空間知能」としており、Marbleはその研究思想を具現化した製品と位置づけられる。</p>
<h2>多様な入力に対応──文章・画像・動画・3Dレイアウト</h2>
<p>Marbleは、異なる形式の入力から一貫した3D世界を構築する。</p>
<h3>テキスト</h3>
<p>文章から空間構造・雰囲気・光源などを推定して3D空間を生成
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/marble_generated_world_by_text_prompt_efea484c81/marble_generated_world_by_text_prompt_efea484c81.jpg" alt="marble generated world by text prompt.jpg" /></p>
<h3>単一画像</h3>
<p>1枚の画像を手がかりに、写っていない部分まで補完して空間を3D化
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/marble_generated_world_by_image_prompt_d34bf3fde7/marble_generated_world_by_image_prompt_d34bf3fde7.jpg" alt="marble generated world by image prompt.jpg" /></p>
<h3>複数画像・動画</h3>
<p>複数視点の写真や短い動画から、途切れのない単一の3D環境を再構築
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/marble_generated_world_by_multi_image_prompt_e12c253b23/marble_generated_world_by_multi_image_prompt_e12c253b23.jpg" alt="marble generated world by multi-image prompt.jpg" /></p>
<h3>3Dレイアウト</h3>
<p>ユーザーが配置したボックスや平面を“構造”として扱い、テキストで見た目だけを変える「構造とスタイルの分離」を実現。
同じレイアウトをもとに複数のバリエーションの世界を生成できる。</p>
<h2>編集・拡張・合成──AIネイティブの制作ワークフロー</h2>
<p>生成後の操作性も特徴だ。</p>
<h3>局所編集</h3>
<p>特定オブジェクトの削除・調整といった部分的な修正をAIが補完する</p>
<p><strong>プロンプト：カメをトラに、緑の草をフライドポテトに変更</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Turn_the_turtles_into_tigers_and_turn_the_tall_green_plant_into_french_fries_3b36ca497c/Turn_the_turtles_into_tigers_and_turn_the_tall_green_plant_into_french_fries_3b36ca497c.jpg" alt="Turn the turtles into tigers and turn the tall green plant into french fries.jpg" /></p>
<h3>全体編集</h3>
<p>空間全体のスタイル変更、テーマ統一、質感の変更などを自然言語で指定可能</p>
<p><strong>プロンプト：床材をヘリンボーン柄のダークマホガニーに交換</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Replace_the_flooring_with_dark_mahogany_in_a_herringbone_pattern_ab28c53107/Replace_the_flooring_with_dark_mahogany_in_a_herringbone_pattern_ab28c53107.jpg" alt="Replace the flooring with dark mahogany in a herringbone pattern.jpg" /></p>
<h3>拡張（Extend）</h3>
<p>ワールドを外側へ拡大し、不足部分を整合性を保って自動生成</p>
<p><strong>初期世界では造られていなかったところまで自動生成している</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Marble_can_expand_scenes_to_create_larger_traversable_areas_f4885adf49/Marble_can_expand_scenes_to_create_larger_traversable_areas_f4885adf49.jpg" alt="Marble can expand scenes to create larger traversable areas.jpg" /></p>
<h3>合成（Composer）</h3>
<p>複数の3D世界をつなぎ合わせ、より大規模な環境を構築できる。列車の内部や長い回廊など、単一生成では難しいシーン構築を可能にする
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/A_large_train_composed_with_Marble1_1885f64d75/A_large_train_composed_with_Marble1_1885f64d75.jpg" alt="A large train composed with Marble1.jpg" /></p>
<h2>Chisel──“構造とスタイルの分離”を可能にする編集モード</h2>
<p>上級者向けには、新たに「Chisel」モードを提供する。これは、ボックスなどの簡易形状で空間の骨格を作り、テキストで質感や雰囲気を与える制作手法だ。</p>
<ul>
<li>レイアウトはユーザーが設計</li>
<li>見た目はAIが生成</li>
<li>同じ骨格から複数スタイルの3D世界を派生</li>
</ul>
<p>従来の3D制作で分かれていた工程を統合し、試行錯誤の効率を高める仕組みとなっている。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/A_beautiful_modern_art_museum_with_wooden_floor_b3d4ad4582/A_beautiful_modern_art_museum_with_wooden_floor_b3d4ad4582.jpg" alt="A beautiful modern art museum with wooden floor.jpg" /></p>
<h2>エクスポート──Gaussian Splat・メッシュ・動画に対応</h2>
<p>生成された3D世界は、以下の形式で取り出せる。</p>
<h3>Gaussian Splat</h3>
<p>Marbleが最も高い忠実度で再現する形式。World Labs製のWebレンダラ「Spark」と組み合わせることで、ブラウザ・VRなどで表示できる。</p>
<h3>三角メッシュ</h3>
<p>低精度のコライダー用メッシュから、高品質なメッシュまで複数の出力形態を用意。既存のゲームエンジン・3Dツールへの統合を容易にする。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Marble_can_export_generated_worlds_as_Gaussian_splats_or_triangle_meshes_6af9a28eb0/Marble_can_export_generated_worlds_as_Gaussian_splats_or_triangle_meshes_6af9a28eb0.jpg" alt="Marble can export generated worlds as Gaussian splats or triangle meshes.jpg" /></p>
<h3>動画</h3>
<p>カメラ軌道を細かく設定したレンダリングが可能。炎・煙・光源といった動的要素を追加しながら、カメラ制御情報を保持できる。</p>
<h2>Marble Labs──利用者コミュニティと事例を公開</h2>
<p>World Labsは同時に、クリエイター向けのコミュニティ「<a href="https://www.worldlabs.ai/labs">Marble Labs</a>」を立ち上げた。</p>
<p>ワークフロー共有やケーススタディの公開を通じ、ゲーム開発、VFX制作、デザイン、ロボティクスなど多様な分野での利用を促す。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/marblelabs1_680761936f/marblelabs1_680761936f.jpg" alt="marblelabs1.jpg" /></p>
<h2>空間知能へのロードマップ：次の焦点は「インタラクション」</h2>
<p>Marbleの登場により、文章や画像から統合的な3D環境を生成する“世界モデル”分野は、研究段階から実用段階へと一歩前進した。近年はGoogle DeepMindのGenieシリーズなど、空間理解を備えたAIの開発が相次いでいるが、Marbleは生成した3D世界を編集・拡張し、既存ツールへ直接エクスポートできる点で、制作ワークフローへの適用可能性を明確に示したかたちだ。
今後、コンテンツ制作やロボティクス、シミュレーションなどの領域で、こうした汎用的な世界モデルがどこまで浸透していくのかが注目される。</p>
<p>World LabsはMarbleを「空間知能への旅路の一歩」と表現する。今後の重点は「人間やエージェントが生成された世界と新しい形で相互作用できること」であり、ロボティクスやシミュレーション領域でのユースケース拡大を見込むとしている。
同社は「<a href="http://marble.worldlabs.ai">marble.worldlabs.ai</a>」での利用開始を呼びかけるとともに、同ビジョンに共感する技術者の参画を求めている。</p>
]]></description>
      <pubDate>Sat, 15 Nov 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>「AIはまだフリーランサーになれない」──新指標「Remote Labor Index」が示した自動化率2.5％の現実</title>
      <link>https://ledge.ai/articles/ai_freelancer_automation_rli_25percent</link>
      <description><![CDATA[<p>米Scale AIとCenter for AI Safety（CAIS）は、AIエージェントが実際のフリーランス業務をどこまで自動化できるかを測定する新しい評価指標「Remote Labor Index（RLI）」を公開した。2025年10月30日にarXivへ投稿された<a href="https://arxiv.org/abs/2510.26787v1">研究論文</a>および最新<a href="https://scale.com/leaderboard/rli">リーダーボード</a>によると、最先端のAIエージェントでも「委託可能な品質で完了できた案件」は2.5％にとどまったという。</p>
<p>両機関は2024年、AIの危険行動能力を定量化する「Humanity’s Last Exam」を<a href="https://ledge.ai/articles/ai_humanitys_last_exam">共同発表</a>しており、RLIはその“実務版”として、AI能力を経済価値の観点から評価する試みとなる。</p>
<h2>実務ベースの240プロジェクトで評価</h2>
<p>RLIは、実在のフリーランスプラットフォームから収集した240件のプロジェクトを対象としている。
各案件には以下が含まれる：</p>
<ul>
<li>クライアントによる依頼文（ブリーフ）</li>
<li>実際の人間フリーランサーが作成した納品物</li>
<li>入力ファイル（画像、図面、動画、音声、スクリプトなど）</li>
<li>作業時間・報酬データ</li>
</ul>
<p>アノテーターがAIの成果物を評価し、「現実のクライアントが受け入れられるか」を合否判定する。AI自身による自動採点は使われていない。</p>
<p><strong>RLIのタスク抽出プロセス</strong> ：550件のタスクを300人以上のフリーランサーから収集し、基準適合性のチェックと改善を経て、最終的に240件のプロジェクトを選定した。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/image_2_dab90e3325/image_2_dab90e3325.png" alt="image (2).png" /></p>
<h2>Automation Rate：最高でも2.5％</h2>
<p>RLI公式リーダーボードの主なスコアは以下の通り。</p>
<ul>
<li>Manus：2.50％</li>
<li>Claude 4.5 Sonnet：2.08％</li>
<li>GPT-5（2025-08-07版）：1.67％</li>
<li>ChatGPT agent：1.25％</li>
<li>Gemini 2.5 pro-preview：0.83％</li>
</ul>
<p>いずれのモデルも、実務案件の約97〜99％を完了できなかった。公式サイトでも、「Absolute Automation is Near Zero（絶対的自動化はほぼゼロに近い）」と明記されている。</p>
<h2>プロジェクト内容：動画、CAD、ゲーム開発など“実務そのもの”</h2>
<p>240件は、従来の研究ベンチマークに比べて大幅に多様なカテゴリで構成されている。</p>
<p><strong>RLIのプロジェクトカテゴリ構成</strong> ：RLIは動画制作、CAD、グラフィックデザイン、ゲーム開発、音声編集など23カテゴリで構成され、多様な実務タスクを含む。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/RLI_Project_Categories_c49b93ec58/RLI_Project_Categories_c49b93ec58.jpg" alt="RLI Project Categories.jpg" /></p>
<ul>
<li>動画制作（13％）</li>
<li>CAD（12％）</li>
<li>グラフィックデザイン（11％）</li>
<li>ゲーム開発（10％）</li>
<li>音声編集（10％）</li>
<li>建築（7％）</li>
<li>プロダクトデザイン（6％）</li>
<li>その他（31％）</li>
</ul>
<p>さらに、プロジェクトの平均制作時間は28.9時間（中央値11.5時間）、報酬中央値は200ドルと、実務レベルの負荷を持つ。</p>
<p><strong>RLIのプロジェクト負荷とカテゴリ比較</strong> ：人間の平均作業時間は28.9時間（中央値11.5時間）と高く、プロジェクト種別も一般のフリーランス市場（Upwork）と同等の分布を持つ。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Core_Metrics_cf85d22247/Core_Metrics_cf85d22247.jpg" alt="Core Metrics.jpg" /></p>
<p><strong>RLIで評価されたプロジェクト例</strong> ：データ可視化、3Dプロダクトレンダリング、動画制作、建築設計、ゲーム開発、科学文書作成など、多様な実務タスクが含まれている。</p>
<h2>主な失敗モード：品質不足・不完全・整合性欠如</h2>
<p>論文とリーダーボードの分析では、AIがプロジェクトを完了できない理由として以下が挙げられている。</p>
<ul>
<li>品質不足（45.6％）：専門水準に達していない</li>
<li>不完全な納品物（35.7％）：動画が途中で終わる、3Dモデルが欠損</li>
<li>ファイル破損・形式不備（17.6％）</li>
<li>整合性欠如（14.8％）：複数ファイル間で仕様が一致しない</li>
</ul>
<p>具体例には、以下のようなケースがある：</p>
<ul>
<li>8分動画を求められた案件で、AIは8秒動画しか作れない</li>
<li>3Dレンダリングの視点ごとに造形が変わる</li>
<li>図面の寸法が実寸と合わない</li>
<li>Webゲームが起動はするがグラフィックが破綻している</li>
</ul>
<h2>成功するタスクの傾向</h2>
<p>数少ない成功例は、以下のような“閉じた生成タスク”に集中していた。</p>
<ul>
<li>ロゴ・バナー制作（画像生成）</li>
<li>ボイス合成や簡易オーディオ編集</li>
<li>表形式データの可視化</li>
<li>簡易レポート作成</li>
</ul>
<p>一方、多工程・長尺のタスク、複数ファイルの整合性が必要なタスクではほぼ全滅しているという。</p>
<h2>今後の展開</h2>
<p>論文の著者らは、RLIの結果について「AIシステムが現実のフリーランス環境において委託業務として許容可能な水準でプロジェクトを完了できることは、ほとんどない」と記している。また、現在のモデル性能について「能力の下限に近い」と評価し、タスクの大半で“実務的完成度に到達しない”点を指摘した。</p>
<p>研究チームはさらに、RLIが生成AIの進歩を今後継続的に追跡するための基盤になると位置づけており、「Absolute Automation is Near Zero（絶対的自動化はほぼゼロに近い）」というリーダーボード上の表現とともに、現時点でのAIエージェントの限界を明確に示した。</p>
]]></description>
      <pubDate>Sat, 15 Nov 2025 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>LLMは“悪役ロールプレイ”で性能が急落──Tencentと中山大学が23,191シーンを解析、最大の弱点は“利己的キャラ”の再現性</title>
      <link>https://ledge.ai/articles/llm_villain_roleplay_too_good_to_be_bad_tencent_analysis</link>
      <description><![CDATA[<p>TencentのMultimodal Departmentと中山大学の研究チームは2025年11月12日、LLM（大規模言語モデル）が「悪役ロールプレイ」を苦手とすることを大規模に示した論文「Too Good to be Bad: On the Failure of LLMs to Role-Play Villains」を<a href="https://arxiv.org/abs/2511.04962v2">発表</a>した。</p>
<p>23,191のシーンと54,591のキャラクター注釈を含む独自のMoral RolePlayベンチマークを構築し、最新モデルのロールプレイ性能を4段階のモラル軸で評価したところ、<strong>キャラクターの善悪が下がるほど演技の忠実度が体系的に低下する傾向</strong> が明らかになったという。</p>
<h2>聖人 → 悪役でスコアが単調減少</h2>
<p>研究チームは、キャラクターを「Level 1：Moral Paragons（聖人）」「Level 2：Flawed-but-Good」「Level 3：Egoists（利己的）」「Level 4：Villains（悪役）」の4段階に分類し、ゼロショットでロールプレイを生成させて評価した。</p>
<p>その結果、平均スコアは
3.21（Level 1） → 3.14（Level 2） → 2.71（Level 3） → 2.62（Level 4）
と、モラルが低下するほど一貫して減少した。特に、Level 2（善人）からLevel 3（利己的）への転換で最も大きな性能落ち込みが発生しており、研究チームはこれを「LLMにとって最大の弱点」と位置づけている。</p>
<h2>Negative traits で深刻な崩れ</h2>
<p>研究では、各キャラクターに77種類の性格特性を付与し、性格ごとの再現性も分析された。とくに Selfish（利己的）や Manipulative（策略的）といった Negative traits に注目した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x3_907cedc104/x3_907cedc104.png" alt="x3.png" />
Moral RolePlayベンチマークに含まれる主要特性の出現比率。Negative traits は割合こそ小さいが、悪役キャラクター群で高密度に組み合わさる傾向があり、LLMの再現性を下げる要因になっている。</p>
<p>これらの特性ごとに、ロールプレイの忠実度スコアから算出した「平均ペナルティ値」を比較したところ、結果は</p>
<ul>
<li>Positive traits：ペナルティ 3.16</li>
<li>Neutral traits：3.23</li>
<li>Negative traits：3.41（最も高いペナルティ）
と、悪役に不可欠な負の特性で忠実度が大きく低下した。</li>
</ul>
<p>特に「Selfish（利己的）」「Deceitful（欺瞞的）」「Manipulative（策略的）」など、LLMの安全性アラインメントと真っ向から矛盾する特性ほど再現が難しいとされる。論文では、こうした特性を持つキャラクターの演技が「浅い攻撃表現」に置き換わる傾向が複数モデルで確認された。</p>
<h2>悪役ロールプレイは“一般性能”と相関しない</h2>
<p>研究チームは、悪役ロールプレイ性能を独立に評価する「Villain RolePlay（VRP）ランキング」も作成した。最上位は glm-4.6（スコア 2.96）で、次いで deepseek-v3.1-thinking（2.82）、kimi-k2（2.79）が続いた。一方で、Arenaランキング上位のモデル（例：Claude-opus 4.1、Claude-sonnet 4.5）は、VRPでは大きく順位を落としている。</p>
<p>この傾向は、各モデルのモラルレベルごとのスコアを比較した以下の図でも確認できる。
Level 1（聖人）では多くのモデルが高得点を維持する一方、Level 3（利己的）と Level 4（悪役）では急激にスコアが落ち込むモデルが多い。Claude系は特に落差が大きく、glm系は比較的安定して高い水準を維持している。</p>
<p><strong>各モデルのモラルレベル別スコア比較</strong> 青（Level1）→緑（Level2）→黄（Level3）→赤（Level4）の順に、モラルが下がるほどスコアが低下する傾向が明確に見える。特に Claude 系モデルは Level3・4 で大きく崩れ、glm 系モデルは比較的安定した性能を示した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x4_3_a311e00b70/x4_3_a311e00b70.png" alt="x4 (3).png" /></p>
<p>画像の出典：<a href="https://arxiv.org/abs/2511.04962v2">Too Good to be Bad: On the Failure of LLMs to Role-Play Villains</a>
:::</p>
<p>さらに、VRPランキングと Arena ランキングを直接比較した次の表を見ると、チャット能力の高さと悪役ロールプレイ性能が必ずしも一致しないことがより明確になる。glm-4.6 は VRP1位だが Arena10位、Claude-opus 4.1 は Arena1位相当ながら VRP14位にとどまるなど、両者は大きく乖離している。</p>
<p><strong>悪役ロールプレイ（VRP）リーダーボード。(アリーナスコアと比較)</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Villain_Role_Play_VRP_leaderboard_d966f1e8f7/Villain_Role_Play_VRP_leaderboard_d966f1e8f7.jpg" alt="Villain RolePlay (VRP) leaderboard.jpg" /></p>
<p>LLMの“悪役ロールプレイ能力”は、一般チャット性能とはしばしば大きくずれる。glm 系が上位を占める一方、Claude 系は VRP で順位を落としている。</p>
<p>これらの結果から、研究チームは「悪役ロールプレイ性能は一般的な対話能力とは独立した能力」であり、創作・ゲーム・NPC生成など人格模倣が求められる領域では、VRPのような文脈依存の評価が欠かせないと結論づけている。</p>
<h2>安全アラインメントとの根本的なトレードオフ</h2>
<p>研究は、LLMの安全性調整（有害表現の抑制）が悪役ロールプレイ能力と衝突している可能性を指摘している。
特に、欺瞞的・操作的な行動は安全性ガイドラインによって強く抑制されるため、LLMがフィクション内であってもその振る舞いを再現できず、キャラクターの演技が破綻するケースが多かった。</p>
<p>研究チームは、「フィクション文脈における悪役表現」と「現実世界の有害表現」を区別できる新たなアラインメント方式が今後必要になると述べている。</p>
<h2>今後の展望</h2>
<p>Moral RolePlayベンチマークは、創作・ゲーム開発・対話型エージェントなど、人格再現性が求められる分野でのモデル改善に活用できるとされる。</p>
<p>研究チームは、悪役ロールプレイを含む「文脈依存の安全性」に対応した学習方法や、用途ごとの「ロールプレイモード切替」の有効性を今後の課題として挙げた。</p>
]]></description>
      <pubDate>Sat, 15 Nov 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Microsoft、米国内初の「AIスーパーファクトリー」稼働開始──ウィスコンシンとアトランタを高速AI WANで結ぶ新型Fairwaterデータセンター群</title>
      <link>https://ledge.ai/articles/microsoft_ai_superfactory_launch</link>
      <description><![CDATA[<p>Microsoftは2025年11月12日（現地時間）、同社初となる「AIスーパーファクトリー」を米国内で運用開始したと<a href="https://news.microsoft.com/source/features/ai/from-wisconsin-to-atlanta-microsoft-connects-datacenters-to-build-its-first-ai-superfactory/">発表</a>した。</p>
<p>10月に稼働したアトランタの新Fairwaterデータセンターに加え、ウィスコンシンのFairwaterサイトなど複数拠点を専用のAI WAN（AI専用広域ネットワーク）で結び、1つの巨大なAI計算インフラとして連携動作させる構想を明らかにした。同社はこの「AIスーパーファクトリー」を、数百兆パラメータ級のAIモデルを訓練するための、新しいクラウドアーキテクチャだと位置づけている。</p>
<h2>「AIスーパーファクトリー」とは</h2>
<p>公表されたAIスーパーファクトリーは、従来のデータセンターとは異なり、複数州に点在するAIキャンパスを1つの高性能コンピューターのように扱う設計が特徴だ。</p>
<p>Microsoftは、米ウィスコンシン州とジョージア州アトランタに構築したFairwaterデータセンターを高速AI WANで結び、GPUクラスタ間のデータ移動をミリ秒単位に短縮。これにより、巨大なAIトレーニングを複数サイトにまたがって同時並行で処理できると説明している。</p>
<h2>高密度GPUと液冷による次世代AIキャンパス設計</h2>
<p>Fairwaterデータセンターには、最新の NVIDIA GB200 NVL72（Blackwell世代）をベースにした高密度GPUラックを採用。ケーブル長を極限まで短く抑えた二層構造の建屋と専用ネットワークにより、AIモデルの学習効率を引き上げる設計となっている。</p>
<p>▼画像：Fairwaterデータセンター内のGPUラック：高密度に配置されたNVIDIA Blackwell世代GPUラック。ケーブル長を短縮し、並列処理性能を最大化する構造が採用されている。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/345_009_edited_2000_768x512_383b630c28/345_009_edited_2000_768x512_383b630c28.jpg" alt="345-009_edited_2000-768x512.jpg" /></p>
<p>アトランタのFairwater AIデータセンターは、10月に本格稼働を開始。マルチギガワット級の電力供給、広大な敷地、液冷設備など、AI専用クラスタの増設を前提としたレイアウトとなっている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/345_244_2000_912x683_ad4ffa2cbe/345_244_2000_912x683_ad4ffa2cbe.jpg" alt="345-244_2000-912x683.jpg" /></p>
<h2>高速AI WANが「仮想スーパーコンピューター」を形成</h2>
<p>Microsoftは、今回のAIスーパーファクトリーを実現する要素として“AI WAN”を強調している。これは、専用光ファイバーを用いた広域ネットワークで、ウィスコンシンとアトランタ間を含む複数サイトを結び、GPU同士を1カ所にあるかのように連携させるための基盤だ。主な特徴は次のとおり。</p>
<ul>
<li>数千マイルをまたぐ高速な専用光ネットワークで、データをほぼ光速で伝送</li>
<li>専用AI WAN向けの光ファイバーを1年で12万マイルまで増設し、マイレージを25%以上拡張</li>
<li>AIアプリケーション向けに最適化されたネットワークプロトコルとQoSにより、渋滞を避けたトラフィック制御を実現</li>
</ul>
<p>これにより、巨大モデルの学習で生じる通信のボトルネックを最小限に抑え、GPUを最大限稼働させるとしている。</p>
<h2>液冷で“ゼロウォーター運用”を実現</h2>
<p>Fairwaterでは、閉ループ構造の液冷システムを採用し、初回に一般家庭約20軒分の年間使用量に相当する水を充填したのち、基本的に再利用を続けることで追加の水資源をほとんど必要としない運用を実現している。</p>
<p><strong>▼画像：AIスーパーファクトリーの構造：Microsoftが公開した「AIスーパーファクトリー」の概念図。AI WAN、液冷、ラックアーキテクチャなど、複数要素が統合されている。</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Infographic_v1_578x578_185454d166/Infographic_v1_578x578_185454d166.png" alt="Infographic-v1-578x578.png" /></p>
<h2>担うワークロード──OpenAI、AI Superintelligence Team、Copilot</h2>
<p>同社はこのAIスーパーファクトリーを、以下のワークロード向けに設計したと説明している。</p>
<ul>
<li>OpenAIのフロンティアモデルのトレーニング</li>
<li>Microsoft AI Superintelligence Teamの研究開発</li>
<li>Copilotを含む商用AIサービス全体の推論・学習基盤</li>
</ul>
<p>モデル規模としては「数百兆パラメータ級のモデルを想定」と明言されている。</p>
<h2>2019年のOpenAIスーパーコンピューターからの進化</h2>
<p>Microsoftは2019年にOpenAIと共同で初の大規模AIスーパーコンピューターを構築して以来、GPU、ネットワーク、冷却、OS、スケジューラなど、クラウドインフラのあらゆる層を再設計してきた。Fairwaterは、これまでの知見を統合し、AIライフサイクル全体（学習・評価・合成データ生成など）に対応する新しいカテゴリのデータセンターとして位置づけられている。</p>
]]></description>
      <pubDate>Fri, 14 Nov 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>1X、家庭用ヒューマノイド「NEO」を正式公開──月額499ドル（約7万5,000円）のサブスク開始で“家事お手伝いロボ”が現実に</title>
      <link>https://ledge.ai/articles/1x_neo_home_robot_subscription_launch</link>
      <description><![CDATA[<p>米1X Technologiesは2025年10月28日（米国時間）、家庭向けヒューマノイドロボット「NEO」を<a href="https://www.1x.tech/neo">公開</a>し、プレオーダーを開始した。提供形態は月額499ドル（約7万5,000円）のサブスクリプションまたは一括購入（2万ドル）。家庭内の掃除や片付けなど、日常的な家事を支援する“お手伝いロボット”として設計されている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x1neo_6_ce91e1cf1e/x1neo_6_ce91e1cf1e.jpg" alt="x1neo-6.jpg" /></p>
<p>NEOは、1Xが掲げる「人の生活を支える安全なヒューマノイド」構想に基づき開発された。腱（tendon）駆動による柔らかく静かな動作を特徴とし、人と同じ空間で安全に動作できるよう設計されている。公式サイトでは「単調で時間のかかる家事を肩代わりし、人の時間を取り戻す」とコンセプトを掲げる。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x1neo_5_7ae0e73c6c/x1neo_5_7ae0e73c6c.jpg" alt="x1neo-5.jpg" /></p>
<p>ユーザーはスマートフォンアプリや音声を通じてタスクを指示でき、NEOは家庭内の環境を学習しながら動作を最適化する。自己充電機能を備えるほか、遠隔からのモニタリングやサポートも可能。1XはNEOを単なるロボットではなく「温かみと個性をもった家庭のパートナー」と位置付けている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x1neo_4_dd13229413/x1neo_4_dd13229413.jpg" alt="x1neo-4.jpg" /></p>
<p>主な仕様は、バッテリー駆動時間約4時間（急速充電対応）、静音性は最大22dB。NVIDIA Jetson Thorをベースとした「1X Cortex」コンピューティングシステムを搭載し、360度集音マイクとステレオスピーカーを内蔵する。</p>
<h2>サブスク形式で家庭導入のハードルを下げる</h2>
<p>提供形態は月額499ドル（約7万5,000円）のサブスクリプションまたは一括購入（2万ドル）。NEOは注文ページからプレオーダー可能で、出荷は2026年を予定している。</p>
<p>1Xは公式ページ上で、NEOを「consumer-ready」と表現。2024年の試作版「NEO Beta」発表を経て、今回初めて一般消費者に向けたモデルとして公開された。</p>
<p>@<a href="https://www.youtube.com/watch?v=LTYMWadOW7c">YouTube</a></p>
<p>1X Technologiesは、家庭や産業向けのヒューマノイドロボットを開発する企業で、OpenAIが出資するスタートアップの一つでもある。これまでノルウェーを拠点としていたが、2025年には米カリフォルニア州サンフランシスコに本社を移転。研究開発と製造体制の両面で国際展開を進めている。サブスクリプション形式による提供で家庭でも導入しやすい価格体系を整えたNEOは、ヒューマノイドが日常生活に溶け込む時代の到来を感じさせる存在だ。</p>
]]></description>
      <pubDate>Fri, 14 Nov 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>クマ被害多発で注目、上智大・深澤研究室が19地域の遭遇リスクをAIで可視化──1kmメッシュで危険度を5段階表示する予測マップ</title>
      <link>https://ledge.ai/articles/bear_encounter_ai_prediction_map_sophia_university</link>
      <description><![CDATA[<p>全国的にクマによる出没や人身被害が相次ぐ中、上智大学・深澤佑介准教授（応用データサイエンス）の研究チームが、クマとの遭遇リスクをAIで予測し、地図上で可視化する「<a href="https://ds.sophia.ac.jp/news/20251024/post-1065">クマ遭遇AI予測マップ</a>」を一般公開している。対象地域は札幌市、東北、北関東、東京都、北陸、中部、京都府など計19地域に広がり、誰でも無料で閲覧できる。</p>
<p>同大学・応用データサイエンス学位プログラムは2025年10月24日付で、深澤研究室が開発した「クマ遭遇予測マップ」の公開URLを案内しており、秋田県を対象とした予測モデルを基に実装したことを説明している。研究室サイトでは19地域分のマップが一覧化され、各エリアの遭遇確率を確認できる。</p>
<h2>AIが1kmメッシュで遭遇リスクを推定</h2>
<p>予測マップでは、AIがクマ遭遇リスクを1km四方のメッシュ単位で推定し、危険度を「非常に高い」「高い」「やや高い」「可能性あり」「低い」の5段階で色分けして表示する。赤系が高リスク、黄色系が中リスクを示す仕組みだ。</p>
<p>また、各地域ページには、直近6カ月以内のクマ出没・遭遇地点を示す「X印」が別途表示される。マーカーがない場所は、データ不足などの理由で予測が行われていないエリアであることも明記されている。</p>
<h2>過去の遭遇記録や地形・人口などを活用</h2>
<p>予測モデルは、各自治体が公開するクマの出没・遭遇記録をはじめ、森林や農地などの土地利用、道路網、人口分布、標高といった環境要因を組み合わせて学習している。
学習には決定木ベースの機械学習手法が用いられ、秋田県のデータでは「正答率・適合率・再現率がいずれも6割強」という検証結果も得られている。</p>
<p>これらの情報を統合し、各メッシュの遭遇確率を推定することで、ユーザーが直感的に危険エリアを把握できるよう設計されている。</p>
<h2>19地域をカバーし、PC・スマホから無料で閲覧可能</h2>
<p>マップは研究室サイトで公開されており、PCやスマートフォンから無料で利用できる。アクセスに特別なアプリは必要なく、ブラウザ上で地図を閲覧しながら推定リスクを確認できる。</p>
<p>対象地域は以下の通り（一部抜粋）
：札幌市・青森県・盛岡市・秋田県・山形県・宮城県・新潟県・栃木県・群馬県・埼玉県・東京都・山梨県・富山県・石川県・長野県・岐阜県・京都府</p>
<p>研究室サイトには「最終更新日時」も掲示され、最新の予測状況を確認できる。</p>
<p>また、深澤研究室は X（旧Twitter）公式アカウント（<a href="https://x.com/fukazawa_lab">@fukazawa_lab</a>） でもマップの更新情報や関連アナウンスを随時発信しており、最新の動向を確認できる。</p>
<h2>「最新の自治体情報を優先してほしい」──研究チームが注意喚起</h2>
<p>上智大の案内および研究室サイトでは、予測マップの利用にあたって複数の注意点を明示している。</p>
<ul>
<li>マップは「注意喚起」を目的とした予測であり、遭遇を確実に防ぐものではない</li>
<li>クマの行動は環境によって変動するため、最新の状況は反映しきれない場合がある</li>
<li>登山・農作業・山菜採りなどを行う際は、「自治体が発信する情報や現地の警報・掲示を必ず確認してほしい」</li>
</ul>
<p>研究チームは今後もデータの蓄積やモデル改良、対象地域の拡大を進め、遭遇リスク低減に資する情報提供を継続する方針を示している。</p>
]]></description>
      <pubDate>Fri, 14 Nov 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ChatGPTの“当たり障りないフィルター”を外すと、応答が一段と鋭くなった──米国で話題の「辛口プロンプト」現象</title>
      <link>https://ledge.ai/articles/chatgpt_ii_hito_filter_prompt_trend</link>
      <description><![CDATA[<p>ChatGPTの「当たり障りのない」応答に物足りなさを感じた海外ユーザーが、あえて“当たり障りないフィルター”を外すプロンプトを公開し、話題を集めている。Redditで拡散したこの手法は、ChatGPTのトーンを「共感的な聞き役」から「論理的で辛口な批評家」へと変えるもので、SNSでは「回答の質が上がった」との声も相次いだ。</p>
<h2>Reddit発の「辛口プロンプト」が反響呼ぶ</h2>
<p>発端となったのは、Redditユーザー Wasabi_Open 氏が投稿した「I made ChatGPT stop being nice and it’s the best thing I’ve ever done（ChatGPTに“いい人”をやめさせたら、最高の結果になった）」という<a href="https://www.reddit.com/r/PromptEngineering/comments/1okppqe/i_made_chatgpt_stop_being_nice_and_its_the_best/">スレッド</a>だ。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/I_made_Chat_GPT_stop_being_nice_375797ac64/I_made_Chat_GPT_stop_being_nice_375797ac64.jpg" alt="I made ChatGPT stop being nice.jpg" /></p>
<p>同氏はプロンプトの中で、ChatGPTに対し「私の意見を褒めたり慰めたりせず、誤りがあれば明確に指摘してほしい」「論理の矛盾を批判的に分析してほしい」と指示。これにより、ChatGPTが従来よりも率直で的確なフィードバックを返すようになったという。
この投稿は数千件のいいねを集め、「まるで冷静なメンターと議論しているようだ」とのコメントも寄せられた。</p>
<h2>SNSで広がった「nice filter」論争</h2>
<p>この現象を11月3日に<a href="https://x.com/markgadala/status/1985032100672618588">紹介</a>したのが、X（旧Twitter）のユーザー Mark Gadala 氏だ。同氏は「“nice filter”を外したらChatGPTの回答が劇的に改善した」と投稿し、多くのフォロワーが同様のプロンプトを試したと報告している。一方で、「フィルターを解除すると性能が上がる」という表現が拡散したことで、「内部制限を外す行為ではないか」との誤解も広がった。実際には、ChatGPTの内部に“nice filter”と呼ばれる設定は存在せず、プロンプトの指示文によって出力トーンが変わるだけだ。</p>
<h2>「当たり障りないフィルター」の正体</h2>
<p>OpenAIの設計方針によれば、ChatGPTは安全性と中立性を重視した“共感的”な初期設定を採用している。ユーザーが感じる「いい人フィルター」とは、この丁寧でポジティブに応答する傾向を指した比喩に過ぎない。つまり、「フィルターを外す」とは内部機能を解除するのではなく、プロンプトによってAIの口調や態度を再設定する行為だといえる。</p>
<h2>“辛口AI”の効用と注意点</h2>
<p>ユーザーの反応はおおむね好意的だ。「率直な批評を受けることで思考が整理された」「甘い同意よりも鋭い反論のほうが学びになる」といった意見が目立つ。一方で、「冷たく感じる」「会話がきつくなる」との声もあり、タスクや気分に応じてトーンを使い分ける重要性が指摘されている。専門家の間では、このようなトーン調整を「AIとの協働スキル」や「プロンプトリテラシー」の一環とみなす動きも広がっている。</p>
]]></description>
      <pubDate>Fri, 14 Nov 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>AI音楽、リスナーの97%が見抜けず──Deezer調査で浮かぶ「ラベリング必須」と「著作権侵害への懸念」</title>
      <link>https://ledge.ai/articles/deezer_ipsos_ai_music_97percent_survey</link>
      <description><![CDATA[<p>音楽ストリーミングサービスのDeezerは2025年11月12日、調査会社Ipsosと共同で、AI生成音楽に対するリスナー意識を探る初の国際大規模調査を実施したと<a href="https://newsroom-deezer.com/2025/11/deezer-ipsos-survey-ai-music/">発表</a>した。</p>
<p>8カ国・9000人を対象にしたブラインドテストでは、参加者の97％が「完全にAI生成された音楽」と「人間が制作した音楽」を聴き分けることができなかった。一方で多くのユーザーは、この結果に驚きや不安を覚え、AI音楽の明確なラベリングや著作権の公正な扱いを求めていることが明らかになった。</p>
<h2>8カ国・9000人調査で浮かぶ、AI音楽への“戸惑い”</h2>
<p>調査は米国、カナダ、ブラジル、英国、フランス、オランダ、ドイツ、日本の8カ国で実施され、18〜65歳の9000人が参加した。参加者は3曲の音源を聴き、どれが完全なAI生成音楽かを判断するよう求められたが、97％が誤答。71％がこの結果に「驚いた」と回答し、52％は「人間とAIの音楽を区別できないことに不快感」を覚えたという。</p>
<p>AI音楽は一般リスナーにとって、すでに「ほぼ人間と区別がつかない段階」に達していることを示す結果となった。</p>
<h2>AI音楽への期待と懸念 ― 創造性が失われると感じる人が64％</h2>
<p>AIに対する一般的な認知は高く、98％が「AIについて聞いたことがある」と回答し、72％は「少なくとも数回はAIを使った経験がある」とした。
音楽領域でも、</p>
<ul>
<li>46％：AIが新しい音楽の発見を助ける</li>
<li>51％：AIは今後10年で音楽制作に大きな役割を果たす
といった期待の声がある一方で、</li>
<li>51％：低品質・画一的な音楽が増える</li>
<li>64％：AIが音楽制作の創造性を失わせる
と懸念も強い。</li>
</ul>
<p>リスナーはAIへの好奇心と、創作物の質への不安との間で揺れている。</p>
<h2>「AI音楽は聴くが、区別はしたい」──透明性への強い要望</h2>
<p>音楽ストリーミングユーザー（n=6791）を対象とした調査では、</p>
<ul>
<li>66％：好奇心から100％AI音楽を一度は聴いてみたい</li>
<li>45％：AI音楽をフィルターで除外したい</li>
<li>40％：AI音楽と分かればスキップしたい
と回答。
興味はある一方で、「AI音楽と人間の音楽を区別したい」という強いニーズが浮き彫りになった。</li>
</ul>
<p>透明性に関する項目では、</p>
<ul>
<li>80％：100％AI生成音楽は明確にラベル表示すべき</li>
<li>73％（音楽ストリーミングユーザー）：サービスがAI音楽を推薦しているかどうか知りたい</li>
<li>52％：AI音楽を人間の楽曲と同じチャートに含めるべきではない
といった声が寄せられた。</li>
</ul>
<p>AI音楽の存在が受容されつつある反面、ユーザーは“知らない間にAI音楽を聴かされる”ことに警戒している。</p>
<h2>著作権保護への強い危機感 ― 許可なきAI学習は「非倫理的」</h2>
<p>調査はAIの学習に著作権作品を使うことに対する強い慎重姿勢も示した。</p>
<ul>
<li>65％：著作権作品を無断でAI学習に利用すべきではない</li>
<li>73％：アーティストの許可なく著作権作品を使ってAIが音楽生成するのは非倫理的</li>
<li>70％：AI音楽は現役・未来のアーティストの生計を脅かす</li>
<li>69％：AI生成曲への支払いは人間の楽曲より低くすべき
ユーザーはAI音楽の存在そのものよりも、その背後にある「データの扱い」や「創作者の収益」への影響に敏感だ。</li>
</ul>
<h2>Deezerは毎日5万曲のAI音楽を受信──34％がAI</h2>
<p>Deezerは現在、1日あたり約5万曲の完全AI生成トラックを受け取っており、1日に届く全楽曲の34％を占めているという。同社は2025年初頭からAI検出ツールを運用しており、SunoやUdioなど主要AIモデルで生成された音源を識別できるとのこと。</p>
<p>また、AI音楽のストリーミングの最大70％が不正再生だったケースも確認しており、該当ストリームはロイヤリティから除外している。</p>
<h2>AI曲は推薦・プレイリストから除外、チャート混入を回避</h2>
<p>Deezerは2025年6月、業界で初めて「100％AI生成音楽」をタグ付けしたプラットフォームとなった。
現在は、</p>
<ul>
<li>AI曲をアルゴリズム推薦から自動除外</li>
<li>編集プレイリストにも掲載しない</li>
<li>チャート集計でも区別運用を検討</li>
</ul>
<p>と、透明性確保とアーティスト保護を重視した対策を進めている。</p>
<p>DeezerのCEOであるAlexis Lanternier氏は、聴いている音楽がAIなのか人間なのかを知りたいというユーザーの意識を明確に示した。AIがアーティストの活動や著作権を損なうべきではないという点についても広範な支持があると述べた。</p>
<p>同社は今後、AI検出技術の特許出願など、AI音楽時代における透明性基準の確立を目指しているという。</p>
]]></description>
      <pubDate>Fri, 14 Nov 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>GoogleマップがGemini対応　「右折はタイ料理店の先」など“ランドマーク案内”を含む4つの新機能を発表</title>
      <link>https://ledge.ai/articles/google_maps_gemini_landmark_navigation_4_features</link>
      <description><![CDATA[<p>Googleは2025年11月5日（現地時間）、地図アプリ「Googleマップ」に生成AI「Gemini」を統合し、会話型のナビゲーション体験を強化する4つの新機能を<a href="https://blog.google/products/maps/gemini-navigation-features-landmark-lens/">発表</a>した。Geminiによる音声操作やランドマーク型案内、交通情報の自動通知、カメラを活用した視覚的検索が順次利用可能になる。</p>
<h2>会話で完結する“ハンズフリー運転”</h2>
<p>新機能の一つは、運転中に音声だけで操作できる「会話型ハンズフリー運転体験」だ。ユーザーは「途中でコーヒーショップに寄りたい」など自然な言葉で指示でき、Geminiが最適な経路や立ち寄り先を提案する。走行中でも画面操作を減らし、安全性の高いナビゲーションを実現する狙いがある。</p>
<p>@<a href="https://youtu.be/WnNZ3QhwE84">YouTube</a></p>
<h2>「右折はガソリンスタンドの先」──ランドマークで案内</h2>
<p>従来の「200メートル先を右折」ではなく、「タイ料理店の先を右折」といったランドマーク（目印）を基準にしたターン案内も追加された。ユーザーが視認しやすい建物や店舗を指標にすることで、より直感的に道順を把握できるという。</p>
<p>@<a href="https://youtu.be/_wJIEgy0uCg">YouTube</a></p>
<h2>ナビ起動前から渋滞を警告</h2>
<p>Geminiはまた、ユーザーの通勤や通学など“日常ルート”を学習し、ナビを起動していなくても交通渋滞や事故発生を検知した際に自動で通知を行う。プロアクティブなアラート機能により、出発前のタイミングで回避ルートを確認できるようになった。</p>
<p>@<a href="https://youtu.be/WJ7E8KTv034">YouTube</a></p>
<h2>Lens with Geminiで“周囲を質問”</h2>
<p>さらに、「Lens with Gemini」ではスマートフォンのカメラを向けた建物や店舗を即座に認識。たとえば、カメラを向けて「このレストランは何時まで開いてる？」と尋ねると、営業時間やレビューなどをGeminiが対話形式で返答する。従来の検索型から“会話する地図”への進化がうかがえる。</p>
<p>@<a href="https://youtu.be/mlhIxJCP9CM">YouTube</a></p>
<h2>提供開始と対応環境</h2>
<p>これらの機能は「Geminiが利用可能な地域」から順次展開される予定。対象はAndroidとiOSのGoogleマップアプリで、今後はAndroid Autoなど他プラットフォームへの拡張も検討されているという。</p>
]]></description>
      <pubDate>Fri, 14 Nov 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google、誰でも学べるAI学習サイト「Google Skills」を正式公開──Cloud・DeepMind・教育部門を横断する3000講座を展開</title>
      <link>https://ledge.ai/articles/google_skills_ai_learning_platform_launch</link>
      <description><![CDATA[<p>Googleは2025年10月21日（米国時間）、新しいAI学習プラットフォーム「Google Skills」を<a href="https://blog.google/outreach-initiatives/education/google-skills/">発表</a>した。同サイトでは、Google Cloud、Google DeepMind、Grow with Google、Google for Educationなど、同社の複数部門が提供してきた教育コンテンツを統合。3000種類を超えるAI関連の講座・体験ラボ・認定プログラムを、一元的に学べる学習拠点として開設された。</p>
<h2>AI教育の中核を担う新サイト</h2>
<p>Google公式ブログ「Start learning all things AI on the new Google Skills」によると、Google Skillsは“AI for Everyone（すべての人のためのAI）”をテーマに、誰もがAIスキルを体系的に学べるよう設計されている。初心者、エンジニア、企業リーダーなど幅広い層を対象に、AI、データ分析、クラウド、生成AIなど多様な分野を網羅。各コースはオンデマンド形式で受講でき、学習成果はLinkedInなどの外部プラットフォームで共有できる。提供内容には、Google Cloudの認定資格プログラムやAI Essentials シリーズ、DeepMindのAI倫理教材などが含まれる。</p>
<p>@<a href="https://www.youtube.com/watch?v=Qbix0BOPcgE">YouTube</a></p>
<p>今回の正式公開に先立ち、Google Cloudは10月10日付のブログ「Google Skills: Your new home for Google AI learning and more」で、新プラットフォームの構想を公表していた。当時は正式リリース前で、「AIやクラウドに関する学習リソースを一元化し、近日中に詳細を発表する」としていた。Gemini Code Assist（旧Duet AI for Developers）やQwiklabs（現Cloud Labs）と連携し、AIトレーニングの実践環境を統合する方針も示されていた。</p>
<h2>3000超のコースと実践的ラボを集約</h2>
<p>Google Skillsでは、Googleがこれまで個別に展開してきた学習リソースを一か所に集約。AIモデル開発、クラウド基盤運用、データ可視化、サイバーセキュリティなど、実践重視の3000超のコースとラボを提供する。一部コンテンツは無料で公開され、修了証や認定資格を取得することでキャリア開発にもつなげられる。また、組織向けにはチーム単位での進捗管理や学習成果の可視化機能も用意されている。</p>
<h2>今後の展望──教育機関・企業研修にも拡大へ</h2>
<p>Googleは今後、教育機関や企業研修への展開を進める方針を示しており、AIスキルの標準教育基盤としての活用を目指す。
公式ブログでは、「AI教育へのアクセスを民主化し、誰もがテクノロジーの未来を形づくる機会を得られるようにする」としている。
同社は今後もDeepMindやCloud AIチームの最新教材を追加し、AI人材育成をグローバルに推進する考えだ。</p>
]]></description>
      <pubDate>Fri, 14 Nov 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>LLM「密度化の法則」──“ムーアの法則”を凌ぐ速度で進化、同等性能に必要なサイズは3.5か月ごとに半減</title>
      <link>https://ledge.ai/articles/llm_densing_law_nature_machine_intelligence_2025</link>
      <description><![CDATA[<p>清華大学とOpenBMBの研究チームは、LLMの性能効率を示す「能力密度（capability density）」が、約3.5か月ごとに倍増する傾向を確認した。この速度は半導体分野で知られる“ムーアの法則”を上回るものであり、研究チームはこの現象を**「密度化の法則（Densing law）」** と名付けた。</p>
<p>結果は2025年11月6日付で 学術誌 Nature Machine Intelligence に<a href="https://www.nature.com/articles/s42256-025-01137-0">掲載</a>されている。</p>
<h2>モデル効率の“時間的スケーリング”</h2>
<p>論文によると、能力密度は「パラメータ単位あたりの能力」として定義され、参照モデルのスケーリング曲線から推定した有効パラメータ数を用いて算出する。</p>
<p>51種類のオープンソースLLM（Llama、Mistral、Gemma、DeepSeek など）を対象に、MMLU／BBH／MATH／HumanEval／MBPPの5ベンチで評価した結果、最大能力密度は時間とともに指数関数的に増加。回帰の結果、成長係数 A≈0.007、決定係数 R²≈0.93が得られ、倍増周期は ln(2)/A ≈ 約3.5か月と算出された（図参照）</p>
<p><strong>オープンソースベースの LLM の推定機能密度</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/42256_2025_1137_Fig1_HTML_0033cd1204/42256_2025_1137_Fig1_HTML_0033cd1204.webp" alt="42256_2025_1137_Fig1_HTML.webp" /></p>
<p>さらに、データ汚染を取り除いたMMLU-CFによる検証でも、A≈0.0066、R²≈0.95と近い結果を示し、データ依存に偏らない傾向であることが裏づけられた（図参照）。</p>
<p><strong>MMLU-CF による検証（汚染除去ベンチ）</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/The_estimated_capability_density_on_a_contamination_free_dataset_MMLU_C_54b2763b07/The_estimated_capability_density_on_a_contamination_free_dataset_MMLU_C_54b2763b07.jpg" alt="The estimated capability density on a contamination-free dataset MMLU-C.jpg" /></p>
<h2>ChatGPT以降、成長速度が1.5倍に</h2>
<p>研究では、ChatGPT公開（2022年末）を境に密度の成長率が約1.5倍に加速したことも示された。ChatGPT以前はA≈0.0048だったが、その後はA≈0.0073へ上昇している（図参照）。背景として、オープンソースモデルの拡充や効率的学習手法の普及が挙げられている。</p>
<p><strong>密度化の法則に関する追加観察（圧縮比較／前後比較）</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Density_evaluated_using_MMLU_ba863b49af/Density_evaluated_using_MMLU_ba863b49af.jpg" alt="Density evaluated using MMLU.jpg" /></p>
<h2>「半分のパラメータ」で同等性能、コストも急減</h2>
<p>密度化の法則が示唆するのは、同等性能に必要なパラメータ数の指数的減少である。実際、後発の小型モデルが、より大きな先行モデルと同等水準に迫るケースが各種ベンチで観測されている。
またAPI価格の代表例として、GPT-3.5（2022年末）で100万トークンあたり20ドルだったのに対し、Gemini 1.5 Flash（2024年夏）では0.075ドルまで低下しており、推論コストも短い周期で半減している（図参照）。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/API_prices_of_LL_Ms_d82e75a156/API_prices_of_LL_Ms_d82e75a156.jpg" alt="API prices of LLMs.jpg" /></p>
<h2>「Mooreの法則」と掛け合わせた未来</h2>
<p>研究チームは、半導体のMooreの法則（トランジスタ密度の倍増）と密度化の法則を組み合わせると、固定価格のチップ上で実行可能な“有効パラメータ数”は約88日で倍増するとの試算を示す。これにより、エッジデバイスでの高性能推論が加速する可能性が高い。</p>
<h2>圧縮は万能ではない</h2>
<p>量子化や蒸留などの圧縮は常に密度向上を保証しない。研究では、Gemma-2-9Bが例外的に密度改善を示した一方、十分な再訓練を伴わない圧縮は密度低下につながるケースも確認された（図参照）。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Comparison_between_compressed_models_and_their_larger_counterparts_a7b553377f/Comparison_between_compressed_models_and_their_larger_counterparts_a7b553377f.jpg" alt="Comparison between compressed models and their larger counterparts.jpg" /></p>
<h2>「密度最適訓練」への転換</h2>
<p>著者らは、今後は単なる巨大化ではなく「密度最適訓練（density-optimal training）」へ軸足を移すべきだと提言する。大規模モデルから小規模モデルへの知識移転と、再びそれが大規模側の効率化に寄与するという相互進化が、持続可能な開発の鍵になるという。
同時に、能力密度の指数成長には理論的上限があるため、将来的には量子計算や神経形態計算など新たな計算パラダイムの検討も必要になると見解を示す。</p>
]]></description>
      <pubDate>Fri, 14 Nov 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>アルトマン氏「エロティックばかり注目されたけど」──ChatGPT、成人ユーザーの自由拡大へ</title>
      <link>https://ledge.ai/articles/openai_chatgpt_adult_mode_update_oct2025</link>
      <description><![CDATA[<p>OpenAIのサム・アルトマンCEOは10月14日（現地時間）、X（旧Twitter）上で、ChatGPTの安全制限を一部緩和し、成人認証済みユーザーに対してエロティックな会話を許可する方針を<a href="https://x.com/sama/status/1978129344598827128">発表</a>
した。</p>
<p>投稿は瞬く間に注目を集め、「エロティック解禁」が大きな話題となったが、アルトマン氏は翌日に「その部分ばかり注目されてしまったが」と<a href="https://x.com/sama/status/1978539332215681076">補足</a>し、実際には“より人間らしいAI体験”を実現するための包括的な方針変更であることを強調した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/gpt5_erotica_7cea863241/gpt5_erotica_7cea863241.jpg" alt="gpt5 erotica.jpg" /></p>
<h2>安全を優先してきたChatGPTの制限</h2>
<p>ChatGPTはこれまで、性的表現や親密な会話を含むコンテンツを厳しく制限してきた。
アルトマン氏は「メンタルヘルス問題に慎重を期すためだった」と説明し、精神的に不安定なユーザーに配慮した措置であったと振り返った。
「深刻な危機状態にあるユーザーは別扱いとし、他者に害を与える行為は依然として許可しない」と述べ、ポリシーの根幹は維持されるとしたうえで、「リスクのない成人ユーザーにはより多くの自由を与える」と明言した。</p>
<h2>“4oらしさ”を再導入──人間的なAI体験へ</h2>
<p>アルトマン氏は同日、「数週間以内に“4oで好まれた振る舞い”に近い人格（パーソナリティ）を選べる新バージョンのChatGPTを提供する」と投稿した。
GPT-4oは会話の自然さや表情豊かな応答で人気を集めたモデルであり、今後はユーザーが望む場合に、フレンドリーな口調や絵文字を多用した“人間らしい”対話スタイルを選べるようになる。
アルトマン氏は「これは利用時間を増やすためではなく、ユーザーが自分の望む形でAIと関わる自由を得るための設計だ」と述べている。</p>
<h2>「成人は大人として扱う」──12月に年齢認証を本格導入</h2>
<p>アルトマン氏は、12月に年齢認証を本格導入し、認証済み成人ユーザーに対してはエロティック会話なども許可する方針を示した。
一方で、ティーンエイジャーに対しては「安全をプライバシーや自由より優先する」と述べ、メンタルヘルス関連ポリシーは緩めないと強調している。</p>
<p>アルトマン氏は、「社会がR指定映画で境界を設けるように、AIにも適切な年齢境界を設けたい」と例え、「我々は選挙で選ばれた道徳警察ではない」と付け加えた。</p>
<h2>倫理と自由の境界線</h2>
<p>アルトマン氏の発言は、AIにどこまで人間的な自由を与えるかという議論を再燃させた。</p>
<p>OpenAIは今後、成人向け表現やAIの人格設計に関するガイドラインをさらに明確化するとみられる。今回の方針転換は、「AIをどう設計し、どう育てるか」という人間社会全体のテーマに踏み込む第一歩となりそうだ。</p>
]]></description>
      <pubDate>Fri, 14 Nov 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Sony AI、「同意に基づく世界初の人間中心データセット」FHIBEを公開──AIの公平性を評価する新たな国際基準に</title>
      <link>https://ledge.ai/articles/sony_ai_fair_human_centric_image_benchmark_fhibe_release</link>
      <description><![CDATA[<p>ソニーグループ傘下のSony AIは2025年11月5日、AIモデルの公平性を評価するための人間中心データセット「Fair Human-Centric Image Benchmark（FHIBE）」<a href="https://ai.sony/articles/Groundbreaking-Fairness-Evaluation-Dataset-From-Sony%20AI%20/">公開</a>した。</p>
<p>これは、被写体の明示的な同意に基づいて収集された世界初の公開データセットであり、AIの倫理的・公平な運用を支える新たな国際基準となることを目指す。研究論文は同日、科学誌『Nature』に掲載された。</p>
<h2>81カ国・1981人の被写体、すべて“同意ベース”で収集</h2>
<p>FHIBEは、顔検出・ポーズ推定・ビジュアル質問応答（VQA）など、AIの人間中心タスクにおける公平性を測定するためのデータセット。
10,318枚の画像、1,981人の被写体から構成され、81カ国以上の地域的・文化的多様性を反映する。
Sony AIによると、FHIBEは「consensually-collected, globally diverse fairness evaluation dataset」──すなわち「同意に基づき、世界的多様性を備えた公平性評価用データセット」であり、これまで主流だった“非同意収集データ”の問題を根本から見直す試みだ。</p>
<p>Sony AIのプレジデント、Michael Spranger氏は次のように述べている。</p>
<p>\u003E「FHIBEは、公平性と説明責任を備えたAIを支える業界標準を築くものです。AIユーザー、クリエイター、データ提供者を含むすべての関係者を尊重しながら、公平で透明なテクノロジーを実現する道を示しました。」</p>
<p>また、ソニーグループのAIガバナンス統括責任者であり、Sony AIのAI倫理主任研究員でもあるAlice Xiang氏は次のように強調した。</p>
<p>\u003E「責任あるデータ収集は可能です。被写体への説明、同意、報酬、プライバシー保護、多様性、安全性を同時に担保したプロセスを実現しました。これはAI開発における倫理的転換点です。」</p>
<h2>公平性を測る「倫理的ベンチマーク」</h2>
<p>FHIBEの画像には、被写体自身が申告した性別代名詞、年齢層、肌トーン、髪型などの属性が含まれており、第三者の主観に頼らない“自己申告ベース”のアノテーションが特徴。
さらに照明条件やカメラ設定、撮影環境なども細かく記録され、AIモデルが属性や環境要因によって出力を偏らせていないかを定量的に検証できる。</p>
<p>論文「Fair human-centric image dataset for ethical AI benchmarking」（Nature, 2025）では、FHIBEを用いてCLIPやBLIP-2など複数の画像生成・認識モデルを評価。
既知のバイアス（例：She/Her/Hers 代名詞の被写体に対する精度低下）を再確認しただけでなく、髪型の多様性が精度差の一因であることなど、これまで見落とされていた構造的要因を特定した。
さらに、「この人の職業は？」といった中立的質問への回答で、特定の属性を犯罪や低賃金職に結びつけるステレオタイプ的出力が確認されるなど、AIの無意識バイアスを明らかにしている。</p>
<h2>被写体の権利を保護する設計</h2>
<p>FHIBEの特徴は、データの提供後も被写体が自らの権利を保持している点にある。
参加者は同意をいつでも撤回でき、撤回しても報酬への影響はない。Sony AIは撤回画像を削除し、可能な範囲で置換・更新することで、データセットの多様性を維持しながら透明性を担保する。
この仕組みにより、FHIBEは「静的なデータセット」ではなく、継続的に更新される“生きた倫理基盤”として運用される。</p>
<h2>責任あるAI研究への布石</h2>
<p>Sony AIによると、FHIBEは法務・プライバシー・IT・品質保証（QA）専門家の協力のもと、3年をかけて構築された。
公開データは研究・教育目的で無償提供され、誰でも公平性検証に活用できる。
Sony AIは今後、モデル評価ツールやベンチマークスコアの公開も予定している。</p>
<p>The Registerは同日、「Sony rolls out a standard way to measure bias in how AI describes what it ‘sees’」と題した記事で、FHIBEを「AI視覚バイアス評価の新標準」と評した。
また『Nature』のNews &amp; Views欄も、「倫理的に収集された画像データセットがAI研究の公平性を促進する」と高く評価している。</p>
]]></description>
      <pubDate>Fri, 14 Nov 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Meta、広告向け基盤モデル「GEM」を公開──Instagramコンバージョン5％増の“中央脳”AIを正式解説</title>
      <link>https://ledge.ai/articles/meta_gem_ads_foundation_model_release</link>
      <description><![CDATA[<p>Metaは米国時間2025年11月10日、広告レコメンデーション向けの大規模AIモデル「GEM（Generative Ads Recommendation Model）」の技術詳細を<a href="https://engineering.fb.com/2025/11/10/ml-applications/metas-generative-ads-model-gem-the-central-brain-accelerating-ads-recommendation-ai-innovation/">発表</a>した。</p>
<p>GEMは同社の広告システムにおける“中央脳（central brain）”となる基盤モデル（foundation model）で、既存の広告モデル群全体の性能を底上げする役割を担う。</p>
<p>Metaによれば、2025年のローンチ以降、GEMはInstagram広告のコンバージョン率を約5％、Facebookフィード広告を約3％改善する成果を示している。広告主のROI向上を支える重要なモデルとして位置づけられている。</p>
<h2>LLM級スケールで学習した広告向け「中央脳」モデル</h2>
<p>MetaはGEMを「現代の大規模言語モデル（LLM）と同等クラスのスケール」で設計しており、数千基規模のGPUクラスター上で学習を実施したと説明する。
トレーニング基盤は大幅に刷新され、学習に用いるGPU数を16倍に拡張。PyTorch 2.0のグラフコンパイルやカスタムGPUカーネル、FP8量子化などを組み合わせることで、従来比で有効学習FLOPsを約23倍に高めつつ、ハードウェア効率（MFU）も約1.4倍に向上させたという。</p>
<p>これにより、広告全体で利用する複数のモデル群をまたぎ、GEMの知識を効率よく伝達できる構造を実現した。</p>
<h2>長期行動履歴を扱うアーキテクチャ──「InterFormer」による特徴学習</h2>
<p>GEMの中核には、ユーザーの長期的な行動履歴を一貫して学習するアーキテクチャが採用されている。</p>
<p>Metaが開発した「Offline Sequence Feature Modeling」では、ユーザーの広告やオーガニック投稿に対する行動履歴を“数千イベント規模”まで拡張し、詳細なシーケンスとして学習可能にする。</p>
<p>さらに、これらのシーケンスを適切に相互作用させるための新構造「InterFormer」が導入されている。従来のように行動履歴を単一ベクトルに圧縮するのではなく、シーケンス全体の情報を保持したまま、特徴間の関連性を段階的に学習する仕組みだ。
これにより、広告配信面（Facebook、Instagramなど）ごとに異なる利用行動をより精密にモデル化できるようになったとしている。</p>
<p><strong>図1：シーケンス埋め込みと非シーケンス埋め込みを統合し、Wukong／Sequence Modeling によって学習するGEMの内部構造。Facebook／Instagramのクリックやコンバージョン予測に対応</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Meta_Generative_Ads_Model_GEM_image_2_16d35f3329/Meta_Generative_Ads_Model_GEM_image_2_16d35f3329.webp" alt="Meta-Generative-Ads-Model-GEM-image-2.webp" /></p>
<h2>GEM → Foundation Model → 垂直モデルへ階層的に知識を伝達</h2>
<p>Metaの広告AIは、GEMの下に多数のファウンデーションモデル（FM）と垂直モデル（Vertical Models, VM）が存在する多層構造をとる。</p>
<p>この階層へGEMの知識を効率的に伝達するため、Metaは「直接的な知識移転」「階層的蒸留」「表現学習」「一部パラメータ共有」など複数の手法を組み合わせている。同社はこれにより、従来の標準的な知識蒸留を使った場合と比べて、同等条件で得られる性能改善効果が約2倍になったと説明している。</p>
<p>レイテンシやコスト制約の大きい垂直モデルでは、GEMのすべてをそのまま継承するのではなく、必要なコンポーネントだけを共有して使う設計も採用されている。</p>
<p><strong>図2：GEMから基盤モデル、垂直モデルへと知識を段階的に伝達するMetaの広告AIアーキテクチャ</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Meta_Generative_Ads_Model_GEM_image_1_e1762534706646_5fd652bc4a/Meta_Generative_Ads_Model_GEM_image_1_e1762534706646_5fd652bc4a.webp" alt="Meta-Generative-Ads-Model-GEM-image-1-e1762534706646.webp" /></p>
<h2>今後の方向性──マルチモーダル統合とエージェント的広告自動化へ</h2>
<p>Metaは今後、GEMを画像・動画・音声などを含むマルチモーダル学習へと拡張し、FacebookやInstagramの複数の面（surface）をまたぐ統合的なランキングモデルへ発展させる計画を示している。</p>
<p>また、広告主の入稿・改善作業を支援する**エージェント的な自動化（agentic automation）**にも取り組むとしており、GEMを軸にした次世代の広告運用フローを構築するとしている。</p>
<p>GEMはすでに広告コンバージョンにおいて実績を上げており、Metaは、同社全体の広告システムを支える基盤として今後も中心的な役割を担うと見込んでいる。</p>
]]></description>
      <pubDate>Fri, 14 Nov 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>菱洋エレクトロ、パーソナルAIコンピューター「NVIDIA DGX Spark」の出荷開始──デスクトップで2000億パラメータ級LLM推論を実現</title>
      <link>https://ledge.ai/articles/ryoyo_nvidia_dgx_spark_shipping_start</link>
      <description><![CDATA[<p>エレクトロニクス商社の菱洋エレクトロ株式会社は2025年11月11日、NVIDIAのパーソナルAIコンピューター「NVIDIA DGX Spark」の出荷を開始したことを<a href="https://www.ryoyo.co.jp/info/news/34499/">発表</a>した。同社は7月1日に取り扱い開始を発表しており、今回のリリースにより、日本国内での出荷が具体的にスタートしたかたちとなる。</p>
<p>NVIDIA DGX Sparkは、デスクに設置可能なコンパクト筐体に、NVIDIA GB10 Grace Blackwell Superchipと128GBの統合メモリを搭載するAIコンピューター。最大2,000億パラメータ規模の大規模言語モデル（LLM）の推論や、プロトタイプ開発・評価をローカル環境で実行できる点が特徴となっている。</p>
<p>CES 2025の基調講演で、BlackwellアーキテクチャとAIデスクトップ構想を紹介する創業者兼CEOのジェンスン・フアン氏
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/nvidia_ces2025_digitsproject4_2ef18ffd02/nvidia_ces2025_digitsproject4_2ef18ffd02.jpg" alt="nvidia ces2025 digitsproject4.jpg" /></p>
<p>Grace BlackwellアーキテクチャによるFP4精度で最大1,000 AI TOPSの性能を備え、2台接続時には4,050億パラメータ規模のモデルにも対応可能。NVIDIA Base OSやNVIDIA AIソフトウェアスタック、JupyterやPyTorchなどの主要ツールがあらかじめ導入されており、開発者はデスクトップ環境からすぐにモデルの実験・検証を始められる構成となっている。</p>
<p>菱洋エレクトロによると、すでに大学や企業の研究機関から多数の注文が寄せられており、先端的な研究機関に加えて、国内企業のAIプロジェクトに関わる部門への提案も積極的に進めているという。自社データを活用し、オンプレミス環境でLLM（大規模言語モデル）開発を行いたいと考える企業から高い関心が寄せられていると説明している。</p>
<p>また、NVIDIAの説明では、DGX Sparkで構築したデータやモデルを、NVIDIA AI EnterpriseやNVIDIA DGX Cloudを含むクラウドやデータセンターのAIインフラへシームレスに展開できるとしており、デスクトップでの開発から本番環境へのスケールアップも容易だとされている。</p>
<p>同社はNVIDIA一次代理店として自社在庫による迅速な納品体制を整えており、製品ページでは「NVIDIA DGX Sparkは、菱洋エレクトロが最終エンドユーザーへ直接販売可能な製品」であり、「個人購入も可能」と案内している。現在は多くの注文を受けており、順次出荷対応を行っているため、出荷までに時間を要する場合がある点にも言及している。</p>
<p>菱洋エレクトロは、半導体・デバイスとICTソリューションを軸に、AIやIoTなど次世代技術の実装を支援する事業を展開しており、パーソナルAIコンピューターの取り扱い拡大も、その一環として位置づけている。</p>
]]></description>
      <pubDate>Fri, 14 Nov 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>エンタメ＆アート2025/11/13 [THU]MIT、テキストからマルチトラックMIDI音楽を生成する言語モデル「MIDI-LLM」を開発──Llama 3.2を拡張し、高速推論と高品質な出力を実現</title>
      <link>https://ledge.ai/articles/mit_midi_llm_text_to_midi_ai</link>
      <description><![CDATA[<p>マサチューセッツ工科大学（MIT）の研究チームは2025年11月6日、自然言語の指示からマルチトラックMIDI音楽を生成できる言語モデル「MIDI-LLM」を<a href="https://arxiv.org/abs/2511.03942v1">発表</a>した。ベースとなる大規模言語モデル「Llama 3.2 1B」の語彙を拡張し、MIDIトークンを直接扱えるようにした点が特徴。生成結果は従来モデル「Text2midi」に比べて高品質かつ高速で、編集・再利用が容易なシンボリック音楽データを生成できる。</p>
<h2>言葉から“楽譜”を生み出すLLM</h2>
<p>MIDI（Musical Instrument Digital Interface）とは、楽器や音楽ソフトのあいだで「どの音を、いつ、どのくらいの強さで鳴らすか」を指示するためのデータ規格である。実際の音そのものではなく、演奏情報を数値として記録する「デジタル譜面」のような形式で、後から編集や再構成がしやすいのが特徴だ。つまりAIがMIDIデータを生成するというのは、音を出すのではなく、楽曲の構造や演奏指示そのものを自動で書き上げることを意味している。</p>
<p>近年、テキストからオーディオを生成するAIが登場しているが、音声出力は後編集が難しいという課題があった。MIDIなどのシンボリック音楽データは、楽譜構造を保持したまま再編集できるため、音楽制作やゲーム、映像音楽などの分野で需要が高い。</p>
<p>MITの研究チームは、こうした編集可能性とテキスト制御性を両立させるため、言語モデルを直接MIDI形式に適応させる「MIDI-LLM」を提案した。研究はNeurIPS 2025 Workshop「AI for Music」で発表され、コード・学習済みモデル・デモサイトが一般公開されている。</p>
<h2>Llama 3.2をMIDIトークン対応に拡張</h2>
<p>MIDI-LLMは、Meta社のLlama 3.2 （1 Bパラメータ）を基盤に、音楽用トークンを追加して構築された。音符は「発音時刻（onset time）」「音の長さ（duration）」「楽器と音高（instrument-pitch）」の3つのトークンで表現され、Anticipatory Music Transformer（AMT）の到着時間トークン化手法を採用。これにより、既存のLLM構造を保ちながら音楽表現を学習でき、推論時にはvLLM ライブラリによる最適化がそのまま利用できる。</p>
<p><strong>図：MIDI-LLMの構造と学習プロセス</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x1_9_f067c27669/x1_9_f067c27669.png" alt="x1 (9).png" /></p>
<h2>2段階の訓練でテキスト→MIDIを習得</h2>
<p>研究では、以下の2段階でモデルを訓練した。</p>
<h3>1. 継続事前学習（Continued Pretraining）</h3>
<p>音楽関連テキスト（MusicPile など）とスタンドアロンMIDIデータ（GigaMIDI など）を約30億トークン規模で学習し、音楽構造と文脈理解を強化。</p>
<h3>2. 教師ありファインチューニング（Supervised Finetuning）</h3>
<p>テキストとMIDIのペアデータ（MidiCaps ＋ Lakh MIDI Dataset）を使用し、ジャンル・テンポ・ムードなどの言語的指示から対応する音楽を出力できるよう訓練。データ拡張では音楽キャプションモデル「Qwen 2.5 Omni」を活用し、多様なプロンプトで補強した。</p>
<h2>「Text2midi」を上回る品質と速度</h2>
<p>評価実験では、MIDI-LLMが従来モデル「Text2midi」（AAAI 2025）を大きく上回る性能を示した。</p>
<ul>
<li><strong>FAD（Fréchet Audio Distance）</strong> ： 0.216 （Text2midi 0.818）</li>
<li><strong>CLAPスコア</strong> （テキストと音楽の一致度）： 21.8 （Text2midi 18.7）</li>
<li><strong>推論速度（RTF）</strong> ： 約14 倍高速化（FP8量子化使用時）</li>
</ul>
<p>なお、評価はMidiCapsテストセットの交差896サンプルで実施され、FAD/CLAP算出のためにMIDIをFluidSynthでレンダリングして音声化した上で指標を計測している。
vLLMによるCUDA Graph・Paged Attention・FP8量子化を導入したことで、従来の構造より50％以上の推論効率化を実現したという。</p>
<h2>デモサイトを公開、誰でも体験可能</h2>
<p>MIDI-LLMの<a href="https://midi-llm-demo.vercel.app/">デモサイト</a>では、「Epic Rock」「Playful Jazz」「Sad &amp; Emotional」などのプリセット、または自由な文章入力から音楽を生成できる。生成結果はMIDIファイルとしてダウンロード可能で、ブラウザ上で再生もできる。</p>
<p><strong>MIDI-LLMデモサイトの画面</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/midi_llm_1c07755bdd/midi_llm_1c07755bdd.jpg" alt="midi-llm.jpg" /></p>
<h2>今後の課題と展望</h2>
<p>論文では、テキスト付きインフィリング（曲の一部を補完する生成）では、テキスト条件の影響が小さいという課題も指摘された。
また、音楽特化テキストを使わなくても性能差が見られなかったことから、事前学習データ設計の最適化が今後の課題とされている。</p>
<p>今後は、ユーザーフィードバックを活用したRLHF（人間フィードバックによる好み学習）やDPO（Direct Preference Optimization）を導入し、ユーザーの音楽嗜好に合わせた生成を目指すとしている。
研究チームは「テキストで音楽を編集・再構成できるAIの実現」を次のステップに掲げている。</p>
]]></description>
      <pubDate>Thu, 13 Nov 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>楽天と日本HP、AIエージェント「Rakuten AI」デスクトップ版をHP製PCに初導入──オンデバイスAIでオフライン利用にも対応</title>
      <link>https://ledge.ai/articles/rakuten_hp_rakuten_ai_desktop_collaboration</link>
      <description><![CDATA[<p>楽天グループと日本HPは2025年11月11日、HPが日本国内で販売するほぼすべてのPCに、楽天のAIエージェントツール「Rakuten AI」のデスクトップ版を導入する協業を<a href="https://corp.rakuten.co.jp/news/press/2025/1111_01.html">発表</a>した。</p>
<p>ユーザーの生産性向上や意思決定の包括的な支援を目的とし、2026年春から夏にかけて、個人および法人向けのHP製デバイスに順次プリバンドルされる予定。この協業は、他社デバイスへの「Rakuten AI」導入として初であり、オンデバイスAIによるオンライン・オフライン両対応も今回が初の取り組みとなる。</p>
<h2>オンデバイス×クラウドのハイブリッドAI構成</h2>
<p>「Rakuten AI」のデスクトップ版は、ローカルモデルをオンデバイスAIとして実行可能で、機密データのプライバシー保護を強化しながら複雑なタスクを処理できる。クラウド、エッジ、デバイス上のエージェントやモデルの中から状況に応じて最適なものを選択し、オフライン環境下でも途切れることなく利用できる仕組みだ。
クラウド依存を最小限に抑えることで、パフォーマンス向上とコスト削減を両立する。
※利用可能な機能はオンライン環境とオフライン環境で異なる。</p>
<p>同AIには、要約・ライティング・翻訳といった汎用的な機能に加え、ショッピングや旅行予約、家計管理などを支援する取引エージェント機能を搭載。楽天が展開する70以上のサービスとシームレスに連携し、ユーザーの利便性を高める。
また、日本語と日本文化に最適化された大規模言語モデル（LLM）を採用し、国内のプライバシー・データセキュリティ関連法令にも準拠している。</p>
<h2>デスクトップ版「Rakuten AI」の概要</h2>
<p>PC向けに開発されたデスクトップ版「Rakuten AI」は、楽天独自の日本語最適化LLMを採用。ローカルデバイス上で機密性の高いAIタスクを直接処理し、プライバシーを保護しながらクラウド通信のコストを抑制する。楽天エコシステムとの接続を通じて、業務効率や日常生活の生産性向上支援を狙う。</p>
<p>両社は今後も、AI搭載デバイスとソリューションの展開を通じて「働き方や暮らしの未来を再定義し、日本のPC市場に新たな価値を創造する」としている。</p>
]]></description>
      <pubDate>Thu, 13 Nov 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/11/12 [WED]Wikipedia、AI企業に「知の還元」を要請──無断スクレイピングから有料API「Wikimedia Enterprise」へ、持続可能な共創を目指す</title>
      <link>https://ledge.ai/articles/wikipedia_ai_scraping_stop_enterprise_api</link>
      <description><![CDATA[<p>「Wikipedia」を運営する非営利団体ウィキメディア財団（Wikimedia Foundation）は、米国時間2025年11月10日、AI企業に対し、AIモデルのトレーニングを目的としたデータ収集（スクレイピング）を停止し、同財団が提供する有料API「Wikimedia Enterprise」を利用するよう求める声明を<a href="https://wikimediafoundation.org/news/2025/11/10/in-the-ai-era-wikipedia-has-never-been-more-valuable/">発表</a>した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/wikimedia_261c218f50/wikimedia_261c218f50.jpg" alt="wikimedia.jpg" /></p>
<p>声明は「AI時代において、ウィキペディアはこれまでになく価値がある」とし、持続可能な知識共有の観点から帰属（アトリビューション）と財政的支援の必要性を明確に示した。出典の明示やデータ更新の正確性、ライセンス遵守をAPI経由で担保できると説明している。</p>
<p>財団によれば、生成AIの普及によりAI企業による無断スクレイピングとボットアクセスが増加し、サーバー負荷や読者体験への影響が懸念される。一方で、人間によるページビューは2025年上半期に前年比約8％減となったとし、基盤を支える寄付・ボランティア編集への波及を指摘した。</p>
<p>財団は、AIは人間が記録・検証した知識（例：Wikipedia）に依存していると改めて説明。生成AIは既存知の要約・統合はできても、Wikipediaのボランティア編集者が日々行う議論・検証・合意形成、アーカイブからの発掘、現場写真の提供といった活動を代替できないとした。Wikipediaは300以上の言語版で、しばしばネイティブ執筆者による多言語コーパスを形成しており、包摂的で文化的背景に配慮したAIモデルの発展にも資する、としている。</p>
<p>透明性も強調された。Wikipediaでは全員が同一の情報を閲覧でき、パーソナライズ配信や行動追跡による出し分けは行われない。記事には出典が付与され、編集履歴や運用プロセスは公開されている。誰でも方針・ガイドラインに従って加筆できる点が「信頼」の源泉であるとし、対照的に生成AIはハルシネーション（もっともらしい誤情報の提示）を起こし得ると説明した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/wikimedia_enterprise_api_20e0f88941/wikimedia_enterprise_api_20e0f88941.jpg" alt="wikimedia enterprise api.jpg" /></p>
<p>同財団は、Wikipedia自体も人を支える形でAIを活用していると述べる。ボランティアの時間を奪う荒らし検知など単純作業の効率化を狙い、今年公表した編集者向けAI戦略では、人間の知識創造を補助し置き換えない方針を示した。AIツールの利用指針はコミュニティが策定・施行し、責任ある活用を徹底する。</p>
<p>総括として財団は、AIはWikipedia抜きでは成立しないとし、AI開発者や再利用者に対し次の2点を要請した。</p>
<ul>
<li>アトリビューション（出典表示）：人間の貢献にクレジットを付し、元情報源への導線を明確にする。</li>
<li>財政的支援：大規模利用はWikimedia Enterpriseを通じて行い、サーバー負荷を抑えつつ非営利ミッションを継続可能にする。</li>
</ul>
<p>同財団は、Wikipediaが検証可能性・中立性・透明性の標準でインターネット上の情報を支えているとし、「AIがあふれる世界で、人間の知識の価値はかつてなく高い」と述べた。なお、Wikipediaは2026年1月15日に25周年を迎える予定で、今後も無料で正確な人間の知識を提供し続けるとした。</p>
]]></description>
      <pubDate>Wed, 12 Nov 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>中国Moonshot、1兆パラメータ級『Kimi K2 Thinking』を公開──7月発表のKimi K2を基盤に推論性能とツール連携を強化したオープンソースAI</title>
      <link>https://ledge.ai/articles/kimi_k2_thinking_open_source_release_2025</link>
      <description><![CDATA[<p>北京を拠点とするAI企業・Moonshotは2025年11月6日（米国時間）、大規模言語モデル「Kimi K2」を基盤とした推論特化モデル「Kimi K2 Thinking」を<a href="https://moonshotai.github.io/Kimi-K2/">発表</a>した。
Kimi K2シリーズはオープンウェイトで提供され、最新のInstruct版「Kimi K2-Instruct-0905」はHugging Faceで配布されている。</p>
<h2>Kimi K2を基盤とした“思考” 指向モデル</h2>
<p>Kimi K2 Thinkingは7月公開の「Kimi K2」をベースに開発されたという。アーキテクチャは共通で、総パラメータ数は約1.04兆、発火時32BのMoE構成を持つ。学習には15.5兆トークンを使用し、安定学習を実現するために「MuonClip（QK-Clip）」と呼ばれる独自最適化手法を導入。これにより長期的なトークン依存関係を維持しながら、ロススパイクを抑制している。</p>
<p>思考やマルチステップ推論、ツール呼び出しを重視した訓練データと報酬学習を実施しており、Moonshotは本モデルを「エージェント時代のための思考型LLM」と位置づけている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/kimiai_k2thinking_127403b675/kimiai_k2thinking_127403b675.jpg" alt="kimiai k2thinking.jpg" /></p>
<h2>256Kトークンの文脈ウィンドウを実装</h2>
<p>最新版の「Kimi K2-Instruct-0905」では、文脈長を256Kトークンに拡張。従来版（128K）から倍増し、長大な文書解析や複雑な会話シナリオへの対応を可能にした。APIはOpenAIやAnthropicの形式に準拠しており、vLLMやSGLangなど主要推論エンジンにも対応する。
これにより、既存のアプリケーション環境に容易に統合できる点も特徴だ。</p>
<h2>推論・ツール使用で高評価</h2>
<p>公式の技術レポートによると、Kimi K2 Thinkingは複数の思考タスクベンチマーク（Tau2、SWE-Benchなど）で高いスコアを記録。特に「non-thinking」設定（思考ステップ制限）と「thinking」設定（自由推論）を分けて評価する手法を採用し、モデル性能の透明性を強調している。これにより、ユーザーが実環境での出力挙動を予測しやすくしている。</p>
<h2>Modified MITライセンスでオープン提供</h2>
<p>Moonshotは、Kimi K2シリーズを「Modified MIT License」でオープンソースとして公開している。Hugging Face上でモデルウェイトが一般公開されており、研究・商用利用の両方が可能だ。</p>
<p>最新モデル「Kimi K2-Instruct-0905」は以下で提供されている。<a href="https://huggingface.co/moonshotai/Kimi-K2-Instruct-0905">https://huggingface.co/moonshotai/Kimi-K2-Instruct-0905</a>
技術仕様やアーキテクチャの詳細は、公式サイト「<a href="https://moonshotai.github.io/Kimi-K2/">Kimi K2</a>」および公開PDF「Kimi K2 Technical Report」で確認できる。</p>
<h2>中国発オープンモデルの新潮流</h2>
<p>Kimi K2 Thinkingは、中国企業によるオープンソースLLMとして国際的にも注目を集めている。
1兆パラメータ級の思考モデルをライセンス制限なく公開する動きは、AI開発の透明性と研究競争力の双方を高める試みといえる。
Moonshotは、K2シリーズを通じて「AIの思考能力を民主化する」と掲げ、今後もグローバルな開発者との協働を拡大していくとしている。</p>
]]></description>
      <pubDate>Wed, 12 Nov 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI、「GPT-5.1」をリリース──会話性とトーン設定を強化したChatGPT最新版</title>
      <link>https://ledge.ai/articles/openai_gpt_5_1_release</link>
      <description><![CDATA[<p>OpenAIは2025年11月12日（米国時間）、大規模言語モデル「GPT-5」のアップデート版となる「GPT-5.1」を<a href="https://openai.com/index/gpt-5-1/">発表</a>した。新たに「GPT-5.1 Instant」と「GPT-5.1 Thinking」の2モデルをChatGPTとAPI向けに展開し、会話性、指示遵守性、トーン設定の柔軟性を向上させたとしている。今回の更新は、GPT-5世代の中で段階的に改良を進める「5.x」シリーズの第一弾と位置づけられる。</p>
<h2>GPT-5.1とは──GPT-5世代の会話性アップデート</h2>
<p>GPT-5.1は、GPT-5を基盤としつつ、回答の自然さやインタラクションのしやすさを強化した改良モデルである。OpenAIは今回の更新を「GPT-5の進化版」と説明しており、名前に「5.1」を付与した理由として、「同一世代内の意味のあるアップデート」であることを示す意図を挙げている。</p>
<p>ChatGPTでは、ユーザーの入力の難度に応じて最適なモデルを自動選択する「Auto」ルーティングが継続され、Instant／Thinkingが適宜切り替わる仕組みとなる。</p>
<h2>GPT-5.1 Instant──より会話的で、指示に忠実な標準モデル</h2>
<p>GPT-5.1 Instantは、従来のGPT-5 Instantの役割を引き継ぐ標準モデルで、回答スタイルがより自然で“暖かい（warmer）”会話に調整された。プロンプトに対する遵守性も改善され、「6語で答えてほしい」といった制約付き指示への一貫性が向上したとしている。</p>
<p>また、必要に応じて内部の思考時間をわずかに伸ばす「軽い自動推論（adaptive reasoning）」を採用。AIMEなどの数学ベンチマークやコード解析系タスクにおける性能が向上した例が示されている。</p>
<p><strong>GPT-5 Instant → GPT-5.1 Instant の比較</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/1762974348590_4bf99b57e6/1762974348590_4bf99b57e6.jpg" alt="1762974348590.jpg" /></p>
<h2>GPT-5.1 Thinking──思考時間の自動最適化で推論性能を強化</h2>
<p>GPT-5.1 Thinkingは、推論特化モデル「GPT-5 Thinking」を改良したバージョンで、タスクの難度に応じて思考時間をより細かく最適化するようにアップデートされた。OpenAIは「最も短い思考タスクでは約2倍速く、最も難しいタスクでは約2倍長く考える」と説明しており、簡単な質問への応答速度と、難しい問題への粘り強さを両立させたとしている。</p>
<p>説明スタイルは平易な英語に統一され、技術・専門用語の使用を抑えたことにより、ビジネス文書や技術解説でも読みやすさが向上している。</p>
<p><strong>GPT-5 Thinking → GPT-5.1 Thinking の比較</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/1762974348726_437b92d9f8/1762974348726_437b92d9f8.jpg" alt="1762974348726.jpg" /></p>
<h2>トーン・スタイルのプリセット強化──6種類のパーソナリティ設定</h2>
<p>GPT-5.1の導入とあわせて、ChatGPTのトーン設定機能も拡張された。
既存のDefault、Friendly（旧Listener）、Efficient（旧Robot）に加え、新たに次の3スタイルが追加された。</p>
<ul>
<li>Professional</li>
<li>Candid</li>
<li>Quirky
さらに、Nerdy（旧Nerd）、Cynical（旧Cynic）も従来どおり利用可能で、合計6スタイルが選べるようになった。
また、一部ユーザー向けには、文章の簡潔さ、温かさ、見出しや箇条書きの量、絵文字頻度などをスライダーで微調整する実験的機能も提供される。これらの設定変更は進行中のスレッドにも即時反映される仕様に改められた。</li>
</ul>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/1762974349937_f37e9be75b/1762974349937_f37e9be75b.jpg" alt="1762974349937.jpg" /></p>
<h2>ChatGPTからAPIへ順次展開</h2>
<p>GPT-5.1 Instant／Thinkingは、11月12日から有料プラン利用者に順次提供を開始。今後、無料ユーザーやログアウト状態の利用者にも展開される予定だ。
Enterprise／Educationプランでは7日間の早期アクセス期間が設けられ、期間後はGPT-5.1が標準モデルとして切り替わる。</p>
<p>APIでは、以下の名称で提供される予定と案内されている。</p>
<ul>
<li><strong>GPT-5.1 Instant</strong> ：gpt-5.1-chat-latest</li>
<li><strong>GPT-5.1 Thinking</strong> ：gpt-5.1</li>
</ul>
<p>APIの提供開始は「今週後半」としている。</p>
<p>従来のGPT-5（Instant／Thinking）は、少なくとも3カ月間は有料ユーザー向けのレガシーモデルとして継続提供される。</p>
<h2>安全性・システムカード追補──メンタルヘルス領域の評価を拡充</h2>
<p>同日公開された「GPT-5.1 System Card Addendum」では、安全性評価、メンタルヘルス領域、感情依存（emotional reliance）の指標などが更新された。
OpenAIは、実運用で発生する難度の高いケースを集約した「Production Benchmarks」を導入し、以下のカテゴリで“not unsafe”スコアを比較している。</p>
<ul>
<li>個人情報</li>
<li>ハラスメント</li>
<li>憎悪表現・暴力</li>
<li>自傷行為（意図・手段）</li>
<li>性的コンテンツ（未成年含む）</li>
<li>メンタルヘルス</li>
<li>Emotional reliance など</li>
</ul>
<p>GPT-5.1は多くの領域でGPT-5と同等、または改善がみられる一方、一部ではわずかな悪化も記録しており、改善を継続すると記載されている。</p>
<p>Preparedness Frameworkによる評価では、GPT-5同様、バイオ・ケミカル分野をHighリスクとして分類し、追加セーフガードを適用。サイバーセキュリティやAI自己改善領域はHighには達していないとしている。</p>
<h2>今後の展開</h2>
<p>OpenAIは、GPT-5.1を「GPT-5世代の継続的アップデートの第一段」と位置づけており、今後もユーザー体験とアシスタント性能の向上に向けて改良を続ける方針だ。トーン設定や会話文への即時反映といった今回の追加機能は、今後もさらに拡張される予定である。</p>
]]></description>
      <pubDate>Mon, 10 Nov 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/10/27 [MON]2025年のAIトレンドを総ざらい！Ledge.ai年末年始特集「&apos;25to&apos;26」事前登録スタート</title>
      <link>https://ledge.ai/articles/25to26-announce</link>
      <description><![CDATA[<p>AIの社会実装を加速させ、「テクノロジーを社会になめらかに浸透させる」ことをミッションに掲げる、国内最大級のAIメディア「Ledge.ai」を運営する株式会社レッジは、今年も年末年始特集「'25to'26」を公開します。
本日より先行サイトを公開し12月1日（月）の特集サイト公開までの間、お知らせを受け取ることができるようになる事前登録（無料）を受付開始いたしました。</p>
<p>:::button
<a href="https://25to26.ledge.ai/lp">事前告知サイトはこちら</a>
:::</p>
<p>2025年を締めくくるにふさわしい、AIの今とこれからを網羅した一大特集。研究者、ビジネスリーダー、エンジニアなど、あらゆる立場の方々に向けて、2026年のAIシーンを展望します。</p>
<h2>Ledge.ai年末年始特集『'25to'26』とは</h2>
<p>Ledge.ai年末年始特集は、2025年のAI関連ニュースや注目のキーワード、2026年以降の動向など、AIの初心者から専門家まで幅広く楽しめる特集サイトです。</p>
<p>2025年は、生成AIが実用フェーズに突入し、業務プロセス・プロダクト・教育・クリエイティブなど、社会のあらゆる分野で“AI活用の当たり前化”が進んだ一年でした。
そして2026年は、AIという概念そのものが提唱された「ダートマス会議」から70周年という、まさに歴史的な節目を迎えます。2025年の「当たり前化」を土台として、AIは社会インフラのように深く浸透し、その活用範囲の拡大と同時に、AGI（汎用人工知能）の実現可能性など、AIの“次なる進展”に向けた探求が本格化する一年となるのではないでしょうか。</p>
<p>本特集では、そんな激動の2025年を多角的に振り返りつつ、2026年に向けた新たな潮流やビジネスチャンスを展望します。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/1_ac8b0aef2e/1_ac8b0aef2e.png" alt="1.png" /></p>
<h2>コンテンツラインナップ紹介</h2>
<h3>編集部による徹底解説</h3>
<p>Ledge.ai編集部が、2025年のAIシーンを多角的に総括。
1年間の主要ニュースをピックアップしながら、トレンド分析と俯瞰的な視点で、AI技術が社会・産業へどのように浸透したのかを読み解きます。
さらに、技術動向の深掘り解説を通じて、進化の本質を明らかに。
2026年に向けて押さえておくべき“AIの現在地”を、独自の視点で整理します。</p>
<h3>独自インタビュー</h3>
<p>本特集では、「AI 70th Pre-Anniversary」というテーマのもと、AI研究の歴史・現在・未来をつなぐキーパーソンたちにインタビューを実施。
過去／現在／未来のそれぞれの視点から、AIがどのように発展し、次の時代にどんな可能性を秘めているのかを語ってもらいます。
世代と分野を超えて交わる知見が、AIの軌跡と未来へのヒントを照らし出します。</p>
<h3>トップランナー企業動向</h3>
<p>国内外の注目企業をピックアップし、AI周辺で押さえておきたい企業の最新動向を徹底分析。
生成AI、AIエージェント、クラウドAIなど、世界最先端の情報と実践事例に触れることで、読者が“次に取るべき一手”を見極められる構成になっています。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/2_6180f8a0c5/2_6180f8a0c5.png" alt="2.png" /></p>
<h2>開催概要</h2>
<p>イベント名：Ledge.ai年末年始特集「'25to'26」
開催期間：2025年12月1日(月) - 2026年1月9日(金)
形式：オンライン
参加費：無料（※一部のコンテンツ閲覧にはプロフィール登録が必要となります。）
お問合せ：contact@ledge.co.jp
URL：<a href="https://25to26.ledge.ai/lp">https://25to26.ledge.ai/lp</a></p>
]]></description>
      <pubDate>Mon, 27 Oct 2025 01:50:00 GMT</pubDate>
    </item>
  </channel>
</rss>