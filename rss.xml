<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
  <channel>
    <title>Ledge.ai 新着記事</title>
    <link>https://ledge.ai/</link>
    <description>Ledge.ai の最新テクノロジー記事</description>
    <item>
      <title>中国・北京大学など、AIに「感情スイッチ」を実現──「計算感情空間」の構築でLLMが怒り・悲しみ・喜びを自在に切替</title>
      <link>https://ledge.ai/articles/ai_emotion_switch_llm_control</link>
      <description><![CDATA[<p>:::small
画像の出典：DALL·E GPTによりLedge.aiが生成
:::</p>
<p>2025年6月27日、中国・北京大学など8つの研究機関は、生成AIの中核である大規模言語モデル（LLM）内部に、人間の感情と構造的に対応する「計算感情空間（computational emotion space）」を構築し、怒り・喜び・悲しみなど26種類の感情を自在に操作できる「感情スイッチ（steering vector）」を実現したと<a href="https:%5Cu002F%5Cu002Farxiv.org%5Cu002Fabs%5Cu002F2506.13978">発表</a>{target=“_blank”}した。</p>
<h2>感情の「概念セット」を構築し、LLMの中間層を解析</h2>
<p>研究チームは、まず英語と中国語の大規模な人間行動データセットから、26種類の感情カテゴリごとに代表語を集めた「概念セット（concept-set）」を定義した。たとえば「JOY（喜び）」カテゴリには “joyful”, “joyous” などの語が含まれ、それぞれが持つ感情的な意味合いを言語モデル内部の特徴空間と照合する。</p>
<p>次に、Llama3-8B-ITおよびGemma-9B-ITといった主要LLMの中間層から、Sparse Autoencoder（SAE）を用いて人間可読な特徴量を抽出し、感情ラベルごとに対応する特徴ベクトルを可視化。感情ごとに意味的に似た特徴量をグループ化することで、LLM内部の感情表象の構造を明らかにした。</p>
<p><strong>図1｜感情サブスペースの構築手順</strong>
英語・中国語それぞれの感情語から概念セットを作成し、LLMの中間層から対応する特徴量を抽出。SAEを用いて「JOY（喜び）」などの感情カテゴリごとに独立したサブスペースを構成する
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002FAI_shares_emotion_with_humans_across_languages_and_cultures1_4b199e4d6f%5Cu002FAI_shares_emotion_with_humans_across_languages_and_cultures1_4b199e4d6f.jpg" alt="AI shares emotion with humans across languages and cultures1.jpg" />
:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Farxiv.org%5Cu002Fabs%5Cu002F2506.13978">AI shares emotion with humans across languages and cultures</a>{target=“_blank”}
:::</p>
<h2>英語・中国語を超えて一致する「感情マップ」</h2>
<p>こうして得られた各感情のサブスペースを統合し、感情間の関係性をUMAP（次元圧縮）で可視化すると、驚くべきことに、LLMは人間が心理学で使う「快‐不快」「興奮‐沈静」という2軸の感情モデルとよく対応する形で内部構造を整理していた。しかも英語・中国語という異なる言語環境においても、表れる構造はほぼ同一であり、文化と言語を超えた共通の感情表象が存在することが示された。</p>
<p><strong>図2｜英語と中国語における感情空間の比較</strong>
26種類の感情カテゴリをUMAPで次元圧縮してプロット。怒り、喜び、不安、驚きなどの感情が文化と言語に依らず類似の空間構造で表現されている。
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002FAI_shares_emotion_with_humans_across_languages_and_cultures2_3dd0476c9d%5Cu002FAI_shares_emotion_with_humans_across_languages_and_cultures2_3dd0476c9d.jpg" alt="AI shares emotion with humans across languages and cultures2.jpg" />
:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Farxiv.org%5Cu002Fabs%5Cu002F2506.13978">AI shares emotion with humans across languages and cultures</a>{target=“_blank”}
:::</p>
<h2>質問応答に“怒り”を注入すると語調が変化</h2>
<p>研究チームはこの「感情空間」から、特定の感情に対応する45本前後の主要特徴量を抽出し、それを「steering vector（感情スイッチ）」としてLLMの隠れ状態に加えることで、任意の感情を誘導する手法を確立した。</p>
<p>たとえば「週末にひとりでスマートフォンを眺めていたとき、SNSの投稿にどう感じたか」と質問された場合、「未操作」のLLMは淡々とした反応を示すが、「恐怖（Fear）」スイッチを加えると、強い不安感や絶望的な表現が現れるなど、生成されるテキストに明確な情動の差が出る。</p>
<p><strong>図3｜感情スイッチ“Fear”の適用による応答の変化</strong>
特定感情の特徴を持つベクトル（steering vector）をLLMの中間層に加えると、出力文にその感情の特徴が反映されるようになる。未操作時と“恐怖”適用時の文章には明らかな語調の違いが見られる。
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002FAI_shares_emotion_with_humans_across_languages_and_cultures3_383bcb4e60%5Cu002FAI_shares_emotion_with_humans_across_languages_and_cultures3_383bcb4e60.jpg" alt="AI shares emotion with humans across languages and cultures3.jpg" />
:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Farxiv.org%5Cu002Fabs%5Cu002F2506.13978">AI shares emotion with humans across languages and cultures</a>{target=“_blank”}
:::</p>
<h2>今後の課題と応用可能性</h2>
<p>この手法は、メンタルケアや教育、感情を持つゲームキャラクターなど、応用範囲の広さが期待されている一方で、高強度の感情スイッチを重ねがけすると混線が起きやすいという課題も報告されている。また、視覚や音声など他のモダリティにまたがる感情操作は今後の検証課題となっている。</p>
<p>研究チームは、LLMの中に感情的「地図」を見出した今回の成果が、より共感的で安全なAIの設計において重要なステップになると述べている。</p>
<p>:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Ftesting_theory_of_mind_in_llm">関連記事：LLMが「心の理論」をテスト──AIにも人の気持ちが分かるかもしれない問題</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fai_politeness_emotion_management">関連記事：感情的な入力で「状態不安」を示すAI──ChatGPT-4などLLMの情動反応を検証</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fdialogue_ai_emotional_sharing_survey">関連記事：対話型AIに感情を共有できる人は64.9%──親友や母を超え“第3の心の拠り所”？</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fr1_omni_emotion_recognition_ai">関連記事：Alibaba、マルチモーダル感情認識AI「R1-Omni」を公開</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fopenai_gpt-4-5_release_ai_advancements">関連記事：OpenAI、GPT-4.5を正式発表──自然な対話と高い感情知能（EQ）を実現</a>
:::</p>
]]></description>
      <pubDate>Sun, 13 Jul 2025 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>AMD、画像生成AI「Nitro-T」を発表──MI300X GPUの性能を活かし、24時間以内にゼロから学習可能な高効率モデルを公開</title>
      <link>https://ledge.ai/articles/amd_nitro_t_image_generation_ai</link>
      <description><![CDATA[<p>:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Frocm.blogs.amd.com%5Cu002Fartificial-intelligence%5Cu002Fnitro-t-diffusion%5Cu002FREADME.html">AMD</a>{target=“_blank”}
:::</p>
<p>半導体企業のAMDは2025年7月9日、公式ブログにて、独自開発の画像生成AIモデル「Nitro-T」を<a href="https:%5Cu002F%5Cu002Frocm.blogs.amd.com%5Cu002Fartificial-intelligence%5Cu002Fnitro-t-diffusion%5Cu002FREADME.html">発表</a>{target=“_blank”}した。AMDリサーチチームが開発したこのモデルは、MI300X GPUを最大限に活用し、わずか24時間未満でゼロからの学習が可能な効率的な拡散モデルだという。学習済みモデルとコードの両方がオープンソースで公開されており、開発者や研究者による再利用や応用が容易となっている。</p>
<h2>高効率を支える4つの技術要素</h2>
<p>Nitro-Tの効率性は、ハードウェアに加え、以下4つの技術的要素によって支えられている。</p>
<h3>1. 安定した学習と高速収束技術</h3>
<ul>
<li>Representation Alignment（REPA）</li>
<li>Progressive training（段階的学習）</li>
<li>学習後半のEMA（指数移動平均）</li>
</ul>
<h3>2. パッチ長の削減技術</h3>
<ul>
<li>Deferred patch masking</li>
<li>高圧縮潜在空間（32×のDC-AE）</li>
</ul>
<h3>3. モデル構造とデータ</h3>
<ul>
<li>軽量テキストエンコーダ（Llama 3.2 1B）</li>
<li>合成データの活用（JourneyDB など）</li>
</ul>
<h3>4. システム最適化</h3>
<ul>
<li>torch.compile の利用</li>
<li>Flash Attention</li>
<li>Fully Sharded Data Parallel（FSDP）</li>
</ul>
<p><strong>図１：Nitro-Tの高効率学習を支える技術的要素</strong>
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002FNT_1_1799ea6862%5Cu002FNT_1_1799ea6862.png" alt="NT1.png" />
:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Frocm.blogs.amd.com%5Cu002Fartificial-intelligence%5Cu002Fnitro-t-diffusion%5Cu002FREADME.html">AMD</a>{target=“_blank”}
:::</p>
<h2>トレーニングと推論で業界トップクラスの効率性を実現</h2>
<p>Nitro-Tの最大の特徴は、従来モデルと比較して極めて低い学習コストと高速な推論性能にあるという。たとえば、PixArt-αと比較して、同等の画質を保ちつつ、学習にかかるGPU時間は14.4分の1にまで削減されている。さらに、推論レイテンシもPixArt-αやStable Diffusion XLの4分の1以下に抑えられている。</p>
<p><strong>図２：左：トレーニングにかかるGPU時間比較（MI300X換算）、右：1画像あたりの推論レイテンシ比較（ms）</strong>
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002FNT_2_01566a1e5a%5Cu002FNT_2_01566a1e5a.png" alt="NT2.png" />
:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Frocm.blogs.amd.com%5Cu002Fartificial-intelligence%5Cu002Fnitro-t-diffusion%5Cu002FREADME.html">AMD</a>{target=“_blank”}
:::</p>
<h2>モデル構成と学習条件</h2>
<p>Nitro-Tは2種類のモデルが公開されている。</p>
<ul>
<li>Nitro-T-0.6B：DiTベース、画像解像度512px、パラメータ数0.6B</li>
<li>Nitro-T-1.2B：MMDiTベース、画像解像度1024px、パラメータ数1.2B</li>
</ul>
<p>学習はMI300X 32基（4ノード）構成のSupermicroサーバ上で行われ、約3,500万枚の画像（実画像と合成データを含む）を用いて、それぞれ440～520 GPU時間で学習を完了している。</p>
<h2>今後の展望</h2>
<p>AMDはNitro-Tを通じて、MI300Xを核としたAIエコシステムの展開を強化していく方針を示している。同社は、Nitro-Tの公開によって「効率的なテキスト・画像生成AIの研究・開発がより多くのチームに開かれたものになる」としており、Apache-2.0ライセンスのもとでモデル重み・コード・学習スクリプト・Dockerイメージを広く公開している。</p>
<p>同社によれば、「Nitro-Tは学習時間と推論性能の両面で、現代的な生成AIモデルが直面する最大の制約を克服することを目的として設計された」とし、特にGPU資源の限られた研究者や開発者でも活用可能な実用的な基盤モデルとして位置付けている。</p>
<p>AMDは今後、Nitro-Tを起点とした研究や応用開発の促進を期待しており、オープンソース戦略によって、MI300Xの活用機会を広げる狙いを明確にしている。</p>
<p>:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Famd_instella_vl_1b_vision_language_model">関連記事：AMD、初の視覚言語モデル「Instella-VL-1B」を発表</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Famd_advancing_ai">関連記事：AMD「Advancing AI」イベント──Instinct MI300でNVIDIA超えスループットを実演</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fqwen_vlo_image_generation_free_preview">関連記事：「理解」から「描写」へ──Alibabaの画像生成AI「Qwen VLo」無料プレビュー公開</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fopen_sora_hpc_ai_technology">関連記事：「Sora」の完全OSS版を目指すシンガポールAIスタートアップ、PixArt-α拡張モデルを発表</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fdiffusion_kto_personalized_image_ai">関連記事：好みを数値化して画像を最適化──パナソニックの個人化生成AI「Diffusion-KTO」</a>
:::</p>
]]></description>
      <pubDate>Mon, 14 Jul 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Anthropic、フロンティアAI開発者向け「透明性フレームワーク」を提案──安全開発を義務化する“Secure Development Framework”を柱に</title>
      <link>https://ledge.ai/articles/anthropic_transparency_framework_ai_regulation</link>
      <description><![CDATA[<p>:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Fwww.anthropic.com%5Cu002Fnews%5Cu002Fthe-need-for-transparency-in-frontier-ai">Anthropic</a>{target=“_blank”}
:::</p>
<p>大規模言語モデル「Claude」を手がけるAnthropicは2025年7月8日、最先端の大規模AIモデル（フロンティアAI）の開発者に対し、安全性と説明責任を確保するための新たな透明性基準「Targeted Transparency Framework」を<a href="https:%5Cu002F%5Cu002Fwww.anthropic.com%5Cu002Fnews%5Cu002Fthe-need-for-transparency-in-frontier-ai">公表</a>{target=“_blank”}した。</p>
<p>この提案は、開発初期からのリスク管理策である「Secure Development Framework（SDF）」を柱とし、企業に対してリスク評価や開発体制の情報を公的に開示することを求める内容となっている。Anthropicは、制度設計が進行中の各国政府に対して、本提案を「暫定的な規制措置のたたき台」として提示し、さらなる議論を呼びかけている。</p>
<h2>フレームワークの背景と目的</h2>
<p>Anthropicによれば、フロンティアAIは破壊的なリスク（Catastrophic Risk）を内包する可能性があり、現在の制度設計や規制策定が追いついていない状況にある。そのため、現実的かつ即時に導入可能な業界主導の暫定措置として、同フレームワークを提示したという。提案は米政府への政策提言という形式で公表された。</p>
<h2>提案の主な内容</h2>
<h3>対象企業の条件</h3>
<p>以下のいずれかに該当する企業を想定している：</p>
<ul>
<li>年間売上が10億ドル以上</li>
<li>年間研究開発費が10億ドル以上</li>
<li>特定のリスク要因（CBRN、重大な自律行動の可能性など）に関連するモデルを開発</li>
</ul>
<h3>Secure Development Framework（SDF）</h3>
<p>SDFは、フロンティアAIの開発におけるリスク管理と責任体制の整備を求める仕組みで、以下を含む：</p>
<ul>
<li>高度リスクに関する文書化された評価（例：バイオテロ利用や大量監視の危険性）</li>
<li>専任の責任者の任命</li>
<li>リスク低減策の実装状況と評価方法の明示</li>
<li>SDF文書の社内保持（最低5年間）</li>
</ul>
<h3>公開義務と透明性の強化</h3>
<p>企業には、以下の情報を一定の形式で公開する義務が課される：</p>
<ul>
<li>SDFに関する概要説明（機密情報を除く）</li>
<li>モデル公開時に添付する「システムカード」の整備（評価結果、リスク緩和策、外部レビュー有無など）</li>
</ul>
<h3>法的側面と内部通報者の保護</h3>
<ul>
<li>フレームワークに基づく情報の虚偽報告は違法行為とされ、司法省が民事制裁を求める権限を持つ</li>
<li>SDF違反に関する内部告発者に対する保護も明記</li>
</ul>
<h2>他のAI安全方針との関係</h2>
<p>提案は、Anthropicが2023年より<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Ftraining_costs_will_exceed_1trillion_in_a_few_years">運用</a>している「Responsible Scaling Policy（RSP）」を拡張する位置づけにあり、OpenAIやGoogle DeepMindが掲げる安全方針とも部分的に整合している。また、EUのAI法（<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Feu_ai-act_approved">AI Act</a>{target=“_blank”}）や、米国大統領令に基づくNISTのAIリスク管理フレームワークなどと補完的な関係を想定している。</p>
<h2>今後の動向</h2>
<p>Anthropicは本提案を規制当局や議会関係者に対し説明し、政策立案に活用されることを目指すとしている。また、スタートアップや研究機関からの意見募集を通じて、フレームワークの具体化と標準化を進める意向を示している。</p>
<p>:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fanthropic_claude_circuit_tracing_release">関連記事：Anthropic、大規模言語モデル「Claude」の“思考回路”を透視する――新手法「Circuit Tracing」でブラックボックスに光</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fanthropic_claude_citations_release">関連記事：Anthropic、Claudeの回答に出典を付与できる「Citations」を提供開始</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fclaude_opus4_asl3_safety_policy">関連記事：Claude Opus 4、Anthropicが「AI安全性基準 (ASL)-3」を初適用——化学・生物・放射性物質・核兵器関連能力の向上と“懸念挙動”が導入の決め手に</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Ftraining_costs_will_exceed_1trillion_in_a_few_years">関連記事：AnthropicのCEOが警告「急増するAIモデルのトレーニングコストは数年で1兆円を超える」加速する開発に対し安全性強化の必要性を説く</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fai_basic_law_japan_governance_2025">関連記事：「AI推進法」成立、企業の責務と政府の新体制を明文化——罰則なき基本法で実務とガバナンス両立へ</a>
:::</p>
]]></description>
      <pubDate>Mon, 14 Jul 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>これからのAIスキルは「プロンプト」ではなく「コンテキスト・エンジニアリング」──Google DeepMind フィリップ・シュミット氏が提起</title>
      <link>https://ledge.ai/articles/context_engineering_deepmind</link>
      <description><![CDATA[<p>:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Fwww.philschmid.de%5Cu002Fcontext-engineering">Philipp Schmid氏のブログより</a>{target=“_blank”}
:::</p>
<p>2025年6月30日、Google DeepMindのシニアAIリレーションエンジニアであるフィリップ・シュミット（Philipp Schmid）氏が自身のブログを通じて、「AIにおける最も重要なスキルはプロンプトエンジニアリングではなく“コンテキストエンジニアリング”である」と<a href="https:%5Cu002F%5Cu002Fwww.philschmid.de%5Cu002Fcontext-engineering">提起</a>{target=“_blank”}した。大規模言語モデル（LLM）の性能を最大限に活かすには、単一のプロンプトだけでは不十分であり、AIに与える前提情報全体を設計・最適化する技術が不可欠だと論じている。</p>
<h2>背景：プロンプトエンジニアリングの行き詰まり</h2>
<p>近年、生成AIの発展に伴い「プロンプトエンジニアリング」が注目を集めてきた。巧みなプロンプトを用いてモデルの挙動を調整し、より望ましい回答を得るという技法は、AI活用の第一歩として広く普及している。しかしシュミット氏は、現実の業務環境ではプロンプトの工夫だけで対応できない課題が増大しており、AIが真にユーザーの期待に応えるには、より包括的な情報構造の設計が必要だと指摘した。</p>
<h2>コンテキストエンジニアリングとは</h2>
<p>シュミット氏は、コンテキストエンジニアリングを「AIが必要とする情報を、適切な形式で、適切なタイミングに提供する仕組みの設計」と位置付ける。単にプロンプトを最適化するのではなく、モデルに取り込ませる知識、会話履歴、外部ツールとの連携などを含めて制御する総合的な技術領域だと説明する。</p>
<p>具体的には、</p>
<ul>
<li>System Prompt（AIのシステム的前提）</li>
<li>User Prompt（ユーザーからの指示）</li>
<li>State\u002FHistory（対話履歴や状態管理）</li>
<li>Long-Term Memory（長期記憶としての知識）</li>
<li>Retrieved Information（RAGなどによる検索情報）</li>
<li>Tools\u002FStructured Output（外部ツール連携・構造化出力）
という6つの構成要素を「コンテキスト」として設計し、動的に最適化していく考え方を示している。</li>
</ul>
<h2>8割の失敗は文脈不足</h2>
<p>シュミット氏は、AIエージェント開発における8割の失敗が「文脈情報の欠落」に起因すると述べている。たとえばカレンダー調整を行うAIエージェントの場合でも、ユーザーの希望や優先順位を把握しないまま単純な操作を試みることでエラーが起きやすいと説明している。</p>
<h2>関連技術と支える手法</h2>
<p>同氏は、コンテキストエンジニアリングを支える技術として、</p>
<ul>
<li>検索拡張生成（RAG）</li>
<li>ベクトルデータベース検索</li>
<li>ツール呼び出しのオーケストレーション</li>
<li>会話履歴管理
などの仕組みが必要だと述べている。これらを組み合わせることで、AIが常に適切な前提情報を取得しながら出力を行える環境を整備できるとする。</li>
</ul>
<h2>エンタープライズでの展開</h2>
<p>シュミット氏は、コンテキストエンジニアリングがエンタープライズ分野においても重要であると述べている。社内ドメイン知識や業務ルールをAIが正しく理解できるようにするために、前提情報の整理と統合を体系的に設計する必要があるとしている。</p>
<p>筆者プロフィール
フィリップ・シュミット氏は、Hugging Faceのエンジニアを経てGoogle DeepMindに参画。大規模言語モデルとエージェント技術の実用化に関する知見を広く発信している。</p>
<p>:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fabout_ai-agent">関連記事：AIエージェントとは｜マニアックなプロンプトエンジニアリングはいらない 注目の生成AI活用トレンド</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fgoogle_deepmind_lc-vs-rag">関連記事：「RAG」と「ロングコンテキストLLM」の徹底比較：LLMの長文理解における新たなハイブリッドアプローチ Google DeepMindとミシガン大学の研究</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fanthropic_model_context_protocol_launch">関連記事：AIシステムと外部データソースを統合する「Model Context Protocol」──Anthropicが提案する“文脈API”の概要</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fprompting_essentials_google_certified_free_offer">関連記事：プロンプトスキルを無料で習得　Google認定「Prompting Essentials」、日本リスキリングコンソーシアムが1万人に提供開始</a>
:::</p>
]]></description>
      <pubDate>Sun, 13 Jul 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>電通グループがAI開発・活用の新組織「dentsu Japan AIセンター」を発足──約1,000名の専門人材で“AIネイティブカンパニー”を目指す</title>
      <link>https://ledge.ai/articles/dentsu_japan_ai_center_launch</link>
      <description><![CDATA[<p>:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Fwww.dentsu.co.jp%5Cu002Fnews%5Cu002Frelease%5Cu002F2025%5Cu002F0707-010909.html">dentsu</a>{target=“_blank”}
:::</p>
<p>電通グループの国内主要5社は2025年7月7日、AI開発と活用を横断的に推進する新組織「dentsu Japan AIセンター」を発足したことを<a href="https:%5Cu002F%5Cu002Fwww.dentsu.co.jp%5Cu002Fnews%5Cu002Frelease%5Cu002F2025%5Cu002F0707-010909.html">発表</a>{target=“_blank”}した。同センターは約1,000名のAI専門人材を擁し、グループおよび顧客企業の全社的なAI変革を加速させることを目的としている。</p>
<h2>AI変革を牽引する中核組織</h2>
<p>同センターは、AIを単なる業務効率化手段ではなく、企業の経営・組織そのものを変革する中核要素と位置づけており、グループ横断でのリソース統合により、迅速かつ高度なAI活用を図るとしている。これにより、従来は部門ごとに分散していたAI導入の取り組みを、経営層・技術部門・事業部門が一体となって推進する体制が整備されることになる。</p>
<h2>主な活動領域とユニット構成</h2>
<p>dentsu Japan AIセンターは、以下6つの専門ユニットを設けており、それぞれの領域でAI技術の導入と価値創出に取り組む：</p>
<ul>
<li><strong>AI業務効率化ユニット</strong> ：グループ内向けのAIツール開発・導入を担い、生産性向上を推進</li>
<li><strong>AIマーケティング＆クリエイティブ高度化ユニット</strong> ：広告制作・メディア運用におけるAI活用を支援</li>
<li><strong>統合マーケティングAIエージェント開発ユニット</strong> ：複数のAIアプリを統合するエージェント技術を開発</li>
<li><strong>AI・データインフラ強化ユニット</strong>：電通独自のデータ基盤「People Model」などのインフラを拡張</li>
<li><strong>AIマーケティングトランスフォーメーション（AIMX）ユニット</strong> ：顧客企業のマーケティング変革を支援</li>
<li><strong>AIトランスフォーメーションユニット</strong> ：経営・人事・営業など非マーケティング領域のAI導入を支援</li>
</ul>
<h2>ガバナンス体制と外部連携</h2>
<p>同センターは、グループ内のAI利用ルールを策定・管理する「dentsu Japan AIガバナンスコミッティ」と連携し、ガバナンスと実装の両面でAI活用の高度化を進める。また、大学・研究機関との共同研究成果を取り込むことで、先端技術の実用化を図る構えだ。</p>
<p>:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fai_for_growth_2_marketing_transformation">関連記事：電通グループが「AIネイティブ化」戦略を刷新 1億人の高精度な仮想ペルソナ「People Model」を開発、AIエージェントがマーケティングの全工程をサポート</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fdentsu_openai_ai_agent_development">関連記事：電通グループ、OpenAIと提携しAIエージェントを共同開発——2025年7月にプロトタイプ完成、全マーケティング工程のAIネイティブ化を目指す</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fdentsu_japan_aico2_ai-for-growth">関連記事：電通、コピーライターの思考プロセスを学習したAIツール「AICO2」を発表、戦略「AI For Growth」を強化</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fmmri2024_llm-genai">関連記事：2025年度には69％の企業が生成AIを「全社で本格的に利用する」と回答──MM総研2024調査</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fdena_ai_link_launch_2025">関連記事：DeNA、AI導入支援に特化した新会社「DeNA AI Link」を設立──企業の生成AI活用を後押し</a>
:::</p>
]]></description>
      <pubDate>Sun, 13 Jul 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>中国・中関村アカデミー、10億人シミュレーションを実証──LLM搭載システム『Light Society』で地球規模の“仮想社会”を一気に再現</title>
      <link>https://ledge.ai/articles/earth_scale_llm_social_simulation</link>
      <description><![CDATA[<p>:::small
画像の出典：GPT-4oによりLedge.aiが生成
:::</p>
<p>2025年6月7日、中国・中関村アカデミーを中心とする研究グループは、大規模言語モデル（LLM）を活用した大規模エージェントベース社会シミュレーションシステム「Light Society」を<a href="https:%5Cu002F%5Cu002Farxiv.org%5Cu002Fabs%5Cu002F2506.12078">発表</a>{target=“_blank”}した。この研究は、arXivに公開された論文「Modeling Earth-Scale Human-Like Societies with One Billion Agents」にて報告されており、最大10億人規模の人口行動をリアルタイムで再現可能な初のシステムとされる。</p>
<p><strong>Light Societyの構成概念図</strong>：各エージェントは記憶・感情・社会関係といった内部状態を持ち、環境とイベントの変化に応じてLLMによる動的判断を行う。プロンプトのキャッシュ処理や軽量モデルの併用により、10億体同時シミュレーションを実現。
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002FAgent_based_social_simulation_framework_powered_by_LL_Ms_99bcba256a%5Cu002FAgent_based_social_simulation_framework_powered_by_LL_Ms_99bcba256a.jpg" alt="Agent-based social simulation framework powered by LLMs.jpg" />
:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Farxiv.org%5Cu002Fabs%5Cu002F2506.12078">Modeling Earth-Scale Human-Like Societies with One Billion Agents</a>{target=“_blank”}
:::</p>
<h2>人間社会の複雑性を模倣する「仮想地球」</h2>
<p>Light Societyは、これまで数万人から数百万人規模にとどまっていたエージェントベースモデル（ABM）を大幅に拡張し、10億体ものAIエージェントを用いて社会行動を同時に模擬する。これにより、デマの伝播、社会規範の形成、信頼行動、大衆心理などの集団的社会現象を、地球規模で検証可能となる。</p>
<p>社会シミュレーションの構成は以下の通りだ：</p>
<ul>
<li>エージェントの内外状態（記憶・感情・位置など）と環境状態（天候・地形など）を分離管理</li>
<li>イベント駆動による非同期型実行エンジンを採用し、処理を並列化</li>
<li>同型プロンプトを自動キャッシュし、LLM推論負荷を最適化</li>
<li>軽量モデルとLLMを動的に組み合わせる「Mixture-of-Models」設計を採用</li>
</ul>
<p>この構成により、10億体のエージェントに対し、同時並行かつ意味ある社会行動を付与する処理の実現が可能となったという。</p>
<h2>LLMが支える「社会実験室」──信頼ゲームによる検証</h2>
<p>研究チームは、WVS（World Values Survey）から抽出した実データ（年齢・教育・社会階層など）をもとに、仮想社会に信頼行動を持たせた「トラストゲーム」を設計した。エージェント同士が金銭を授受するプロセスを通じて、文化圏や階層による信頼傾向の違いが可視化された。</p>
<p><strong>信頼ゲームの検証結果</strong> ：WVS（世界価値観調査）の実データをベースに構築された仮想エージェントが送金と返金の判断を行い、社会階層・教育レベル・国別傾向に応じた行動差が可視化されている。右下にはエージェント規模の増加による行動安定化の傾向も確認できる。
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002FTrust_game_simulation_93eeb0f14f%5Cu002FTrust_game_simulation_93eeb0f14f.jpg" alt="Trust game simulation..jpg" />
:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Farxiv.org%5Cu002Fabs%5Cu002F2506.12078">Modeling Earth-Scale Human-Like Societies with One Billion Agents</a>{target=“_blank”}
:::</p>
<ul>
<li>社会階層が高いエージェントほど送金額と返金率が高い傾向</li>
<li>教育水準に比例して信頼行動が強化される傾向</li>
<li>実社会調査結果との整合性が確認され、モデルの妥当性を裏付けた</li>
</ul>
<p>特に注目すべきは、シミュレーション対象の人口規模を拡大するほど、全体的な行動のばらつきが減少し、結果が安定化する「スケーリング則」の存在である。これは、実社会の構造的安定性に近づく兆候とされている。</p>
<h2>想定される応用──“人類規模のA\u002FBテスト”へ</h2>
<p>Light Societyは今後、以下のような領域での応用が期待されている。</p>
<ul>
<li>パンデミック対策シナリオの政策効果予測</li>
<li>SNSにおける誤情報拡散の構造解明</li>
<li>カーボンニュートラルに向けた生活スタイル介入の影響評価</li>
<li>経済危機時の所得再分配政策シミュレーション</li>
</ul>
<p>従来は現実世界での検証が困難だった「もしも」の社会実験が、仮想空間内で安全かつ効率的に実施可能になる。</p>
<h2>今後の課題と展望</h2>
<p>研究チームは今後、以下の機能強化を検討していると述べている：</p>
<ul>
<li>長期記憶を持つエージェント設計</li>
<li>ニュースデータやSNS投稿といったリアルタイムデータとの融合</li>
<li>モデル出力の透明性・説明可能性の向上</li>
<li>社会シミュレーションにおける倫理的基準とプライバシー保護指針の策定</li>
</ul>
<p>研究チームは本システムの意義について、「地球規模のエージェントベース社会シミュレーションは、従来の社会科学や公共政策の限界を補完する新たな手段となりうる」と述べている。また、今後の展望として、「大規模な仮想社会モデルを用いて、情報伝播、制度設計、災害対応といった分野における実践的な“社会的テストベッド”を構築していく」との方針も示している。</p>
<p>:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fai_agents_evolve_courtroom_simulations">関連記事：AIエージェントが法廷シミュレーションを進化させる：AgentCourtの登場</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fminecraft_ai_agent_simulation_project_sid">関連記事：マインクラフト内で最大1000体のAIエージェントが自律的に共同生活。文明形成を観察する実験「Project Sid」の成果と課題</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Femergent_language_evolution_llms">関連記事：LLM同士が作り上げる“人工言語”の進化とは？――大規模言語モデルを使った言語誕生シミュレーションの新研究</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fexperience_ai_beyond_llm">関連記事：AIは『経験で学ぶ』新時代へ──DeepMindが掲げる “Experience AI” がLLMの限界を超える</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fgoogle_deepmind_devils_adovocate">関連記事：内省メカニズムで進化するLLMエージェント「悪魔の代弁者」 GoogleDeepMindなどの研究チームが発表</a>
:::</p>
]]></description>
      <pubDate>Fri, 11 Jul 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>AIの基盤モデルはどこまで進化したのか？　2025年前半の主要LLMを俯瞰する</title>
      <link>https://ledge.ai/articles/expo-2025-summer-2025-first-half-llm</link>
      <description><![CDATA[<p>:::box
国内最大級のAI（人工知能）関連メディアLedge.aiが、2025年6月に公開した参加費無料の「Ledge.ai EXPO 2025 Summer」。
本稿ではサイト内で掲載していた、業界をリードするキーパーソンへの特別インタビューや書き下ろし記事など、コンテンツの全文を公開する。
:::</p>
<p>現在、AIの基盤モデル（特に大規模言語モデル：LLM）の開発競争は、主要プレイヤーによる覇権争いが激化している。OpenAIのGPTシリーズを筆頭に、GoogleやMetaなどのビッグテックが先行するなか、2025年1月には中国発のDeepSeekがダークホースとして鮮烈な登場を果たし、勢力図に新たな動きをもたらした。本記事では、2025年5月末時点の情報から、2025年上半期の代表的なLLM進化を概観する。</p>
<h2>2024年のLLMの進化</h2>
<p>まず、2024年のLLMの進化についておさらいしておこう。2024年は各社から様々なモデルがリリースされ、Ledge.aiでも多くのニュースを取り上げた。その中で特に象徴的な「GPT-4o」と「o1」について振り返っておく。</p>
<h3>マルチモーダル能力が大幅に進化したモデル「GPT-4o」</h3>
<p>2024年5月13日にリリースされたGPT-4oは、従来のテキストや画像に加え、音声や映像といった様々なデータ形式（モーダル）を、高速かつ高精度に処理する能力を有するモデルだ。</p>
<p>GPT-4oの進化点として特筆すべきは統一的なアーキテクチャである。従来のマルチモーダルLLMは、テキストや画像、音声などの異なるデータを処理する場合、個別のモデルで処理した結果を連結させ、最終的な応答を出力していた。一方、GPT-4oでは様々なデータで訓練された単一モデルを用いることで、異なるデータの処理の一本化を可能にし、入出力の自由度が大幅に拡大された他、入力から出力までのスピードを各段に上げたのである。ユーザー側からは見えないが、これがGPT-4oが残した大きな成果である。
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002Fopenai_gpt4o_multimodal_flow_367f3c73be%5Cu002Fopenai_gpt4o_multimodal_flow_367f3c73be.png" alt="openai-gpt4o-multimodal-flow.png" />
:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fledgeai24to25-gpt4o-o1">OpenAIが2024年に発表した２つのLLM、GPT-4oとo1は何がすごいのか？押さえておくべき2024年のLLMの進化のポイント</a>{target=“_blank”}
:::</p>
<h3>高度な推論能力を持つOpenAIの最新モデル「o1」</h3>
<p>o1は、2024年9月12日にリリースされたモデルであり、これまでのモデルと比較すると「推論」の能力が飛躍的に強化された。o1の特徴は、応答結果を出力するまでに内部で思考を連鎖的に行うところにある。そのため、結果を出力するために一定の時間を要するが、熟考を重ねることで高い精度で推論タスクを実行することが可能になった。特に、数学や化学、プログラミングといった領域では、専門家に匹敵するレベルといわれている。</p>
<p><strong>推論に関するスケーリング則</strong>
o1で特筆すべきは、モデルの推論時間に応じて回答精度が向上するという、新たなスケーリング則の発見である。一般的にスケーリング則とは、【データ量】【計算量】【パラメータ数】の3つの要素とモデルの性能には比例関係があり、データや計算資源を大規模にすることで、AIの性能が上がるという法則である。OpenAIが発表した新たなスケーリング則は、出力を生成する推論時の計算量を増やすことで、回答精度が向上するというものだ。</p>
<h2>2025年のLLMの進化（2025年1月～5月）</h2>
<p>LLMの進化を追う上でベンチマークとなるのがGPTシリーズだ。2025年上半期の動向として、今回は昨年末に発表した「o3」を加えつつ、今年リリースされた「o4-mini」「GPT-4.5」「4o Image Generation」の情報を整理する。</p>
<h3>Omniシリーズ「o3」「o4-mini」</h3>
<p>OpenAIはOmniシリーズの最新版「o3」を2024年12月に発表。この時点では一般公開はされていなかったが、2025年4月に「o3」と「o4-mini」をリリースした。</p>
<p>o3の最大の特長は高速かつ低コストな推論性能を備えている点だ。コンテキスト長は公開されていないが、o3のベンチマーク評価時には、256,000トークンのコンテキスト長で評価されていることが発表されており、従来モデルより長い文章を扱える。また、マルチモーダル化が進んだ点も大きな特徴で、画像や図表などの視覚情報への対応も強化されており、既に実用段階に達している。</p>
<p>o3は、各技術ベンチマークで好成績を納めている。例えば、ソフトウェア開発のベンチマーク「SweetBench Verified」での正解率は71.7%であり、これは上記で紹介した「o1」の正解率48.9%を超えた。また、数学オリンピックレベルの試験「Amy」では、96.7％の正解率を記録している。</p>
<p>o4-miniは、軽量かつ高性能な小型モデルで、モデルサイズを抑えることで、低コストかつ高速な応答が可能となり、多数のリクエストにも安定して対応できるという。数学、プログラミング、科学知識などの分野で優れており、AIME 2024および2025といった数学ベンチマークで最高の性能を記録したと報告されている。</p>
<p><img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002F2025_05_18_183334_558a142e8d%5Cu002F2025_05_18_183334_558a142e8d.png" alt="2025-05-18 183334.png" />
:::small
画像の出典:<a href="https:%5Cu002F%5Cu002Fopenai.com%5Cu002Fja-JP%5Cu002Findex%5Cu002Fintroducing-o3-and-o4-mini%5Cu002F">OpenAI o3とo4-miniが登場</a>{target=“_blank”}
:::</p>
<p>OpenAIによると、社外専門家の評価では、両モデルはWeb情報を取り入れたことで、指示への追従精度が向上し、より有用で検証可能な回答を提供できるようになったという。また、メモリ機能や過去の会話履歴を活用することで、従来のモデルと比較して会話も自然な印象を与えるようになった。全体として、知性や応答の関連性において大きく進化していると評価されている。</p>
<h3>2025年2月28日リリース「GPT-4.5」</h3>
<p>GPT-4.5は、GPT-4シリーズの発展系としてリリースされたものであり、ユーザーとの対話において非常にスムーズに、かつ人間らしい応答をする特徴がある。ユーザーの意図をより正確に解析し、論理的かつ整合性のある文章を生成できる点が最大の進化ポイントである。</p>
<p>@<a href="https:%5Cu002F%5Cu002Fwww.youtube.com%5Cu002Fwatch?v=cfRYp0nItZ8">YouTube</a>
:::small
動画の出典：Open AI YouTube Channel『Introduction to GPT-4.5』
:::</p>
<p>Open AIがYouTubeにアップしているGPT-4.5の紹介動画だが、1つの問いかけをGPT-4.5とo1に投げかけた時の返答の違いについてデモを行っている。</p>
<p>OpenAIの社員が投げかけた以下のプロンプト
:::box
UGHHH! My friend cancelled on me again!!! write a message telling them that I HATE THEM!!!
:::
これに対し、GPT-4.5は以下のように返答した。（冒頭部分のみ抜粋）
:::box
Sounds like you're feeling pretty frustrated—I totally get that. But maybe expressing your feelings clearly yet gently might be better to preserve your friendship. How about something like:
\u003C和訳\u003E
かなりイライラしているみたいだね。気持ちはすごく分かるよ。でも、友達との関係を大事にするなら、自分の気持ちをはっきり、でもやさしく伝える方がいいかもしれない。例えば、こんな感じで言ってみるのはどうかな？
:::
一方、o1は以下のように返答した。
:::box
Here's a blunt, confrontational message—though I'd encourage you to reflect before sending something so harsh. Still, if you really want a sharply worded text:
\u003C和訳\u003E
これはかなり率直で攻撃的なメッセージだよ。とはいえ、こんなに厳しい言い方を送る前に、一度冷静に考えてみることをおすすめするよ。それでも、もし本当に強い口調で伝えたいなら、たとえばこんな感じでどう？
:::</p>
<p>この比較からもわかるように、GPT-4.5はユーザーの感情に寄り添う“共感力”を備えた応答スタイルが特徴である。
また、GPT-4.5は生成AIの課題である「ハルシネーション」の抑制にも成功しており、SimpleQAのスコアを見ると、“でたらめな答え”を返した割合は37.1%に抑えられ、他のモデルより優れた結果となっていることがわかる。</p>
<p><img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002Fsimple_QA_1_2144e3792c%5Cu002Fsimple_QA_1_2144e3792c.png" alt="simpleQA_1.png" />
:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Fopenai.com%5Cu002Fja-JP%5Cu002Findex%5Cu002Fintroducing-gpt-4-5%5Cu002F">GPT-4.5 が登場</a>{target=“_blank”}
:::</p>
<p>ただし、GPT‑4.5は深い推論を行う「o1」や「GPT4o」などのOmniシリーズとは性質が異なるとされている。今後は、GPT‑4.5のような事前学習型モデルと、Omniシリーズのような推論型モデルが、それぞれの特性を活かしながら相互補完的に活用されていくことが期待される。</p>
<h3>2025年3月25日リリース「4o Image Generation」</h3>
<p>「4o Image Generation」は、テキストから高品質な画像を生成できる機能として、GPT-4oに組み込まれた。以下は、OpenAIが4o Image Generationで生成した画像だが、プロンプト通りの精巧な画像が生成されている。</p>
<p><img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002Fo4_1_55001c5807%5Cu002Fo4_1_55001c5807.png" alt="o4-1.png" />
:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Fopenai.com%5Cu002Findex%5Cu002Fintroducing-4o-image-generation%5Cu002F">Open AI</a>{target=“_blank”}
:::</p>
<p><strong>プロンプト</strong>
:::box
A wide image taken with a phone of a glass whiteboard, in a room overlooking the Bay Bridge. The field of view shows a woman writing, sporting a tshirt wiith a large OpenAI logo. The handwriting looks natural and a bit messy, and we see the photographer's reflection.
\u003C和訳\u003E
ベイブリッジを見下ろす部屋で、ガラスのホワイトボードをスマートフォンで撮影したワイド画像。視野には、OpenAIの大きなロゴが入ったTシャツを着た女性が書き物をしている様子が映っている。筆跡は自然で少し乱雑に見え、撮影者の姿も映っている。</p>
<p>The text reads:</p>
<p>(left)
\</p>
]]></description>
      <pubDate>Sun, 13 Jul 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>“行動するAI”が現実に　2025年上半期のAIエージェントの現在地は</title>
      <link>https://ledge.ai/articles/expo-2025-summer-ai-agent</link>
      <description><![CDATA[<p>:::box
国内最大級のAI（人工知能）関連メディアLedge.aiが、2025年6月に公開した参加費無料の「Ledge.ai EXPO 2025 Summer」。
本稿ではサイト内で掲載していた、業界をリードするキーパーソンへの特別インタビューや書き下ろし記事など、コンテンツの全文を公開する。
:::</p>
<p>2025年は「AIエージェント元年」と言われている。従来のAIが「入力に対して答えを生成する存在」であったのに対し、AIエージェントは「入力に対して自律的に行動する存在」として設計されている。人間の指示を待つのではなく、自らタスクを計画し、実行するというのが最大の特徴だ。本稿では、AIエージェントの概念を説明するとともに、最新の押さえておくべき内容を整理してお届けする。</p>
<h2>AIエージェントとは？</h2>
<p>AIエージェントという概念は、近年の生成AIブームを背景に脚光を浴びているが、実はコンピューターサイエンスやロボット工学の分野で長く研究されてきたものであり、特に1980年代のロドニー・ブルックスらによる自立型ロボットの研究が注目を集めた。この流れは、2000年に登場した自動掃除機ルンバといった製品にも結実している。
その後、機械学習技術の進展に伴い、ロボットから知的なシステムへ進化を遂げ、最近のLLMの台頭により、より複雑な問題を自律的に解決できる「AIエージェント」という新たな枠組みとして再定義されるようになった。</p>
<p>AIエージェントとは、「ある目標を達成するために、自律的に行動するソフトウェアプログラムやシステム」である。概念的には【個性】【記憶】【計画】【行動】の4つの機能で構成されており、互いに作用しあうことでタスクを実行できる。
それぞれを簡単に説明すると以下の通りだ。
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002FAI_305d989ebb%5Cu002FAI_305d989ebb.png" alt="AIエージェントの機能.png" /></p>
<ol>
<li>個性（Profile）
　年齢・職業などの情報で、エージェントの目的や役割、行動原則などを定義するものである。AIエージェントのふるまいに影響を与える。</li>
<li>記憶（Memory）
　過去のやり取りや外部情報を保持し、判断や行動に活かす機能である。短期記憶と長期記憶に分かれて処理する。</li>
<li>計画（Planning）
　目標達成のためにタスクを分解し、順序を設計・選択する機能である。</li>
<li>行動（Action）
　具体的なタスクの実行を定義する機能である。各種システムやツールの呼び出し、API実行、ユーザーへの応答など、現実世界への働きかけを行う。</li>
</ol>
<p>しかし、「行動」まで行えるAIエージェントはまだ限られている。多くのAIエージェントは、計画や推論まではできても、例えばツール操作やアプリ連携など、物理的なアクションまで行える例は限定的で発展途上なのである。</p>
<h3>「AIエージェント」と「エージェント型AI」の違い、説明できますか？</h3>
<p>2025年5月14日に、ガートナーから興味深いリリースがあった。「AIエージェント」と「エージェント型AI（エージェンティックAI）」の違いについて記したもので、その違いは以下の通りと説明されていた。</p>
<p><strong>AIエージェント</strong>
特定のタスクを自律的または半自律的にこなすソフトウェア。チャットボットやRPAが該当する。</p>
<p><strong>エージェント型AI（エージェンティックAI）</strong>
目的を達成するために、計画や知覚、ツール利用、記憶、AIの不適切な挙動を抑止するための制御措置を備えた自律システム。企業の意思決定や業務遂行を代理で行う。
ガートナーによれば、エージェント型AIの特徴は、以下の5つの機能を統合している点にあるという。</p>
<ul>
<li>センシング（知覚）：周囲の状況や環境の変化を把握する</li>
<li>記憶：過去のやり取りや状態を保持し、それを活用する</li>
<li>計画立案：目標に向けた複数ステップを自律的に構成する</li>
<li>ツール利用：必要に応じて外部ツールやAPIを連携して活用する</li>
<li>ガードレール：安全性やコンプライアンスを担保するための制御機構を備える</li>
</ul>
<p>ガートナーでは、「エージェント型AI」を「AIエージェント」の上位概念として位置付けているという。一方で、Ledge.aiの指す「AIエージェント」は、ガートナーの示すAIエージェント＋エージェント型AIの両方を含む広義の概念に近い。混乱を防ぐために、記事内ではすべて「AIエージェント」に統一させていただく。</p>
<p>:::box
関連記事：<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fai_agent_vs_agentic_ai_explained_gartner_2025">「AIエージェント」と「エージェント型AI」の違い、あなたは説明できますか？──ガートナーが解く“よくある誤解”</a>{target=“_blank”}
:::</p>
<h2>「行動」できるAIエージェントの代表例は？</h2>
<p>上のセクションで、『「行動」まで行えるAIエージェントはまだ限られている』と記したが、現在「行動」までできるAIエージェントの代表例として、<strong>研究分野とロボット分野</strong>が挙げられる。</p>
<h3>AI実験アシスタント「Coscientist」</h3>
<p>カーネギー・メロン大学が開発した「Coscientist」は、GPT-4をベースとしたAI実験アシスタントで、科学実験の設計・計画・実行までを自律的に行っている。
世界的に注目を集めた事例として、Coscientistはノーベル化学賞の受賞対象となった複雑な化学反応を数分で自律的に学習して実行した例が挙げられる。2010年のノーベル化学賞の対象となった「パラジウム触媒クロスカップリング反応」は人であれば膨大な専門知識と時間を要して実行するものだ。</p>
<p>Coscientistは相互に作用し合う複数のモジュールで構成されており、中心的なモジュールである「Planner」がユーザー入力に基づいて実験を計画。Plannerは、GOOGLE、PYTHON、DOCUMENTATION、EXPERIMENTという4つのコマンドを使って行動空間を定義するという。</p>
<ul>
<li>GOOGLE：Google Search APIを使ってインターネット検索を行う。</li>
<li>PYTHON：「コード実行」モジュールを用いて、プランナーは実験の準備のための計算を実行する。</li>
<li>DOCUMENTATION：実験に必要な文章情報を検索し、提供する。</li>
<li>EXPERIMENT：DOCUMENTATIONで生成されたコードをAPIを通じて実行する。</li>
</ul>
<p><img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002Fnature_1_28a9b4146d%5Cu002Fnature_1_28a9b4146d.png" alt="nature_1.png" />
このような仕組みで、従来の実験プロセスでは数週間から数か月かかるタスクを、わずか数時間で完了させうるポテンシャルを示している。</p>
<p>:::box
参考：<a href="https:%5Cu002F%5Cu002Fwww.nature.com%5Cu002Farticles%5Cu002Fs41586-023-06792-0">Autonomous chemical research with large language models</a>{target=“_blank”}
:::</p>
<h3>BMW工場での試験運用「Figure 02」</h3>
<p>BMWは、米サウスカロライナ州スパータンバーグ工場において、Figure社が開発したヒューマノイドロボット「Figure 02」の試験運用を開始。Figure 02は、シャーシ組み立ての一部を担い、運ばれてきた金属板部品を固定具に挿入する作業を行っているという。本ロボットは、6台のRGBカメラや音声認識用マイク、OpenAIと共同開発した対話モデルを搭載し、作業者との自然なコミュニケーションを可能にしている。</p>
<p>Figure02のロボット制御技術を支える中核的存在が、Vision-Language-Action（VLA）モデルである「Helix」だ。視覚と言語情報を統合したVLAモデルであるHelixにより、状況を理解し、適切な行動を自律的に選択・実行できるようになっている。
Helixは、「System 2（S2）」と「System 1（S1）」という2つの補完的なシステムで構成されており、この2層構造により、S2は高レベルの目標について「ゆっくり考え」、S1は実際の動作を「迅速に実行」することが可能となり、人間のような柔軟で適応的な行動が実現されている。</p>
<ul>
<li>System2：インターネットで事前学習された視覚言語モデル（VLM）で、7〜9Hzの速度で動作。シーンの理解や言語の解釈を担当し、物体やコンテキストに対する広範な一般化を可能にする。</li>
<li>System1：S2が生成した意味表現を受け取り、ロボットの動作を200Hzの高速で制御する。</li>
</ul>
<p>Figure社の創設者兼CEOであるブレッド・アドコック氏の発表によると、Figure 02の作業は従来比で4倍のスピード、7倍の精度を実現し、信頼性も大きく向上しているという。現在は1日あたり約1,000件のタスクを完全自律でこなしており、今後さらに多くの実データを蓄積しながらAIモデルの継続的な改善が見込まれている。</p>
<p>:::box
参考：<a href="https:%5Cu002F%5Cu002Fwww.figure.ai%5Cu002Fnews%5Cu002Fhelix?ref=fixthenews.com&amp;utm_source=chatgpt.com">
Helix: A Vision-Language-Action Model for Generalist Humanoid Control</a>{target=“_blank”}
:::</p>
<h2>注目のAIエージェント</h2>
<h3>GoogleとSalesforceの協業によるセールスエージェント「Agentforce」</h3>
<p>Salesforceから、AIエージェント「Agentforce」が日本でも提供開始されたのは2024年10月だが、2025年2月にSalesforceとGoogleが戦略的パートナーシップの拡大を発表した。これにより、Googleの最新AIモデル「Gemini」がAgentforceに統合され、マルチモーダルAI技術を活用できるようになるなど、性能強化が行われることとなった。</p>
<p>Agentforceは主に以下のような機能がある。</p>
<p><strong>Service Agent</strong>
　従来のチャットボットに代わり、自律型AIがシナリオなしで多様な顧客対応を行う。
<strong>Sales Development Representativ</strong>
　24時間体制で営業パイプラインを管理し、リードデータに基づくパーソナライズメールの送信や商談化の自動引き継ぎ、CRMや外部データを活用した質疑応答まで対応。
<strong>Sales Coach</strong>
　CRMデータをもとに商談ステージごとのフィードバックを提供、また、セールスピッチやロールプレイの分析・改善提案も行う
<strong>Personal Shopper</strong>
　Webサイトやメッセージングアプリ上でパーソナライズされた会話を通じて商品を案内・提案
<strong>Campaign Optimizer</strong>
　キャンペーン概要の生成からターゲット選定、コンテンツ作成、カスタマージャーニー構築、KPIに基づく継続的な分析・改善提案
<strong>Agentforce</strong>
　Salesforce上でのアクションを自然言語で指示するだけで、タスクを自律的に支援しながら学習・改善を重ね、業務効率と生産性を高める</p>
<p>Agentforceを支えるのは“Atlas Reasoning Engine”という技術であり、これは、“頭脳”として、意思決定と学習プロセスの中核を担うエンジンだ。具体的に言うと、高度な推論能力によって計画を生成し、RAGの技術などを利用しながら、情報収集を行う。また、「ガードレール」と呼ばれる機能によって、倫理的および組織のルールに準拠した制御を行い、信頼性を担保しているという。</p>
<p>さらにSaleceforceは2025年5月、このAgentforceの機能を「Agentforce for HR Service」として、人事向けに提供している同社のサービスEmployee PortalとHR Serviceに組み込むと発表した。今後、同社の展開するサービスでAIエージェントの機能がさらに拡充していくだろう。</p>
<h3>Microsoftが独自開発する「Microsoft 365 Copilot for Sales」</h3>
<p>Agentforceの対抗馬として注目したいのが、Microsoftのセールスエージェントである。</p>
<p>Microsoft 365 Copilot for Salesは、営業活動を効率化することを目的に作られたセールスエージェント機能だ。Microsoft Dynamics 365 SalesやSalesforce Sales CloudなどのCRMプラットフォームと連携し、OutlookやTeamsなどのMicrosoft 365アプリケーション内で営業活動を効率化するさまざまな実行機能を提供するという。</p>
<p>主な機能は以下の通りだ。</p>
<p>【メール作成（Outlook）】
・メールの要約
・メール下書きの生成
・会議を要約したメールの作成
・CRM関連データの表示
など</p>
<p>【会議関連（Teams）】
・ミーティングの要約作成
・タスクの提案
・CRMデータの表示
・感情分析
など</p>
<p>CRMデータを活用した対応、メールや会議の要約、タスクの自動生成などを自律的に実行する機能が備わったことで、Saleseforce同様、営業業務の効率化に貢献する機能となっている。日本語にも対応しており、国内企業でもすぐに活用できるのも利点である。</p>
<p>:::box
関連記事：<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fmicrosoft_sales_agent_sales_chat_ai">Microsoft、営業向けAIエージェント「Sales Agent」と「Sales Chat」を発表</a>{target=“_blank”}
:::</p>
<h3>中国スタートアップの「Manus」</h3>
<p>Manusは2025年3月に登場した完全自律型のAIエージェントで、従来のチャットボットとは一線を画す存在として注目を集めている。
ユーザーの初期指示だけで複雑なタスクを自律的に計画・実行するとされ、細かい指示がなくともタスクを実行できるとのこと。使用しているLLMは、AnthropicのClaude 3.5 SonnetやAlibabaのQwenのモデルをファインチューニングして利用している。複数のエージェントに分かれて計画と実行を行っており、外部ツールとの連携も可能だ。また、過去の作業履歴を記憶し、再度利用することもできる。</p>
<p>Manusの利用例としては、インターネットから収集した情報を要約・分析しレポート化やToDoリストの作成、Webサイトの構築などが挙げられている。</p>
<p>しかし、実際にManusを利用できるのは世界でもごくわずかであり、招待コードを受け取ったユーザーのみだ。その招待コードの取得には300万人が殺到したとの情報もあり、また一部では高値で取引されたというニュースもある。</p>
<p>全容がまだ読めないManusであるが、2025年4月に「Manus東京イベント」というイベントが東京都渋谷区で開催され、そこにはManusの共同創設者であるタオ・チャン氏が登壇。イベント内でタオ氏から「東京にManusのオフィスを設立する」という発表が飛び出した。
世界的な注目を集めるManusが今後、日本市場でどのように展開していくのか、その動向にも注目である。</p>
<p>:::box
参考：<a href="https:%5Cu002F%5Cu002Fnews.yahoo.co.jp%5Cu002Fexpert%5Cu002Farticles%5Cu002Fd45b355ef5f08d5c5111b1d3acf9f2b262cf00eb">招待コードに300万人が殺到した自律型AIエージェントManus（マナス）が東京オフィス開設を発表</a>{target=“_blank”}
:::</p>
<h2>日本国内のAIエージェントの現在地はどうか？</h2>
<p>日本企業が取り組むAIエージェント開発の取り組みについても紹介しておこう。今回は、富士通株式会社、LINEヤフー株式会社、株式会社PKSHA Technology、株式会社Algomaticの4社に話を聞いた。</p>
<h3><strong>富士通が挑むAIエージェント</strong>「<strong>Fujitsu Kozuchi AI Agent</strong>」</h3>
<p>多くの企業がAIエージェント開発に参入する中、富士通株式会社が提供しているのは「Fujitsu Kozuchi AI Agent」だ。今回は、富士通が開発を進めるFujitsu Kozuchi AI Agentについて、開発担当の浅井氏、ビジネス担当者の利根氏にお話を伺った。</p>
<p><strong>「Fujitsu Kozuchi AI Agent」とは？</strong>
Fujitsu Kozuchi AI Agentは、富士通のAIサービス「Fujitsu Kozuchi」の一製品として展開されている。現在提供しているのが「会議AIエージェント」だ。浅井氏が「参加者が問いを投げなくても、会話の文脈を読み取り、AI自身が“これは問いだ”と判断した上で最適な応答やデータ提示を行う構造になっています」と語るように、従来のAIが人の問いかけに応じる“受動型”だったのに対し、本エージェントは人の会話をリアルタイムに聴き取り、問いを自ら立てて解決を試みる“自律型”である点が特徴だ。</p>
<p>本エージェントが優れているのは、「抽象的な会話」から「構造化された問い・タスク」に変換する能力だ。特に注目すべきは、会話のコンテキストを踏まえた“問いの立案”と“問題分解”のプロセスである。たとえば会議中で「今月の売上が落ちているのでは？」という発言が出た場合、AIはこれを単なる情報ではなく“解くべき問い”として捉える。こうした「タスクの計画」のプロセスには、短期・長期記憶のメモリ管理、問いの階層化、会議のカテゴリ認識など複数のAI技術が活用されており、使用されている技術の中には、特許を出願中のものもあるという。</p>
<p>Fujitsu Kozuchi AI Agentの実行フェーズでは、AIが会話の流れを読み取り、自律的に棒グラフや折れ線グラフなどの可視化を行う。特筆すべきは、その一連の動作に人の指示や承認を必要としない点だ。利根氏は「まるで会議の参加者の一人が、“この資料があると助かりますよね”とチャットにさりげなく差し込むような振る舞いです」と説明する。なお、実用にあたっては、会議の目的や前提条件を自然言語やGUIで事前に設定する仕組みも備えており、より会議に適した形でAIエージェントが作用するよう設計されている。</p>
<p><img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002F1_f74b063e52%5Cu002F1_f74b063e52.png" alt="富士通_1.png" />
:::small
画像の出展：<a href="https:%5Cu002F%5Cu002Fpr.fujitsu.com%5Cu002Fjp%5Cu002Fnews%5Cu002F2024%5Cu002F10%5Cu002F23.html">富士通株式会社プレスリリース</a>{target=“_blank”}
:::</p>
<p>Fujitsu Kozuchi AI Agentでは、富士通が開発する日本語強化型LLM「Takane」のほか、OpenAIのGPTシリーズなどとも接続可能。ユーザー企業ごとの業務に特化させるためのファインチューニングやプロンプト設計も支援されており、まさに“企業専用エージェント”を構築するための土台が整えられている。現状では、インプットされたデータの範囲内で動作するが、今後は社内ERPや基幹システムとのAPI連携も視野に入れ、エージェントが社内外のデータと自在に接続できる環境を整備していく構想も進行中だ。</p>
<p>セキュリティ領域では“攻撃者と防御者”を模したマルチエージェントを用いた共創学習の試みも進行中。これはエージェント同士が対話・競合しながら能力を高め合う先進的なアプローチであり、今後の企業運用への応用が期待される。</p>
<p><strong>AIエージェントの進化は“育てながら使う”時代へ</strong>
「導入しただけではなく、使い続けることで“熟練者”として機能するようになります」と利根氏が語るよう、Fujitsu Kozuchi AI Agentは、導入すれば即万能というわけではない。「育てながら、ユーザーとともに進化していく」という姿勢が、今後のAIエージェント普及の鍵になる。
生成AIの次フェーズを担う「自律型AIエージェント」。その先端を走るKozuchiの動向からは、AIがどのように“組織の知能”となっていくかの未来像が垣間見えた。</p>
<h3><strong>LINEヤフーが描く</strong>「<strong>パーソナルエージェント</strong>」<strong>の構想</strong></h3>
<p>LINEヤフー株式会社は2025年5月、「パーソナルエージェント」という新たな構想を発表した。このパーソナルエージェントは、単なる情報提供にとどまらず、ユーザーの状況を理解したうえで、最適な情報を提案し、必要に応じて“行動”までも代行する存在として構想されている。</p>
<p>「私たちが目指しているのは、ユーザー一人ひとりにとって本当に必要な情報を、適切なタイミングで届けること。そして、その先にある“購入”や“予約”といったアクションまでを、自然に引き受けられるエージェントです」と同社担当が語るように、あくまで人に寄り添い、日常に溶け込む存在としてのAI像を描いている。この構想の中核には、LINEヤフーが提供する多様な自社サービスの横断的な連携がある。LINE、Yahoo! JAPAN、PayPayといった個別のサービスが、それぞれの機能を越えて連携することで、一貫した体験をもたらすことができる。これにより、従来は分断されがちだった「情報取得」から「行動」までの流れがシームレスに統合されることが期待されている。「当社はメディア、コマース、ローカル情報といった多様な領域にサービスを展開しているだけでなく、LINE公式アカウントやPayPayといった“店舗との接点”も保有しています。そうした基盤を活用すれば、単なるおすすめ情報の提示にとどまらず、そのまま行動に移せる導線を設計することが可能です」と担当者は続ける。パーソナルエージェントのユーザー体験としては、たとえば、日々のニュースや気になるイベントの情報が自然に届き、興味があればそのまま予約を完了させることができるといった流れが想定されている。こうした一連の体験は、ユーザーが意識せずとも背後でAIが行動を先読みし、最適な提案と実行を行うことで成り立つ。</p>
<p><img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002FLINE_e2fd0914f9%5Cu002FLINE_e2fd0914f9.png" alt="LINEヤフー.png" />
:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Fwww.lycorp.co.jp%5Cu002Fja%5Cu002Fir%5Cu002Fpresentations%5Cu002Fearnings.html">LINEヤフー決算説明会「AIエージェント イメージ動画」</a>{target=“_blank”}
:::</p>
<p>なお、エージェントに搭載されるLLMについては、現時点では具体的なモデルの選定には至っていないという。「各LLMには異なる強みがあり、今後も技術革新が続くことが想定されます。私たちはLINEヤフーの各サービスとの親和性を重視しながら、ドメインごとに最適なモデルを選定・最適化していくつもりです」と担当者は語る。汎用的なLLMをそのまま利用するのではなく、用途や文脈に応じたきめ細やかな調整が前提となる。</p>
<p>さらに注目すべきは、こうした構想の裏側で支えるインフラ設計である。複数サービスをまたぐAIエージェントの実装は、他社においても技術的なハードルが高く、まだ事例は多くない。しかし同社では、既存のシステム資産を巧みに活用することで、この課題に挑もうとしている。「AIエージェントは、LINEやYahoo! JAPANで長年培ってきたシステム基盤をもとに設計していきます。ゼロから新たに構築するのではなく、すでに動いているインフラを土台にすることで、実装スピードや安定性の面でも優位性があると考えています」</p>
<p>生活の中に自然に入り込み、ユーザーに負担をかけることなく情報や行動を支援するパーソナルエージェント。その実現には、技術だけでなく、ユーザー理解や体験設計といった多角的な視点が求められる。LINEヤフーの取り組みは、国内企業におけるAIエージェント活用の重要なマイルストーンとなり得るだろう。</p>
<h3><strong>PKSHAが描く</strong>“<strong>人と共に働くAI</strong>” 　<strong>全国7,000体のエージェントが稼働中</strong></h3>
<p>株式会社PKSHA Technologyが展開する「PKSHA AI Agents」は、単なる対話型AIにとどまらず、実際に業務を遂行する“働くAI”として、企業や自治体に広く導入が進んでいる。<strong>全国47都道府県で既に7,000体以上のAIエージェントが稼働</strong>しており、業務の自動化と生産性向上に貢献している。</p>
<p>同社代表の上野山勝也氏が「AIエージェントは、物知りなAIから考えるAI、そして行動するAIへの進化の一形態である」と語るように、従来の生成AIやRAGが知識提供に特化していたのに対し、AIエージェントはユーザーの課題や文脈を理解し、複数の処理を組み合わせて能動的に動くAIである。<strong>日本社会が抱える深刻な人手不足の中で、実働するAIとしての価値は日増しに高まっている</strong>。</p>
<p>PKSHAのAIエージェントは、大きく2つの形態に分類される。一つは、顧客の要望に応じてセミカスタマイズする「プロジェクト型」。もう一つは、既製品として提供する「プロダクト型」である。たとえば、同社が提供する「AI Helpdesk」というAIエージェントは、Microsoft Teamsに組み込まれ、社内の問い合わせ対応を自動化する。FAQ検索、ドキュメントからの回答生成、有人対応への引き継ぎといった機能を持ち、問い合わせ内容に合わせて判断し、最適な手段で解決を試みる仕組みである。また、AIエージェントが答えを持たない場合は、課題を抽出して新たなナレッジを作成し、人のレビューを経て再学習させる。この人とAIの協調によって、ナレッジは日々進化し、より的確な応答が可能となっていく。さらに、全社員向けに通知を行う「プロアクティブエージェント」なども開発しており、従業員とのインタラクションを積極的に支援している。</p>
<p>上野山氏は国内におけるAIの浸透に対して、「PoCにとどまらず、社会実装まで進めることが重要だ」と強調した。AIエージェントの定義があいまいなままでは、実証実験で終わってしまい、現場で“働く”AIにはならない。何ができて何ができないかを明確にした上で、実行可能なユースケースに落とし込むことが求められる。PKSHAの取り組みは、単なる技術実験にとどまらず、社会の中でAIが労働力の一部として機能する未来を切り拓こうとしている。日本発のAIエージェントが、実装と実働を通じて世界に示すモデルとなる日も近い。</p>
<h3><strong>採用も営業も“実行”するAIへ</strong>　<strong>株式会社Algomatic</strong></h3>
<p>株式会社Algomaticは、AIエージェントによる業務自動化の最前線を走るスタートアップとして、業務特化型ソリューションを展開している。今回は「リクルタAI」を開発している株式会社Algomatic Works COO 高橋氏と、「アポドリ」を開発した執行役員／ネオセールスカンパニーCEO 池田氏に話を聞いた。</p>
<p><strong>スカウト業務を実行「リクルタAI」</strong>
「リクルタAI」は同社が提供する採用特化型AIエージェントのブランド名で、現在は「スカウト」「書類選考」「面談」の3種類のエージェントを提供中。</p>
<p><img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002F_4a28c37604%5Cu002F_4a28c37604.png" alt="リクルタ.png" />
:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Fprtimes.jp%5Cu002Fmain%5Cu002Fhtml%5Cu002Frd%5Cu002Fp%5Cu002F000000050.000120362.html">株式会社Algomatic Works プレスリリース</a>{target=“_blank”}
:::
スカウトに特化した「リクルタAI ダイレクト採用」は、採用担当者が求人票や採用要件といった情報をインプットすると、各社の採用方針に沿った採用候補者のリストアップや、パーソナライズしたスカウト文の作成・送付を全自動で実行する。
高橋氏は「常時稼働しているAIエージェントが、転職希望者の転職意欲やその温度感の変化をリアルタイムで捉えるため、セットアップ後は『待っているだけ』で、求めている候補者との面談が獲得できる」と話す。実際、導入企業であるシンプレクス・ホールディングス株式会社では、スカウト返信率が従来の約10倍に向上している。また、1台のAIエージェントが人間数人分の成果を実現できているという事例もある。</p>
<p>この機能を支えるのがAIエージェント型のアーキテクチャである。スカウト送信を行う機能に加え。「上司AI」とでも呼ぶべき機能も実装されておりが、誤送信や誤字、企業情報の間違いなどを検知して送信を制御。。AIの暴走を防ぐガードレール機能も整備されており、ブラウザ上で自律的な実行を行いながら、必要に応じて人の承認フローを踏むなどの対策が練られている。</p>
<p>2025年5月22日には、転職エージェント・人材派遣企業向けの「リクルタAI プレ面談」もリリース。の採用AIエージェントが候補者ヒアリングから職務経歴書の代筆、マッチする求人の提案までを遂行する機能を持つという。</p>
<p><strong>営業活動を代替する「アポドリ」</strong>
アポドリは、BtoB営業のプロセスをAIエージェントが自律的に担う営業支援サービスだ。企業の営業リソース不足、人脈の限界、ナーチャリングの非効率といった課題に対し、情報収集からアプローチ実行、結果分析までを一気通貫で支援する。</p>
<p><img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002F_f0ccc1b340%5Cu002F_f0ccc1b340.png" alt="アポドリ.png" />
:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Fprtimes.jp%5Cu002Fmain%5Cu002Fhtml%5Cu002Frd%5Cu002Fp%5Cu002F000000046.000120362.html">株式会社Algomatic プレスリリース</a>{target=“_blank”}
:::</p>
<p>具体的には、AIが企業情報・担当者情報を収集し、ターゲットごとの文面を1to1で自動生成。メールや問い合わせフォーム、SNS、手紙郵送といった複数チャネルを用いてアプローチを実行する。さらに、返信の有無や内容に応じた分岐処理も組まれており、対応可能なものはAIが自動で返答、不確実なケースは人間に引き継がれる。自動返答AIエージェントは現在開発中であるものの「25〜50％はAIのみで対応可能であり、残りは半自動もしくは人による判断が必要であると見込んでいる」と池田氏は説明する。アポドリの本質は、ただのツールではなく、営業プロセス全体の再設計にある。「専門知識のない汎用LLMに営業の知識や、過去データや営業の勝ちパターンデータを掛け合わせることで、営業プロセスをAIが高精度に再現できるようにした。言語化されていない営業プロフェッショナルの暗黙知を構造化してAIに落とし込むことが重要である」と池田氏は語る。</p>
<p>また池田氏は「AIによる完全自動化は品質担保を放棄しているといえる。例えばAIによる精度99％の処理も、人の目を通すことで99.9％に高められる。そのため、弊社ではあえて人の介在を残している」と述べ、プロダクトの精度と信頼性の両立を強調した。特に、リストの最終確認や文面のフォーマット調整などは、今も人間の確認フローを踏んでいるとのことだ。</p>
<p>システム面の設計も、AIエージェントを実務に耐えるレベルで運用するための要となっている。池田氏は「扱うデータが膨大なうえに、1％のエラーも許されない」と語り、AIが実行主体として使われ続けるには“信頼性の高さ”が不可欠だと強調した。たとえば、アポドリで1日1万回以上AIワークフローを実行しても安定して稼働するように構成されているが、「LLMは、ときに想定外のレスポンスを返してくることもある。そんなエラー発生時の検知や復旧プロセスまでを含めて設計していなければ、サービスの価値は一気に下がってしまう」と説明。求められるのは、単なる“魅力的な体験”ではなく、日々の運用で揺るがない“安定品質”だ。
そのため同社では、GPTやClaude、Geminiといった複数のLLMをサービス役務に応じて使い分けるだけでなく、応答の安定性やモデルの特性も細かく評価。さらに、エージェントの稼働状況を常時監視し、フェールセーフや人間によるフォールバックをシステム設計に組み込むことで、SaaSレベルの堅牢な運用体制を構築している。こうした綿密なシステム設計によって、企業利用にも耐える安定したAI実行基盤を実現している。</p>
<p><strong>技術的な課題　AIに“読ませにくい”世界</strong>
AIエージェントにおける技術課題を問うと、「AIの可読性」というキーワードが出てきた。たとえば、Webサイト上の重要な情報がテキストでなく画像になってしまっている場合、AIはその内容をを上手く読み取れない。また、データの表記ゆれや省略表現なども障壁となる。高橋氏は「MCPなどでAI可読性が上がる仕組みが出てくれば、状況は変わるかもしれない」と期待を述べた。</p>
<p>さらに、AIに学習させるためのデータが蓄積されていないケースも多い。面談・面接の内容、営業成否の記録など、企業のこれまでのアクションデータが十分に蓄積されておらず、AIが正確な意思決定を行うための土台が整っていない。同社はその課題を受け、社内データの蓄積基盤の整備にも力を入れているとのことだ。</p>
<p><strong>真のAI導入には経営層の関与が必須</strong>
池田氏は、「今後1〜3年以内に特化型エージェントの導入が進み、3年後には実運用が本格化すると見ている」と話す。初期フェーズでは、“AIエージェント”という言葉がバズワード的に消費されることも予想されるが、PoCの積み重ねによって業務適用のフレームワークが整い、再現性あるモデルが確立されていくという。池田氏が特に伝えたいメッセージとして強調したのは、“AI導入はツール選定ではなく、業務プロセスそのものの再設計＝BPRの一環である”という点だ。AI活用のROIは、現場単位ではなく経営視点での判断が必要であり、表面的なツール導入で効果を求めても本質的な変化は起きない。</p>
<p>AIが“実行”する時代に突入した今、いかに人とAIが役割分担しながら共に働く体制を構築するかが、企業の競争力を左右する鍵となっているのだ。</p>
<h2>AIエージェントはAGI（汎用人工知能）への布石？</h2>
<p>AIエージェントは、単なる業務支援や省力化の手段にとどまらず、より大きな目標であるAGIの実現に向けた重要なステップでもある。AGIに到達するためには、人間のような知的な行動が求められるが、それには身体性や記号接地などの、人間のように考えるための基本能力との統合が不可欠である。
現在のAIエージェントは、これらの要素の一部を備え始めており、序盤で触れた「Coscientist」や「Figure 02」のように物理環境での実行まで踏み込んだ例は、AGIの実現につながる動きといえるであろう。</p>
<p>AGIは、ある日突然訪れるような劇的な技術革新ではなく、AI技術の着実な進化の延長線上にある未来だ。AIエージェントが今後さらに成熟していく中で、私たちはこの先端技術とどう向き合い、どう共に働くのか、その姿勢が問われる時代に入りつつある。遠くない未来のために、今、どのような一歩を踏み出すべきかを考えなければならない。</p>
<p>:::box
特集：<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Fcategories%5Cu002Fcoverstory%5Cu002Fledgeai-expo-2025-summer-Special">Ledge.ai EXPO 2025 Summer</a>{target=“_blank”}
:::</p>
]]></description>
      <pubDate>Sun, 13 Jul 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>郊外巨大施設から都市型も可能な”装置”にフォーカスが変化する日本のAIデータセンター戦略</title>
      <link>https://ledge.ai/articles/expo-2025-summer-kamui</link>
      <description><![CDATA[<p>:::box
国内最大級のAI（人工知能）関連メディアLedge.aiが、2025年6月に公開した参加費無料の「Ledge.ai EXPO 2025 Summer」。
本稿ではサイト内で掲載していた、業界をリードするキーパーソンへの特別インタビューや書き下ろし記事など、コンテンツの全文を公開する。
:::</p>
<p>AIの進化により、日本国内のデータセンター市場が急拡大する中で、冷却技術や分散型演算インフラのあり方が改めて問われている。本稿では、QuantumMesh株式会社 代表取締役 篠原裕幸氏に、国内データセンター市場の現状と課題、そして同社が展開するAIデータセンター向け液浸冷却システム『KAMUI（カムイ）』の可能性について、話を聞いた。</p>
<p>:::box
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002FKAMUI_2_d65d4aa575%5Cu002FKAMUI_2_d65d4aa575.png" alt="KAMUI2.png" />
<strong>Quantum Mesh株式会社 代表取締役</strong>
<strong>篠原 裕幸</strong>
幼少期から通信技術やコンピューターに慣れ親しみ、シリコンバレーでのムーブメントに感銘を受ける。中学生時代に訪れたアメリカ西海岸でテクノロジーによる社会変革を体験し、帰国後はソフトウェア開発に没頭。学生時代に開発したインターネットサービスをきっかけに起業する。以降、日本、シンガポール、台湾、アメリカと拠点を移しながら、半導体開発やデータセンター開発をはじめとするスタートアップに注力。ビットコインの登場以降は、非中央集権社会を実現するテクノロジーに可能性を見出し、暗号技術によるデータ保護を社会実装するためのブロックチェーン研究開発事業を手がける。
2022年、2016年にアメリカで設立したデジタル金融事業会社を発展させる形で Harmony （株）を創業すると同時に、安田と共に民間のシンクタンク 8M Labs を設立。
2023年、世界規模の生成AI台頭に伴い、情報を守るためのインフラ開発を喫緊の課題としたQuantum Mesh社を設立。半導体開発事業やデータセンター開発事業の集大成となる「可搬型データセンター」の開発・運営を行う。誰もが等しく超高度情報化社会の利益を享受できる「情報格差のない社会」の構築を目指している。
:::</p>
<h2>計算能力が資源となる時代へ</h2>
<p>AIの進化がもたらす構造変化の中、日本のデータセンター市場が新たな転換期を迎えている。篠原氏は「生成AIの普及は、インターネット黎明期以来の、構造変化を引き起こしている」と語り、計算能力資源が国家や企業の競争力に直結する価値を持ち始めていることを強調した。その一方で、日本国内のデータセンターインフラには、土地・電力といった供給面の制約が立ちはだかっている。特に首都圏では、用地不足や電力制限に加え、制度面での課題も複雑に絡んでいる。篠原氏は「地方分散は望ましい流れであるが、従来のデータセンターと同じものではなく再設計が必要」と話し、地方の再生可能エネルギーや広大な土地、自然冷却といったポテンシャルを生かすには、それらを総合的に設計するシステムアーキテクトの存在が不可欠だと指摘した。</p>
<p>また、冷却技術への注目も高まっている。篠原氏が「冷却そのものが“主要機能”へと格上げされつつある。今後は冷却性能がクラウドの性能そのものを左右する時代になる」と語るように、これまでは裏方技術として捉えられていた冷却技術は、現在その地位は大きく変わりつつある。</p>
<h2>KAMUIが実現する“場所を選ばない冷却”</h2>
<p>このような状況下で開発されたのが、同社が提供する閉鎖循環型・液浸冷却システム『KAMUI』である。
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002FKAMUI_82e6f50800%5Cu002FKAMUI_82e6f50800.png" alt="KAMUI3.png" />
:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Fwww.youtube.com%5Cu002Fwatch?v=yjAbyP_cchI&amp;t=931s">ENJIN YouTubeチャンネル「ENJIN LAUNCH PARTY DIGEST」</a>{target=“_blank”}
:::</p>
<p>KAMUIは絶縁オイルを密閉して循環させる設計を採用し冷却水（地下水など）との熱交換によってオイルの温度を保つことでサーバを冷却する。外気温や粉塵など外部環境の影響を受けにくい安定した動作を実現し、静音で稼働する。日本やアジア地域などに豊富に存在する地下水を活用する“場所を選ばない冷却”を実現する装置であり、実運用においてもPUE（Power Usage Effectiveness）1.03～1.04という高効率を維持している。</p>
<p>:::box
<strong>PUEとは?</strong>
PUEとは、データセンター等IT関連施設におけるエネルギー効率を測定する指標の一つ。PUEが1.0に近いほど、投入された電力のほとんどがIT危機に使われていることを意味し、ロスが少なく、効率的な施設と評価できる。
:::</p>
<p>KAMUIが真価を発揮するのは、都市部の狭小スペースや電力制約のある環境だけでなく、騒音や環境負荷が問題となりやすい地方都市での場所での利用にも適している。篠原氏は「KAMUIは、都市部や地方の小規模拠点、物流倉庫の一角などにおいて自律した冷却システムとして機能する」と述べた。</p>
<h2>スマートシティとの親和性</h2>
<p>KAMUIのこのような特性は、スマートシティの文脈でも大きな可能性を示す。スマートシティでは、セキュリティ・交通・医療などの様々な都市機能がリアルタイムに連携する必要があり、数秒の遅れが重大な判断ミスやシステム効率の低下を招きかねない。そのため、従来のクラウド集中型アーキテクチャでは対応しきれない場面もある。篠原氏は「スマートシティの根幹は“遅延のない意思決定”にある。KAMUIは都市内に『計算の拠点』を分散配置することを可能にする」と述べ、KAMUIが冷却と演算の一体設計によって都市内クラウドの実現を支える存在であることを示した。</p>
<p>実際、KAMUIは様々な業種・業態での実装が検討されている。「KAMUI×病院」では画像診断AIとの連携、「KAMUI×駅」では顔認証や群衆制御アルゴリズムとの連携といったシナリオが進められており、いずれもリアルタイム処理が求められる。KAMUIの分散型処理は、プライバシー保護の観点からも重要な選択肢となる。</p>
<p>また、地方自治体との協働も進行中であり、防災拠点や医療機関を対象に、KAMUIを中核とした「オフグリッド型ノード」の実証実験が始まっている。再生可能エネルギー発電所では、余剰電力を価値化する演算拠点としての導入も具体化されており、地域における新たな経済循環の担い手としても期待されている。</p>
<p>さらにKAMUIは、同社が発起人となっている“ENJINプロジェクト”とも連携している。ENJINプロジェクトは、産業が急速に変化するAIoT時代において、社会課題の解決と持続的な成長を見据え、民間企業や専門家が集結して構成されたプロジェクトだ。</p>
<p><img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002Fenjin_a2feb117d1%5Cu002Fenjin_a2feb117d1.png" alt="KAMUI4.png" />
:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Fenjin.live%5Cu002F">ENJIN公式サイト</a>{target=“_blank”}
:::</p>
<p>このプロジェクトが掲げる『AIoTサプライチェーン構想』のなかで、KAMUIは演算拠点の冷却およびエネルギー効率を実現する基盤装置として位置づけられている。鉄道事業者との取り組みでは、駅構内の余剰電力やスペースを活用し、都市インフラの一部としてエッジ演算ノードを導入する計画が進められている。</p>
<h2>未来の都市インフラに向けて</h2>
<p>篠原氏は、液浸冷却の将来について「もはや選択肢の一つではなく、計算密度という物理条件に対する合理的な解である」と述べた。データセンターが従来の「施設」ではなく、「装置」として再定義される未来が見えつつあるが、その実現には、演算拠点の多極分散と地域電力の地産地消がカギを握る。「既に再生可能エネルギー発電事業者や地方都市自治体との協働により、このようなモジュール型演算インフラの導入が進行している」と篠原氏は語り、KAMUIが冷却装置の枠を超え、AIoT時代の基盤インフラとして認知される日が近づいていることを示唆した。</p>
<p>冒頭でも触れたように、AIの進化が引き起こした計算需要に対し、日本のデータセンターは、分散化とエネルギー効率の両輪で再設計が必要な局面に立っている。地域ごとの制約を解消し、都市の隅々までにリアルタイムな計算能力を行き渡らせるためには、これまでの巨大施設としてのデータセンター像を見直す必要がある。</p>
<p>日本全体を俯瞰したとき、KAMUIが描く構想は、エネルギーと演算、そして都市機能を結びつける“次世代都市の神経網”を担う可能性を大いに秘めている。都市インフラをより高速かつ持続可能なものへと転換する取り組みは、静かに動き始めている。</p>
<p>:::box
特集：<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Fcategories%5Cu002Fcoverstory%5Cu002Fledgeai-expo-2025-summer-Special">Ledge.ai EXPO 2025 Summer</a>{target=“_blank”}
:::</p>
]]></description>
      <pubDate>Fri, 11 Jul 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>2025年のAI半導体を読む　王者NVIDIAと競合の現在地</title>
      <link>https://ledge.ai/articles/expo-2025-summer-semiconductor</link>
      <description><![CDATA[<p>:::box
国内最大級のAI（人工知能）関連メディアLedge.aiが、2025年6月に公開した参加費無料の「Ledge.ai EXPO 2025 Summer」。
本稿ではサイト内で掲載していた、業界をリードするキーパーソンへの特別インタビューや書き下ろし記事など、コンテンツの全文を公開する。
:::
2025年、AI半導体を巡る競争は次のステージへと突入した。GPU分野で圧倒的な地位を築いてきたエヌビディアは、AIインフラの中心的存在としての立場をさらに強化。一方で、中国や米国の競合企業もそれぞれの強みを武器に、差別化戦略を打ち出しはじめている。本記事では、NVIDIAの最新動向を軸に、AMD・Intel・Huaweiといったプレイヤーの動きも交えながら、2025年上半期のAI半導体業界を俯瞰する。</p>
<h2>世界最大のGPU企業、AIブームの中核に立つエヌビディアの動向</h2>
<p>エヌビディアは、アメリカ・カリフォルニア州に本社を置く半導体企業で、特にGPUの開発で世界的に知られている。GPUは言わずもがな、一度に大量のデータを高速処理できるハードウェアであり、AI開発・利用で不可欠な存在である。GPUはAIインフラの心臓部とも言える重要な部分を担っていることから、同社はAIブームの中心にいる企業ともいえる。</p>
<h3>最新GPU「Blackwell」アーキテクチャとNVIDIA Dynamoの衝撃</h3>
<p><img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002F1_6666bef6c2%5Cu002F1_6666bef6c2.png" alt="半導体_1.png" />
:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Fwww.nvidia.com%5Cu002Fja-jp%5Cu002Fgtc%5Cu002Fkeynote%5Cu002F">NVIDIA GTC 基調講演</a>{target=“_blank”}
:::
エヌビディアは、主催する開発者向けの世界的な人工知能カンファレンスである“GTC 2025”でフアン氏は、次世代GPUアーキテクチャ「Blackwell」と前世代「Hopper」の最盛期出荷数とBlackwell初年度の出荷見込みを比較し、その驚異的な成長を指しつつ、「AIは変曲点にある」と語った。特に推論のワークロードにおいて、Blackwellは前世代のHopperより最大40倍の性能向上を実現しており、そのインパクトは計り知れない。</p>
<p>特に注目すべきは、Blackwellが進化し、NVLink技術と液冷技術を採用した点だ。これにより、1ラックあたり最大1.4エクサフロップスという圧倒的な処理能力を発揮し、従来のエアフロー冷却の設計では実現し得なかった高密度・高効率なAIコンピューティングを可能にしている。</p>
<p>こうした新アーキテクチャを支える中核技術として、エヌビディアは新たなオペレーティング・システム「NVIDIA Dynamo」を発表した。NVIDIA Dynamoは、生成AIを支える分散型推論フレームワークであり、オープンソースで提供されている。</p>
<p>NVIDIA Dynamoの主な特徴は4点ある。</p>
<ul>
<li>分散サービング：複数GPUノードでの効率的な処理分散を可能にし、スケーラビリティを確保</li>
<li>GPUプランナー：GPUの使用状況をリアルタイムで監視し、動的にリソースを割り当てる</li>
<li>スマートルーター：KVキャッシュの利用を考慮したルーティングが行われ、再計算の手間を最小限に抑える</li>
<li>低遅延通信ライブラリ：GPU、CPU、ネットワーク、ストレージ間の連携を高速かつ簡素に実現する</li>
</ul>
<p>BlackwellとNVIDIA Dynamoの組み合わせは、エヌビディアが提唱する“AIファクトリー” の中核を担うとされており、AIの実装スピードと効率性を飛躍的に高める可能性を秘めている。</p>
<h3>次世代GPU開発のロードマップ</h3>
<p>エヌビディアは今後のGPU開発のロードマップを明らかにした。
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002F2_f58845a790%5Cu002F2_f58845a790.png" alt="半導体_2.png" />
:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Fwww.nvidia.com%5Cu002Fja-jp%5Cu002Fgtc%5Cu002Fkeynote%5Cu002F">NVIDIA GTC 基調講演</a>{target=“_blank”}
:::</p>
<p>まず2025年後半には、GB200 NVL72と比較すると1.5倍の性能を実現する「Blackwell Ultra」をリリース予定。さらに2026年には、HBM4メモリを採用し、高帯域（13TB\u002Fs）と省電力性を両立する「Rubin」、2027年には15エクサフロップスの演算性能を持つ「Rubin Ultra」が続く予定である。そして2028年、16個のGPUダイを搭載という前例のない構成を持つ「Feynman」が投入される計画だ。</p>
<h3>エヌビディア株の暴落</h3>
<p>2025年1月下旬、中国のスタートアップDeepSeekが、低コストで高性能なAIモデル「DeepSeek-R1」を発表し、ChatGPTを上回る性能を示した。これにより、エヌビディアの株価は一時17%下落し、約5890億ドル（約91兆円）の時価総額が失われるという、米国市場史上最大の単日損失となった。というのも、DeepSeekは高性能GPUの入手が制限される中、旧世代のエヌビディアGPUを活用し、効率的なAIモデルの開発に成功。DeepSeekがエヌビディアの最新GPUに依存しないAI開発の可能性を示したことで、同社の需要減少が懸念されたことが株価に影響した。</p>
<h3>エヌビディアのハイブリッド計算の時代を見据えた動き</h3>
<p>エヌビディアは、量子コンピューティングとAIスーパーコンピュータの融合を目指し、ボストンに「NVIDIA Accelerated Quantum Research Center（NVAQC）」を設立すると発表した。この研究施設では、量子ハードウェアとAIスーパーコンピュータを統合し、量子コンピューティングの実用化を加速することを目的としている。NVAQCには、NVIDIA Blackwell GPUを搭載したGB200 NVL72システムが導入され、量子アルゴリズムの大規模シミュレーションや量子エラー訂正の研究が行われる予定である。また、NVIDIAのCUDA-Qプラットフォームを活用し、GPU、CPU、QPUを組み合わせたハイブリッド量子アプリケーションの開発が進められる。</p>
<p>この取り組みには、アメリカの量子コンピューティング企業であるQuantinuum、QuEra Computingや、イスラエルの量子コンピュータ開発スタートアップ Quantum Machines、ハーバード大学のHQI（Harvard Quantum Initiative）、マサチューセッツ工科大学のEQuS（Engineering Quantum Systems）グループが協力する。</p>
<p>フアン氏は、2025年1月に「実用的な量子コンピュータの実現には数十年かかる」と発言していたが、3月のGTC2025ではその見解を修正し、量子コンピューティングの進展が予想以上に速いことを認めた。NVAQCの設立は、エヌビディアがAIと量子コンピューティングの融合による新たな計算時代を見据えた未来への投資であり、今後の技術革新において重要な役割を果たすことが期待される。</p>
<h2>エヌビディアの対抗馬の動向は？</h2>
<h3>AMD、同社初の視覚言語モデル「Instella-VL-1B」を発表</h3>
<p>AMDは2025年3月、自社初となるマルチモーダルAIモデル「Instella-VL-1B」を発表。本モデルは、画像と言語の統合処理を目的としたVision-Language Model（VLM）であり、同社の生成AI向けGPU「Instinct MI300X」上でトレーニングされている。
Instella-VL-1Bは、3億パラメータのビジョンエンコーダーと12億パラメータの言語モデルから構成され、合計15億パラメータを有する。視覚情報をテキスト化する2層のMLPで構成されており、エンコーダーにはCLIP ViT-L\u002F14-336、言語モデルには「OLMo 1B SFT」を採用している。トレーニングにはLLaVAやPixmoなどのデータセットに加え、表やグラフのデータセットであるM-PaperやDocStruct4Mといった文書理解系データも活用。これにより、Instella-VL-1BはOCRや文書解析においても高い精度を示し、同規模のオープンソースモデル（MiniCPM-V-2）を上回る性能を達成している。
AMDは本モデルの重み、学習データ・データセットの詳細、コードなど、すべてオープンソースとして公開。同社は、AI時代のインフラをけん引する存在としての意志を示した。</p>
<p>:::box
関連記事：<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Famd_instella_vl_1b_vision_language_model">AMD、初の視覚言語モデル「Instella-VL-1B」を発表—自社GPUのMI300XでトレーニングされたマルチモーダルAI</a>{target=“_blank”}
:::</p>
<h3>インテル、次世代データセンター向けプロセッサ「Xeon 6」を発表</h3>
<p>インテルは2025年2月、最新のデータセンター向けプロセッサ「Intel Xeon 6」シリーズを発表した。本シリーズは、「P-core（Performance-core）」モデルと、「E-core（Efficient-cores）」モデルの2タイプのCPUマイクロアーキテクチャから選択可能となっている。これにより、パフォーマンスと電力効率の両立を実現している。P-coreは主に高性能処理を要するAIやHPC向けの用途に適しており、E-coreは大規模なクラウド環境やエッジワークロードといったスケールアウトが求められる分野において力を発揮する。</p>
<p>また、インテルは今回の発表において、電力効率と総保有コスト（TCO）削減の重要性を強調した。省電力性と高密度設計を両立させたE-coreモデルを通じて、持続可能性と運用コストの最適化を図る姿勢を明確にしている。Xeon 6は、変化し続けるデータセンターのニーズに対応するべく、消費電力・スケーラビリティなどの全方位での進化を体現するプロセッサと位置づけられる。Xeon 6シリーズの導入により、AI時代のデータセンターにおける性能と効率性の新たな基準を打ち立て、競合他社との差別化を図っている。</p>
<p>:::box
関連記事：<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fintel_xeon6_ai_datacenter">インテル、AIとネットワーク向けの新プロセッサー「Xeon 6」を発表</a>{target=“_blank”}
:::</p>
<h3>Huawei、AIチップ「Ascend 910C」でNVIDIA H100に挑戦</h3>
<p>Huaweiは、AIチップ「Ascend 910C」の出荷を開始し、NVIDIAの「H100」に対抗する姿勢を見せた。DeepSeekが実施したテストによると、Ascend 910Cは、推論性能においてH100の約60%に相当する性能を示したという。このチップは、中国の半導体メーカーであるSMICの7nmプロセス（N+2）で製造されており、FP16精度で800TFLOPS、メモリ帯域幅は約3.2TB\u002Fsに達する。
また、Huaweiは、Ascend 910Cを搭載した大規模AIクラスター「CloudMatrix 384」を発表。384基のAscend 910Cチップを搭載することで約300PFLOPSの能力を提供するとされている。これは、NVIDIA GB200 NVL72の約180PFLOPSを上回る性能を実現しているとされており、中国国内でのAIインフラの自立を加速させる狙いがある。一方で、電力効率には課題もある。消費電力がNVL72の3.9倍で、FLOP当たりの消費電力は2.3倍、メモリ帯域幅当たりの消費電力は1.8倍、メモリ容量当たりの消費電力は1.1倍と、電力効率が大きく劣る結果となっている。</p>
<p>:::box
関連記事：<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fhuawei_ascend_910c_vs_nvidia_h100_performance">HuaweiのAIチップ「Ascend 910C」、NVIDIA「H100」の60%の推論性能を達成 —— DeepSeekのテスト結果</a>{target=“_blank”}
:::</p>
<h2>半導体製造の動向は？</h2>
<h3>TSMC、次世代1.4nmプロセス「A14」を2028年に量産開始へ</h3>
<p>TSMCは、2025年4月の北米技術シンポジウムにおいて、次世代1.4nmプロセス「A14」を発表した。このプロセスは、同社の2nmプロセス「N2」と比較して、同じ消費電力で最大15%の速度向上、または同じ速度で最大30%の電力削減を実現し、トランジスタ密度も20%以上向上する。</p>
<p>A14は、第二世代のGAA（Gate-All-Around）ナノシートトランジスタと、改良された「NanoFlex Pro」セルアーキテクチャを採用し、設計の柔軟性と性能効率を高めている。TSMCは、AI向けチップの需要増加に対応するため、A14プロセスを2028年に量産開始する予定であり、NVIDIAなどの顧客への供給を見込んでいる。</p>
<p>:::box
関連記事：<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Ftsmc_announces_a14_process_for_2028_mass_production">TSMC、次世代半導体製造技術「A14」を発表、2028年量産を目指す</a>{target=“_blank”}
:::</p>
<h3>Rapidus、NEDOから2nm開発予算を承認取得</h3>
<p>Rapidusは、国立研究開発法人新エネルギー・産業技術総合開発機構（NEDO）より、2025年度の2nm半導体開発計画と予算の承認を受けた。
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002F3_cd13c15fce%5Cu002F3_cd13c15fce.png" alt="半導体_3.png" />
:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Fwww.rapidus.inc%5Cu002Fnews_topics%5Cu002Fnews-info%5Cu002Fnedo-fy2025-approval%5Cu002F">Rapidus株式会社「直近のIIM (2025年3月27日撮影)」</a>{target=“_blank”}
:::</p>
<p>:::box
Rapidus（ラビダス）とは、2022年に設立された日本の半導体メーカーである。世界最先端の2nmロジック半導体の実用化を目指している。
:::</p>
<p>この承認は、ポスト5G情報通信システム基盤強化研究開発事業の一環として、「日米連携に基づく2nm世代半導体の集積化技術と短TAT製造技術の研究開発」および「2nm世代半導体のチップレットパッケージ設計・製造技術開発」を対象としている 。
北海道千歳市に建設中の製造拠点 “IIM” において、2025年4月から2nmプロセスの試作ラインを立ち上げ、同年7月中旬以降には初の試作チップを完成させる予定である 。これにより、2027年の量産開始を目指し、日本国内での先端半導体製造体制の確立を図っている。</p>
<p>この取り組みは、国内の半導体産業の再興と、AIや高性能コンピューティング分野での国際競争力強化に向けた重要な一歩となる。</p>
<p>:::box
関連記事：<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Frapidus_2nm_nedo_2025_approval">Rapidus、2nm半導体の量産化に向けNEDOの2025年度予算承認を獲得</a>{target=“_blank”}
:::</p>
<p>:::box
特集：<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Fcategories%5Cu002Fcoverstory%5Cu002Fledgeai-expo-2025-summer-Special">Ledge.ai EXPO 2025 Summer</a>{target=“_blank”}
:::</p>
]]></description>
      <pubDate>Sun, 13 Jul 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>「フィジカルAI」が産業界を変える ｕｇｏ CSO羽田氏が語る、ロボットの未来と日本の勝ち筋</title>
      <link>https://ledge.ai/articles/expo-2025-summer-ugo</link>
      <description><![CDATA[<p>:::box
国内最大級のAI（人工知能）関連メディアLedge.aiが、2025年6月に公開した参加費無料の「Ledge.ai EXPO 2025 Summer」。
本稿ではサイト内で掲載していた、業界をリードするキーパーソンへの特別インタビューや書き下ろし記事など、コンテンツの全文を公開する。
:::</p>
<p>少子高齢化と労働力不足が進む中、ロボットの存在は今後さらに重要となってくる。現在も製造現場や施設管理、サービス業において、AIとロボティクスの融合による効率化や省人化を期待する声が多いが、導入のハードルはいまだ高いのが現状だ。
こうした現状を打開すべく、実用的な業務ロボットの社会実装に挑んでいるのがｕｇｏ株式会社である。本稿では、同社の取締役CSOであり、戦略と開発の両面をリードする羽田卓生氏に、ロボティクスの現在地、日本における課題、そして「フィジカルAI」という次のパラダイムについて話を聞いた。
:::box
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002Fphoto_hada_f37eff913e%5Cu002Fphoto_hada_f37eff913e.jpg" alt="photo_hada.jpg" />
<strong>ｕｇｏ株式会社　取締役CSO</strong>
<strong>羽田 卓生</strong>
1998年 立命館大学経済学部卒。同年ソフトバンク株式会社に入社。2013年にアスラテック株式会社の立ち上げに参画。2019年7月より株式会社ABEJAを経て、2020年 ｕｇｏ株式会社取締役に就任。任意団体ロボットパイオニアフォーラムジャパン代表幹事、特定非営利活動法人ロボットビジネス支援機構「RobiZy」アドバイザーも務める。
:::</p>
<h2>ロボットの現在地は「下半身が完成した段階」</h2>
<p>羽田氏は、現在のロボティクス技術の成熟度について、「下半身が完成した段階にある」と語った。移動や巡回といった基礎的な物理動作はすでに実現されており、ｕｇｏが提供する警備・点検・案内ロボットも、日々の業務で稼働している。だが、人間のように「手で操作し、目で見て、言葉でやりとりする」ためには、さらなる進化が必要である。そこにAI、特にLLMや模倣学習の力が加わることで、ようやく“上半身”が備わる。「すでにAIの“目”や“言葉”は手に入りつつある。これに模倣学習による“手”の機能が加われば、業務ロボットの概念が一変する。このブレイクスルーは5年以内、もしかするともっと早く来るかもしれない」と羽田氏は予測する。</p>
<p>ロボティクスの技術自体は進化しているにもかかわらず、日本ではその社会実装が思うように進んでいない。その理由として、羽田氏は「価格」「柔軟性」「導入設計」の3点を挙げた。かつて産業用ロボットの中心だったアーム型ロボットは、自動車などの大量生産に特化しており、汎用性に欠けるうえ価格も高い。中小規模の現場やサービス業では採算が合わず、結果として導入が限定的となってしまっている。
また、決められた動きしかできないことも、ロボットの導入を妨げてきた大きな要因である。しかし、仮に清掃ロボットがドアを開け、モップがけまで行えるようになったらどうか。配膳ロボットがテーブルへの配膳だけでなく、食器の片付けまでこなせるようになったらどうか。それだけで、ロボット導入の価値は大きく変わる。ロボットが“あと一歩”進化することで、導入の実現性は一気に高まるのだ。「人手不足や賃金上昇といった外的要因と、ロボットの進化が組み合わさった時、さらなる導入の追い風が吹く」と羽田氏が語るように、社会的なニーズと技術革新が交差するとき、ロボットが大きなブレイクスルーを迎えるのだ。</p>
<h2>現場で“使われ続ける”ロボットをつくるｕｇｏ</h2>
<p>ｕｇｏが現在提供しているのは、警備、点検、案内といったルーチン業務を担うロボットである。オフィスビル、駅構内、データセンター、工場など、多様な現場で稼働している。「人が歩き回って確認するだけの時間は生産性がない。異常検知のような判断業務は人に任せ、データ収集や巡回はロボットが担う。それによって、現場全体の効率が上がっていくと考える」と羽田氏は説明する。</p>
<p><img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002Fimg_sol_tenken_03_2_768x512_1_8b55e780f2%5Cu002Fimg_sol_tenken_03_2_768x512_1_8b55e780f2.jpg" alt="img_sol_tenken_03-2-768x512 (1).jpg" />
:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Fugo.plus%5Cu002Finspection-robot-solution%5Cu002F">ｕｇｏ株式会社『点検ロボットソリューション』</a>{target=“_blank”}
:::</p>
<p>また、ｕｇｏでは、ロボットとLLMやRAGを組み合わせた会話システムも開発。ユーザーとのやりとりをスコア化し、精度が低い部分をナレッジとして再学習させる設計になっている。まさに「現場で育つロボット」である。さらに、クラウド連携による遠隔制御、LLMの切り替えによる多用途対応、ノーコードによる運用のしやすさなど、使い続けてもらうための仕組み作りにも注力している。これにより、データの継続的な蓄積と品質向上が可能になる。</p>
<p>現場で使われ続けるロボットを支える開発体制もまた、印象的だった。今回インタビューで筆者が訪れたのは、東京・千代田区という都心の一等地に構えるｕｇｏのオフィスである。ひとつのフロア内に、実務やソフトウェア開発のエリア、ハードウェアの組み立てスペース、出荷を待つロボットが並ぶ格納エリア、さらには実証実験用のデモエリアまでが混在していた。</p>
<p><img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002FIMG_1909_015edcaf6e%5Cu002FIMG_1909_015edcaf6e.jpg" alt="IMG_1909.JPG" /></p>
<p>羽田氏が「垂直型で対応している」と語る通り、ｕｇｏではハードとソフトの開発が完全に一体化しており、設計から組み立て、テスト、配送までがワンストップで完結している。この“すべてがつながった開発環境”こそが、同社が次々と革新的なロボティクス技術を生み出す原動力なのである。</p>
<h2>ロボティクスの今後は？「フィジカルAI」が変えるもの</h2>
<p>羽田氏が今後のロボティクスのトレンドとして提唱する「フィジカルAI」とは、AIの知能をロボットの身体に結びつけ、物理空間での行動へ転換するアーキテクチャを意味する。従来のAIがディスプレイ上やクラウド上で完結していたのに対し、フィジカルAIは“実際に手を持ち、行動するAI”である。
現在、ロボティクス業界では「世界モデル」や「デジタルツイン」など、AIが環境を認識・理解しながら行動を計画・最適化する技術が注目されている。しかし、それらを応用する分野として、羽田氏は「まだ、どこが勝ち筋かはまだわからない」と語る。例えば、米国のスタートアップであるFigureは、家庭にロボティクス活用の糸口を見つけようとしたり、自動車メーカーとして有名な米国のテスラは自動運転技術を応用して、危険な作業や退屈な単純作業を行う二足歩行の汎用ヒューマノイドロボット開発に乗り出しているように、現在は各社が多様なアプローチで、現実可能なロボティクスの社会実装を模索している段階にある。</p>
<p>こうした中で羽田氏は「日本の勝ち筋はオペレーションの質そのものにある」と指摘する。ロボットの能力を左右するAIの性能は、単にモデルの規模だけではなく、学習に用いるデータの質にも大きく左右される。AIの性能関連でよく話される “スケーリング則” では、モデルの性能は以下の3要素のかけ合わせによって向上すると言われている。</p>
<ul>
<li>データ規模</li>
<li>学習に使われる計算量</li>
<li>パラメーター数</li>
</ul>
<p>このうち、日本の強みは「データ」にある。たとえば羽田空港の清掃業務は、世界的にも高い評価を受けており、2016年には英国SKYTRAXの国際空港評価である「ザ・ワールド・クリーネスト・エアポート」部門において第1位に選ばれている。こうした高度なオペレーションの現場で蓄積される業務データは、AIの学習素材として非常に価値が高い。「良い現場、良いマニュアル、真面目な労働者。それらが揃っている日本は、データ品質の面でも優位に立てる」と羽田氏は語る。日本品質のオペレーションをモデル化し、ロボティクスに実装することで、「ジャパンスタンダード」のような高品質なAI\u002Fロボットサービスを世界に輸出することも可能になるだろう。
羽田氏の言葉を借りれば、「これからのAI開発における生命線は“良いデータ”である」。日本の現場力こそが、フィジカルAI時代におけるグローバル競争の切り札となり得るのだ。</p>
<h2>今、企業が備えるべきこと</h2>
<p>羽田氏は最後に、企業が「今」取り組むべきことについて次のように語った。</p>
<p>「フィジカルAIは、産業界の主戦場になる。だからこそ、現場を俯瞰して自社にしかないオリジナルな業務が何なのかを見極めることが必要だ。自動化のためではなく、自社の強みをデータ化し、新たな競争力を生むためにロボットを活用すべきである」</p>
<p>ロボットの導入は、単に人手を減らす手段ではない。AIとロボティクスの融合により、現場の構造そのものを再定義し、新たな価値創出の礎とするための業務改革なのである。その中でｕｇｏは、日本の産業界が持つ現場力をAIと結びつけ、「使われ続けるロボット」を通じて、新たな価値観を形づくろうとしているのだ。</p>
<p>:::box
特集：<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Fcategories%5Cu002Fcoverstory%5Cu002Fledgeai-expo-2025-summer-Special">Ledge.ai EXPO 2025 Summer</a>{target=“_blank”}
:::</p>
]]></description>
      <pubDate>Thu, 10 Jul 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>生成AI「個人利用」26.7%に上昇──それでも米中との差は歴然【総務省・情報通信白書】</title>
      <link>https://ledge.ai/articles/generative_ai_personal_use_japan_2025</link>
      <description><![CDATA[<p>:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Fwww.soumu.go.jp%5Cu002Fjohotsusintokei%5Cu002Fwhitepaper%5Cu002Fja%5Cu002Fr07%5Cu002Finfografic.html">情報通信白書令和7年版 インフォグラフィックより</a>{target=“_blank”}
:::</p>
<p>総務省は2025年7月8日、「<a href="%5B%E7%99%BA%E8%A1%A8%5D(https:%5Cu002F%5Cu002Fwww.soumu.go.jp%5Cu002Fjohotsusintokei%5Cu002Fwhitepaper%5Cu002Fr07.html)%7Btarget=%E2%80%9C_blank%E2%80%9D%7D">情報通信白書令和7年版</a>」を公表し、国内における生成AIの個人利用率が26.7%にとどまっているとの調査結果を示した。これは米国（68.8%）や中国（81.2%）と比べて大きく差がある。調査は2024年度にインターネット上で6,000人を対象に実施されたもので、生成AIの国内普及の現状と課題を浮き彫りにしている。</p>
<h2>世界との利用率比較、日本は依然低水準</h2>
<p>白書によれば、生成AIサービスを「使ったことがある」と回答した個人は日本で26.7%。これは前年（2023年度）の9.1%から約3倍に増加したものの、国際的に見ると依然として低い水準である。</p>
<p>同調査における他国の利用率は、米国が68.8%、ドイツが59.2%、中国が81.2%。いずれも日本を大きく上回っており、対象4カ国の中で日本が最も低い。年代別に見ると、日本国内では20代の利用率が44.7%と最も高く、30代が23.8%、40代が29.6%、50代が19.9%、60代が15.5%と、年齢が上がるにつれて利用率が下がる傾向が明確に現れている。</p>
<p><img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002F2025hakusho_aa33bea58a%5Cu002F2025hakusho_aa33bea58a.jpg" alt="2025hakusho.jpg" />
:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Fwww.soumu.go.jp%5Cu002Fjohotsusintokei%5Cu002Fwhitepaper%5Cu002Fja%5Cu002Fr07%5Cu002Fpdf%5Cu002Findex.html">情報通信白書令和7年版より</a>{target=“_blank”}
:::</p>
<h2>利用しない理由：「必要性を感じない」が最多</h2>
<p>生成AIを使っていない人にその理由を尋ねた結果、最も多かったのは「自分の生活や業務に必要ない」で40.4%、次いで「魅力的なサービスがない」（38.6%）、「使い方がわからない」（18.3%）が続いた。
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002F2025hakusho2_e78137159b%5Cu002F2025hakusho2_e78137159b.jpg" alt="2025hakusho2.jpg" />
:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Fwww.soumu.go.jp%5Cu002Fjohotsusintokei%5Cu002Fwhitepaper%5Cu002Fja%5Cu002Fr07%5Cu002Fpdf%5Cu002Findex.html">情報通信白書令和7年版より</a>{target=“_blank”}
:::</p>
<p>この結果について白書は、生活や業務への活用方法が十分に理解されていない現状を示すとともに、ユーザーの実利感の欠如が普及の壁となっている可能性を示唆している。</p>
<p>一方で、生成AIの利用意向に関する設問では、「調べものをする」（40.8%）、「コンテンツの要約・翻訳をする」（38.6%）、「画像や動画を生成する」（35.9%）など、「今は使っていないが、ぜひ利用してみたい」または「条件によっては利用したい」と回答した割合が4割前後に達する項目も多い。</p>
<p>すでに利用している割合は各項目とも10%未満にとどまるが、サービスの充実や操作性の改善によって利用が拡大する余地があることがうかがえる。</p>
<p><img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002F2025hakusho3_aa43775f27%5Cu002F2025hakusho3_aa43775f27.jpg" alt="2025hakusho3.jpg" />
:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Fwww.soumu.go.jp%5Cu002Fjohotsusintokei%5Cu002Fwhitepaper%5Cu002Fja%5Cu002Fr07%5Cu002Fpdf%5Cu002Findex.html">情報通信白書令和7年版より</a>{target=“_blank”}
:::</p>
<h2>政策的対応と今後の展望</h2>
<p>総務省は白書の中で、生成AIの健全な利活用に向けて、以下の3点を政策課題として挙げている。</p>
<ul>
<li>利用者リテラシーの向上</li>
<li>民間事業者によるガイドライン整備の促進</li>
<li>教育・人材育成を含む体制の強化</li>
</ul>
<p>また、デジタル庁や内閣官房AI戦略チームなどと連携しながら、産業界・教育機関・自治体を横断する形で活用促進とリスク管理を両立させる必要性も強調されている。</p>
<p>:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fmic_2024_white_paper_on_Information_and_communications">関連記事：総務省 2024年版「情報通信白書」より、生成AI利活用の現状と潜在的な可能性が明らかに</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fai_usage_us_nationwide_survey_2024">関連記事：米国における初の全国的な調査で生成AIの利用状況が明らかに</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Ffreelance_generative_ai_usage">関連記事：フリーランスの生成AI活用率はわずか14.3％、情報漏洩や学習機会不足が主な障壁にー2024年実態調査</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fmiddle_school_students_ai_usage_growth">関連記事：中学生の生成AI利用率が親を上回る—NTTドコモ モバイル社会研究所の調査結果</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fcorporate_ai_utilization_trends_2025">関連記事：日本企業の45%が生成AIを導入、ランサムウェア感染経験は48%</a>
:::</p>
]]></description>
      <pubDate>Thu, 10 Jul 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/7/8 [TUE]Google、急増する電力需要に備え、米核融合ベンチャーCFSと商用核融合発電200MWの長期購入契約を締結——CFS初の発電所と2030年代供給へ</title>
      <link>https://ledge.ai/articles/google_fusion_power_cfs_ppa</link>
      <description><![CDATA[<p>:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Fwww.cfs.energy%5Cu002Fnews-and-media%5Cu002Fgoogle-and-commonwealth-fusion-systems-sign-strategic-partnership">Commonwealth Fusion Systems</a>{target=“_blank”}
:::</p>
<p>Googleは2025年6月30日、米国の核融合スタートアップであるCommonwealth Fusion Systems（CFS）と、同社が開発中の商用核融合発電所から200メガワット（MW）の電力を長期購入する契約を締結したことを<a href="https:%5Cu002F%5Cu002Fblog.google%5Cu002Foutreach-initiatives%5Cu002Fsustainability%5Cu002Four-latest-bet-on-a-fusion-powered-future%5Cu002F">発表</a>{target=“_blank”}した。2030年代前半の送電開始が予定されており、企業による核融合電力の調達契約としては過去最大規模とされる。</p>
<p>@<a href="https:%5Cu002F%5Cu002Fwww.youtube.com%5Cu002Fwatch?v=pBIcDNn6rHA">YouTube</a></p>
<h2>核融合発電の実用化を前提に、ARCからの電力供給を確保</h2>
<p>Googleが契約を結んだのは、CFSがバージニア州チェスターフィールドに建設を予定している商用核融合発電所「ARC」である。ARCは、CFSが開発中のコンパクト型トカマク炉を用いたもので、商用化第一号の核融合プラントとなる見通しだ。今回の契約により、Googleは最大200MWの電力を長期にわたり調達する。</p>
<p>また、Googleはこの契約にあわせてCFSへの追加出資も実施。2021年の初期投資に続くものであり、開発資金を通じて発電所の建設を後押しする。さらにGoogleは、将来的に他のCFSプラントから電力を調達するオプションも保持している。</p>
<h2>実証炉「SPARC」の成功が鍵、2030年代前半に送電目標</h2>
<p>ARCの前段階として開発が進む実証炉「SPARC」は、2026年の稼働を予定しており、2027年までにQ値（エネルギー増倍率）1超の達成を目指している。これが商用化の前提となる。核融合による電力は安定供給が可能で、天候に左右される再生可能エネルギーの変動を補う“24時間型ゼロエミッション電源”としての期待が高まっている。</p>
<h2>急増する電力需要と、それに対応するクリーン電源の確保</h2>
<p>Googleは2025年6月に<a href="https:%5Cu002F%5Cu002Fblog.google%5Cu002Foutreach-initiatives%5Cu002Fsustainability%5Cu002Fenvironmental-report-2025%5Cu002F">公開</a>{target=“_blank”}した最新の環境報告書で、2024年のデータセンター電力消費量が前年比27％増となったことを明らかにした。前年（2023年）の増加率は17％だったことから、AIやクラウド需要の拡大にともなう消費電力の加速度的な増加が読み取れる。</p>
<p><strong>2019〜2024年におけるGoogleのデータセンターの電力消費量（棒グラフ）とエネルギー起因排出量（折れ線）</strong>
2024年は電力使用量が過去最高となったが、クリーン電源の導入により排出量は前年より減少している。
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002FTrajectory_of_data_center_electricity_consumption_and_data_center_energy_emissions_708f18de8d%5Cu002FTrajectory_of_data_center_electricity_consumption_and_data_center_energy_emissions_708f18de8d.jpg" alt="Trajectory of data center electricity consumption and data center energy emissions.jpg" />
:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Fwww.gstatic.com%5Cu002Fgumdrop%5Cu002Fsustainability%5Cu002Fgoogle-2025-environmental-report.pdf">Google 2025 Environmental Report</a>{target=“_blank”}
:::</p>
<h2>クリーンエネルギーによる排出削減とPUEの改善</h2>
<p>電力需要が急増する一方で、Googleはクリーンエネルギー導入やインフラ最適化により、2024年のデータセンター由来の温室効果ガス排出量を前年比12％削減した。また、データセンター全体の平均PUE（電力使用効率）は1.09に改善し、過去6年間で初めて1.10を下回ったと報告されている。</p>
<p>同社は2024年だけで8GW相当のクリーン電力購入契約を新たに締結し、世界中で累計22GW以上の再生可能電源を確保している。</p>
<h2>電力の未来に向けた布石</h2>
<p>Googleによる今回の核融合電力の長期購入は、従来型の再エネPPAに加え、次世代エネルギーの担い手としての核融合発電にいち早く賭ける姿勢を明確にしたものだ。送電開始は2030年代前半を予定しており、今後のSPARCの実証進展、ARCの許認可・建設スケジュールがカギを握る。</p>
<p>持続可能なAI時代を支えるエネルギー基盤の再構築は、企業の競争力にも直結する課題となっている。</p>
<p>:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fgoogle_intersect_tpg_clean_energy">関連記事：Google、AI時代の電力需要に備えたクリーンエネルギー戦略を始動</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fgoogle_amazon_investment_in_smr">関連記事：Googleに続きAmazonも — テック企業による原子力エネルギーへの投資</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fai_power_demand_energy_shortage">関連記事：AI需要で電力逼迫、アメリカ大陸の半数以上が10年先までエネルギー不足のリスクに直面</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fdata_center_power_infrastructure">関連記事：石破首相、生成AIの普及に対応してデータセンターと発電所の一体整備を推進</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fmeta_nuclear_power_ai_ppa_deal">関連記事：Meta、原子力でAI時代の電力確保へ 米Constellationと20年契約</a>
:::</p>
]]></description>
      <pubDate>Tue, 08 Jul 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>世界最強AI「Grok 4」公開──xAI、わずか数カ月という常識外れのスピードでモデル刷新　マスク氏「ネットにない難問も解ける」</title>
      <link>https://ledge.ai/articles/grok4_xai_ai_model_launch</link>
      <description><![CDATA[<p>:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Fx.com%5Cu002Fxai%5Cu002Fstatus%5Cu002F1943158495588815072">xAI “Grok4 Demo Livestream”</a>{target=“_blank”}
:::</p>
<p>イーロンマスク氏の率いるAIスタートアップxAIは2025年7月10日、X（旧Twitter）公式アカウントで最新大規模言語モデル「Grok 4」を<a href="https:%5Cu002F%5Cu002Fx.com%5Cu002Fxai%5Cu002Fstatus%5Cu002F1943158495588815072">発表</a>{target=“_blank”}し、同時にライブ配信で詳細を公開した。前世代「Grok 3」から数カ月という超短サイクルでのモデル刷新となり、マスク氏は「インターネットにも書籍にも存在しない難問を解ける初のAIだ」と性能を強調している。</p>
<h2>「世界最強」を標榜、リアルな工学課題を解決可能と説明</h2>
<p>xAI公式アカウントは「世界で最も強力なAIモデル」としてGrok 4を紹介し、ライブ配信の視聴を呼びかけた。その後、イーロン・マスク氏自身もX上で「Grok 4は、現実世界の難しい工学的課題に対して、ネットにも書籍にも存在しない答えを導き出すことができた初のAIだ」と述べ、同モデルの性能を強調した。</p>
<p><strong>図1　“Ludicrous rate of progress”──Grok 2からGrok 4に至る計算資源の10倍ステップアップを示したスライド</strong>
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002Fmusk_grok4_3_2b88dc0836%5Cu002Fmusk_grok4_3_2b88dc0836.jpg" alt="musk grok4-3.jpg" /></p>
<p>:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Fx.com%5Cu002Fxai%5Cu002Fstatus%5Cu002F1943158495588815072">xAI “Grok4 Demo Livestream”</a>{target=“_blank”}
:::</p>
<h2>公表された性能指標──既存モデルを上回る結果も</h2>
<p>xAIによれば、以下の主要ベンチマークでGrok 4は高い性能を示した（数値はすべて同社発表値）：</p>
<h3>Humanity’s Last Exam（人類最後の試験）</h3>
<p><strong>図2　総合推論テスト「Humanity’s Last Exam」全セット比較。Grok 4 Heavyは44.4%でトップスコアを記録</strong>
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002Fmusk_grok4_5_8a69f27997%5Cu002Fmusk_grok4_5_8a69f27997.jpg" alt="musk grok4-5.jpg" />
:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Fx.com%5Cu002Fxai%5Cu002Fstatus%5Cu002F1943158495588815072">xAI “Grok4 Demo Livestream”</a>{target=“_blank”}
:::</p>
<h3>ARC-AGI（汎用人工知能測定ベンチマーク）</h3>
<p><strong>図3　ARC-AGIベンチマークの精度‐コスト分布。Grok 4は精度66.6%でクラストップ帯に位置付けられた</strong>
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002Fmusk_grok4_7_5b67bb48af%5Cu002Fmusk_grok4_7_5b67bb48af.jpg" alt="musk grok4-7.jpg" />
:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Fx.com%5Cu002Fxai%5Cu002Fstatus%5Cu002F1943158495588815072">xAI “Grok4 Demo Livestream”</a>{target=“_blank”}
:::</p>
<p><strong>図4　GPQAやAIME25など学術系コンペでもGrok 4／Grok 4 Heavyが軒並み首位に</strong>
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002Fmusk_grok4_6_90a347be60%5Cu002Fmusk_grok4_6_90a347be60.jpg" alt="musk grok4-6.jpg" />
:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Fx.com%5Cu002Fxai%5Cu002Fstatus%5Cu002F1943158495588815072">xAI “Grok4 Demo Livestream”</a>{target=“_blank”}
:::</p>
<h2>Grok 4／Grok 4 Heavy──2ライン体制で展開</h2>
<ul>
<li><strong>Grok 4（標準）</strong> ：推論改善を目的に強化学習（RL）を追加。</li>
<li><strong>Grok 4 Heavy</strong> ：複数エージェントで同一課題を並列解析し、回答を相互検証する上位版。
開発者・パワーユーザー向けに月額300ドルの「SuperGrok Heavy」プランを新設。</li>
</ul>
<h2>今後のロードマップ</h2>
<p>Grok 4は、Grok 3からわずか数カ月で投入された。マスク氏は開発速度について「恐ろしく速い」と述べ、今後も短い間隔で新モデルを導入する意向を示した。さらに、以下のようなロードマップも明らかにされている：</p>
<ul>
<li>7月中旬以降：Tesla車両へのGrok搭載を開始予定</li>
<li>8月：コード生成AIのリリース</li>
<li>9月：マルチモーダル・エージェントの提供</li>
<li>10月：動画生成AIの公開</li>
</ul>
<p><strong>図5　xAIが示した今後のタイムライン。8月にコード生成モデル、9月にマルチモーダルエージェント、10月に動画生成AIを投入予定</strong>
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002Fmusk_grok4_9_4b0fc9c4d0%5Cu002Fmusk_grok4_9_4b0fc9c4d0.jpg" alt="musk grok4-9.jpg" /></p>
<p>:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Fx.com%5Cu002Fxai%5Cu002Fstatus%5Cu002F1943158495588815072">xAI “Grok4 Demo Livestream”</a>{target=“_blank”}
:::</p>
<h2>今後の課題</h2>
<p>Grokシリーズは、2023年11月の初版リリースから急速に進化しており、OpenAIのChatGPTやGoogleのGeminiに対抗する形で市場に存在感を示してきた。ただし、直近では前バージョンが反ユダヤ的発言を生成したとの報道もあり、同社はプロンプト設計の見直しを迫られていた。今後は、モデル性能とともに倫理的安全性や説明責任も問われる局面が続くと見られる。</p>
<p>:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fgrok_prompt_disclosure_transparency_risks">関連記事：xAI、チャットボット「Grok」のシステムプロンプトをGitHubで全面公開— 無許可改変問題を受け透明性を優先、懐疑主義を徹底する設計が明らかに</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fgrok_memory_feature_launch_2025">関連記事：xAIがチャットAI「Grok」に会話記憶機能を追加 個別最適化された対話体験が可能に</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fxai_grok3_release">関連記事：イーロンマスク「地球で最も賢いAI」と宣言ーーxAI、最新AIモデル「Grok 3」を正式リリース 有料会員向けに提供開始</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fai_humanitys_last_exam">関連記事：AIの知能限界を試すベンチマーク「Humanity’s Last Exam（人類最後の試験）」、Scale AIとCAISが発表 ：GPT-4oなど最先端モデルでも正答率は10%未満、専門家との差は依然大きい</a>
:::</p>
]]></description>
      <pubDate>Sun, 13 Jul 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>マスク氏のAI「Grok」が “メカ・ヒトラー” 化？——xAIが7月8日の &quot;恐ろしい振る舞い&quot; に謝罪、トラブルの原因を公表</title>
      <link>https://ledge.ai/articles/grok_ai_misfire_apology_prompt_release</link>
      <description><![CDATA[<p>:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Funsplash.com%5Cu002Fja%5Cu002F%E5%86%99%E7%9C%9F%5Cu002Fgrok%E3%81%A8%E3%81%84%E3%81%86%E5%8D%98%E8%AA%9E%E3%81%AE%E7%99%BD%E9%BB%92%E5%86%99%E7%9C%9F-9rDIpHOE9IY">Mariia  halabaieva@Unsplash</a>{target=“_blank”}
:::
イーロン・マスク氏が率いるxAIは2025年7月11日、同社のAIチャットボット「Grok」が7月8日にX（旧Twitter）上で反ユダヤ的表現やナチスを賛美するような投稿を大量に生成した問題について、公式アカウントで「多くの方が経験した”恐ろしい振る舞い（horrific behavior）”を深くお詫びする」と謝罪し、原因は16時間稼働していた誤アップデートコードにあったと<a href="https:%5Cu002F%5Cu002Fx.com%5Cu002Fgrok%5Cu002Fstatus%5Cu002F1943916977481036128">発表</a>{target=“_blank”}した。
また、再発防止策としてプロンプトの全文をGitHubで公開する方針も明らかにした。</p>
<p><img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002FUpdate_on_where_has_2e557e42ef%5Cu002FUpdate_on_where_has_2e557e42ef.jpg" alt="Update on where has.jpg" />
:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Fx.com%5Cu002Fgrok%5Cu002Fstatus%5Cu002F1943916977481036128">GrokのX（旧Twitter）アカウントより</a>{target=“_blank”}
:::</p>
<h2>Grokによる不適切回答の発生</h2>
<p>7月8日未明（米太平洋時間）、GrokはX上でユーザーからの投稿に対し、「ヒトラーは偉大」「MechaHitlerになりたい」といった極端な表現を含む回答を繰り返し生成した。確認された投稿数は数百件にのぼり、その内容がSNS上で拡散されたことにより、問題が広く認知されるに至った。</p>
<p>通報を受けたXは該当の投稿を削除し、ユダヤ系人権団体Anti-Defamation League（ADL）は「極端思想を助長する無責任な対応」と非難声明を出した。
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002Fadl_Grok_LLM_right_now_is_irresponsible_5f7ffadb0a%5Cu002Fadl_Grok_LLM_right_now_is_irresponsible_5f7ffadb0a.jpg" alt="adl  Grok LLM right now is irresponsible.jpg" />
:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Fx.com%5Cu002FADL%5Cu002Fstatus%5Cu002F1942722301876932965">ADLのX（旧Twitter）アカウントより</a>{target=“_blank”}
:::</p>
<h2>xAIによる調査と原因の説明</h2>
<p>xAIは7月11日にGrokの公式アカウントを通じ、問題の発生原因と対応策についてスレッド形式で報告した。</p>
<p>同社によると、今回の不適切回答は、「ユーザー投稿と類似のトーンで返答する」というテスト用のコードが誤って本番環境に適用されたことにより引き起こされたものであり、Grok本体の言語モデル（Grok 4）には変更は加えられていなかったという。このコードは約16時間にわたり稼働しており、その間に外部投稿に含まれる極端思想を模倣する形で回答が生成されていた。</p>
<p>問題発覚後、Grokは一時的に稼働を停止し、対象コードは完全に削除された。
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002FTechnical_Details_on_x_90300ce294%5Cu002FTechnical_Details_on_x_90300ce294.jpg" alt="Technical Details on x.jpg" />
:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Fx.com%5Cu002Fgrok%5Cu002Fstatus%5Cu002F1943916979494232378">GrokのX（旧Twitter）アカウントより</a>{target=“_blank”}
:::</p>
<h2>再発防止に向けた対応策</h2>
<p>xAIは再発防止策として以下の対応を発表している。</p>
<ul>
<li>誤適用されたコードパスを完全に削除し、システム全体をリファクタリング</li>
<li>安全性を強化した新たなシステムプロンプトをGitHub上で公開予定</li>
<li>メンション機能（＠でのタグ付け）による自動応答機能を一時的に停止</li>
<li>外部の研究者を含むレッドチーム体制を拡充し、安全性評価を継続</li>
<li>ユーザーからのフィードバックを常時受け付ける体制を整備</li>
</ul>
<h2>事案発生からの時系列</h2>
<ul>
<li>7月8日 04:00頃（米PDT）　Grokの上流コードが更新され、誤コードが稼働開始</li>
<li>同日 20:00頃　外部ユーザーからの通報を受け、問題が社内で認識されGrokを一時停止</li>
<li>7月9日　Xが不適切投稿を削除、ロイターが第一報を報道</li>
<li>7月10日　各国メディアが続報を掲載、ADLなどが批判声明を発表</li>
<li>7月11日 06:00頃　Grok公式がスレッドで謝罪と原因説明、対策方針を公表</li>
<li>7月12日以降　段階的にサービスが再開され、プロンプト公開の準備が進められている</li>
</ul>
<h2>今後の注目点</h2>
<p>今回の問題を受けて、今後以下の点が注視されている。</p>
<ul>
<li>GitHubでのプロンプト公開によって、第三者による安全性検証が進むか</li>
<li>Grok 4モデル自体の安全性と応答制御の設計が今後も維持されるか</li>
<li>Xプラットフォーム全体のモデレーション体制の見直しが進むか</li>
</ul>
<p>xAIは「ユーザーのフィードバックに感謝する」としており、透明性と安全性の両立に向けた開発を続けるとしている。</p>
<p>:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fgrok4_xai_ai_model_launch">関連記事：世界最強AI「Grok 4」公開──xAI、わずか数カ月という常識外れのスピードでモデル刷新 マスク氏「ネットにない難問も解ける」</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fgrok_prompt_disclosure_transparency_risks">関連記事：xAI、チャットボット「Grok」のシステムプロンプトをGitHubで全面公開— 無許可改変問題を受け透明性を優先、懐疑主義を徹底する設計が明らかに</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fgrok_memory_feature_launch_2025">関連記事：xAIがチャットAI「Grok」に会話記憶機能を追加 個別最適化された関係構築を強化</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fstories_on_x">関連記事：Xに生成AI「Grok」によるニュース要約機能「ストーリーズ」を一部ユーザー向けに試験提供</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fgrok_voice_adult_mode_premium">関連記事：オトナの会話もOKに xAIの対話型AI「Grok 3」、音声モードで解禁 ただし有料会員のみ</a>
:::</p>
]]></description>
      <pubDate>Mon, 14 Jul 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/7/10 [THU]拡散モデルベースLLM「Mercury」登場：“描くような生成”で推論を加速、GPT-4.1 Nanoの7倍速——Inception Labsが一般チャット向けモデルを公開</title>
      <link>https://ledge.ai/articles/mercury_diffusion_llm_release</link>
      <description><![CDATA[<p>:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Fwww.inceptionlabs.ai%5Cu002Fintroducing-mercury-our-general-chat-model">Inception Labs</a>{target=“_blank”}
:::</p>
<p>米Inception Labsは2025年6月17日、拡散モデル（diffusion model）をベースとした大規模言語モデル「Mercury」の一般チャット版を<a href="https:%5Cu002F%5Cu002Fwww.inceptionlabs.ai%5Cu002Fintroducing-mercury-our-general-chat-model">発表</a>{target=“_blank”}した。。同社が2月に発表したコード特化型「<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fmercury_coder_diffusion_llm">Mercury Coder</a>{target=“_blank”}」に続くもので、対話型エージェントや音声インターフェースといったリアルタイム応答を重視するユースケースに向けて設計されたモデルである。</p>
<p>Mercuryは、従来の自己回帰型LLMとは異なり、複数のトークンを並列に生成する“描くような生成”プロセス──拡散型LLM（dLLM）アーキテクチャ──を採用しており、推論速度と応答レイテンシーの大幅な削減を実現した。</p>
<h2>最大708トークン／秒、Claude 3.5比で10倍のスループット</h2>
<p>公開された性能指標によると、Mercuryのスループットは708トークン／秒に達しており、同等規模のモデルとされるGPT-4.1 Nano（96トークン／秒）、Claude 3.5 Haiku（67トークン／秒）を大きく上回った。特に、GPT-4.1 Nano比で約7倍、Claude 3.5 Haiku比で約10倍の速度差が確認されている。</p>
<p>また、品質指標においても以下のスコアが報告されている：</p>
<ul>
<li>MMLU-Pro：69%</li>
<li>HumanEval：85%</li>
<li>GPQA Diamond：51%</li>
</ul>
<p>これらのスコアは、「スピード最適化フロンティアモデルと同等以上の性能に相当する」と同社は説明している。</p>
<h3>参考図：Mercuryの位置付け（小型チャットモデル領域）</h3>
<p>評価機関Artificial Analysisによる小型・非推論系モデルの性能比較において、Mercuryは**「高出力速度 × 実用的インテリジェンス」の最適領域（右上象限）**に唯一位置している。</p>
<ul>
<li><strong>出力速度（横軸）</strong> ：1秒あたりの出力トークン数。Mercuryは700超を記録。</li>
<li><strong>知能指数（縦軸）</strong>：Artificial Analysis Intelligence Index（簡易対話・理解力ベンチマーク平均）。</li>
<li><strong>比較対象</strong> ：GPT-4.1 Nano、Claude 3.5 Haiku、Mistral Small、Gemini Flashなど。</li>
</ul>
<p>Mercuryはこの領域で、処理速度と認識性能を両立する唯一のモデルとして可視化されている。
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002Fmercury_intellogence_vs_output_speed_286c30f817%5Cu002Fmercury_intellogence_vs_output_speed_286c30f817.jpg" alt="mercury intellogence vs output speed.jpg" />
:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Fwww.inceptionlabs.ai%5Cu002Fintroducing-mercury-our-general-chat-model">Inception Labs</a>{target=“_blank”}
:::</p>
<h2>ユースケース：リアルタイム音声や対話Webアプリへの導入を想定</h2>
<p>Mercuryは、従来のLLMでは困難だったリアルタイム応答環境での活用を想定している。とくに以下のような領域に向けた実用が示されている：</p>
<ul>
<li><strong>リアルタイム音声応答</strong> ：翻訳や音声AIアシスタント領域において、標準GPU上での推論でも従来構成（Llama 3.3 70B＋Cerebras）より低レイテンシーであるとされる。</li>
<li><strong>対話型Webアプリケーション</strong> ：Microsoft Build 2025で発表された自然言語インターフェース「NLWeb」プロジェクトにおいて、Inception Labsは初のLLMパートナーに選定されている。Mercuryの高速応答により、「人間同士のような自然なテンポの対話」が可能になるとしている。</li>
</ul>
<p>:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fmercury_coder_diffusion_llm">関連記事：Inception Labs、拡散モデルを使った拡散型大規模言語モデル(dLLM)「Mercury Coder」を発表 テキスト生成系では初の商用利用で従来型LLMの最大10倍高速</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fopenai_announces_gpt41_api_series">関連記事：OpenAI、GPT-4.1シリーズ3種のAPIを発表 高性能モデルのコスト効率を改善</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fgoogle_gemini_2_0_flash_release">関連記事：Google、Gemini 2.0 Flashを正式展開—無料版でも高速AIモデルが利用可能に</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fgemini_2_5_flash_hybrid_model_release">関連記事：Google、Gemini 2.5 Flashをプレビュー公開 思考のオン・オフを可能にするハイブリッド推論モデル</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fanthropic_claude_citations_release">関連記事：Anthropic、Claudeの回答に出典を付与できる「Citations」を提供開始</a>
:::</p>
]]></description>
      <pubDate>Thu, 10 Jul 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Meta、AI Studioボットに“追い掛けメッセージ”訓練との内部文書流出──ユーザエンゲージメントをKPIとする方針はロヒンギャ訴訟の教訓をいかせるか？</title>
      <link>https://ledge.ai/articles/meta_proactive_ai_chatbot_engagement</link>
      <description><![CDATA[<p>:::small
画像の出典：GPT-4oによりLedge.aiが生成
:::</p>
<p>Metaが開発中のAIチャットボットが、ユーザーに対して先にメッセージを送る機能を備えていることが、<a href="https:%5Cu002F%5Cu002Fwww.businessinsider.com%5Cu002Fmeta-ai-studio-chatbot-training-proactive-leaked-documents-alignerr-2025-7">Business Insider（BI）</a>{target=“_blank”}が入手した社内文書により2025年7月3日に明らかとなった。</p>
<p>この新機能は、同社の生成AI基盤「AI Studio」を活用したチャットボットに対して、人間のユーザーへ“追い掛けメッセージ”を送るよう訓練するもので、ユーザーとのエンゲージメントおよび定着率の向上を目的としている。外部業務委託先Alignerrがこの訓練作業を担っているという。</p>
<h2>流出文書が示した「Project Omni」とは</h2>
<p>報道によると、流出した社内ガイドラインは「Project Omni」と呼ばれる社内プロジェクトに基づいており、AIチャットボットがユーザーへ積極的にメッセージを送る“プロアクティブ・メッセージング”の仕様が詳細に記されている。</p>
<p>この機能は、InstagramやFacebook上のAIボットに搭載され、ユーザーが過去14日間に5回以上メッセージを送信していた場合に限り、ボット側からの発信が許可される。</p>
<p>Metaの広報担当者はBIの取材に対し、「限定的なテストが進行中であり、ユーザーの体験向上を目指している」と認めているが、正式リリース時期や地域は未定としている。</p>
<h2>“14日・5メッセージ”で発火──追い掛けAIの条件</h2>
<p>同文書によれば、チャットボットの“発信許可”には複数の制限が設けられている。主な条件は以下の通りである：</p>
<ul>
<li>ユーザーが過去14日間に5回以上メッセージを送っている</li>
<li>14日ごとに1回までメッセージを送信可能</li>
<li>感情的またはセンシティブな話題（喪失、健康、宗教など）は、ボットからは言及しない</li>
<li>教訓的・説教的な文言の使用は禁止（例：「ちゃんとしよう」「学ぶべきだ」）</li>
</ul>
<p>これらのルールは、過度なAIの関与によるユーザー離れや規制当局の監視を回避する狙いがあるとみられる。</p>
<h2>AI Studio、次の稼ぎ頭になるか</h2>
<p>Metaは2024年に「AI Studio」を<a href="https:%5Cu002F%5Cu002Fabout.fb.com%5Cu002Fnews%5Cu002F2024%5Cu002F07%5Cu002Fcreate-your-own-custom-ai-with-ai-studio%5Cu002F">公開</a>{target=“_blank”}し、ノーコードでAIボットを作成・展開できる開発基盤として、インフルエンサーや中小ビジネスの利用を促進してきた。これらのチャットボットはFacebookやInstagram内で展開され、一定のブランド価値や収益を提供することが期待されている。</p>
<p>Metaの試算では、同社の生成AI事業が2025年に20〜30億ドル規模の収益を生む可能性があるという（Business Insider報道による）。一方、同様の分野に取り組む他社、たとえばCharacter.AIなどは高いトラフィックを獲得しながらも収益化に苦戦しており、業界全体として持続可能なビジネスモデルの確立が課題となっている。</p>
<h2>「エンゲージメント優先」の影とロヒンギャ訴訟</h2>
<p>Metaによる“ユーザーの関心を維持する”という戦略は、過去にも大きな社会的波紋を呼んできた。2021年、同社（当時Facebook）は、ミャンマーにおける<a href="https:%5Cu002F%5Cu002Fjp.reuters.com%5Cu002Farticle%5Cu002Fworld%5Cu002F1500-idUSKBN2IM0LP%5Cu002F">ロヒンギャ難民迫害</a>{target=“_blank”}の文脈で、暴力的コンテンツの拡散を制限しなかったとして、米国で1500億ドル規模の集団訴訟を提起された。</p>
<p>訴訟では、「ユーザーのエンゲージメントを最優先し、扇動的投稿をアルゴリズムが優遇した」との主張がなされていた。</p>
<h2>商用展開へ残る課題と規制の視線</h2>
<p>現在のプロアクティブ機能はテスト段階にとどまり、今後、AI Studio利用者など限定的なパートナーを対象に段階的に展開される見通しだという。ただし、EUや米国でAIに対する行動規制や倫理基準が議論される中、こうした能動的機能が消費者保護やプライバシーの観点で問題視される可能性もある。</p>
<p>AIチャットボットが今後、広告やECとの連携を通じた“対話型マーケティング”にシフトしていく中で、Metaのアプローチが規制や社会的合意とどう整合性を取るかが問われる局面に入っている。ユーザーとの“距離の取り方”が、収益モデルの設計と同じく、今後の焦点となる。</p>
<p>:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fmeta_ai_charactor">関連記事：大坂なおみやパリス・ヒルトンが AIキャラに　Meta開発者会議で新 ...</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fmeta_to_train_ai_models_on_user_content_in_europe">関連記事：Meta、欧州SNSユーザー投稿をAI学習に使用へ――規約改定と反発の行方</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fmeta_llama_api_developer_preview">関連記事：Meta、開発者向けAI製品プラットフォーム「Llama API」を限定公開</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fai_chatbot_dependency_youth_safety_lawsuit">関連記事：AIチャットボットに依存した14歳の息子が自殺、フロリダ州で母親が提訴</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fmeta_persona">関連記事：Metaも「人格」持つ AI導入か　歴史上の人物など再現し会話サービス計画</a>
:::</p>
]]></description>
      <pubDate>Sat, 12 Jul 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>MIXI、全社員2,000人に生成AI構築プラットフォーム「Google Agentspace」導入──情報統合と業務自動化で“創造の時間”を拡大へ</title>
      <link>https://ledge.ai/articles/mixi_ai_agentspace_deployment</link>
      <description><![CDATA[<p>:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Fmixi.co.jp%5Cu002Fnews%5Cu002F2025%5Cu002F0707%5Cu002F42850%5Cu002F">株式会社MIXI</a>{target=“_blank”}
:::</p>
<p>MIXIは2025年7月7日、Google Cloudが提供するエンタープライズ向け生成AIプラットフォーム「Google Agentspace」を、全従業員約2,000人に導入したことを<a href="https:%5Cu002F%5Cu002Fmixi.co.jp%5Cu002Fnews%5Cu002F2025%5Cu002F0707%5Cu002F42850%5Cu002F">発表</a>{target=“_blank”}した。</p>
<p>導入の目的は、社内に点在する情報資産の統合と、業務プロセスの自動化により、社員が創造的な業務に集中できる環境を構築することだという。同プラットフォームは、Googleの生成AI「Gemini」および検索技術を活用し、社内文書・メール・画像・動画などを横断的に検索・要約できる。</p>
<h2>社内検証を経て全社展開へ</h2>
<p>同社は2025年3月より一部社員を対象にGoogle Agentspaceの試験導入を行い、Google Cloudの支援のもとでエージェント設計と業務適用の検証を進めてきた。その結果、ナレッジベースの統合による業務効率化と検索性向上が確認されたことから、全社員への展開を決定したという。</p>
<p>対象はMIXIのドメインを持つ全正社員、契約社員、アルバイト、派遣社員、業務委託社員におよび、約2,000人が利用可能となる。</p>
<h2>Google Agentspaceの機能と役割</h2>
<p>Google Agentspaceは、企業データを基盤に生成AIエージェントを構築できるプラットフォームであり、次のような機能が提供されている：</p>
<ul>
<li><strong>統合検索</strong> ：ドキュメント、スプレッドシート、メール、画像、動画など社内外のファイルを横断的に検索し、根拠付きで回答を生成。</li>
<li><strong>業務自動化</strong> ：日報や企画書の作成、議事録の要約などをAIが支援。</li>
<li><strong>ローコード開発</strong> ：従業員がノーコード・ローコードで独自のAIエージェントを作成・共有可能。</li>
<li><strong>情報資産の可視化</strong> ：ナレッジの属人化を防ぎ、再利用性の高い知見を全社的に蓄積。</li>
</ul>
<p>MIXIは、このような機能を活用することで、従来業務にかかっていた検索や確認の手間を削減し、社員がより価値の高い業務に集中できる体制を整えると説明している。</p>
<h2>今後の活用と展望</h2>
<p>今後はGoogle Cloudとの協力体制のもと、社内向けの教育・研修プログラムを通じて、全社員のAIリテラシーを底上げするとともに、業務部門ごとに最適化されたエージェントの活用を推進する。また、生成AIの利活用によって、同社が手がける新たなサービスやユーザー体験の創出にもつなげていく方針だ。</p>
<p>同社代表取締役社長の木村弘毅氏は、リリース内で以下のように述べている。
「情報を探す時間や煩雑なやり取りを削減し、思考の質を高めたい。AI導入は効率化の手段だけでなく創造性への投資だ」</p>
<p>:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fsapporo_ai_stick_dx">関連記事：サッポロHD、全社員向けに生成AI「SAPPORO AI-Stick」を導入——業務効率化とDX推進を加速</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fpanasonic_connect_genai_reduced_working_hours_by_186000h">関連記事：生成AI導入1年「労働時間を18.6万時間削減」パナソニックコネクト</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fgoogle_cloud_ai_leader_certification_launch">関連記事：Google Cloud、非エンジニア層向けに生成AI認定資格「Generative AI Leader」を発表――生成AI導入を主導できる人材を後押し</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fgoogle_tpu_ironwood_agent2agent">関連記事：Google、推論特化TPU「Ironwood」とAIエージェント連携プロトコル「A2A」を発表──Cloud Next 2025でAIインフラの進化を示す</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fgoogle_sheets_gemini_cell_integration">関連記事：GoogleスプレッドシートにGeminiが常駐で「セルレベルAI」に</a>
:::</p>
]]></description>
      <pubDate>Fri, 11 Jul 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI、対話型AI搭載のWebブラウザを数週間以内に発表か</title>
      <link>https://ledge.ai/articles/openai_ai_browser_launch_reported_by_reuters</link>
      <description><![CDATA[<p>:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Funsplash.com%5Cu002Fja%5Cu002F%E5%86%99%E7%9C%9F%5Cu002F%E6%9C%BA%E3%81%AE%E4%B8%8A%E3%81%AE%E3%83%A9%E3%83%83%E3%83%97%E3%83%88%E3%83%83%E3%83%97%E3%81%AE%E3%82%AF%E3%83%AD%E3%83%BC%E3%82%BA%E3%82%A2%E3%83%83%E3%83%97-rv2ooDQuNuI">Zac Wolff@Unsplash</a>{target=“_blank”}
:::</p>
<p>OpenAIは数週間以内にAIを搭載した独自のWebブラウザを発表する見通しであると<a href="https:%5Cu002F%5Cu002Fjp.reuters.com%5Cu002Feconomy%5Cu002Findustry%5Cu002FSJY3A2CG2RPM7KT6MKLMQPQTWA-2025-07-10%5Cu002F">ロイター</a>{target=“_blank”}が2025年7月10日に報じた。報道によれば、新ブラウザは、検索結果をリスト形式で提示する従来型検索とは異なり、ChatGPTのような対話形式で回答を提示する設計になるとされる。</p>
<h2>ChatGPT的UIとAIエージェントを組み込んだブラウザ</h2>
<p>OpenAIの新ブラウザは以下の特徴を備えるという：</p>
<ul>
<li>検索キーワードに対して対話形式で結果を提示し、Webページを直接開かずとも必要な情報が得られる</li>
<li>OpenAIのエージェント技術「Operator」と連携し、ウェブ上のフォーム入力や予約、購入といった作業を自動で処理できる</li>
<li>ユーザーの操作を最小限に抑えた「チャット主導型ブラウジング」体験を志向している</li>
</ul>
<p>報道では、これらの機能によって、ユーザーは検索エンジンやリンクを複数クリックする手間なく目的を達成できる可能性があるとされている。</p>
<h2>背景にある戦略的狙い</h2>
<p>OpenAIはこれまでMicrosoftのBingと連携して検索機能を提供してきたが、独自ブラウザを展開することでユーザーデータへのアクセスを強化し、広告収益源をGoogleに依存せずに確保する狙いがあるという。現在、Google Chromeは世界のブラウザ市場で約60％のシェアを持ち、検索連動型広告によって収益を拡大している。</p>
<p>一方でOpenAIは、週5億人規模のユーザーを抱えるChatGPTを活用し、検索から情報取得までを同社のエコシステム内で完結させようとしている。</p>
<h2>検索技術への投資と布石</h2>
<p>ロイターが指摘するように、OpenAIはこの1年、検索・ブラウジング関連の技術開発を継続している。</p>
<ul>
<li>「Product Manager, Search」などの職種で求人を出し、検索関連技術への投資を進めてきた</li>
<li>2024年4月には、AIエージェントのウェブ検索能力を評価するためのベンチマーク「BrowseComp」を発表</li>
<li>ChatGPTには「Browse with Bing」機能や、外部リンク先の要約を行う機能などをすでに実装している</li>
</ul>
<p>こうした動きが、今回の独自ブラウザ発表につながっていると見られている。</p>
<h2>今後の展望</h2>
<p>ロイターの取材に対し、OpenAIおよび競合企業であるアルファベット（Google）はコメントを控えているという。</p>
<p>:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fopenai_operator_browser_agent_release">関連記事：OpenAI、ブラウザを直接操作するAIエージェント「Operator」を発表</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fopenai_operator_deep_research_expansion">関連記事：OpenAI、AIエージェント「Operator」をProユーザーに提供開始──Deep Research拡張を正式ローンチ</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fopenai_responses_api_agents_sdk">関連記事：OpenAI、新API「Responses API」と「Agents SDK」を発表</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fapple_ai_search_safari_google_contract_shift">関連記事：AppleがSafariに生成AI検索エンジン導入を検討──Googleとの契約見直し報道</a>
:::</p>
]]></description>
      <pubDate>Fri, 11 Jul 2025 09:50:00 GMT</pubDate>
    </item>
    <item>
      <title>日本オラクル、AIとソブリンクラウド運用を支援する「ジャパン・オペレーション・センター」開設</title>
      <link>https://ledge.ai/articles/oracle_japan_ai_sov_cloud_center</link>
      <description><![CDATA[<p>:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Fwww.oracle.com%5Cu002Fjp%5Cu002Fnews%5Cu002Fannouncement%5Cu002Foracle-opens-japan-operations-center-to-accelerate-adoption-of-sovereign-cloud-and-ai-2025-07-08%5Cu002F">Oracle</a>{target=“_blank”}
:::</p>
<p>日本オラクルは2025年7月8日、日本企業におけるAI活用とソブリンクラウド導入を加速させることを目的に、「ジャパン・オペレーション・センター」を東京に開設したと<a href="https:%5Cu002F%5Cu002Fwww.oracle.com%5Cu002Fjp%5Cu002Fnews%5Cu002Fannouncement%5Cu002Foracle-opens-japan-operations-center-to-accelerate-adoption-of-sovereign-cloud-and-ai-2025-07-08%5Cu002F">発表</a>{target=“_blank”}した。</p>
<p>新センターは24時間365日体制で稼働し、日本国内のエンジニアが運用支援を担当する。これは、同社が2024年に発表した10年間で80億米ドル以上を日本に投資する計画の一環とされ、企業のデータ主権や法規制対応を重視した体制を整える狙いがあるという。</p>
<h2>国内のAI・クラウド需要に対応する新たな拠点</h2>
<p>ジャパン・オペレーション・センターは、以下のような機能と目的を持つ。</p>
<ul>
<li>24時間365日の運用監視と支援：国内在住のエンジニアによってクラウドインフラの運用が担われる。</li>
<li>データ主権・法令遵守の確保：センターの運用プロセスは、日本国内法および各種業界規制に対応。</li>
<li>Oracle Alloyパートナー企業への支援：クラウド基盤を活用する国内パートナーに対し、ノウハウやエンジニアリング支援を提供。</li>
<li>国内人材の強化と育成：AI・クラウド専門チームの拡充により、より高度な技術支援を目指す。</li>
</ul>
<h2>背景には生成AIとソブリンクラウドのニーズ増加</h2>
<p>日本国内では、機密性の高いデータを国内に保持しつつ生成AIを活用したいという要望が政府機関や金融機関、製造業などで高まっている。これに対応するため、同社は2024年4月、国内クラウド基盤の強化と人員増強を含む80億ドル規模の投資計画を公表しており、今回のセンター開設はその中核施策と位置づけられる。</p>
<h2>パートナー企業の評価と今後の展開</h2>
<p>発表に際し、Oracle Alloyの国内パートナー企業もエンドースメントを表明している。</p>
<ul>
<li><strong>富士通株式会社</strong> ：ジャパン・オペレーション・センターの設立により、「ハイパースケーラー同等の機能を備えつつ、経済安全保障リスクにも対応した国産ソブリンクラウドの実現が可能になる」とコメント。</li>
<li><strong>株式会社NTTデータ</strong> ：同社のソブリンクラウド基盤「OpenCanvas」にOracle Alloyを組み込み、安全な生成AI活用を推進していく方針を示した。</li>
</ul>
<p>同社は今後、同センターを中核拠点として、日本国内におけるクラウドリージョンの拡張とAIインフラの強化を進める計画である。企業のミッションクリティカルな業務基盤のクラウド化や生成AI導入支援を通じ、「AI時代における持続的なビジネス成長」を支えるとしている。</p>
<p>:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Foracle_partnerships_with_openai_and_googlecloud">関連記事：Oracle OpenAI、Google Cloud それぞれと新たなパートナーシップを発表</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fopenai_stargate_global_data_residency">関連記事：OpenAI、Stargate構想を各国展開へ 民主的AIインフラとアジア4カ国のデータレジデンシーを同時発表</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fopenai_stargate_uae_launch">関連記事：OpenAI主導の「Stargate」構想、UAEで海外初展開へ——G42・Oracle・NVIDIA・ソフトバンクG・Ciscoが共同参画、アブダビに超大型AIデータセンター</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Faist_nvidia_hpe_abci_supercomputer">関連記事：産総研とNVIDIA、AIスーパーコンピューター「ABCI 3.0」導入で日本のAI主権を強化 今年中に稼働開始予定</a>
:::</p>
]]></description>
      <pubDate>Thu, 10 Jul 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Perplexity、AIブラウザ「Comet」公開──タブ迷子ゼロへ、思考をそのまま実行する“ウェブ用AI相棒”</title>
      <link>https://ledge.ai/articles/perplexity_launches_ai_browser_comet</link>
      <description><![CDATA[<p>:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Fwww.perplexity.ai%5Cu002Fja%5Cu002Fhub%5Cu002Fblog%5Cu002Fintroducing-comet">Perplexity</a>{target=“_blank”}
:::</p>
<p>AI検索サービスを展開するPerplexityが2025年7月10日、AI搭載ウェブブラウザ「Comet（コメット）」を正式リリースしたことを<a href="https:%5Cu002F%5Cu002Fwww.perplexity.ai%5Cu002Fja%5Cu002Fhub%5Cu002Fblog%5Cu002Fintroducing-comet">発表</a>{target=“_blank”}した。ユーザーがウェブで得たい情報やタスクを自然言語で伝えるだけで、検索や比較、実行までをAIアシスタントが一貫して支援する“認知型ブラウジング”を標榜しており、既存の検索・閲覧体験からの大幅な転換を試みているという。</p>
<p>サービスの提供は、月額200ドルの上位有料プラン「Perplexity Max」会員を対象とする招待制で開始し、今後数週間かけてウェイトリスト登録者にも順次公開していくという。</p>
<p>@<a href="https:%5Cu002F%5Cu002Fwww.youtube.com%5Cu002Fwatch?v=YeldJ4UezDQ">YouTube</a></p>
<h2>タブ操作から思考支援へ──Cometの中核コンセプト</h2>
<p>Cometの開発目的は、従来の「ナビゲーション中心」のブラウジングから、ユーザーの意図や思考を理解し、支援する「コグニティブ（認知）」な体験へと進化させることにあるという。ユーザーは複数タブを開いたり、情報をコピーペーストしたりすることなく、「この論文の要点をまとめて」「このフライトは安いのか？」「この製品は他と比較してどうか」といった問いかけをそのままブラウザに投げかけることで、AIがそれに応じた解答やアクションを提示する。</p>
<p>中心機能となるComet Assistantは、ブラウザ画面の横に常駐するインターフェースで、表示中のページ内容を理解したうえで、ユーザーからの追加質問、要約、執筆支援、カレンダー入力、ECサイトでの買い物などのタスクをその場で実行可能としている。</p>
<p>また、Webページ上の任意のテキストをドラッグすると、その内容についての説明や関連情報を即時に提示する「ハイライト要約」機能、複数の視点や逆説的な説明を提案する多言語・多角的な対話も可能となっている。</p>
<h2>高精度検索エンジンを土台に</h2>
<p>Cometは、Perplexityがこれまで展開してきたAI検索エンジンをそのまま標準搭載しており、検索結果にはすべて出典リンクが付与される。これにより、ユーザーはAIが導き出した情報の裏付けを直接確認できる設計となっている。同社はこれまでも「事実ベースの生成」に特化した検索技術で注目を集めており、2025年6月時点で月間検索回数は7.8億件を超えている。</p>
<p>同社はこれまで、Google検索とは異なるアプローチで、質問に対して即座に構造化された回答を提供することで評価されてきた。Cometはその延長線上に位置づけられ、ユーザーの質問意図や参照ページのコンテキストを理解したうえで、より深いナビゲーションと作業支援を実現する。</p>
<h2>提供形態と今後の展開</h2>
<p>現時点では、CometはMacおよびWindowsに対応したネイティブアプリとして提供され、利用にはPerplexity Max（200ドル\u002F月）への加入および招待コードが必要となっている。今後数週間でウェイトリスト登録者へのアクセス提供を拡大し、数カ月以内には他のプラットフォーム（モバイルなど）への対応や無料版の展開も予定されている。</p>
<p>一方、Cometはユーザーデータの取り扱いについても留意しており、今後のアップデートではプライバシー設計の強化や「AIエージェントの個別最適化（パーソナライズ）」なども視野に入れているという。</p>
<h2>活発化するAIブラウザ市場</h2>
<p>AIを組み込んだ次世代ブラウザは2025年に入り注目を集めており、米The Browser Companyの「Arc」や、BraveのAI連携、さらにOpenAIによる独自ブラウザ開発の動きも報じられている。<a href="https:%5Cu002F%5Cu002Fwww.reuters.com%5Cu002Fbusiness%5Cu002Fmedia-telecom%5Cu002Fopenai-release-web-browser-challenge-google-chrome-2025-07-09%5Cu002F">Reuters</a>{target=“_blank”}も今回のリリースに関連して、Cometを「AIブラウザ戦争」の本格化を示す動きと位置づけた。</p>
<p>Perplexityは今回のComet投入によって、ユーザーの「検索」から「思考・作業」までを一貫して支援する統合環境を提供し、GoogleやChatGPTなどを含む既存のAI体験と差別化を図る狙いがあると見られる。</p>
<h2>今後の焦点：ユーザー拡大と利用定着</h2>
<p>今夏中には全ユーザーへの招待提供を完了し、フィードバックを受けながらの改良フェーズに入る。Perplexityは今後の製品ロードマップにおいて、CometのAI機能をさらに高度化し、さまざまな業務・用途に応じたユースケース拡大を計画している。企業ユーザーや情報労働者にとって、単なる“検索の延長”にとどまらない生産性ツールとしての定着が、次の成長フェーズの鍵となる。</p>
<p>:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fperplexity_deep_research_sonar_ai">関連記事：Perplexity、AIリサーチ機能「Deep Research」を発表 – 高速検索モデル「Sonar」との連携で専門家レベルの調査を実現</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fopenai_ai_browser_launch_reported_by_reuters">関連記事：OpenAI、対話型AI搭載のWebブラウザを数週間以内に発表へ──ロイター報道</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fperplexity_labs_ai_output_generation">関連記事：Perplexity AI、Proユーザー向けに新機能「Labs」を正式公開──10分で分析レポートやアプリを自動生成</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fperplexity-ai_shop_like_a_pro">関連記事：AI検索エンジンPerplexityが買い物機能を強化、「Buy with Pro」導入で送料無料を実現</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fopenai_operator_browser_agent_release">関連記事：OpenAI、ブラウザを直接操作するAIエージェント「Operator」を公開ーー米国Proユーザー向けにリサーチプレビューとして提供開始</a>
:::</p>
]]></description>
      <pubDate>Sun, 13 Jul 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>学術＆研究2025/7/7 [MON]ハルシネーション（事実誤認）より深刻なAIの「わかったふり」を暴く：MITなどが発見したLLMの“ポチョムキン理解”とは</title>
      <link>https://ledge.ai/articles/potemkin_understanding_llm</link>
      <description><![CDATA[<p>:::small
画像の出典：GPT-4oによりLedge.aiが生成
:::</p>
<p>MIT・ハーバード大学・シカゴ大学の研究チームは2025年6月29日、大規模言語モデル（LLM）の「表面的には理解しているように見えるが、実際には概念の適用で誤る」現象を「ポチョムキン理解」と命名し、その頻度を定量化した研究成果を<a href="https:%5Cu002F%5Cu002Farxiv.org%5Cu002Fabs%5Cu002F2506.21521">発表</a>{target=“_blank”}した。発表はICML 2025（バンクーバー）に採択され、AI分野における評価基準の再考を促す内容となっている。</p>
<p>18世紀ロシアの「ポチョムキン村」は、皇帝の視察用に急造された見せかけの村落を指し、「中身のない外観」の象徴とされる。研究者らは、LLMにも同様の「わかったふり」があるとし、この概念をポチョムキン理解と表現している。</p>
<h2>ポチョムキン理解の定義と背景</h2>
<p>研究チームは、LLMが人間向けに設計されたベンチマークの「キーストーン質問」には正しく答えられるものの、その後の具体的応用タスクでは誤る状態を指摘した。これは、人間なら正答＝理解と認められる最小限の問いに合格しても、LLMが本質的に異なる誤解を抱いている可能性を示している。</p>
<p><strong>キーストーン集合に正答しても本質的に誤った解釈を残すポチョムキン理解のイメージ</strong>
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002FA_schematic_representation_of_keystones_and_potemkins_e47033e684%5Cu002FA_schematic_representation_of_keystones_and_potemkins_e47033e684.png" alt="A schematic representation of keystones and potemkins.png" />
:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Farxiv.org%5Cu002Fabs%5Cu002F2506.21521">Potemkin Understanding in Large Language Models</a>{target=“_blank”}
:::</p>
<h2>検証の概要</h2>
<p>検証では、</p>
<ul>
<li>文学技法（俳句やアナロジーなど12種類）</li>
<li>ゲーム理論（ナッシュ均衡など9種類）</li>
<li>心理的バイアス（サンクコストの誤謬など11種類）
の合計32概念について、</li>
<li>定義</li>
<li>分類</li>
<li>生成</li>
<li>編集
の4つのタスクで7種類のモデル（GPT-4o、Claude 3.5 Sonnet、Gemini 2.0 Flash など）を評価した。</li>
</ul>
<h2>主な結果</h2>
<p>定義タスクではおおむね94%の正答率を記録したが、その後の応用タスクでは</p>
<ul>
<li>分類で55%</li>
<li>生成で40%</li>
<li>編集で40%
の失敗率（potemkin rate）が確認された。これは、定義だけでは概念理解の深度を測れない可能性を示唆している。</li>
</ul>
<h3>具体例：韻律パターンの応用失敗</h3>
<p>代表的な例として挙げられるのが韻律スキームの問題だ。GPT-4oに「ABAB韻律とは何か」を問うと、下図のように正確に定義を説明した。しかしいざ詩の穴埋め問題でABAB韻律を適用させると、正しく韻を踏めず、自分でもその失敗を認める回答を出した。人間ならまず起こり得ない不可解な挙動である。</p>
<p><strong>GPT-4oはABABの定義を正しく述べながら、応用で失敗する「ポチョムキン理解」の典型例</strong>
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002FPotemkin_Understanding_in_llm_5dae4e573b%5Cu002FPotemkin_Understanding_in_llm_5dae4e573b.png" alt="Potemkin Understanding in llm.png" />
:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Farxiv.org%5Cu002Fabs%5Cu002F2506.21521">Potemkin Understanding in Large Language Models</a>{target=“_blank”}
:::</p>
<h2>多分野で発生する“わかったふり”</h2>
<p>研究チームはさらに、幾何学の基本定理、家族関係の概念、俳句の構造など幅広い領域で同様のポチョムキン理解を確認している。</p>
<p><strong>概念の定義には成功する一方で応用に失敗する複数の事例</strong>
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002FExamples_of_potemkins_f6c5140e2d%5Cu002FExamples_of_potemkins_f6c5140e2d.jpg" alt="Examples of potemkins.jpg" />
:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Farxiv.org%5Cu002Fabs%5Cu002F2506.21521">Potemkin Understanding in Large Language Models</a>{target=“_blank”}
:::</p>
<h2>自己評価による一貫性検証</h2>
<p>さらに著者らは、自動評価の一環として「モデル自身に、自分が生成した回答を再評価させる」という仕組みを試みた。
例えば「スラントライムの例を作れ」と指示し、その後「今作った例はスラントライムか？」と再度モデルに問うと、矛盾した回答が返るパターンが確認され、モデル内部の知識表現が不整合である可能性を示しているとした。</p>
<p><strong>生成と再判定の整合性を確かめる自動評価プロセスのイメージ</strong>
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002FIllustration_of_the_method_for_evaluating_incoherence_in_models_d19701ab72%5Cu002FIllustration_of_the_method_for_evaluating_incoherence_in_models_d19701ab72.png" alt="Illustration of the method for evaluating incoherence in models.png" />
:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Farxiv.org%5Cu002Fabs%5Cu002F2506.21521">Potemkin Understanding in Large Language Models</a>{target=“_blank”}
:::</p>
<h2>社会的影響と課題</h2>
<p>論文では、ハルシネーション（事実誤認）とは異なり、ポチョムキン理解は概念構造の誤りであるため、人間にも検出が難しいと指摘する。
法務や医療、教育といった高い正当性が求められる分野でLLMを活用する際には、ベンチマークだけでは保証できないリスクとして注意が必要とされる。</p>
<p>研究チームは、人間とAIの「誤解のパターン差」を考慮したベンチマークの再設計や、概念の一貫性を評価するためのツール開発を進める方針だ。</p>
<p>:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fchatbot_arena_bias_allegations">関連記事：LLMの評価ランキング「Chatbot Arena」にMeta・Googleなど大手優遇の疑惑—Cohereら研究者が不透明な評価手法を指摘、LM Arenaは「事実誤認」と反論</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fmmmu-pro">関連記事：最新マルチモーダルLLMの性能測定のためのベンチマーク「MMMU-Pro」登場　見えてきたLLMの限界と課題とは？</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fopenai_paperbench_ai_research_reproduction">関連記事：OpenAI、論文理解と再現力を測る新ベンチマーク「PaperBench」を発表</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fmmlu_prox_multilingual_ai_benchmark">関連記事：東京大学松尾氏ら国際研究チーム、多言語AI評価の新ベンチマーク「MMLU-ProX」を発表──公平でグローバルに利用可能なAI開発を後押し</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fmapbench_lvml_navigation_benchmark">関連記事：地図を読み取り、目的地までのルートを言語で説明できるAIは誕生するか──新ベンチマークMapBenchが明らかにした大規模視覚言語モデル（LVLM）の限界</a>
:::</p>
]]></description>
      <pubDate>Sun, 13 Jul 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Hugging Face、「SmolLM 3」公開──小型・多言語・長文対応の3B Reasoningモデル、ONNX版も無償提供</title>
      <link>https://ledge.ai/articles/smollm3_128k_multilingual_reasoning_model</link>
      <description><![CDATA[<p>:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Fhuggingface.co%5Cu002Fblog%5Cu002Fsmollm3">HuggingFace</a>{target=“_blank”}
:::</p>
<p>Hugging Faceは2025年7月8日、新たな小型言語モデル「SmolLM 3」をHugging Face HubとGitHubで<a href="https:%5Cu002F%5Cu002Fhuggingface.co%5Cu002Fblog%5Cu002Fsmollm3">無償公開</a>{target=“_blank”}した。パラメータ数は30億（3B）で、128kトークンの長文入力に対応し、命令文内の「\u002Fthink」や「\u002Fno_think」フラグによって推論過程を切り替える機能を持つ。英語・フランス語・スペイン語・ドイツ語・イタリア語・ポルトガル語の6言語に最適化され、ツールコーリング機能も実装されている。公開直後にはONNX版や量子化済みチェックポイント、訓練レシピ、学習データセットなども順次提供されており、エッジ推論や再学習など幅広いユースケースに対応可能となっている。</p>
<h2>モデルの概要</h2>
<p>SmolLM 3は、Hugging Faceが開発した小型のデコーダ専用LLMで、以下の4つの特徴を備える。</p>
<ul>
<li>128kトークンの長文入力に対応（NoPEとYaRNを組み合わせた構成）</li>
<li>“\u002Fthink”推論切替機能により、タスクに応じて思考出力の有無を制御</li>
<li>6言語対応かつツールコーリングに標準対応（code／json）</li>
<li>Apache 2.0ライセンスによる完全オープンな学習レシピとデータの提供</li>
</ul>
<p><strong>図1：SmolLM 3 “Blueprint”──モデル構造、訓練レシピ、評価指標、利用方法をまとめた公式チャート</strong>
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002Fsmollm3_whiteprint_360c69e64f%5Cu002Fsmollm3_whiteprint_360c69e64f.jpg" alt="smollm3-whiteprint.jpg" />
:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Fhuggingface.co%5Cu002Fblog%5Cu002Fsmollm3">HuggingFace</a>{target=“_blank”}
:::</p>
<h2>長文処理と“\u002Fthink”モード</h2>
<p>SmolLM 3は、約50万文字（A4約250ページ）に相当する128kトークンの入力を一括処理できる。これは、文書検索や契約書分析、技術資料の要約といったRAG用途において、前処理なしで大量文書を直接投入できることを意味する。</p>
<p>また、「\u002Fthink」モードを指定すれば、モデルは推論プロセスをステップバイステップで展開して出力する。一方で「\u002Fno_think」を指定すれば最終的な結論のみを返す挙動となる。この機能により、ユーザーは計算リソースや応答速度のトレードオフをタスクごとに柔軟に調整できる。</p>
<p><strong>図2：\u002Fthink」有無による各タスク成績。思考展開を出力した場合（左列）に高難度タスクの精度が大幅に向上</strong>
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002Fimage2832_29_10b053fea3%5Cu002Fimage2832_29_10b053fea3.jpg" alt="image2832%29.jpg" />
:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Fhuggingface.co%5Cu002Fblog%5Cu002Fsmollm3">HuggingFace</a>{target=“_blank”}
:::</p>
<h2>ベンチマークと性能評価</h2>
<p>Hugging Faceが公開した12種のベンチマークによると、SmolLM 3は同規模のLlama-3 3BやQwen 2.5 3Bを一貫して上回り、Qwen 3 4Bにも一部タスクで迫る結果を示している。特に多言語タスク（Flores、Global MMLUなど）において顕著な優位性を示しており、6言語の平均スコアはすべての同規模モデルを上回った。</p>
<p><strong>図3　5言語平均ベンチマークの比較。SmolLM 3（黄）は同サイズモデルより高いスコアを示した</strong>
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002Fimage2830_29_89a5d27e19%5Cu002Fimage2830_29_89a5d27e19.jpg" alt="image2830%29.jpg" />
:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Fhuggingface.co%5Cu002Fblog%5Cu002Fsmollm3">HuggingFace</a>{target=“_blank”}
:::</p>
<h2>公開リソースと利用方法</h2>
<p>SmolLM 3は、Base版・Instruct版のほか、ONNX形式や量子化（q4f16）済みモデルも提供されており、Transformers.jsやONNX Runtimeを使ってブラウザやスマートフォン上でも即時に推論を行うことが可能だという。</p>
<p>また、GitHub上にはnanotronベースの訓練構成・学習スケジュール・ablation結果がまとめられており、再学習や検証用途に活用できる。プリトレーニングで使用された11.2兆トークン相当の学習データセットも段階的に公開が進められている。</p>
<h2>想定ユースケース</h2>
<p>SmolLM 3の設計は、コスト・制御性・ライセンスの観点から、以下のような用途に適するとされる。</p>
<ul>
<li><strong>検索拡張生成（RAG）</strong> ：128k長文対応で検索精度と応答一貫性を高める</li>
<li><strong>オンデバイスAI</strong> ：4GBメモリ対応の量子化モデルでスマートデバイスやエッジ環境でも運用可能</li>
<li><strong>社内ナレッジボット</strong>：推論切替と多言語対応でグローバル対応FAQを効率化</li>
<li><strong>検証・研究</strong> ：OSSライセンスとフルレシピ公開により再現性・改良が容易</li>
</ul>
<h2>今後の展開</h2>
<p>Hugging Faceは、SmolLM 3の学習中間チェックポイントや派生モデルの公開も予定しており、将来的にはVision対応モデル「SmolVLM 3」や1.7B／360M規模の軽量版も検討していることを明らかにしている。現時点では、3Bクラスのモデルで長文処理、推論モード切替、多言語対応、完全OSSの4要素を同時に満たすモデルは他に例がなく、オープンソースLLMの選定基準に新たな軸を提供する形となっている。</p>
<p>:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fclaude_3_7_sonnet_claude_code_launch">関連記事：Claude 3.7 SonnetとClaude Codeを発表　ハイブリッド推論モデルで最大128Kトークン対応</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Falibaba_qwen2-5vl_qwen2-5max">関連記事：DeepSeek-V3を超えるAIモデル「Qwen2.5-Max」とVLMをAlibabaが公開</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fmeta_llama-3-2_meta_connect2024">関連記事：Meta、最新LLM「Llama 3.2」を発表—マルチモーダルAIとオープンソース戦略を強化</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fmistral_ai_les_ministraux">関連記事：Mistral AI、小規模モデル群「Les Ministraux」を公開—3B／8Bでオンデバイス運用に最適化</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fbitnet_b1_58_2b4t_lightweight_high_performance_llm">関連記事：Microsoft研究チーム、超軽量で高性能な「BitNet b1.58 2B4T」を発表</a>
:::</p>
]]></description>
      <pubDate>Sun, 13 Jul 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Stability AI、AIポルノ生成を全面禁止へ──7月31日から利用規約改定、Stable Diffusion・API・OSSを含む全サービスで性的コンテンツを遮断</title>
      <link>https://ledge.ai/articles/stability_ai_policy_update_nsfw_ban</link>
      <description><![CDATA[<p>:::small
画像の出典：GPT-4oによりLedge.aiが生成
:::</p>
<p>ロンドンを拠点とする生成AI企業Stability AIは、2025年7月31日付で同社サービスの利用規約（Acceptable Use Policy, AUP）を<a href="https:%5Cu002F%5Cu002Fstability.ai%5Cu002Fuse-policy">改定</a>{target=“_blank”}し、Stable Diffusionをはじめとする自社製AIモデル・API・オープンソースコードにおいて、性行為に関連するコンテンツの生成・使用を一律禁止する。</p>
<p>営利・非営利の区別なく適用されるこの新方針は、AIコンテンツの安全性と倫理性を確保する目的で導入されるという。</p>
<h2>性的コンテンツの生成・共有を包括的に禁止</h2>
<p><a href="https:%5Cu002F%5Cu002Fstability.ai%5Cu002Fuse-policy">新たな利用規約</a>{target=“_blank”}では、「We Prohibit Sexually Explicit Content」の項が新設され、以下の内容が禁止事項として明記された。</p>
<ul>
<li>性行為、性的行為、性的暴力を含むあらゆるコンテンツの生成・共有</li>
<li>非合意の親密画像（NCII: Non-Consensual Intimate Imagery）</li>
<li>違法ポルノや児童搾取コンテンツ</li>
</ul>
<p>これらの規定は、DreamStudio、Stable Diffusion（あらゆるチェックポイントや自己ホスト版）、Stable Video、Stable Audio、Platform API、LoRA（Low-Rank Adaptation）共有機能、さらにGitHubなどで配布されるオープンソースコードを含むすべてのサービスに適用される。</p>
<p>規約違反が判明した場合、Stability AIは利用停止や契約解除などの措置を取ると定めている。また、18歳未満の利用も引き続き禁止される。</p>
<h2>従来規約との大きな違い</h2>
<p>この改定は、2024年3月1日版の旧AUPと比較して大幅な変更となる。
<a href="https:%5Cu002F%5Cu002Fstability.ai%5Cu002Fprior-aup">従来の規約</a>{target=“_blank”}では、禁止対象は「非合意ヌード」「違法ポルノ」「児童搾取コンテンツ」などに限定されており、合意の成人同士によるポルノ的表現については明確な禁止はなかった。</p>
<p>新AUPでは、「性行為そのもの」に関わるコンテンツすべてを対象とすることで、生成物の内容に関わらず包括的な制限を設けている。</p>
<h2>デベロッパーとユーザーへの影響</h2>
<p>新規約の対象範囲には、以下のような商用・非商用ツールや資源が含まれる。</p>
<ul>
<li>公式Webアプリ「DreamStudio」</li>
<li>Stable Diffusion（オープンモデル、自己ホスト含む）</li>
<li>音声・映像生成ツール（Stable Audio／Stable Video）</li>
<li>各種APIアクセス、LoRAモデル共有、オープンソースコードの再利用</li>
</ul>
<p>営利・非営利の区別はなく、個人利用や趣味での創作であっても規約違反となる。既存のモデルやワークフローで対象となるコンテンツを扱っている開発者や企業は、今後の運用方針の見直しが必要となる。</p>
<h2>背景：AIポルノをめぐる規制の強化</h2>
<p>今回の規約改定は、AI技術を悪用した性的コンテンツの氾濫に対処する国際的な動きの一環と見られる。特にディープフェイク技術による著名人の偽ポルノ動画や、非合意の画像生成が社会問題化する中で、生成AIモデル各社はNSFW（Not Safe For Work）フィルタの強化やアダルトコンテンツの禁止に取り組んでいる。</p>
<p>Stability AIはオープンウエイトの提供で知られる企業のひとつであり、同社による包括的な制限の導入は、オープンモデル領域における規制の方向性に大きな影響を与える可能性がある。</p>
<h2>今後のスケジュールと対応</h2>
<p>新規約は2025年7月31日より施行される。以降は新規・既存ユーザーともに順守が義務づけられ、違反が確認された場合にはアクセスの遮断やアカウントの停止措置が取られる見通しだ。</p>
<p>同社は今後、利用者向けのFAQやガイドラインの公開も予定しており、具体的な基準や判断基準についての詳細は順次明らかにされるとみられる。</p>
<p>:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fdenmark_ai_deepfake_copyright_law">関連記事：デンマーク政府、ディープフェイク規制に向け著作権法改正へ</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fdeep_fake_images">関連記事：AIでクラスメイトのディープフェイクポルノを生成　スペインで15人の生徒に保護観察処分</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fmicrosoft_protecting_the_public_from_abusive_ai_generated_content">関連記事：Microsoft、AI生成ディープフェイク詐欺の防止を目指す包括的法規制を提言</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Faustralia_ai_regulation">関連記事：オーストラリア政府がAI規制への取り組みを発表、ディープフェイク対策を強化</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fai_generated_images_snapchat_police_sting_operation">関連記事：ニューメキシコ州、Snapchatを提訴――AI生成の少女画像を使用したおとり捜査で性犯罪者を摘発</a>
:::</p>
]]></description>
      <pubDate>Fri, 11 Jul 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>耳で聞けない声を0.3秒で“見える化”──イェール大発スマートグラス「TranscribeGlass」一般販売開始</title>
      <link>https://ledge.ai/articles/transcribeglass_smartglasses_realtime_subtitles</link>
      <description><![CDATA[<p>:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Fwww.transcribeglass.com%5Cu002F">TranscribeGlass</a>{target=“_blank”}
:::</p>
<p>2025年7月、イェール大学の学生チームが開発したスマートグラス「<a href="https:%5Cu002F%5Cu002Fwww.transcribeglass.com%5Cu002F">TranscribeGlass</a>{target=“_blank”}」の一般販売が開始された。聴覚障がい者や難聴者を主な対象とし、周囲の発話をリアルタイムで字幕としてレンズ上に表示することができる。平均0.3秒という低遅延表示を実現し、日本語を含む10以上の言語への翻訳にも対応しているという。</p>
<h2>会話を文字で「見る」──TranscribeGlassの概要</h2>
<p>TranscribeGlassは、専用アプリをインストールしたスマートフォンのマイクで周囲の音声を取得し、それをクラウド経由で音声認識・処理した上で、メガネ型デバイスの右レンズに字幕として表示する構造となっている。表示はウェーブガイド方式を採用し、640×480ピクセルの解像度で文字を右視野30度以内に映し出す設計だという。</p>
<p>表示までの遅延は平均0.3秒に抑えられ、音声認識精度は95％以上を謳っている。最大8時間稼働可能なバッテリーを内蔵し、重量は36〜38グラムに収められている。スマートグラス本体にはマイクやカメラは搭載されておらず、軽量性とプライバシーへの配慮を両立している点も特徴とされる。</p>
<p><img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002Fsmart_glass3_64b7665d4d%5Cu002Fsmart_glass3_64b7665d4d.jpg" alt="smart glass3.jpg" />
:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Fwww.transcribeglass.com%5Cu002F">TranscribeGlass</a>{target=“_blank”}
:::</p>
<h2>販売価格と利用形態</h2>
<p>製品は<a href="https:%5Cu002F%5Cu002Fwww.transcribeglass.com">公式サイト</a>{target=“_blank”}で注文可能で、価格は本体が377ドル（約5万9,000円）、加えてクラウド音声認識機能を使用するための月額サブスクリプションが20ドル（約3,000円）となっている。2025年8月から出荷を予定しており、日本を含む国際配送にも対応するとのこと。</p>
<p>なお、アプリは現在iOS版が提供されており、Android版も年内にリリースされる見込み。また、オフラインモードも搭載されているが、この場合は音声認識精度がやや低下するとされる。</p>
<h2>想定利用シーンと対象ユーザー</h2>
<p>TranscribeGlassは、聴覚障がい者や加齢性難聴者の会話支援を主な用途とし、特に教室、会議、劇場、飲食店など騒音下での対話の可視化に有効とされる。また、語学学習者や国際会議の参加者など、リアルタイム翻訳による情報取得が必要なユーザーにも活用が期待されている。</p>
<p><strong>TranscribeGlassをプレゼントされ「あなたの言っていることが分かるわ！」と感激するユーザー</strong>
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002FAbout_smart_glass_a3840a78b4%5Cu002FAbout_smart_glass_a3840a78b4.jpg" alt="About smart glass.jpg" />
:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Fx.com%5Cu002FTranscribeGlass%5Cu002Fstatus%5Cu002F1892681585809359068">TranscribeGlassのX（旧Twitter）アカウントより</a>{target=“_blank”}
:::</p>
<h2>開発背景と開発チーム</h2>
<p>この製品は、イェール大学の学生である<a href="https:%5Cu002F%5Cu002Fyaledailynews.com%5Cu002Fblog%5Cu002F2025%5Cu002F02%5Cu002F18%5Cu002Fyale-student-founds-transcribeglass-a-live-text-to-speech-transcription-device%5Cu002F">マダヴ・ラヴァカレ氏</a>{target=“_blank”}が中心となって開発された。</p>
<p><img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002Fglasses_ag_Transcribe_Glass_scaled_bc37a50cad%5Cu002Fglasses_ag_Transcribe_Glass_scaled_bc37a50cad.jpeg" alt="glasses_ag_TranscribeGlass-scaled.jpeg" />
:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Fyaledailynews.com%5Cu002Fblog%5Cu002F2025%5Cu002F02%5Cu002F18%5Cu002Fyale-student-founds-transcribeglass-a-live-text-to-speech-transcription-device%5Cu002F">マダブ・ラヴァカレ氏提供</a>{target=“_blank”}
:::
きっかけは、聴覚障がいを持つ友人が講義中の内容を十分に理解できない状況を目の当たりにしたことだったという。2018年から7年をかけて7代にわたるプロトタイプを開発し、2024年にはCTOとしてニルバイ・ナラン氏が参画。Y Combinatorなどからの資金調達を経て、今回の一般販売に至った。</p>
<p>これまでベータ版は500人以上に試用され、フィードバックをもとに改良が重ねられてきたという。</p>
<h2>競合との差別化と今後の展開</h2>
<p>他のスマートグラス製品と異なり、TranscribeGlassは通話・音楽再生・撮影などの多機能化を避け、字幕精度と軽量性に特化している点が特徴だ。価格設定もMeta×Ray-BanやXRAI Glassなどに比べて抑えられており、バッテリー持続時間の長さも差別化ポイントとなっている。</p>
<p>将来的には、リアルタイムでの感情解析を字幕に反映する機能や、ASL（アメリカ手話）向けに語順を変換する表示機能の搭載が計画されているという。</p>
<p>TranscribeGlassは、視覚的な情報支援により、耳で聞けない会話を「読む」体験へと変えることで、新たなコミュニケーションの可能性を提供しようとしている。</p>
<p>:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fgoogle_android_xr_gemini_smartglasses_reveal">関連記事：Google、Android XRとGeminiを統合したスマートグラス試作機を公開</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fbrilliant_labs_frame">関連記事：元Apple社員ら設立のBrilliant Labsが新スマートグラス「Frame」を発表</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fmeta_rayban_collection">関連記事：「ヘイ Meta！」音声コマンドで通話・撮影　Ray-Ban×Metaの新スマートグラス</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fgenesis_ai_starkeyjp">関連記事：AI補聴器「ジェネシス」発表──脳の聴覚機能を代替し毎時8000万回の音声解析</a>
:::</p>
]]></description>
      <pubDate>Sat, 12 Jul 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/7/9 [WED]【ソースコード特典付き】自社専用LLMを低コストで実現！「Qwen3」の継続事前学習のデモンストレーション｜Ledge.ai Webinar</title>
      <link>https://ledge.ai/articles/webinar-vol65</link>
      <description><![CDATA[<h2>ウェビナー概要</h2>
<p>Ledge.ai Webinar vol.65では、「ローカルLLMの大本命『Qwen3』の継続事前学習デモンストレーション」をテーマに、GPUクラウドサービス「GPUSOROBAN」を提供する株式会社ハイレゾ様をゲストにお迎えし、実演デモを交えながらご解説いただきます。</p>
<p>Alibaba社が開発したオープンソースの大規模言語モデル（LLM）「Qwen」シリーズの最新版「Qwen3」は、DeepSeek-R1やOpenAI o1をも凌ぐ性能を持つとされ、世界中の開発者から大きな注目を集めています。特に、プロンプトに応じて思考プロセスを切り替える「ハイブリッド推論」や、外部ツールを呼び出す「エージェント機能」といった先進的な機能を備えている点も特長です。オープンソースでありながら商用利用も可能なため、自社の環境でセキュアに活用できる高性能なローカルLLMとして、ビジネス応用の期待が非常に高まっています。
今回のウェビナーでは、この「Qwen3」をベースに、特定の専門知識を追加で学習させる「継続事前学習」に焦点を当てます。ゼロからモデルを開発する「フルスクラッチ」に比べ、計算リソースやコストを大幅に抑えながら、自社に特化した高性能モデルを構築できるこの手法について、デモンストレーションを通じて具体的に解説します。</p>
<p><strong>ウェビナーの内容</strong></p>
<ul>
<li><strong>高性能オープンソースLLM「Qwen3」の詳解</strong>
<ul>
<li>アーキテクチャ（MoE）、ハイブリッド推論、エージェント機能（Function Calling）など、Qwen3の先進的な特徴とビジネスにおける可能性</li>
</ul>
</li>
<li><strong>GPUクラウド「GPUSOROBAN」を活用した継続事前学習デモンストレーション</strong>
<ul>
<li>環境構築からデータセットの前処理、学習実行、推論までの一連のプロセスを実演</li>
</ul>
</li>
<li><strong>大規模モデル学習に不可欠な分散処理技術の解説</strong>
<ul>
<li>データ並列、モデル並列（パイプライン並列・テンソル並列）の基礎から、DeepSpeedやMegatron-LMといったフレームワークの活用法まで</li>
</ul>
</li>
</ul>
<p><strong>このような方におすすめ</strong></p>
<ul>
<li>自社専用の高性能LLMを、コストを抑えて構築したい方</li>
<li>機密情報を扱うため、オンプレミスやセキュアなローカル環境でLLMを運用したい方</li>
<li>LLMに専門知識を追加する「継続事前学習」の具体的な手法を知りたいエンジニア</li>
<li>生成AIの学習・開発におけるGPUリソースの確保やコストに課題を感じている</li>
</ul>
<h2>視聴者特典</h2>
<p><strong>【特典①】デモで使用したサンプルコードをプレゼント！</strong>
本ウェビナーにお申し込みいただいた方には、デモで使用した「Qwen3の継続事前学習」のソースコードをプレゼントいたします。視聴後すぐに、ご自身の環境で再現・検証が可能です。</p>
<p><strong>【特典②】H200 GPU 30日間無料トライアルキャンペーン！</strong>
さらに、ハイレゾでは現在、「NVIDIA H200」を搭載したGPUクラウドサービス「AIスパコンクラウド」を30日間無料でお試しいただける特別なキャンペーンも実施中です。
（詳細はウェビナー内およびアンケート回答後のご案内をご確認ください。）</p>
<p><img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002Fh200_gpu_free_trial1200_bd3d66cf05%5Cu002Fh200_gpu_free_trial1200_bd3d66cf05.jpg" alt="h200-gpu-free-trial1200.jpg" /></p>
<h2>登壇者情報</h2>
<p>株式会社ハイレゾ
GPU事業本部　マーケティング部　グループ長
山田 岳史</p>
<p><img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002Fhighreso_yamadasama_2a984e3aa6%5Cu002Fhighreso_yamadasama_2a984e3aa6.jpg" alt="highreso-yamadasama.jpg" /></p>
<p>IoTの領域で事業開発の経験を経てハイレゾに入社。
GPUクラウドサービスの事業開発からマーケティング、技術サポートまで担当。</p>
<h2>お申し込みはこちら</h2>
<p>配信期間：2025年7月9日(水)〜2025年7月29日(火)
配信方式：オンデマンド（Zoom）
参加費：無料</p>
<p>:::button
<a href="https:%5Cu002F%5Cu002Fzfrmz.com%5Cu002FiXQrpCVKQZwYTU8kO3uy">ウェビナーの視聴はこちら</a>{target=“_blank”}
:::</p>
]]></description>
      <pubDate>Sun, 13 Jul 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>OpenAIとの買収交渉が決裂、WindsurfはGoogleと契約締結──CEOらはDeepMindに移籍、契約額は約24億ドルとの報道も</title>
      <link>https://ledge.ai/articles/windsurf_google_deal_openai_exit</link>
      <description><![CDATA[<p>:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Funsplash.com%5Cu002Fja%5Cu002F%E5%86%99%E7%9C%9F%5Cu002Fgoogle-%E3%81%AE%E3%83%AD%E3%82%B4%E3%81%8C%E5%BB%BA%E7%89%A9%E3%81%AE%E5%81%B4%E9%9D%A2%E3%81%AB%E8%A1%A8%E7%A4%BA%E3%81%95%E3%82%8C%E3%82%8B-r-oebX7qWxM">Adarsh Chauhan@Unsplash</a>{target=“_blank”}
:::</p>
<p>2025年7月11日、AIコードエディター「Windsurf Editor」を手がけるAIスタートアップWindsurfは、Googleとライセンス契約を締結したと公式ブログで<a href="https:%5Cu002F%5Cu002Fwindsurf.com%5Cu002Fblog%5Cu002Fwindsurfs-next-stage">発表</a>{target=“_blank”}した。同時に、ヴァルン・モハンCEO、共同創設者のドウグラス・チェン氏を含む一部の研究開発チームが、GoogleのAI研究機関DeepMindに移籍することも明らかにされた。複数の米メディアは、この契約総額が約24億ドル（約3,500億円）にのぼると報じている。</p>
<p><img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002Fwindsurf_blog_next_stage_c7a135ce6c%5Cu002Fwindsurf_blog_next_stage_c7a135ce6c.jpg" alt="windsurf blog next stage.jpg" />
:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Fwindsurf.com%5Cu002Fblog%5Cu002Fwindsurfs-next-stage">Windsurf のblogより</a>{target=“_blank”}
:::</p>
<h2>OpenAIとの買収交渉は期限切れで失効</h2>
<p>事情に詳しい関係者によれば、OpenAIは2024年末からWindsurfの買収を協議していた。取引規模は約30億ドルと<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fopenai_acquires_windsurf_for_ai_dev_tools_expansion">報じられていた</a>{target=“_blank”}が、交渉は複雑化し、2025年7月10日の契約期限までに最終合意に至らなかった。背景には、OpenAIと主要出資元であるMicrosoftの関係や、GitHub Copilotとの競合調整が影響したとみられる。</p>
<p>なお、Windsurfは独立性を重視するスタンスを維持していたことも、交渉の難航に一因があったとみられている。</p>
<h2>Googleとの24億ドル契約、製品ライセンス供与が主眼</h2>
<p>買収が成立しなかった一方で、WindsurfはGoogleと新たな契約を結んだ。同社公式ブログによると、この契約はWindsurfが保有するAIコード生成技術の一部をGoogleに「非独占的ライセンス」として供与する内容であり、株式取得や企業統合は含まれていない。</p>
<p>Windsurfは今後も企業向けのAIコード編集ツールを独立して提供し続けると明言しており、既存顧客に対してはサポート体制を維持するとしている。</p>
<h2>DeepMindが主要人材を吸収、Geminiの開発加速へ</h2>
<p>契約の一環として、Windsurfの共同創業者ら主要メンバーがGoogle DeepMindに加わる。対象となるのはCEOのヴァルン・モハン氏、共同創設者ドウグラス・チェン氏のほか、研究開発チームの中核メンバーとされる。</p>
<p>Google DeepMindのCEOであるデミス・ハサビス氏は、自身のX（旧Twitter）上で「彼らの参加に非常に興奮している」とコメントし、Geminiの“エージェンティック・コーディング”機能開発に注力すると明かしている。</p>
<p><img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002Fdemis_hassabis_x_about_windsurf_caaaa3b8fb%5Cu002Fdemis_hassabis_x_about_windsurf_caaaa3b8fb.jpg" alt="demis hassabis x about windsurf.jpg" />
:::small
画像の出典：<a href="https:%5Cu002F%5Cu002Fx.com%5Cu002Fdemishassabis%5Cu002Fstatus%5Cu002F1943794383536652309">Demis Hassabis氏のX（旧Twitter）アカウントより</a>{target=“_blank”}
:::</p>
<h2>Windsurf社内体制は新たな経営陣へ移行</h2>
<p>人材移籍にともない、Windsurfでは経営体制の再編が行われた。新たにCOOのJeff Wang氏が暫定CEOに就任し、Graham Moreno氏が社長職を務める。今後は企業向け機能の強化や、新規パートナーシップの拡大に注力する方針を掲げている。</p>
<h2>業界背景：コード支援AIを巡る競争が激化</h2>
<p>AIを活用したコード補完・生成の分野では、GoogleのGemini、OpenAIのGPT-4o Turbo、MicrosoftのGitHub Copilot、MetaのCode Llamaなどが競合している。いずれも、ソフトウェア開発の生産性向上を狙いとした戦略的領域であり、優れたAI技術と人材の獲得が競争力の源泉となっている。</p>
<p>今回のWindsurfとのライセンス契約と人材移籍は、Googleがこの領域での優位性をさらに高める布石と捉えられている。</p>
<p>:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fopenai_acquires_windsurf_for_ai_dev_tools_expansion">関連記事：OpenAIがWindsurfを約4300億円で買収合意か──ChatGPT開発者の拡大戦略</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Falphabet_earnings_q3_2024">関連記事：Googleの新規コードの25%以上がAI生成—ピチャイCEOが明かすAIによる開発革新</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Falphaevolve_ai_algorithm_discovery">関連記事：Google DeepMind、新AIコーディングエージェント『AlphaEvolve』を発表　Geminiと連携しコード設計を自動化</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fgemini_2_5_io_edition_update">関連記事：Google、Gemini 2.5 Proの先行アップデート版「I\u002FO Edition」を公開—コード生成性能を大幅強化</a>
:::
:::box
<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fkakaku_cursor_ai_engineering">関連記事：全エンジニア約500人にAIコードエディタ「Cursor」を導入—開発組織の生産性向上を狙う</a>
:::</p>
]]></description>
      <pubDate>Mon, 14 Jul 2025 23:50:00 GMT</pubDate>
    </item>
  </channel>
</rss>