<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
  <channel>
    <title>Ledge.ai 新着記事</title>
    <link>https://ledge.ai/</link>
    <description>Ledge.ai の最新テクノロジー記事</description>
    <item>
      <title>OpenAI、「ChatGPT for Teachers」を無償提供──米国K-12教員向けに“教育専用ワークスペース”を開放</title>
      <link>https://ledge.ai/articles/chatgpt_for_teachers_us_k12_free_workspace</link>
      <description><![CDATA[<p>OpenAIは2025年11月19日、米国のK-12教員・学校スタッフ向けにカスタマイズされた教育用AI「ChatGPT for Teachers」の提供を<a href="https://openai.com/index/chatgpt-for-teachers/">発表</a>した。対象となる教員・関係者は、2027年6月まで無料で利用できる。専用ワークスペースを備え、教材作成から学区単位の協働管理まで、学校現場での利用に特化した機能が特徴だという。</p>
<p>サービスは、認定された米国のK-12学校に所属する教員・スタッフ・学校リーダー、学区管理者を対象としたもの。利用にはSheerIDによる在職確認が必要で、資格が確認された教育者は専用の「Teachers ワークスペース」を作成できる。学生アカウントについては、今後の提供を見据えて準備中とのこと。</p>
<h2>教材作成・レッスン設計を支援するフル機能のChatGPT</h2>
<p>「ChatGPT for Teachers」では、OpenAIのフラッグシップモデルに加え、ファイルアップロード、Google DriveやMicrosoft 365との連携、画像生成、音声モード、データ分析、ブラウジングなど、一般版ChatGPTが提供する主要機能をそのまま利用できる。</p>
<p><a href="https://openai.com/index/chatgpt-for-teachers/">公式ページ</a>では、単元計画、学習目標に応じたレッスン案、ワークシート生成、生徒向け課題の分岐作成、保護者向け文書の翻訳など、教師の業務を支援する具体的な活用例が示されている。教師が自身のスタイルや生徒情報を入力すれば、回答を継続的にパーソナライズする「メモリ」機能も利用可能だ。</p>
<h2>学校・学区単位の導入を想定したワークスペースと管理機能</h2>
<p>教育機関向けに設計された「Teachers ワークスペース」は、教材・指示文・共同プロジェクトなどの共有を前提としたコラボレーション機能を備える。教員同士でテンプレートを共有したり、共同でカスタムGPTを作成したりすることが可能だ。</p>
<p>管理者向けには、ドメインクレーム（学校ドメインの登録）、ロールベースのアクセス制御（RBAC）、メンバー管理、SAML SSOなどの機能が提供される。学区全体での展開を想定した構成で、大規模な導入を希望する学校・学区向けには追加相談窓口が用意されている。</p>
<h2>学習データへの不使用と暗号化を含むデータ保護</h2>
<p>OpenAIは、教育者向けアカウントで共有された内容はデフォルトでAIモデルの訓練に使用しないと説明している。データは保存時・転送時の双方で暗号化され、MFA・SSOなどのアカウント保護機能も利用できる。</p>
<p>また、米国の教育記録保護法（FERPA）に準拠する形で、学校・学区が必要とするコンプライアンス要件に対応しているとされる。会話データは、不正利用の調査など限定的な目的で権限を持つ担当者がアクセスする場合があるが、範囲は管理されたものだとしている。</p>
<h2>利用開始の方法</h2>
<p>利用を希望する教員は、公式ページから資格確認を行うことで、自身のワークスペースを開設できる。学区管理者は、組織全体のワークスペースを立ち上げ、教員を一括管理することも可能だ。</p>
<p>OpenAIは、無料提供期間終了後（2027年6月以降）について、価格や条件に変更が生じる場合は事前通知するとしており、「教育者にとって手頃な価格を維持することを目指す」と説明している。</p>
]]></description>
      <pubDate>Wed, 26 Nov 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google、Geminiに「AI生成画像の検証」機能を導入──目に見えない電子透かし技術「SynthID」を活用</title>
      <link>https://ledge.ai/articles/google_gemini_synthid_image_verification</link>
      <description><![CDATA[<p>Googleは2025年11月20日、Geminiアプリにおいて、画像をアップロードして「この画像はAIで生成または編集されたものか」を判定できる新機能を<a href="https://blog.google/technology/ai/ai-image-verification-gemini-app/">公開</a>した。同社は2023年に、画像の見た目に影響を与えず識別情報を埋め込む電子透かし（Invisible watermark）技術「SynthID」を開発しており、今回のアップデートにより、この電子透かしをGeminiアプリ上で直接検出できるようになる。</p>
<h2>AI生成画像の真偽をアプリ上で判定</h2>
<p>Geminiアプリでは、ユーザーが画像をアップロードし、「Was this created or edited by Google AI?」といった質問を投げかけることで、画像にSynthIDの電子透かしが含まれているかを判定できる。
電子透かしが検出された場合は、GoogleのAIによって生成または編集された可能性が高いと返答される。一方で電子透かしが見つからない場合は、「GoogleのAIによるものではない可能性が高い」と示される。ただし、他社のAIモデルによる生成画像まで識別できるわけではなく、あくまでGoogleのAIに適用された電子透かしのみが検知対象となる。</p>
<h2>利用条件と注意点</h2>
<p>Googleサポートページによれば、検証は1枚ずつ（100MB以下）の画像に対して実施され、コラージュ画像など複数の画像を含む形式は推奨されない。また、電子透かしが画像全体ではなく一部に適用されている場合、AIで編集された領域が検出されることもある。</p>
<h2>「SynthID」とは</h2>
<p>SynthIDは、画像の外観に影響を与えず、機械的に読み取ることができる目に見えない電子透かし（Invisible watermark）技術を採用している。Geminiアプリは、この電子透かしをもとに生成・編集プロセスを判定する仕組みを持つ。
公式ブログでは、Googleが今後、画像だけでなく動画や音声への対応拡大も計画していることが示されている。</p>
<h2>Googleの透明性強化の取り組み</h2>
<p>GoogleはSearchにおけるAI生成画像のラベル表示を拡充しているほか、C2PA（Content Provenance and Authenticity）に準拠したメタデータ対応も進めている。公式ブログによれば、Geminiアプリ・Vertex AI・Google Adsで生成される画像については、今週よりC2PA準拠のメタデータを自動付与する対応を開始するという。対象となるのはNano Banana Pro（Gemini 3 Pro Image）で生成された画像で、生成プロセスに関する追加情報を示すことで透明性を高めるとしている。</p>
]]></description>
      <pubDate>Tue, 25 Nov 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>KADOKAWA×はてな「カクヨム」、生成AI利用作品に3種のタグ付けを推奨──“本文50％以上／50％未満／補助利用”を明確化</title>
      <link>https://ledge.ai/articles/kakuyomu_ai_tag_guideline_update</link>
      <description><![CDATA[<p>小説投稿サイト「カクヨム」を共同運営するKADOKAWAとはてなは2025年11月19日、生成AIを利用した作品投稿について、利用状況に応じたタグ付けを推奨すると<a href="https://kakuyomu.jp/info/entry/geneai_tag">発表</a>した。対象となるタグは「AI本文利用」「AI本文一部利用」「AI補助利用」の3種類で、本文中のAI生成割合やAIの関与範囲に応じて区分を設ける。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/kakuyomu_genai_tag_0e3bbfab95/kakuyomu_genai_tag_0e3bbfab95.jpg" alt="kakuyomu genai tag.jpg" /></p>
<p>発表によると、「AI本文利用」は本文の大半（目安50％以上）がAIによって生成された場合、または軽微な修正のみを加えて利用した場合に該当する。「AI本文一部利用」は本文の一部（目安50％未満）がAI生成の場合で、こちらも軽微な修正にとどまるケースを想定する。「AI補助利用」は、AIで得たアイデアや資料をもとに作者が本文を書く場合や、作者が書いた文章の校正など、創作の補助的にAIを使用した場合を指す。3つの基準はあくまで目安とし、投稿者は自身の利用実態に合わせて選択する。</p>
<p>11月21日には、「AI補助利用」の適用範囲に関する問い合わせが多数寄せられたとして、公式サイトに追記を行った。追記では、作品執筆の一環として作者が意識的に生成AIツールを使用した場合を想定していると説明。一方で、検索サービスなど日常的に利用するツールのAI要約機能や、文書ソフトの自動校正機能を用いた作品は該当しないとしている。また、投稿者に対して生成AI利用の具体的内容を示す必要はないとも明記した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/kakuyomu_ai_tag2_e153549b82/kakuyomu_ai_tag2_e153549b82.jpg" alt="kakuyomu ai tag2.jpg" /></p>
<p>なお、KADOKAWAが主催する日本最大級の小説コンテスト「カクヨムコンテスト11」に応募する作品については、生成AIを利用している場合、該当タグの付与が必須とのこと。</p>
]]></description>
      <pubDate>Tue, 25 Nov 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>xAI、サウジアラビアと500MW超の次世代AIデータセンターを共同開発──Grokを国家規模で展開へ</title>
      <link>https://ledge.ai/articles/xai_humain_saudi_500mw_grok_national_ai_layer</link>
      <description><![CDATA[<p>サウジアラビア政府系AI企業のHUMAINは2025年11月19日、米ワシントンD.C.で開催された U.S.–Saudi Investment Forum 2025 において、米xAIとの大型フレームワーク契約を<a href="https://www.humain.com/en/news/humain-and-xai-partner-to-build-next-generation-ai-compute-power-and-deploy-grok-in-the-kingdom-to-support-the-most-ai-enabled-nation-objectives">発表</a>した。両社はサウジアラビア国内に、500MW超のフラッグシップ施設を核とした複数拠点のGPUデータセンター網を共同で開発し、xAIの大規模言語モデル「Grok」を国家規模で展開する。</p>
<h2>次世代AIデータセンター網を共同構築</h2>
<p>HUMAINとxAIは、複数のGPUデータセンターで構成される次世代のハイパースケールAIインフラを共同設計・建設・運用する。フラッグシップとなる500MW超の施設は、「世界でも最も高度なAIコンピュートハブの一つ」と位置づけられている。</p>
<p>今回のプロジェクトは、xAIにとって米国外で初めての大規模コンピュート展開となり、同社が既に保有する米国内スーパーコンピュータ群を補完する計算基盤として機能するという。</p>
<p>HUMAINは、AIインフラの設計から建設・運用までを低コストで提供する能力を強みとしており、xAIはフロンティアAIモデルおよび大規模演算最適化の技術を持つ。両社はこれらを組み合わせることで、世界規模でスケール可能な次世代AI開発基盤を整備するとしている。</p>
<h2>Grokを全国展開し“国家AIレイヤー”を実装</h2>
<p>提携には、xAIの大規模言語モデル「Grok」をサウジアラビア全土で展開する計画も含まれ、HUMAINのエージェント基盤「HUMAIN ONE」と統合される。</p>
<p>これにより、以下の機能が国家レベルで提供されるという。</p>
<ul>
<li>リアルタイムインテリジェンス</li>
<li>自律ワークフロー</li>
<li>AIコパイロット</li>
<li>行政・企業・社会をまたぐ意思決定支援</li>
</ul>
<p>HUMAINはこの仕組みを「統合されたナショナルAIレイヤー」と表現し、サウジアラビアの「Most AI-Enabled Nation」（世界で最もAI活用が進む国家）の実現を支える基盤になると説明している。</p>
<p>xAI創業者のElon Musk氏は、「Grokを一国全体に展開する初の取り組みであり、大規模で効率的なコンピュートと高度なAIモデルが未来の知能を形作る」と述べ、提携の意義を強調した。</p>
<h2>サウジアラビアのAI戦略</h2>
<p>発表が行われた U.S.–Saudi Investment Forum では、サウジアラビア政府系投資機関のINVEST SAUDIが「同国が“Oil factories（油の工場）”から“AI factories（AIの工場）”へ変革を進めている」と強調。国家規模のAIインフラを戦略的に整備していく姿勢が示された。</p>
<h2>フォーラムで示されたHUMAINのAIインフラ戦略</h2>
<p>同フォーラムでHUMAINは、NVIDIAとの戦略的パートナーシップを拡大し、今後3年間で最大60万GPUをサウジアラビアと米国の新データセンターに展開する計画も<a href="https://www.humain.com/en/news/humain-expands-strategic-partnership-nvidia">発表</a>した。</p>
<p>さらに、NVIDIA Nemotronを用いたアラビア語モデルの強化、Omniverseを活用した国家規模のデジタルツイン構築など、AWSやGlobal AI、xAIとともにグローバルなAIインフラ基盤づくりを進める枠組みも示されている。</p>
<p>これらの発表は、HUMAINが推進するグローバルAIインフラ戦略の一環として位置づけられている。</p>
<p>500MW超のデータセンター網の詳細仕様や着工時期はまだ公表されていないが、Grokの全国展開とあわせて、サウジアラビアのAI主導型国家戦略を支えるプロジェクトとして、今後はデータセンター網の具体像や運用計画に注目が集まりそうだ。</p>
]]></description>
      <pubDate>Tue, 25 Nov 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Meta、「SAM 3」を発表──テキストや画像例をプロンプトに、画像・動画内の物体を一括検出・分割・追跡　併せて「SAM 3D」で人物・物体の3D生成にも対応</title>
      <link>https://ledge.ai/articles/meta_sam3_sam3d_release</link>
      <description><![CDATA[<p>Metaは2025年11月19日（現地時間）、画像および動画に含まれるオブジェクトを、プロンプトを基点に一括で検出・分割・追跡できる新モデル「Segment Anything Model 3（SAM 3）」を<a href="https://ai.meta.com/blog/segment-anything-model-3/">発表</a>した。同日、2D画像から人物や物体の3Dモデルを生成する「SAM 3D」も<a href="https://ai.meta.com/blog/sam-3d/">公開</a>している。</p>
<p>Metaの公式Xアカウント（@AIatMeta）は、両モデルを「新しい世代のSegment Anything Models」と<a href="https://x.com/AIatMeta/status/1991178519557046380">紹介</a>し、開発者・研究者向けのメディアワークフローを大きく拡張するとしている。</p>
<h2>SAM3：プロンプトから“概念”ベースで検出・分割・追跡</h2>
<p>SAM 3は、短いテキストフレーズ（例：「yellow school bus」）や、画像内のサンプルオブジェクトを示す “example prompt” を入力すると、該当するすべてのオブジェクトを一括して検出・分割し、動画では継続的に追跡できるモデル。Metaはこれを「Promptable Concept Segmentation（PCS）」と定義している。</p>
<p>@<a href="https://www.youtube.com/watch?v=G4OLPDjwncw">Youtube</a></p>
<p>アーキテクチャは、DETRベースの画像検出器と、SAM 2を基盤とした動画トラッカーを単一バックボーンに統合したもの。論文(https://ai.meta.com/research/publications/sam-3-segment-anything-with-concepts/)では、認識と位置特定を切り離す“Presence Head”を新たに採用し、概念ベースの検出精度を向上させたと説明されている。</p>
<p><strong>SAM 3は、テキストプロンプトを用いた検出と追跡を単一アーキテクチャで統合する</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/SAM_3_architecture_overview_0af9a61c98/SAM_3_architecture_overview_0af9a61c98.jpg" alt="SAM 3 architecture overview.jpg" /></p>
<h2>400万概念を含む新データセット「SA-Co」</h2>
<p>SAM 3の開発にあたり、Metaは大規模データセット「<a href="https://github.com/facebookresearch/sam3/blob/main/README.md#sa-co-dataset">SA-Co（Segment Anything with Concepts）</a>」を新たに構築した。SA-Coは、数百万枚規模の高品質画像と約400万のユニーク概念、数千万のマスク情報を含むもので、モデルが幅広い概念を学習できるよう設計されている。</p>
<p>論文で示されたベンチマーク「SA-Co/Gold」では20万以上の概念を評価対象とし、SAM 3はLVISのゼロショットMask APで48.8を記録。従来モデル（38.5）を大きく上回った。</p>
<p><strong>SA-Coは、画像中の多様な物体を高精度にラベル付けする大規模データセット。色ごとに異なる概念カテゴリが示されている</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/SA_Co_907d839e43/SA_Co_907d839e43.jpg" alt="SA-Co.jpg" /></p>
<p>SAM 3は、Metaの研究用ウェアラブルデバイス「<a href="https://ledge.ai/articles/meta_aria_gen2_ai_research">Aria Gen 2</a>」で撮影された一人称視点の映像に対しても高い性能を示す。動きの速さや視点の揺れが大きいファーストパーソン映像でも、対象物の分割と追跡を安定して行える点が特徴だ。</p>
<p>Metaは、Aria Gen 2 Pilot Datasetの一部を<a href="https://www.aidemos.meta.com/segment-anything">Segment Anything Playground</a>上で公開しており、これにより、人間の視点から世界を理解する“コンテクスチュアルAI”や、ロボティクス、機械知覚といった応用領域におけるSAM 3の有用性を示している。</p>
<h2>SAM 3D：1枚の画像から人物・物体の3Dモデルを生成</h2>
<p>同時に公開された「SAM 3D」は、人物に特化した「SAM 3D Body」と一般物体向けの「SAM 3D Objects」から構成される。Metaは、単一の2D画像から高精度で3D形状を復元でき、テクスチャとメッシュの情報を従来手法より忠実に再現できる点を強調している。</p>
<p>@<a href="https://www.youtube.com/watch?v=B7PZuM55ayc">Youtube</a></p>
<h2>2D解析から3D復元までを一貫化</h2>
<p>Metaは、SAM 3とSAM 3Dをセットで発表することで、画像・動画内のオブジェクト理解（SAM 3）から3D形状復元（SAM 3D）までを一貫して扱える視覚AI基盤を提示した。動画編集、AR/VR、ロボティクス、ECなど、多数の応用領域で利用可能性があるとしている。</p>
<p>公式Xでは、今回の発表を「新しい世代のSegment Anything Models」と説明し、SAM 3 と SAM 3D が画像・動画・3Dを横断する基盤技術として進化した点を強調している。投稿では、短いテキスト指示や具体例となる画像を用いた物体の検出・分割・追跡（SAM 3）、そして単一画像から人物や物体の3Dモデルを生成する機能（SAM 3D）が紹介され、「開発者と研究者が新しいメディア処理ワークフローを構築するためのツール」と位置づけている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/segment_anything_models_921509ea34/segment_anything_models_921509ea34.jpg" alt="segment anything models.jpg" /></p>
<p>:::box
[関連記事：AIの\</p>
]]></description>
      <pubDate>Mon, 24 Nov 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>英国、動物実験の段階的廃止ロードマップを発表──AIと臓器チップで2030年までに犬・霊長類の薬物試験削減へ</title>
      <link>https://ledge.ai/articles/uk_animal_testing_phaseout_roadmap_2030</link>
      <description><![CDATA[<p>英国政府は2025年11月11日、動物実験を代替法へ置き換えるための包括的戦略「Replacing Animals in Science Strategy」を<a href="https://www.gov.uk/government/news/animal-testing-to-be-phased-out-faster-as-uk-unveils-roadmap-for-alternative-methods">公表</a>した。臓器チップや3D細胞モデル、遺伝子ベースアッセイなど、多様な非動物技術（Non-Animal Technologies：NATs）の活用を体系的に進める内容で、この中にはAIによる毒性予測や計算モデルも含まれる。</p>
<p>戦略の実行を支えるため、政府と研究機関は総額約7,500万ポンドを投じる。AIを含む計算科学的手法は、初期スクリーニングの効率化に加え、薬物動態の検討段階でも補完的な役割を担うとされ、代替法を広げるうえで「重要な柱」と位置づけられている。</p>
<h2>2026〜2030年で段階的に移行</h2>
<p>今回公表されたロードマップには、具体的な削減目標が盛り込まれた。</p>
<ul>
<li>2026年末まで：皮膚刺激、眼刺激、皮膚感作などの安全性試験を非動物法へ全面移行</li>
<li>2027年まで：ボツリヌス毒素（ボトックス）強度試験におけるマウス試験を終了し、DNAベース評価法へ切り替え</li>
<li>2030年まで：犬・非ヒト霊長類を用いた薬物動態（PK）試験の依存度を削減</li>
</ul>
<p>ただし政府は、代替法が「同等の科学的妥当性と安全性を提供できる場合」に限り動物使用を停止するとしており、実施可能な領域から段階的に置き換えていく現実的なアプローチを取る。</p>
<h2>代替技術の実用化が追い風に</h2>
<p>今回の戦略策定の背景には、代替手法が実用段階へ近づいていることがある。</p>
<ul>
<li><strong>臓器チップ</strong> ：ヒト細胞で臓器環境を再現し、薬物応答を高精度に評価</li>
<li><strong>AIモデル</strong> ：分子構造や実験データから毒性を予測し、動物試験前のスクリーニングを効率化</li>
<li><strong>3Dバイオプリント組織</strong> ：肝臓や皮膚などの組織を立体的に再現し、化学物質の影響評価に利用</li>
</ul>
<p>これらの技術には英国企業・研究機関が積極的に取り組んでおり、産業競争力の強化にもつながると期待されている。</p>
<h2>英国の動物実験の現状</h2>
<p>内務省が公表した2023年の統計では、動物を用いた科学的手技は約268万件で、マウスやラット、魚類が大半を占める。猫・犬・霊長類の使用は全体の0.2％程度と少ないものの、医薬品開発における安全性評価など、依然として役割の大きい領域が残る。</p>
<p>特に犬・非ヒト霊長類のPK試験は、医薬品の臨床前評価で重要とされてきた領域であり、2030年を目標とした依存度の削減は、医薬品開発プロセスにおける大きな転換点といえる。</p>
<h2>完全な代替にはなお課題</h2>
<p>一方で、動物実験の完全廃止には技術的・規制的な課題が残る。</p>
<p>臓器チップやAIモデルでは、免疫系を含む全身の長期的な複合反応を完全に再現することは難しい。また、規制当局が代替法を受け入れるには、精度や再現性、妥当性の検証が不可欠で、導入には時間を要する。動物研究支援団体 Understanding Animal Research も「代替技術は進展しているが、多くの領域ではまだ動物を使わない方法が存在しない」と指摘している。</p>
<h2>資金投下と実行体制</h2>
<p>今回の約7,500万ポンドの投資は、研究開発だけでなく、規制受け入れや教育、データ標準化などを含む包括的な体制整備に充てられる。</p>
<ul>
<li>政府の6,000万ポンド：代替法の検証拠点や規制支援センターの整備</li>
<li>1590万ポンド（MRC・Wellcome Trustなど）：ヒト由来 in vitro モデルの研究支援</li>
</ul>
<p>政府は関係府省・研究機関・製薬企業・動物福祉団体で構成される委員会を設置し、進捗指標（KPI）を設定して透明性を確保する方針だ。英国政府は今回の戦略を「世界で最も詳細なロードマップの一つ」と位置づけ、科学技術と倫理の両面から「動物実験のない研究」への移行を進めていくとしている。</p>
]]></description>
      <pubDate>Mon, 24 Nov 2025 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google、「Gemini 3」を正式発表──推論・マルチモーダル性能を強化した最新モデル、本日より提供開始</title>
      <link>https://ledge.ai/articles/gemini_3_google_launch</link>
      <description><![CDATA[<p>Googleは2025年11月18日（米国時間）、生成AIサービス「Gemini」において最新のAIモデル群「Gemini 3」の中核となる「Gemini 3 Pro」のプレビュー提供を開始したと公式ブログで<a href="https://blog.google/products/gemini/gemini-3/">発表</a>した。</p>
<p>同モデルを「our most intelligent model yet（これまでで最も知的なモデル）」と位置づけ、推論能力、マルチモーダル理解、コード生成などを大幅に強化。Geminiアプリや開発者向けAPI、Google製品群への展開もあわせて示し、次世代AI基盤として同社の戦略を前進させる。</p>
<p>@<a href="https://www.youtube.com/watch?v=98DcoXwGX6I">YouTube</a></p>
<h2>ベンチマークはGemini 2.5 Proを全項目で上回る</h2>
<p>GoogleはGemini 3 Proが「主要ベンチマークのすべてでGemini 2.5 Proを上回った」と説明している。
具体的には以下の例を挙げた。</p>
<ul>
<li>LMArena：Elo 1501</li>
<li>Humanity’s Last Exam：ツール非使用で37.5%</li>
<li>GPQA Diamond：91.9%</li>
<li>MathArena Apex：23.4%</li>
</ul>
<p>マルチモーダル分野でも、MMMU-Proで81%、Video-MMUで87.6%、SimpleQA Verifiedで72.1%など、幅広い領域で高い性能が示されている。Googleはこれらの結果を「科学・数学・事実性など、知識領域をまたぐモデルの信頼性向上の証拠」と述べた。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/gemini_3_table_final_HLE_Tools_on_731f6cc1a4/gemini_3_table_final_HLE_Tools_on_731f6cc1a4.jpg" alt="gemini_3_table_final_HLE_Tools_on.jpg" /></p>
<h2>Deep Thinkモードも発表──高度推論向けに拡張</h2>
<p>Gemini 3には、新たに「Deep Thinkモード」が追加される。これは高度な推論・意思決定・マルチステップ推論を必要とするタスク向けに最適化した拡張モードで、Gemini 3 Proの上位に位置づけられる。</p>
<p>Deep Thinkモードは、以下のように通常のGemini 3 Proを上回るスコアを示した。</p>
<ul>
<li>Humanity’s Last Exam：41.0%</li>
<li>GPQA Diamond：93.8%</li>
<li>ARC-AGI-2（コード実行あり）：45.1%</li>
</ul>
<p>提供に先立ち、安全性テストと外部専門家による評価を経て、Google AI Ultra加入者向けに段階的に公開される予定だ。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/final_dt_blog_evals_2_33b1154911/final_dt_blog_evals_2_33b1154911.jpg" alt="final_dt_blog_evals_2.jpg" /></p>
<h2>Searchにも同日から導入──「AI Mode」でGenerative UIを採用</h2>
<p>Gemini 3は発表当日からGoogle検索の「AI Mode」に採用された。GeminiモデルがSearchに同日から搭載されるのは今回が初となる。</p>
<p>AI Modeでは、検索クエリに応じて生成される「Generative UI」を導入。地図、一覧表、画像レイアウト、ツールシミュレーションなど、検索内容に合わせたインタラクティブな表示が自動生成される。Googleはこれを「従来の検索を超えて、発見・理解・計画を支援する体験」と説明している。</p>
<p>Geminiアプリでも、デフォルトモデルがGemini 3 Proとなり、長文コンテキスト（最大100万トークン）やマルチモーダル推論を活かした学習支援・要約・説明などの機能が強化された。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/A_Imode_d2647e4cb3/A_Imode_d2647e4cb3.jpg" alt="AImode.jpg" /></p>
<h2>開発者向けにはAI Studio・Vertex AI・新IDE「Google Antigravity」を提供</h2>
<p>Gemini 3 Proは、以下の開発者向けツールで利用可能になる。</p>
<ul>
<li>Google AI Studio（Gemini API）</li>
<li>Vertex AI</li>
<li>Gemini CLI</li>
<li>Google Antigravity（新エージェント指向IDE）</li>
<li>Cursor、GitHub、JetBrains、Replitなど一部のサードパーティ環境</li>
</ul>
<p>特に「Google Antigravity」はGemini 3を前提とした“エージェント・ファースト”の開発環境で、エージェントがコードエディタ・ブラウザ・ターミナルに直接アクセスし、タスクを自律的に進める仕組みを採用する。</p>
<p>@<a href="https://youtu.be/22B5Yu0oVS0">YouTube</a></p>
<p>企業向けには「Gemini Enterprise」やVertex AIを通じた導入が開始され、Google Cloud顧客にも順次展開される。</p>
<h2>長期計画のタスクでもトップスコアを記録</h2>
<p>Googleは、長期計画タスクのベンチマーク「<a href="https://andonlabs.com/evals/vending-bench-arena">Vending-Bench 2</a>」でGemini 3がトップスコアを記録したと説明する。</p>
<p>また、1年分のシミュレーションで安定した意思決定とツール利用が確認されたとしており、今後予定される「Gemini Agent」機能（例：メール整理、予約手続きなどマルチステップの実行支援）の基盤になるとした。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/vending_bench_2_final_width_1000_format_webp_40e02b2245/vending_bench_2_final_width_1000_format_webp_40e02b2245.webp" alt="vending_bench_2_final.width-1000.format-webp.webp" /></p>
<h2>「最も安全なモデル」として開発──シコファンシー耐性・プロンプトインジェクション対策を強化</h2>
<p>GoogleはGemini 3を「Google史上最も安全なモデル」と表現し、以下の改善を挙げた。</p>
<ul>
<li>迎合的な回答（シコファンシー）の抑制</li>
<li>プロンプトインジェクション耐性の向上</li>
<li>サイバー攻撃・悪用に対する防御強化</li>
</ul>
<p>また、Frontier Safety Frameworkに基づく内部評価に加え、UK AISIなどの外部機関や独立した専門家によるアセスメントを受けたとしている。「Gemini 3 model card」には詳細な安全性情報がまとめられている。</p>
<h2>今後はDeep Thinkの段階的提供、追加モデルの投入も</h2>
<p>Gemini 3 Proは18日から、Search・Geminiアプリ・AI Studio・Antigravity・Vertex AIなど複数のプロダクトで利用可能となった。今後はDeep Thinkモードが安全性評価を経てGoogle AI Ultra加入者に提供されるほか、Gemini 3シリーズとして追加モデルも順次投入される計画だという。</p>
]]></description>
      <pubDate>Mon, 24 Nov 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/11/23 [SUN]「普通のメガネ」のようで“静かに非凡”──情報を前面・背面に分離するレイヤードHUDとカメラレス設計、指先操作のR1が示すG2／R1のスマートグラス体験の次章</title>
      <link>https://ledge.ai/articles/quiet_tech_smartglasses_even_g2_r1_launch</link>
      <description><![CDATA[<p>ウェアラブルデバイスを手がける Even Realities は、2025年11月13日（現地時間）、新型スマートグラス「Even G2」とスマートリング「Even R1」を<a href="https://x.com/EvenRealities/status/1988629357036753060">発表</a>した。同社は「Quietly Extraordinary Technology」というコンセプトを掲げ、テクノロジーが生活に干渉しすぎず“静かに寄り添う”デザイン思想を中心に据えたことが説明されている。</p>
<p>外観は「普通のメガネ」さながらだが、視界の前面と背面に情報を分離して重ねる独自のレイヤードHUDや、カメラを排した設計、指先ジェスチャーで操作できるR1など、同社が掲げる「Quietly Extraordinary Technology」を体現する機能を備える。録画や音声出力を中心に進化してきた従来のスマートグラスとは異なり、G2／R1はユーザー本人の使いやすさと周囲のプライバシーの両立を追求した“静かな次章”を提示している。</p>
<p>@<a href="https://www.youtube.com/watch?v=tAIhp9hia90&amp;t=4s">YouTube</a></p>
<h2>カメラレス・スピーカーレスという選択</h2>
<p>G2 の最大の特徴は、カメラと外向きスピーカーを搭載しない点にある。近年のスマートグラスは録画や配信、音声出力を強化する方向で進化してきたが、同社は「ユーザーと周囲のプライバシーに配慮するため」として、これらをあえて採用していない。
音声操作や撮影を前提としないことで、外部への音漏れや“撮られているかもしれない”という不安を生まない構造として設計されている。</p>
<h2>手前と奥に情報を分けて表示する“レイヤードHUD”</h2>
<p>G2 の光学システムには、独自の「HAO 2.0」ディスプレイが採用された。超小型マイクロLEDプロジェクターと導波管レンズにより、約2メートル先に自然な形で情報が投影される。</p>
<p>特徴的なのは、表示情報を二層に分離する“レイヤードHUD”だ。</p>
<ul>
<li><strong>前面のレイヤー</strong> ：通知など即時性の高い情報</li>
<li><strong>奥のレイヤー</strong> ：翻訳、ナビゲーションなど継続的な情報</li>
</ul>
<p>というように、視界の中で情報の優先度や性質に応じて層を分ける。
これにより、HUDが“情報過多”になりがちな従来のスマートグラスと異なり、視界のノイズを増やすことなく必要なタイミングで必要な情報のみを提示する設計になっている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/HUD_9e8b648e77/HUD_9e8b648e77.jpg" alt="レイヤードHUD.jpg" /></p>
<h2>更新された主要機能と会話アシスト「Conversate」</h2>
<p>主要機能には、複数ウィジェットを扱うダッシュボード、地磁気センサーによるナビゲーション、リアルタイム双方向翻訳、テレプロンプターなどが含まれる。また、独自LLM「Even AI」の高速化により、音声操作やアプリ起動の応答が向上したという。</p>
<p>新たに追加された「Conversate」は、会話中に文脈に応じた補助情報を視界に表示する機能で、会議や対話の場面で用語の説明や要点整理などを控えめに提示する。</p>
<h2>スマートリング「R1」：操作とウェルビーイングを担う指先デバイス</h2>
<p>G2 の操作体系を補完するのが、スマートリング「Even R1」だ。リング内に静電容量式タッチパッドを内蔵し、タップ、スワイプ、長押しといった指先ジェスチャーでスマートグラスを操作する。素材は医療グレードのセラミックとステンレスを採用し、耐久性と装着性を両立した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/r1_5fbc5fd7f6/r1_5fbc5fd7f6.jpg" alt="r1.jpg" /></p>
<p>R1 にはウェルビーイング機能も搭載され、心拍数、血中酸素濃度、体温、睡眠、歩数、消費カロリーなどを計測。取得データから「生産性スコア」を算出し、その日の集中力傾向を把握する指標として利用できるという。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/wellbeing_2d6509b5cf/wellbeing_2d6509b5cf.jpg" alt="wellbeing.jpg" /></p>
<h2>プライバシーの設計思想</h2>
<p>Even Realities は、カメラを排除した設計に加え、データはユーザーの明示的な同意なしにクラウドへ保存されないと説明している。必要なデータは暗号化され、個人を特定可能な情報を保持しない方針を示している。</p>
<p>G2 と R1 は、録画・配信・音声出力を中心に進化してきた他社製スマートグラスとは異なる方向性を示す。
同社は、情報過多を避け、ユーザー本人の使いやすさと周囲のプライバシーを両立させる「Quiet Tech」アプローチを採用し、視界に重ねる情報をレイヤーで整理し、操作は指先で静かに完結させる設計を打ち出した。こうした設計思想は、スマートグラス市場における一つの選択肢として位置づけられそうだ。</p>
]]></description>
      <pubDate>Sun, 23 Nov 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>日本IBMとセガXD、生成AIを学べるカードバトル研修「バトルワーカーズ」──業務内容をAIがカード化し対戦形式で学習</title>
      <link>https://ledge.ai/articles/generative_ai_card_game_training_battle_workers_ibm_segaxd</link>
      <description><![CDATA[<p>日本IBMは2025年11月17日、生成AIの基礎やプロンプト設計、リスク理解をカードバトル形式で学べる企業向け研修サービス「Generative AI Card Game Training – バトルワーカーズ」を<a href="https://jp.newsroom.ibm.com/2025-11-17-generative-ai-card-game">発表</a>した。研修設計は、ゲーミフィケーション専門企業である株式会社セガ エックスディー（セガXD）が監修した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Press2_Generated_Cards_0743fa59b7/Press2_Generated_Cards_0743fa59b7.jpg" alt="Press2_GeneratedCards.jpg" /></p>
<h2>業務内容を入力するとAIがカード化</h2>
<p>参加者自身の「仕事内容」を入力すると、生成AIがその内容をもとに バトルカード3枚とサポートカード1枚 を自動生成する仕組みを採用。カードには業務内容を反映した特徴や強み、スコアが記載され、研修参加者はこれらのカードを用いて対戦形式のゲームを行う。</p>
<p>カード生成の結果は入力する文章内容に大きく左右されるため、参加者は自然と「どの情報を、どのように記述すればAIが適切に解釈するか」を体験的に学ぶことができる。プロンプトの書き方による出力の違いを、そのままゲームの手札として実感できる構造だ。</p>
<h2>プロンプト設計・リスク理解も体験的に学べる</h2>
<p>研修のねらいとして同社は、生成AIの仕組み理解に加え、AI実務で重要となるプロンプト設計スキルやハルシネーション対策、著作権・情報管理を含むリスク理解の習得を挙げている。</p>
<p>業務内容の入力時には、どこまで詳細を書けるのか、機密情報は避けるべきか、といった情報マネジメント意識が問われる。また、生成AIが業務を誤解してカード化するケースもあり、ハルシネーション（AIの誤生成）への注意点を議論する導入にもつながるという。</p>
<h2>生成AIリテラシー格差への対応</h2>
<p>企業における生成AI活用が急速に広がる一方で、社員間のリテラシー格差は依然として課題とされる。
日本IBMは、ゲームという直感的な学習形式を取り入れることで、初心者でも無理なく生成AIの特性を掴める研修として位置づけている。参加者が「自分の業務」を題材に学ぶため、通常の座学研修よりも実務への応用をイメージしやすい点も特徴だ。</p>
<p>セガXDは、ゲームデザインや参加意欲を高める仕組みづくりに強みを持つ企業で、今回の研修ではバトル形式の導入やカード構造の設計に協力した。</p>
<p>研修を通じて同社は、企業における生成AI活用の基礎力向上を図るとともに、今後もリテラシー教育コンテンツの拡充を進める方針だ。</p>
]]></description>
      <pubDate>Sun, 23 Nov 2025 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>AIモデルが“毎分”アナログ時計を描くとどうなる？──9つのAIの“苦手さ”が見える実験サイト『AI World Clocks』</title>
      <link>https://ledge.ai/articles/ai_world_clocks_ai_models_generate_clocks_every_minute</link>
      <description><![CDATA[<p>ロサンゼルス拠点のアーティスト、Brian Moore 氏が、生成AIの“揺れ”を視覚的に楽しめるウェブサイト「<a href="https://clocks.brianmoore.com/">AI World Clocks</a>」を公開した。ページを開くと、毎分更新で9種類のAIモデルが描いたアナログ時計が並ぶ。時間ごとに姿を変える時計群からは、AIが“同じ指示でも同じものを作れない”という現実がよく見えてくる。海外コミュニティでの投稿やメディア報道から、2025年11月中旬に公開されたとみられる。</p>
<p>例：とある日の午後4時22分の表示
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/ai_world_clock_collage_6bc41e0c7f/ai_world_clock_collage_6bc41e0c7f.jpg" alt="ai world clock collage.jpg" /></p>
<h2>同じプロンプトでも毎回バラバラ　― 9モデルが1分ごとに新しい時計を生成</h2>
<p>「AI World Clocks」は以下の仕様で動作している。</p>
<ul>
<li>９つの異なるAIモデルが、毎分新しい時計を生成して表示</li>
<li>すべてのモデルに 同じプロンプト を提示（「指定時刻のアナログ時計をHTML/CSSで記述し、マークダウンなしで返す」）</li>
<li>モデルごとに 2000トークンの制限を設定</li>
<li>生成されたHTML/CSSをそのままブラウザでレンダリング</li>
</ul>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/about_ai_world_cloks_29c55a0f77/about_ai_world_cloks_29c55a0f77.png" alt="about ai world cloks.png" /></p>
<h2>“完成度は高くない”が想定通り　― 開発者自身が明言</h2>
<p>同プロジェクトを手がけた Brian Moore 氏は、ロサンゼルスを拠点とするアーティスト／クリエイティブディレクターで、インタラクティブなウェブ作品や“奇妙なインターネット体験”をテーマにした実験的プロジェクトを数多く公開してきた人物だ。</p>
<p>Moore氏の<a href="https://brianmoore.com/715-999-7483/">サイト</a>には、AIが苦手なことに挑ませたり、不条理なインタラクションを活かしたりするユニークな作品群が並んでおり、同氏自身も「AIが時計を描くのは苦手だ（It sucks at it）」と説明している。AIの不完全さをあえて作品の中心に据えるスタイルを得意としており、「AI World Clocks」もその延長線上にある。</p>
<h2>現れる時計はモデルごとにまったく違う</h2>
<p>実際にサイトを見ていると、AIごとの挙動の違いがよく分かる。</p>
<ul>
<li>針が欠けているもの</li>
<li>文字盤が過剰に装飾されるもの</li>
<li>レイアウトが描画エラーで崩れるもの</li>
<li>時刻が正しく反映されないもの</li>
</ul>
<p>同一プロンプトでもまったく異なる出力が生まれ、HTML/CSSのように構造が厳密に求められるタスクではモデル間の差が顕著になる。</p>
<h2>開発者が挙げた“モデルの性格”も面白い</h2>
<p>Moore 氏は Hacker News 上の<a href="https://news.ycombinator.com/item?id=45930151">投稿</a>で、モデルごとの“味”についてこう語っている。</p>
<ul>
<li>Kimi K2：「もっとも時間に正確。ただし最も退屈」</li>
<li>Qwen 2.5：「もっともクレイジーで、一番笑わせてくれる」</li>
</ul>
<p>作者本人がこのように“軽妙な温度感”で語っている点からも、同作品がAIの不完全さを楽しむためのプロジェクトであることが理解できる。</p>
<p>「AI World Clocks」は、AIモデルが同一の構造化タスクに取り組んだ際にどのような差異や揺らぎが生じるかを、逐次的に確認できるプロジェクトである。毎分変化する時計表示は、モデル間の安定性や挙動の違いを視覚的に把握しやすく、AIによるコード生成の現状を観察する素材として活用できる設計になっている。</p>
]]></description>
      <pubDate>Sun, 23 Nov 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/11/22 [SAT]ヤン・ルカン博士、Metaを年末に退社──「次のAI革命」Advanced Machine Intelligence研究を推進する新会社を設立へ</title>
      <link>https://ledge.ai/articles/yann_lecun_leaves_meta_ami_startup_2025</link>
      <description><![CDATA[<p>米MetaでチーフAIサイエンティストを務め、同社のAI研究組織「FAIR（Facebook AI Research）」の創設者として知られるヤン・ルカン（Yann LeCun）博士が、12年間在籍したMetaを年末に退社する。2025年11月20日（現地時間）、自身の<a href="https://www.facebook.com/yann.lecun/posts/pfbid0iU3ECamGohUvpmKozEq24RGowPoiu3D7J6vHXJxzKGT4hZittF2UK43oZkjSqVexl">Facebookへの投稿</a>で明らかにした。</p>
<p>ルカン氏は投稿の冒頭で「うわさや最近の報道で耳にした人もいるだろうが、12年間在籍したMetaを離れる計画だ」と述べ、退社を正式に表明した。FAIRの初代ディレクターとして5年、チーフAIサイエンティストとして7年を務めたと振り返り、「FAIRの創設は、自身の“最も誇りに思う非技術的な業績”」と強調した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/yann_lecun_facebook_3d6a371abb/yann_lecun_facebook_3d6a371abb.jpg" alt="yann lecun facebook.jpg" /></p>
<h2>Advanced Machine Intelligence（AMI）研究を継続する独立スタートアップを設立</h2>
<p>投稿によると、ルカン氏は「Advanced Machine Intelligence（AMI）」と名付けた研究プログラムを継続するため、新たなスタートアップを立ち上げる。同プログラムは、近年FAIRやニューヨーク大学（NYU）などの研究者と進めてきたもので、次のような能力を備えたAIシステムを目指すという。</p>
<ul>
<li>物理世界を理解する</li>
<li>持続的な記憶を持つ</li>
<li>推論できる</li>
<li>複雑な行動計画を実行できる</li>
</ul>
<p>ルカン氏は、これを「次の大きなAI革命」につながる研究領域だと位置づける。スタートアップの詳細は後日公表するとしており、社名や資金調達の状況は明かしていない。</p>
<h2>Metaは新会社のパートナーに──Zuckerberg氏らへの感謝も明記</h2>
<p>投稿では、Mark Zuckerberg氏、Andrew Bosworth氏（Boz）、Chris Cox氏、Mike Schroepfer氏の4名に対し、FAIRおよびAMI研究への継続的な支援への謝意を述べている。そのうえで、「Metaは新会社のパートナーとなる」と記し、退社後も協力関係が続くことを示した。</p>
<p>AMI研究のアプリケーションについては「Metaの商業領域と重なる部分もあれば、重ならない領域もある」とし、独立した組織で取り組むことが「広範なインパクトを最大化する手段」だと説明している。</p>
<h2>研究者としての役割は継続、年末まではMetaに在籍</h2>
<p>ルカン氏は現在、MetaのチーフAIサイエンティストとNYUの教授職を兼務している。投稿では、これらの役割に関する変更には触れておらず、研究活動は継続する見通しだ。</p>
<p>また、「年末まではMetaにとどまる」とも述べており、移行期間中は同社での業務を続けながら新会社の準備を進めるとしている。</p>
]]></description>
      <pubDate>Sat, 22 Nov 2025 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/11/21 [FRI]Google、「Nano Banana Pro」を発表──4K対応・多言語テキスト描画を強化した最新画像生成AI</title>
      <link>https://ledge.ai/articles/google_nano_banana_pro_release</link>
      <description><![CDATA[<p>Google は2025年11月20日（米国時間）、画像生成AI「Nano Banana」の最新バージョンとなる 「Nano Banana Pro」 を<a href="https://blog.google/intl/ja-jp/company-news/technology/nano-banana-pro/">発表</a>した。Gemini 3 Pro Image を基盤とした新しい画像生成システムで、、4K解像度の高精細生成や多言語テキスト描画の精度が大幅に向上しているという。</p>
<p>@<a href="https://www.youtube.com/watch?v=UQsJIo46ZR8&amp;t=3s">YouTube</a></p>
<h2>4K出力と多言語テキスト生成を強化</h2>
<p>公式ブログによると、Nano Banana Pro は従来モデルから画質と制御性能が大幅に進化し、最大 4K 解像度の画像生成 に対応した。広告制作、資料作成、スライドデザインなど、高精細なビジュアルを必要とする用途での活用が見込まれる。</p>
<p>Google は発表の中で、照明や構図のコントロールが可能になった事例を複数紹介している。</p>
<p><strong>・照明・フォーカス調整の例</strong>
シーンを昼から夜へと変更し、照明バランスを最適化するデモ（プロンプト：このシーンを夜にしてください）
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/WM_Fox_93f02b0061/WM_Fox_93f02b0061.jpg" alt="WM-Fox.jpg" /></p>
<p><strong>・ドラマチックな照明効果の付与</strong>
人物ポートレートに拡散光を加え、印象を変えるデモ（プロンプト：このポートレートの照明を、左からの柔らかな拡散光に変更してください）
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/WM_Studio_Quality2_81a14549aa/WM_Studio_Quality2_81a14549aa.jpg" alt="WM-Studio-Quality2.jpg" /></p>
<p><strong>・被写界深度の調整</strong>
焦点を被写体の花に合わせ、構図の要素を際立たせるデモ（プロンプト：花に焦点を合わせてください）
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/WM_Flower_Girl_16a2874c39/WM_Flower_Girl_16a2874c39.jpg" alt="WM---Flower-Girl.jpg" /></p>
<p>また、日本語・韓国語・アラビア語などの複雑な文字体系にも対応し、多言語の画像内テキスト生成がより自然で正確になった。文字組みや行送りなど、これまで課題となっていたレイアウト要素の精度も改善している。</p>
<p><strong>・テキストローカライズの例</strong>
英語で書かれた飲料キャンペーンの文言を韓国語に翻訳し、デザイン要素を保持したまま再描画するデモ（プロンプト：3つの黄色と青の缶に書かれている英語のテキストを韓国語に翻訳し、他の要素は変更しないでください）
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Final_Nano_Banana_Translate_Can_4ac7f9e843/Final_Nano_Banana_Translate_Can_4ac7f9e843.jpg" alt="Final_NanoBanana_TranslateCan.jpg" /></p>
<h2>実世界知識の向上と複数画像入力</h2>
<p>Nano Banana Pro は Google Search の最新情報を参照し、現実世界の文脈を反映した画像生成に対応する。Google は、科学・歴史・文化的トピックを視覚化する例を公式ページで紹介している。</p>
<p><strong>・科学トピックの視覚化例</strong>
アイザック・ニュートンの光と色彩の理論を、ミニマルなフラットレイ構図で説明するデモ
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/unnamed_5_75517229b0/unnamed_5_75517229b0.webp" alt="unnamed (5).webp" /></p>
<p>また、最大14枚の画像入力に基づき、スタイルやレイアウトの一貫性を保ちながら新たな画像を生成できる。人物の再現についても、最大5人までの整合性を維持可能としている。</p>
<p><strong>・複数キャラクターの配置例</strong>
14体のキャラクターを並んで座らせ、一貫したスタイルで描画するデモ
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Fluffy_Monsters_a42f31b8d0/Fluffy_Monsters_a42f31b8d0.jpg" alt="Fluffy-Monsters.jpg" /></p>
<p>生成画像には AI 生成物識別技術 SynthID の透かしが自動で埋め込まれ、透明性と著作権管理への配慮も進めている。</p>
<h2>Workspace・NotebookLMなど各サービスで提供開始</h2>
<p>Nano Banana Pro は、Gemini アプリに加えて Google Slides、Google Vids、NotebookLM など複数の Google サービスで利用可能となる。Google Workspace では、11月20日から最大15日間かけて段階的に展開する。</p>
<h2>開発者・企業向けの提供も本格化</h2>
<p>Google は開発者向けブログも公開し、Nano Banana Pro の 2K/4K 出力、複数画像入力、レンダリング精度の向上などを説明した。Vertex AI を通じた API 提供が開始され、Google Cloud 上での画像生成ワークフローへの統合も可能となる。</p>
<p>Google は Gemini 3 系列を軸にマルチモーダル AI の強化を進めており、Nano Banana Pro をその画像生成技術の中心モデルとして位置づけている。多言語描画と高精細画像への対応について、同社は広告・教育・資料作成など幅広い領域での活用を想定していると説明している。</p>
]]></description>
      <pubDate>Fri, 21 Nov 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>AIには「翻訳・予測」、人間には「医療・買い物」──博報堂DYのAI利用調査で浮かぶ“任せたい仕事／任せたくない仕事”の境界線と、生活者が描く望ましい未来像</title>
      <link>https://ledge.ai/articles/ai_job_boundary_future_expectations_hakuhodo_survey</link>
      <description><![CDATA[<p>博報堂DYホールディングス（東京都港区）の研究機関「Human-Centered AI Institute」は11月17日、全国の15～69歳を対象に実施した「AIと暮らす未来の生活調査2025」の結果を<a href="https://www.hakuhodody-holdings.co.jp/news/corporate/2025/11/5995.html">発表</a>した。</p>
<p>生成AIの認知率は85.3％、利用率は33.6％で、利用者の45.3％が「2〜3日に1回以上」AIを使うヘビーユーザーであることが分かった。さらに、AIに任せたい仕事／人が行うべき仕事の線引きや、将来AIに期待する役割など、生活者の価値観が鮮明になった。</p>
<h2>若年層ほど利用率が高く、10代は62.6％</h2>
<p>年代別では、10代の利用率が62.6％と際立って高く、AIネイティブとして日常的に生成AIを使いこなす姿が浮かぶ。一方、50代以上でも24.6％が利用しており、「4人に1人」が生成AIを生活に取り入れている。生成AIの利用は特定の世代に限らず幅広く浸透している状況だ。</p>
<p><strong>■ 生活者全体の85.3%が生成AIを認知し、33.6%が生成AIを利用
利用者の45.3%が2~3日に1回以上利用するヘビーユーザー</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/AI_reseach_0a4eb4f889/AI_reseach_0a4eb4f889.jpg" alt="AI reseach①.jpg" /></p>
<p><strong>■ AIネイティブである10代の生成AI利用率は62.6%
50代以上でも「4人に1人」が利用</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/AI_reseach_285ca8efe9/AI_reseach_285ca8efe9.jpg" alt="AI reseach②.jpg" /></p>
<h2>プライベート利用が9割超、身近な生活ツールに</h2>
<p>生成AI利用者の92.6％がプライベート・学業でAIを活用しており、ビジネスのみの利用（7.4％）を大きく上回った。利用者の多くが、生成AIを身近な生活ツールとして位置づけている。</p>
<p>利用者が生成AIをどう捉えているかを尋ねた項目では、「便利な道具」（43.6％）が最も多かったものの、10代では「悩みを相談できる存在」「遊び相手」といった情緒的な関係性を持つ回答も目立った。一方、50代以上は「サポート要員」としてより道具的に捉える傾向が強かった。</p>
<p><strong>■ 10代にとっては「悩みを相談できる相手」「遊び相手」
50代以上にとっては「サポート要員」</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/AI_reseach_3c033d9a3c/AI_reseach_3c033d9a3c.jpg" alt="AI reseach③.jpg" /></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/AI_reseach_5085c2a869/AI_reseach_5085c2a869.jpg" alt="AI reseach④.jpg" /></p>
<p><strong>■ 生成AIの利用者は92.6%がプライベートで利用、生活に身近なツールに</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/AI_reseach_9360b23539/AI_reseach_9360b23539.jpg" alt="AI reseach⑤.jpg" /></p>
<p><strong>■ プライベートでは「悩みを相談できる存在」「遊び相手」
ビジネスでは「仕事をサポートする存在」</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/AI_reseach_2183b61666/AI_reseach_2183b61666.jpg" alt="AI reseach⑥.jpg" /></p>
<h2>半数がAI情報を信頼、一方で“AIだけでは不十分”との声も</h2>
<p>生成AIが提供する情報を「信頼している」と回答したのは55.1％で、過半数がAIを情報源として受容している。一方、「生成AIの情報だけでは不十分。他メディアからの情報も必要」と答えたのは48.3％に上り、多くの生活者がAIを“単独の情報源”としては使わず、従来メディアとの併用を前提としていることも明らかになった。</p>
<p><strong>■ 「生成AIの提供情報を信頼している」のは、利用者の55.1%で過半数を超える</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/AI_reseach_0ab2e39a47/AI_reseach_0ab2e39a47.jpg" alt="AI reseach⑦.jpg" /></p>
<p><strong>■ 48.3%と約半数が「生成AIの情報だけでは不十分、マスメディアなど他の情報も必要」と考えている</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/AI_reseach_c365104cbe/AI_reseach_c365104cbe.jpg" alt="AI reseach⑧.jpg" /></p>
<h2>AIに任せたい仕事は「ルーティン・翻訳・予測分析」</h2>
<p>AIと人間それぞれに「どんな仕事を任せたいか」を尋ねた項目では、明確な線引きが見えた。</p>
<h3>AIに任せたい仕事（上位）</h3>
<ul>
<li>ルーティンワーク・単純作業（43.4％）</li>
<li>翻訳（41.2％）</li>
<li>環境モニタリング（40.7％）</li>
<li>予測分析（39.2％）</li>
</ul>
<h3>人間がやるべき仕事（上位）</h3>
<ul>
<li>日々の買い物（38.8％）</li>
<li>医療処置・手術支援（35.5％）</li>
<li>教育支援（34.2％）</li>
<li>医療診断（34.0％）</li>
</ul>
<p>AIに任せたい仕事として “機械的・定量的に処理できる”と認識されるカテゴリーが上位に並んだ。一方で人間には「買い物」を人間が行うべきとする声が多く、“楽しみを伴う体験はAIに委ねたくない”という生活意識が表れた。また、医療や教育といった「判断」「共感」「責任」を伴う領域には、AIに任せすぎることへの慎重姿勢も読み取れる。</p>
<p><strong>■ 人間がやるべき仕事は「医療」「教育」
AIがやるべき仕事は「ルーティンワーク」「翻訳」、楽しみたい「買い物」は人間のテリトリー</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/AI_reseach_d206da2b2b/AI_reseach_d206da2b2b.jpg" alt="AI reseach⑨.jpg" /></p>
<h2>生活者が期待する未来は「心と体のケアまでAIが担う世界」</h2>
<p>AIとの“望ましい未来像”に関する質問では、最も多かった回答が「リアルタイム翻訳で、言語の壁がなく国際的な仕事ができる」（20.6％）。次いで「AIのパーソナルドクターが肉体・精神ケアをしてくれる」（17.1％）、「精神的に疲れる業務をAIに任せるようになる」（15.7％）が続き、心身の負荷を軽くする方向でのAI活用に期待が集まった。</p>
<p>生活者はAIを単なる作業代行者としてではなく、今後は身体的・精神的な支援を行う“パートナー”として位置づけることを望む傾向が見られる。</p>
<p><strong>■ 望ましい未来：将来的には生成AIに「心と体のサポート」も期待</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/AI_reseach_3910f444e8/AI_reseach_3910f444e8.jpg" alt="AI reseach⑩.jpg" /></p>
<h2>生成AIが日常の中心に入りつつある一方、人間ならではの領域も明確に</h2>
<p>今回の調査は、生成AIが急速に生活へ浸透しつつある一方で、「AIに任せたい仕事」と「人が担うべき仕事」の境界線が生活者の中で明確に形成されつつあることを示している。また、AIの情報を信頼しつつも、人間が発信する情報の価値を依然として重視する両面性も明らかになった。</p>
<p>生活に寄り添うAI活用の広がりと、未来に向けた役割分担の模索は、今後さらに深化していくとみられる。</p>
]]></description>
      <pubDate>Fri, 21 Nov 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Meta、1600言語対応の「Omnilingual ASR」を公開──低リソース言語500超を含む世界最大級の音声認識モデル</title>
      <link>https://ledge.ai/articles/meta_omnilingual_asr_1600_languages_release</link>
      <description><![CDATA[<p>MetaのAI研究部門であるMeta FAIRは2025年11月10日（現地時間）、1600以上の言語に対応する自動音声認識システム「Omnilingual ASR」を<a href="https://ai.meta.com/blog/omnilingual-asr-advancing-automatic-speech-recognition/">発表</a>した。</p>
<p>これまでASRが文字起こしできなかった500超の低リソース言語を含む世界規模の音声データを収集し、話し言葉をテキスト化できるモデルとしてGitHubにオープンソース（Apache 2.0）で<a href="https://github.com/facebookresearch/omnilingual-asr">公開</a>する。wav2vec 2.0とテキスト埋め込みを組み合わせ、LLMと共通の“次トークン予測”で学習する新アーキテクチャにより、幅広い言語で高精度な認識を実現。言語アクセス格差の解消を目指す取り組みだ。</p>
<p>@<a href="https://www.youtube.com/watch?v=ab-GIqDQn7k">YouTube</a></p>
<h2>世界1600言語を対象に構築──“未対応言語”500以上を含む大規模ASR</h2>
<p>Meta FAIRによると、Omnilingual ASRはこれまで自動音声認識モデルが対応できなかった多数の低リソース言語を含め、1600以上の言語で話し言葉を文字化できるのが特徴だ。対応言語数は、既存の多言語ASRモデルを大きく上回る。</p>
<p>こうした言語拡張を支えるのが、世界各地での音声データ収集プロジェクトである。同社は論文の中で、地域コミュニティと協働して多様な話者データを収集した事例を紹介している。</p>
<h2>コミュニティ協働によるデータ収集</h2>
<p>Pakistan のデータ収集風景：パキスタンでは、現地話者がコーパス作成に参加し、話し言葉の録音作業が進められた。これまでデジタル音声データが十分に存在しなかった言語の可視化につながる取り組みだ。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/pakistan_data_collection_3657e54b1c/pakistan_data_collection_3657e54b1c.jpg" alt="pakistan_data_collection.jpg" /></p>
<p>Liberia のデータ収集風景：リベリアでも同様に、地域住民が音声収録プロジェクトに参加。Metaは、言語アクセス格差の背景にある「データ不足」を現地と共同で解消するアプローチを強調している。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/liberia_data_collection_a713ad6e66/liberia_data_collection_a713ad6e66.jpg" alt="liberia_data_collection.jpg" /></p>
<p>こうしたプロジェクトは、低リソース言語の話者をデジタル社会に包摂するための基盤になるとして、言語学・AI研究双方から注目されているという。</p>
<h2>LLMと共通の学習方式を採用──次トークン予測で統一</h2>
<p>Omnilingual ASR は、音声処理部に wav2vec 2.0 を採用し、テキスト側には埋め込み行列を用いる。さらに、Transformer デコーダが音声入力からテキストを逐次生成する構造を取り、学習には大規模言語モデル（LLM）と同じ 次トークン予測（NTP） を用いる点が特徴だ。</p>
<p><strong>LLM-ASR の基本構造図</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/LLM_ASR_e4f4e0436d/LLM_ASR_e4f4e0436d.png" alt="LLM-ASR.png" /></p>
<p>この構造により、音声認識と自然言語処理の学習方式を統一。LLM の発展に合わせてASRも改善しやすくなるという。</p>
<h2>文脈（コンテキスト）を与えることで精度向上</h2>
<p>Omnilingual ASR は、ターゲット音声の前に複数の「文脈音声＋文脈テキスト例」を与えることで精度が向上するという。
話者の発音や文体、言語固有の特徴をモデルが前もって学習しやすくなる。</p>
<p><strong>コンテキスト例つきのモデル構造図</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/context_1_f86d2e3ac9/context_1_f86d2e3ac9.jpg" alt="context (1).jpg" /></p>
<p>この手法は特に低リソース言語で顕著な効果があると報告されており、地域性の強い発音を含むケースでも安定した認識を実現する。</p>
<h2>研究・教育・文化保全に向けた応用</h2>
<p>Omnilingual ASRはオープンソースで公開されることで、研究者や開発者が多言語音声認識の仕組みを検証・拡張できるようになる。Metaは教育、医療支援、フィールド調査、文化・言語のアーカイブなど、多様な領域で利用が想定されるとし、低リソース言語を含む幅広い言語の文字化や記録作成を支援する基盤として活用される可能性を示している。</p>
]]></description>
      <pubDate>Fri, 21 Nov 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>NTT、人が見た映像や思い浮かべた光景を文章化する「マインド・キャプショニング」開発──脳活動から非言語思考をテキスト化</title>
      <link>https://ledge.ai/articles/ntt_mind_captioning_brain_to_text</link>
      <description><![CDATA[<p>NTTは2025年11月17日、脳活動データをもとに、人が見ている映像や頭の中で思い浮かべた光景を文章として生成する脳解読技術「マインド・キャプショニング（Mind Captioning）」を開発したと<a href="https://group.ntt/jp/newsrelease/2025/11/17/251117a.html">発表</a>した。研究成果は米科学誌 Science Advances に掲載されている。</p>
<h2>視覚体験・想起したイメージを“脳から直接”テキスト化</h2>
<p>マインド・キャプショニングは、動画視聴時や想起時の脳活動（fMRI）から内容に対応する意味情報を抽出し、深層言語モデルを使って文章として復元する技術である。NTTは本技術について、「視覚体験や頭の中のイメージといった非言語的な思考を言語化するための新たなアプローチ」と説明している。</p>
<p>技術は大きく以下の2段階で構成される。</p>
<ol>
<li>脳活動 → セマンティック特徴（意味特徴）の変換</li>
<li>意味特徴に合わせて文章を反復最適化し生成</li>
</ol>
<h2>脳活動から文章が“洗練されていく”仕組み</h2>
<p>文章生成プロセスでは、まず未知トークンを含む初期文を作り、脳活動から推定した意味特徴と一致するように文を何度も書き換える。この「反復最適化（iterative refinement）」により、視覚的特徴に沿った文章へと徐々に近づいていく。</p>
<p><strong>脳活動から抽出した意味特徴を手がかりに、mask付きの文を何度も書き換えて最適化する仕組み（左）。右は言語野を含む／含まない脳領域での精度比較で、視覚領域主体でも一定の生成性能が得られることを示す</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/251117ac_68b513e1ec/251117ac_68b513e1ec.jpg" alt="251117ac.jpg" /></p>
<p>上図が示すように、言語野の活動を含めない場合でも精度が大きく低下するわけではなく、視覚領域の脳活動だけを頼りに文章を生成できる点が同技術の特徴だという。</p>
<h2>視覚していない「想起」状態からも文章化に成功</h2>
<p>実験には日本人6名が参加し、約2,180本の動画クリップ（人物・風景・アニメ・物体など多様）を視聴。その後、映像内容を思い浮かべる“想起タスク”でも脳活動を計測した。</p>
<p>論文では、</p>
<ul>
<li>視聴時（知覚時）</li>
<li>想起時（頭の中で思い浮かべている状態）</li>
</ul>
<p>いずれの脳活動からも内容に対応する文章が生成できたと報告されている。</p>
<p>特に、再現が難しい“単発の想起データ”でも文章生成が可能であった点は、夢や記憶、非言語的思考の理解に応用できる可能性を示す成果だ。</p>
<h2>技術全体の流れ──二段階構造で脳活動を言語へ</h2>
<p>研究では、同技術が次の二段階構成で成り立つとしている。
Stage 1：意味特徴デコーダーの学習（動画視聴 × 言語モデル）
Stage 2：脳活動から推定された特徴に合わせて文章を最適化</p>
<p><strong>マインド・キャプショニングの二段階構造。Stage 1では動画視聴データを用いて意味特徴デコーダーを学習し、Stage 2では脳活動から推定された特徴に基づき文章を反復最適化して生成する</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/251117ab_edd7d6cd67/251117ab_edd7d6cd67.jpg" alt="251117ab.jpg" /></p>
<p>このフローにより、脳の非言語的な表現を、言語モデルが解釈可能な意味空間へと徐々に写像していく。</p>
<h2>言語野に依存しない“非言語思考の言語化”を実証</h2>
<p>生成された文章は、主に視覚情報処理に関わる脳領域の活動から得られた特徴量を利用している。研究チームは「言語野が十分に働かない状況でも、視覚内容を文章として表現できる」としており、言語障害を持つ人のコミュニケーション支援などへの応用可能性を示唆している。</p>
<p>一方で論文では、脳解読技術の本質的リスクについても注意喚起している。
モデルのバイアスが文章へ反映される可能性</p>
<ul>
<li>意図しない思考内容が出力されるリスク</li>
<li>少量データで個人を跨いだデコードが可能になる可能性（プライバシー問題）</li>
</ul>
<p>研究チームは、明確な同意・倫理的ガイドライン・精神的プライバシー（mental privacy）の保護を強く求めている。</p>
<h2>NTTは基礎研究として公開、応用領域の探求へ</h2>
<p>NTTは今回の研究を「脳とAIをより柔軟に結びつけるための基盤研究」と位置づけており、今後は、認知科学・医療・マンマシンインタフェース領域などでの応用を想定している。</p>
]]></description>
      <pubDate>Thu, 20 Nov 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Amazon創業者のジェフ・ベゾス氏、AI企業「プロジェクト・プロメテウス」の共同CEOに就任── “実世界AI” に本格参入</title>
      <link>https://ledge.ai/articles/jeff_bezos_project_prometheus_co_ceo_appointment</link>
      <description><![CDATA[<p>Amazon創業者のジェフ・ベゾス氏が、AIスタートアップ「Project Prometheus（プロジェクト・プロメテウス）」の共同最高経営責任者（CEO）に就任した。2025年11月17日に<a href="https://www.nytimes.com/2025/11/17/technology/bezos-project-prometheus.html">ニューヨークタイムズ</a>
など複数の海外メディアが報じたもので、2021年のAmazon CEO退任以来、同氏が企業運営の最前線に復帰するのは初めてとなる。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/bezos_creates_ai_startup_ba276b805d/bezos_creates_ai_startup_ba276b805d.jpg" alt="bezos creates ai startup.jpg" /></p>
<h2>製造・ロボティクス・航空宇宙…“物理経済”を理解するAIを開発</h2>
<p>報道によると、Prometheusはこれまでに62億ドル（約6,200億円）規模を調達したAI企業で、生成AIとは異なる領域—工場、ロボット、自動車、航空宇宙など、“物理世界の複雑なシステムを理解し操作できるAI”の開発を目標としている。</p>
<p>同社が取り組む領域は「physical economy（物理経済）」と呼ばれ、ソフトウェア領域にとどまらず、機械・装置・製造現場の挙動を扱う。そのため、今後の産業インフラや高度製造分野で存在感を高める可能性があるとみられている。</p>
<h2>共同CEOは元Google X幹部のVik Bajaj氏</h2>
<p>Prometheusは共同CEO制を採用しており、ベゾス氏とともに経営を担うのは、Google X やVerilyで幹部を務めたVik Bajaj（ヴィク・バジャージ）氏。Bajaj氏は工学・バイオ・ロボティクス領域の研究と事業化に長く携わり、ディープテック企業の経営にも実績を持つという。</p>
<h2>ベゾス氏の「経営トップ」復帰は4年ぶり</h2>
<p>ベゾス氏はAmazon CEO退任後、宇宙開発企業Blue Originの運営や複数企業のオーナー業を継続してきたが、実務責任を伴うトップ職に戻るのは約4年ぶり。AI産業の拡大とともに、製造AI・工学AI分野の競争が激化するなかでの復帰として注目が集まっている。</p>
]]></description>
      <pubDate>Thu, 20 Nov 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>AWS、「Kiro」を一般提供──仕様駆動のAIコーディングをIDEとターミナルで一元化</title>
      <link>https://ledge.ai/articles/aws_kiro_general_availability</link>
      <description><![CDATA[<p>AWSは2025年11月18日、AIエディター／開発ツール「Kiro（キロ）」の一般提供を<a href="https://aws.amazon.com/jp/blogs/news/introducing-kiro-cli/">発表</a>した。Kiroは、7月の<a href="https://ledge.ai/articles/aws_ai_ide_kiro_release">プレビュー版公開</a>から機能追加を重ねてきたAI開発ツールで、仕様（Spec）を中心にコード生成・検証・リファクタリングを行える点が特徴だ。一般提供に合わせて、プロパティベーステスト、エージェント動作の巻き戻し機能、ターミナル向けの「Kiro CLI」、AWS IAM連携のチーム管理機能を搭載したという。</p>
<h2>仕様に基づく開発を強化：プロパティベーステストに対応</h2>
<p>Kiroの正式版では、仕様と実装の整合性を検証するための「プロパティベーステスト（property-based testing）」が導入された。開発者がEARS形式で記述した仕様から、Kiroがテストすべき“性質（プロパティ）”を自動抽出し、数百〜数千のランダムテストを生成して実装を検証する仕組みだ。反例が見つかった場合は、原因特定のため条件を徐々に単純化する「縮小（shrinking）」も実行される。</p>
<p>これにより、個別のテストケースでは検出しづらい欠陥を発見しやすくなり、仕様ベースでの正確な実装を支援する。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/correctness_f57a485373/correctness_f57a485373.png" alt="correctness.png" />
Kiro IDEが生成したプロパティベーステストの例。仕様から抽出した“チケットIDが常に一意であること”という性質を可視化し、関連する要件・実装タスク・テスト結果を紐づけて表示する。</p>
<h2>チェックポイントで開発プロセスを巻き戻し可能に</h2>
<p>一般提供版では、エージェントが行った操作や変更内容を段階的に保存する「チェックポイント」機能を追加した。エージェントの提案やコード修正を進めた後でも、必要に応じて任意の状態へ戻れるため、実装方針の変更や手戻りが発生した際のコストを抑えられる。</p>
<p>また、1つのKiroワークスペースで複数のプロジェクトルートを扱える「マルチルート」構成にも対応し、マイクロサービスやモノレポなど複雑な構成の開発にも利用できる。</p>
<h2>IDEとターミナルを共通化する「Kiro CLI」</h2>
<p>正式版の大きな追加点として、ターミナルで利用できる「Kiro CLI」が提供された。開発者は、IDE と同じエージェント環境をターミナルでもそのまま利用でき、Claude Sonnet 4.5 や Haiku 4.5 などのモデル、Autoエージェント、MCP（Model Context Protocol）ツールによるローカル操作やAPI呼び出しも実行できる。</p>
<p>CLIとIDEは同一のステアリングファイルや認証情報、コンテキストを共有するため、開発環境を切り替えても同じワークフローを継続できる。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/cli_5921207104/cli_5921207104.png" alt="cli.png" />
一般提供版で新たに追加された「Kiro CLI」。IDEと同じエージェント環境をターミナルでも利用でき、AutoエージェントやMCPツールによる操作が可能になる。</p>
<h2>AWS IAM連携のチーム管理と、スタートアップ向け1年無償プラン</h2>
<p>Kiroは、AWS IAM Identity Centerと統合され、組織アカウントによるログインや、Pro／Pro+／Powerティアの利用制限、MCP設定、請求管理などをAWS側で一元的に管理できる。</p>
<p>また、シリーズBまでの対象スタートアップには「Kiro Pro+」を1年間無償提供するオファーを開始した。既存のAWS Activateクレジットと併用可能で、2025年12月31日まで申請を受け付ける。</p>
<h2>“AI駆動開発”の基盤へ</h2>
<p>Kiroは、仕様駆動の開発、プロパティベーステスト、IDE／CLI共通のAIエージェント環境、IAM連携によるチーム運用など、開発プロセス全体をAIと統合する機能を備えた。Kiro公式ブログでは「This is just the start.」と記されており、今後もアップデートを継続する方針を示している。</p>
]]></description>
      <pubDate>Wed, 19 Nov 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>クマ被害多発で注目、上智大・深澤研究室が19地域の遭遇リスクをAIで可視化──1kmメッシュで危険度を5段階表示する予測マップ</title>
      <link>https://ledge.ai/articles/bear_encounter_ai_prediction_map_sophia_university</link>
      <description><![CDATA[<p>全国的にクマによる出没や人身被害が相次ぐ中、上智大学・深澤佑介准教授（応用データサイエンス）の研究チームが、クマとの遭遇リスクをAIで予測し、地図上で可視化する「<a href="https://ds.sophia.ac.jp/news/20251024/post-1065">クマ遭遇AI予測マップ</a>」を一般公開している。対象地域は札幌市、東北、北関東、東京都、北陸、中部、京都府など計19地域に広がり、誰でも無料で閲覧できる。</p>
<p>同大学・応用データサイエンス学位プログラムは2025年10月24日付で、深澤研究室が開発した「クマ遭遇予測マップ」の公開URLを案内しており、秋田県を対象とした予測モデルを基に実装したことを説明している。研究室サイトでは19地域分のマップが一覧化され、各エリアの遭遇確率を確認できる。</p>
<h2>AIが1kmメッシュで遭遇リスクを推定</h2>
<p>予測マップでは、AIがクマ遭遇リスクを1km四方のメッシュ単位で推定し、危険度を「非常に高い」「高い」「やや高い」「可能性あり」「低い」の5段階で色分けして表示する。赤系が高リスク、黄色系が中リスクを示す仕組みだ。</p>
<p>また、各地域ページには、直近6カ月以内のクマ出没・遭遇地点を示す「X印」が別途表示される。マーカーがない場所は、データ不足などの理由で予測が行われていないエリアであることも明記されている。</p>
<h2>過去の遭遇記録や地形・人口などを活用</h2>
<p>予測モデルは、各自治体が公開するクマの出没・遭遇記録をはじめ、森林や農地などの土地利用、道路網、人口分布、標高といった環境要因を組み合わせて学習している。
学習には決定木ベースの機械学習手法が用いられ、秋田県のデータでは「正答率・適合率・再現率がいずれも6割強」という検証結果も得られている。</p>
<p>これらの情報を統合し、各メッシュの遭遇確率を推定することで、ユーザーが直感的に危険エリアを把握できるよう設計されている。</p>
<h2>19地域をカバーし、PC・スマホから無料で閲覧可能</h2>
<p>マップは研究室サイトで公開されており、PCやスマートフォンから無料で利用できる。アクセスに特別なアプリは必要なく、ブラウザ上で地図を閲覧しながら推定リスクを確認できる。</p>
<p>対象地域は以下の通り（一部抜粋）
：札幌市・青森県・盛岡市・秋田県・山形県・宮城県・新潟県・栃木県・群馬県・埼玉県・東京都・山梨県・富山県・石川県・長野県・岐阜県・京都府</p>
<p>研究室サイトには「最終更新日時」も掲示され、最新の予測状況を確認できる。</p>
<p>また、深澤研究室は X（旧Twitter）公式アカウント（<a href="https://x.com/fukazawa_lab">@fukazawa_lab</a>） でもマップの更新情報や関連アナウンスを随時発信しており、最新の動向を確認できる。</p>
<h2>「最新の自治体情報を優先してほしい」──研究チームが注意喚起</h2>
<p>上智大の案内および研究室サイトでは、予測マップの利用にあたって複数の注意点を明示している。</p>
<ul>
<li>マップは「注意喚起」を目的とした予測であり、遭遇を確実に防ぐものではない</li>
<li>クマの行動は環境によって変動するため、最新の状況は反映しきれない場合がある</li>
<li>登山・農作業・山菜採りなどを行う際は、「自治体が発信する情報や現地の警報・掲示を必ず確認してほしい」</li>
</ul>
<p>研究チームは今後もデータの蓄積やモデル改良、対象地域の拡大を進め、遭遇リスク低減に資する情報提供を継続する方針を示している。</p>
]]></description>
      <pubDate>Wed, 19 Nov 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ChatGPTの“当たり障りないフィルター”を外すと、応答が一段と鋭くなった──米国で話題の「辛口プロンプト」現象</title>
      <link>https://ledge.ai/articles/chatgpt_ii_hito_filter_prompt_trend</link>
      <description><![CDATA[<p>ChatGPTの「当たり障りのない」応答に物足りなさを感じた海外ユーザーが、あえて“当たり障りないフィルター”を外すプロンプトを公開し、話題を集めている。Redditで拡散したこの手法は、ChatGPTのトーンを「共感的な聞き役」から「論理的で辛口な批評家」へと変えるもので、SNSでは「回答の質が上がった」との声も相次いだ。</p>
<h2>Reddit発の「辛口プロンプト」が反響呼ぶ</h2>
<p>発端となったのは、Redditユーザー Wasabi_Open 氏が投稿した「I made ChatGPT stop being nice and it’s the best thing I’ve ever done（ChatGPTに“いい人”をやめさせたら、最高の結果になった）」という<a href="https://www.reddit.com/r/PromptEngineering/comments/1okppqe/i_made_chatgpt_stop_being_nice_and_its_the_best/">スレッド</a>だ。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/I_made_Chat_GPT_stop_being_nice_375797ac64/I_made_Chat_GPT_stop_being_nice_375797ac64.jpg" alt="I made ChatGPT stop being nice.jpg" /></p>
<p>同氏はプロンプトの中で、ChatGPTに対し「私の意見を褒めたり慰めたりせず、誤りがあれば明確に指摘してほしい」「論理の矛盾を批判的に分析してほしい」と指示。これにより、ChatGPTが従来よりも率直で的確なフィードバックを返すようになったという。
この投稿は数千件のいいねを集め、「まるで冷静なメンターと議論しているようだ」とのコメントも寄せられた。</p>
<h2>SNSで広がった「nice filter」論争</h2>
<p>この現象を11月3日に<a href="https://x.com/markgadala/status/1985032100672618588">紹介</a>したのが、X（旧Twitter）のユーザー Mark Gadala 氏だ。同氏は「“nice filter”を外したらChatGPTの回答が劇的に改善した」と投稿し、多くのフォロワーが同様のプロンプトを試したと報告している。一方で、「フィルターを解除すると性能が上がる」という表現が拡散したことで、「内部制限を外す行為ではないか」との誤解も広がった。実際には、ChatGPTの内部に“nice filter”と呼ばれる設定は存在せず、プロンプトの指示文によって出力トーンが変わるだけだ。</p>
<h2>「当たり障りないフィルター」の正体</h2>
<p>OpenAIの設計方針によれば、ChatGPTは安全性と中立性を重視した“共感的”な初期設定を採用している。ユーザーが感じる「いい人フィルター」とは、この丁寧でポジティブに応答する傾向を指した比喩に過ぎない。つまり、「フィルターを外す」とは内部機能を解除するのではなく、プロンプトによってAIの口調や態度を再設定する行為だといえる。</p>
<h2>“辛口AI”の効用と注意点</h2>
<p>ユーザーの反応はおおむね好意的だ。「率直な批評を受けることで思考が整理された」「甘い同意よりも鋭い反論のほうが学びになる」といった意見が目立つ。一方で、「冷たく感じる」「会話がきつくなる」との声もあり、タスクや気分に応じてトーンを使い分ける重要性が指摘されている。専門家の間では、このようなトーン調整を「AIとの協働スキル」や「プロンプトリテラシー」の一環とみなす動きも広がっている。</p>
]]></description>
      <pubDate>Wed, 19 Nov 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>“自分の分身AI”同士を討論させると何が起きる？──筑波大学とMicrosoft、AIが媒介する新しい「自己省察」の学びを報告</title>
      <link>https://ledge.ai/articles/digital_human_debates_reflecting_with_ai</link>
      <description><![CDATA[<p>筑波大学とMicrosoftの研究者らは2025年11月17日、利用者自身の価値観や思考パターンを反映した「分身AI（Digital Human）」同士を討論させ、本人がその様子を傍観するという実験の結果をまとめた論文を<a href="https://arxiv.org/abs/2511.13046">公開</a>した。研究チームは、参加者がAIの能力理解を深めただけでなく、「自分ではない自分」を観察することで、自身の思考や価値観を客観的に見つめ直す新たな学習効果を確認したという。</p>
<h2>“自分の分身”を作り、その議論を観察するという新しい構図</h2>
<p>研究が扱ったのは、次の3つの視点の中間に位置する体験である。</p>
<ul>
<li>一次的視点（First-person）：自分が他者と直接対話する</li>
<li>三人称視点（Third-person）：第三者同士の会話を観察する</li>
<li>今回の視点（AI-mediated）：自分を反映した「Digital Me」が他人の「Digital You」と議論し、その様子を本人が観察する</li>
</ul>
<p>実験では、学生が自ら設計した分身AIが、自律的に討論を展開する。設計者の価値観や思考は反映されるが、議論の流れは完全には制御できない。この「似ているが、完全な自分ではない」という距離感が、客観的な自己観察を可能にするという。</p>
<p><strong>コミュニケーション構造の比較図</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x2_7_2d4879fc05/x2_7_2d4879fc05.png" alt="x2 (7).png" /></p>
<h2>Digital Human Debates（DHD）の設計：プロンプトとRAGで“自分”を埋め込む</h2>
<p>参加したのは中高生9名（3チーム）。それぞれが6か月にわたり、自分の分身AIを次の3要素で設計した。</p>
<ul>
<li>system_prompt_template.txt：性格・口調・議論戦略・思考様式</li>
<li>interview_transcript.txt：個人の価値観・経験・背景</li>
<li>Documents（RAG 文書）：議論に利用する外部知識（両立場の資料を収集）</li>
</ul>
<p>システムは GPT-4o と LangChain を基盤に構築され、音声入力→議論生成→音声合成→Lip-sync 動画生成までを自動化。</p>
<p><strong>システム構成図</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x4_1_5978703b2d/x4_1_5978703b2d.png" alt="x4 (1).png" /></p>
<h2>どのように討論したのか</h2>
<p>議論のトピックは、以下の4種類からルーレットでランダム選択された。</p>
<ul>
<li>高齢者の免許返納</li>
<li>リモートワークの恒久化</li>
<li>安楽死の合法化</li>
<li>ベーシックインカムの導入</li>
</ul>
<p>討論は約20分。Constructive speech、Cross-examination、Rebuttal など、全国高校ディベート選手権の形式に準じて進行し、勝敗は3名の審査員が判定した。</p>
<p><strong>討論フロー図</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x5_3_aa3dfafba5/x5_3_aa3dfafba5.png" alt="x5 (3).png" /></p>
<p>また、実際の議論分析では、分身AIの発話は以下の3層で構成されていたことが確認された。</p>
<ul>
<li>Personal Context：学生本人の経験や価値観（例：留学経験）</li>
<li>RAG-sourced Evidence：外部文書を参照した事実情報</li>
<li>AI-generated Insight：LLM が独自に構築した新しい主張・質問</li>
</ul>
<p><strong>発話構成の三層分析図</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x6_2_ebcfc4cb09/x6_2_ebcfc4cb09.png" alt="x6 (2).png" /></p>
<h2>発見1：AIが“自分の代わり”ではなく“自分の鏡”として機能する</h2>
<p>論文で最も強調されている成果は、参加者が 「Reflecting with AI」 と呼ばれる新たな学習体験を得た点である。</p>
<h3>「自分が話すより冷静に見られる」</h3>
<p>AIが自分の価値観を用いて議論するものの、その展開は必ずしも本人の想定どおりではない。
この“半自律性”が、次のような客観視を可能にした。</p>
<ul>
<li>「私は衝動的に話しがちだと気づいた」</li>
<li>「AIの方が論理がブレず、どこが弱いのかがわかった」</li>
</ul>
<p>参加者は AI の議論を自分とは別の存在として受け止めつつ、「しかし自分の思考が反映されている」と感じる。この微妙な距離感が、強いメタ認知効果を生んだという。</p>
<h2>発見2：プロンプト設計の違いが“AIの人格”に明確に表れる</h2>
<p>3チームのアプローチは大きく異なり、プロンプト設計の個性が発話に直接反映された。</p>
<ul>
<li><strong>Team A：徹底したキャラクター設計</strong> 「認知停止を破壊する」というテーマを持つ強烈な人格を設定し、攻撃的な論法を組み込んだ。</li>
<li><strong>Team B：繰り返し修正する“チューニング型</strong> 芥川龍之介やガンジーなど歴史人物をモチーフに、AIの出力を評価→修正する反復設計を採用。</li>
<li><strong>Team C：最小限の人格＋大量の論拠データ</strong> キャラ設定は控えめにし、RAGで大量の論理情報を与える“ロジック重視”型。</li>
</ul>
<p>いずれも、設計者の価値観や思考癖がそのままAIのふるまいに組み込まれていたという。</p>
<h2>発見3：AIリテラシー全体をカバーする学習効果</h2>
<p>研究チームは、今回の取り組みを“ジェネレーティブAIリテラシーの実践的な総合モデル”と位置づけている。
実際、参加者は以下の能力を幅広く活用していた。</p>
<ul>
<li>モデル特性の理解</li>
<li>論拠の収集と検証</li>
<li>プロンプト設計</li>
<li>出力の評価と改善</li>
<li>自己の思考の客観視（Reflecting with AI）</li>
</ul>
<p>ただし、倫理や法的側面の深い学習までは含まれておらず、今後の課題とされている。</p>
<p>研究では、「AIとの協働」ではなく “AIを通して自分を理解する” という新しい可能性を示しており、分身AIが自律的に議論する様子を観察することで、ユーザーは自らの論理構造・思考の癖を外在化し、客観的に省察できるという。研究チームは、この“Reflecting with AI”が今後のAIリテラシーにおける重要な能力になると位置づけている。</p>
]]></description>
      <pubDate>Wed, 19 Nov 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>生成AI画像に“著作権”成立と判断　千葉県警、無断複製で27歳男を書類送致──全国初の摘発</title>
      <link>https://ledge.ai/articles/gen_ai_image_copyright_case_chiba_police</link>
      <description><![CDATA[<p>千葉県警生活経済課は2025年11月20日、生成AIで作成された画像を無断で複製したとして、神奈川県大和市の無職の男性（27）を著作権法違反（複製権侵害）の疑いで千葉地検に書類送致したと<a href="https://www.police.pref.chiba.jp/kohoka/orders_prefecture_03461.html">発表</a>した。県警によれば、生成AIで作られた画像を著作物として扱い、著作権法違反で摘発するのは全国で初めてとみられる。</p>
<p>事件は8月25日昼頃に発生。男性は、千葉県我孫子市の男性が生成AIを用いて制作し、SNSに投稿していたコンピューターグラフィックスを、著作権者の承諾を得ずに外部サーバーへ送信し複製した疑いが持たれている。県警は、被害者が生成過程で入力した多数のプロンプト（指示）や内容、表現に至るまでの過程を確認し、創作性のある表現が認められるとして著作物性を判断した。</p>
<p>報道によれば、容疑者は複製した画像を自身が販売する電子書籍の表紙として使用していたとされる。容疑者は「作品に合う素材だった」と供述しているという。県警は、被害者が画像生成において具体的な指示を重ね、独自の表現に到達していた点を踏まえ、無断複製が著作権法上の侵害に当たると判断した。</p>
<p>著作権法では、人の創作的関与によって生まれた表現が保護の対象となる。生成AIが作成した画像については、どの程度の人間による関与が「創作性」として評価されるかが議論されてきた。今回の事案では、AIへの指示内容や生成工程が「人による具体的な創作行為」と認められた点が重要となる。</p>
<p>生成AIで作成された画像については、人の具体的な指示や制作過程がどの程度関与しているかが著作物性の判断要素とされる。同事案では、プロンプトの内容や生成工程に人の創作的関与が認められた点が、著作権侵害容疑の判断材料となった。SNS上で公開された生成AI画像であっても、創作性が認められる場合には著作権が成立するため、複製や利用には権利者の許諾が必要となる。</p>
]]></description>
      <pubDate>Wed, 19 Nov 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google、誰でも学べるAI学習サイト「Google Skills」を正式公開──Cloud・DeepMind・教育部門を横断する3000講座を展開</title>
      <link>https://ledge.ai/articles/google_skills_ai_learning_platform_launch</link>
      <description><![CDATA[<p>Googleは2025年10月21日（米国時間）、新しいAI学習プラットフォーム「Google Skills」を<a href="https://blog.google/outreach-initiatives/education/google-skills/">発表</a>した。同サイトでは、Google Cloud、Google DeepMind、Grow with Google、Google for Educationなど、同社の複数部門が提供してきた教育コンテンツを統合。3000種類を超えるAI関連の講座・体験ラボ・認定プログラムを、一元的に学べる学習拠点として開設された。</p>
<h2>AI教育の中核を担う新サイト</h2>
<p>Google公式ブログ「Start learning all things AI on the new Google Skills」によると、Google Skillsは“AI for Everyone（すべての人のためのAI）”をテーマに、誰もがAIスキルを体系的に学べるよう設計されている。初心者、エンジニア、企業リーダーなど幅広い層を対象に、AI、データ分析、クラウド、生成AIなど多様な分野を網羅。各コースはオンデマンド形式で受講でき、学習成果はLinkedInなどの外部プラットフォームで共有できる。提供内容には、Google Cloudの認定資格プログラムやAI Essentials シリーズ、DeepMindのAI倫理教材などが含まれる。</p>
<p>@<a href="https://www.youtube.com/watch?v=Qbix0BOPcgE">YouTube</a></p>
<p>今回の正式公開に先立ち、Google Cloudは10月10日付のブログ「Google Skills: Your new home for Google AI learning and more」で、新プラットフォームの構想を公表していた。当時は正式リリース前で、「AIやクラウドに関する学習リソースを一元化し、近日中に詳細を発表する」としていた。Gemini Code Assist（旧Duet AI for Developers）やQwiklabs（現Cloud Labs）と連携し、AIトレーニングの実践環境を統合する方針も示されていた。</p>
<h2>3000超のコースと実践的ラボを集約</h2>
<p>Google Skillsでは、Googleがこれまで個別に展開してきた学習リソースを一か所に集約。AIモデル開発、クラウド基盤運用、データ可視化、サイバーセキュリティなど、実践重視の3000超のコースとラボを提供する。一部コンテンツは無料で公開され、修了証や認定資格を取得することでキャリア開発にもつなげられる。また、組織向けにはチーム単位での進捗管理や学習成果の可視化機能も用意されている。</p>
<h2>今後の展望──教育機関・企業研修にも拡大へ</h2>
<p>Googleは今後、教育機関や企業研修への展開を進める方針を示しており、AIスキルの標準教育基盤としての活用を目指す。
公式ブログでは、「AI教育へのアクセスを民主化し、誰もがテクノロジーの未来を形づくる機会を得られるようにする」としている。
同社は今後もDeepMindやCloud AIチームの最新教材を追加し、AI人材育成をグローバルに推進する考えだ。</p>
]]></description>
      <pubDate>Wed, 19 Nov 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>LLMはSNSの“ジャンク投稿”で脳が腐る？──米研究チームが「Brain Rot（脳腐敗）仮説」を提唱、推論力や安全性の劣化を確認</title>
      <link>https://ledge.ai/articles/llm_brain_rot_hypothesis_ai_junk_data_risk</link>
      <description><![CDATA[<p>テキサス大学オースティン校、テキサスA&amp;M大学、パデュー大学の研究者チームは2025年10月15日、LLM（大規模言語モデル）がSNSに溢れる “ジャンクデータ” によって徐々に劣化する可能性を示す研究「LLMs Can Get “Brain Rot”!」を<a href="https://llm-brain-rot.github.io/">発表</a>した。</p>
<p>研究チームは、X（旧Twitter）から収集した投稿データを用いた制御実験を行い、低品質テキストの混入が推論力・長文理解・安全性・人格的特性に一貫して悪影響を与えると報告している。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/teaser_cad83675ae/teaser_cad83675ae.png" alt="teaser.png" /></p>
<h2>SNSの“低品質テキスト”がモデル品質を損なう可能性</h2>
<p>研究チームが着目したのは、Web全体に含まれるユーザー生成コンテンツ、とりわけXに特徴的な「エンゲージメントは高いが内容が浅い投稿」だ。
これらは</p>
<ul>
<li>誤情報</li>
<li>攻撃的な表現</li>
<li>情報量の乏しい短文</li>
</ul>
<p>などが混在しやすく、AIモデルが大量に取り込むとどうなるかは十分に検証されてこなかった。</p>
<p>今回の研究では、こうした投稿を「ジャンクデータ」として体系的に扱い、その影響を測定するための仮説を「LLM Brain Rot（脳腐敗）仮説」と名づけた。</p>
<h2>実験方法：X投稿を「品質」と「人気度」で分類しモデルを再学習</h2>
<p>研究者らは、Xの投稿を以下の2軸で分類した。</p>
<ul>
<li>テキストの品質（semantic quality）</li>
<li>エンゲージメント（人気度）</li>
</ul>
<p>この組み合わせで複数のデータセットを構築し、既存のオープンソースLLMに対して再学習（fine-tuning）を実施。
評価には、</p>
<ul>
<li>推論タスク（ARC Challenge、ARC-Easy など）</li>
<li>長文コンテキスト理解（RULER）</li>
<li>安全性（有害発言誘発テスト）</li>
<li>“人格特性”に関する心理尺度（ナルシシズム、マキャベリズム、サイコパシーなど）</li>
</ul>
<p>が利用された。</p>
<h2>主な結果①：推論・読解能力が段階的に悪化</h2>
<p>研究では、低品質テキストの割合を増やすほど、推論・長文理解タスクのスコアが段階的に低下することが確認された。研究者らは変化の様子を「dose–response（用量反応）」に似ていると指摘している。</p>
<p>この “劣化の全体像” は、下図のように整理されている。</p>
<p><strong>〈図1〉推論タスク・人格特性の変化（Effective Size）</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/effective_size_82aa2fbc8f/effective_size_82aa2fbc8f.png" alt="effective_size.png" />
低品質SNSデータで再学習すると、推論・読解（左）が悪化し、人格特性（右）も望ましくない方向へ変化する傾向が示された。</p>
<p>この図が示すように、</p>
<ul>
<li>ARC Challenge</li>
<li>RULER</li>
<li>HH-RLHF（安全性関連）</li>
</ul>
<p>ではいずれもスコアが低下。また人格特性に関しても、ナルシシズム・サイコパシー傾向などが上昇する結果となった。</p>
<p>さらに、モデルがどのように失敗するようになったかも分析されている。</p>
<p><strong>〈図2〉失敗パターンの増加（Failure Count）</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/failure_mode_barplot_count_f4b1cff90d/failure_mode_barplot_count_f4b1cff90d.png" alt="failure_mode_barplot_count.png" />
低品質データで再学習したモデルほど、思考プロセスの失敗（“考えていない”“誤った論理”“事実誤認”など）が大幅に増加した。</p>
<p>この図が表している通り、</p>
<ul>
<li>no thinking（思考が全く行われない）</li>
<li>wrong logic in plan（計画の論理破綻）</li>
<li>factual error（事実誤認）</li>
</ul>
<p>などの“思考の崩壊”が顕著に増加している。</p>
<h2>主な結果②：安全性と“人格特性”にも悪影響</h2>
<p>推論能力の低下だけでなく、モデルの「振る舞い」や「性格的傾向」にまで変化が現れた。</p>
<ul>
<li>有害な発言を誘発しやすくなる</li>
<li>攻撃的・支配的・自己中心的な回答を返す傾向が強まる</li>
<li>不正確な主張を自信満々に述べるケースが増える</li>
</ul>
<p>研究チームはこれを、SNS特有の“攻撃性・扇動性のある投稿”を多く学習した結果として説明している。</p>
<h2>“主な結果③：“脳腐敗”は簡単には治らない</h2>
<p>モデルの性能が落ちたあとに、高品質データで“洗浄（wash-out）”すれば回復するのではないか──
研究者らはこの仮説も検証した。</p>
<p>結果は次の通り。</p>
<p><strong>〈図3〉洗浄（wash-out）実験：完全には回復しない</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/wash_out_scaling_1114dafe8f/wash_out_scaling_1114dafe8f.png" alt="wash_out_scaling.png" />
ジャンクデータで劣化したモデルを高品質データで再学習しても、ARC-CやRULERなど複数タスクで性能が回復しきらず“残留劣化”が確認された。</p>
<p>グラフが示すように、洗浄後のモデルは“部分的な回復”こそ見られるものの、元の性能には戻らなかった。
研究者らはこれを「残留ダメージ（residual damage）」と呼び、“悪いデータを食べさせると、後から取り返しがつかない”可能性を指摘している。</p>
<h2>ChatGPT・Gemini・Claude・Grokにも影響しうる？</h2>
<p>研究はオープンソースLLMを用いた実験だが、論文は「Webテキストを学習するすべてのLLMに関わる問題」だと指摘している。</p>
<p>ChatGPT、Gemini、Claude、Grok など商用モデルがどの程度SNSテキストを取り込んでいるかは公開されていないが、Web全体のクロールデータを使う以上、“ジャンク比率”がモデル品質を左右する可能性は避けられないとする。</p>
<h2>今後の焦点：AIの“健康診断”として利用される可能性</h2>
<p>研究チームは、</p>
<ul>
<li>データ品質フィルタリングの強化</li>
<li>SNS由来データの評価と管理</li>
<li>「Brain Rot」兆候の定期的検査</li>
</ul>
<p>などを訓練パイプラインに組み込む必要性を訴える。</p>
<p>また、GitHub上では実験コードが公開されており、企業が自社モデルで同様の検証を再現できるようになっている。研究者らは、「SNS時代のデータ汚染がAIモデルに及ぼす影響を体系的に測定する第一歩になった」としており、今後は 他言語・他SNS・他文化圏データでの再検証が進むと見られる。</p>
]]></description>
      <pubDate>Wed, 19 Nov 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>アルトマン氏「エロティックばかり注目されたけど」──ChatGPT、成人ユーザーの自由拡大へ</title>
      <link>https://ledge.ai/articles/openai_chatgpt_adult_mode_update_oct2025</link>
      <description><![CDATA[<p>OpenAIのサム・アルトマンCEOは10月14日（現地時間）、X（旧Twitter）上で、ChatGPTの安全制限を一部緩和し、成人認証済みユーザーに対してエロティックな会話を許可する方針を<a href="https://x.com/sama/status/1978129344598827128">発表</a>
した。</p>
<p>投稿は瞬く間に注目を集め、「エロティック解禁」が大きな話題となったが、アルトマン氏は翌日に「その部分ばかり注目されてしまったが」と<a href="https://x.com/sama/status/1978539332215681076">補足</a>し、実際には“より人間らしいAI体験”を実現するための包括的な方針変更であることを強調した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/gpt5_erotica_7cea863241/gpt5_erotica_7cea863241.jpg" alt="gpt5 erotica.jpg" /></p>
<h2>安全を優先してきたChatGPTの制限</h2>
<p>ChatGPTはこれまで、性的表現や親密な会話を含むコンテンツを厳しく制限してきた。
アルトマン氏は「メンタルヘルス問題に慎重を期すためだった」と説明し、精神的に不安定なユーザーに配慮した措置であったと振り返った。
「深刻な危機状態にあるユーザーは別扱いとし、他者に害を与える行為は依然として許可しない」と述べ、ポリシーの根幹は維持されるとしたうえで、「リスクのない成人ユーザーにはより多くの自由を与える」と明言した。</p>
<h2>“4oらしさ”を再導入──人間的なAI体験へ</h2>
<p>アルトマン氏は同日、「数週間以内に“4oで好まれた振る舞い”に近い人格（パーソナリティ）を選べる新バージョンのChatGPTを提供する」と投稿した。
GPT-4oは会話の自然さや表情豊かな応答で人気を集めたモデルであり、今後はユーザーが望む場合に、フレンドリーな口調や絵文字を多用した“人間らしい”対話スタイルを選べるようになる。
アルトマン氏は「これは利用時間を増やすためではなく、ユーザーが自分の望む形でAIと関わる自由を得るための設計だ」と述べている。</p>
<h2>「成人は大人として扱う」──12月に年齢認証を本格導入</h2>
<p>アルトマン氏は、12月に年齢認証を本格導入し、認証済み成人ユーザーに対してはエロティック会話なども許可する方針を示した。
一方で、ティーンエイジャーに対しては「安全をプライバシーや自由より優先する」と述べ、メンタルヘルス関連ポリシーは緩めないと強調している。</p>
<p>アルトマン氏は、「社会がR指定映画で境界を設けるように、AIにも適切な年齢境界を設けたい」と例え、「我々は選挙で選ばれた道徳警察ではない」と付け加えた。</p>
<h2>倫理と自由の境界線</h2>
<p>アルトマン氏の発言は、AIにどこまで人間的な自由を与えるかという議論を再燃させた。</p>
<p>OpenAIは今後、成人向け表現やAIの人格設計に関するガイドラインをさらに明確化するとみられる。今回の方針転換は、「AIをどう設計し、どう育てるか」という人間社会全体のテーマに踏み込む第一歩となりそうだ。</p>
]]></description>
      <pubDate>Wed, 19 Nov 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google、気象予報AI「WeatherNext 2」を発表──従来モデルの8倍速で予報を生成し、最大1時間までの解像度を実現</title>
      <link>https://ledge.ai/articles/weathernext_2_google_ai_weather_forecasting_model_release</link>
      <description><![CDATA[<p>Googleは2025年11月17日（現地時間）、気象予報向けAIモデル「WeatherNext 2」を<a href="https://blog.google/technology/google-deepmind/weathernext-2/">発表</a>した。天気がサプライチェーンや航空路、日常の移動など幅広い領域の意思決定に影響を与えることを指摘し、近年のAI技術が予報能力を大きく向上させているという。</p>
<h2>気象予報の高速化と高解像度化</h2>
<p>WeatherNext 2は、Google DeepMindとGoogle Researchが開発した予報モデルで、従来のWeatherNextモデルと比べ8倍速で予報を生成し、最大1時間解像度の予測に対応する。公式ブログでは、WeatherNext 2を「最も高度で効率的な予報モデル」と位置づけている。</p>
<p>@<a href="https://www.youtube.com/watch?v=YQwqoEm_xis">YouTube</a></p>
<h2>数百のシナリオ生成に対応</h2>
<p>WeatherNext 2は、一つの予報結果だけでなく、数百の可能性（シナリオ）を生成する仕組みを備える。これにより、気象機関が複数のシナリオを比較しながら判断できる環境を提供する。Googleは実験的なサイクロン予測を通じて、この技術が意思決定の支援に使われた例を紹介している。</p>
<p><strong>WeatherNext 2 によるシナリオ生成の仕組み：</strong> 異なるランダム性を加えて複数の予報をつくり、時間の経過と共に分岐が広がっていく
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Weather_Next_2_blog_figure_03_lar_width_1000_format_webp_8aa488b043/Weather_Next_2_blog_figure_03_lar_width_1000_format_webp_8aa488b043.webp" alt="WeatherNext_2-blog-figure-03_lar.width-1000.format-webp.webp" /></p>
<h2>研究成果の提供範囲を拡大</h2>
<p>GoogleはWeatherNext 2の研究成果を「研究室から実環境へ展開する」として、以下のサービスで利用可能にした。</p>
<ul>
<li>Earth Engine：気象データ分析向け</li>
<li>BigQuery：予測データの活用拡大</li>
<li>Vertex AI：カスタム推論に使える早期アクセスプログラムを提供</li>
</ul>
<p>企業や研究者が独自の予報処理を行える仕組みが整備されつつある。</p>
<h2>Googleサービス全般の天気機能を更新</h2>
<p>WeatherNext技術は、Googleの各種サービスでも順次反映されている。</p>
<ul>
<li>Google検索</li>
<li>Gemini</li>
<li>Pixel Weather</li>
<li>Google Maps Platform の Weather API</li>
</ul>
<p>これらの天気情報は、WeatherNext 2の技術に基づいた内容にアップグレードされた。Googleは「数週間以内にGoogleマップにも反映される」としている。</p>
<h2>技術基盤：Functional Generative Networks（FGN）</h2>
<p>WeatherNext 2の基盤となる仕組みは、DeepMindが開発したFunctional Generative Networks（FGN）と呼ばれる技術だ。FGNは「大気の動き方そのものを学習し、時間とともにどのように変化していくかを再現する」点が特徴とされる。これにより、台風の渦の広がり方や降水帯の移動といった複雑で変化の激しい気象現象も、連続した流れとして予測しやすくなる。</p>
<p><strong>FGNの生成プロセスの概念図</strong> ：複数モデルとノイズ注入を組み合わせ、異なる気象シナリオを生成する
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/fgn_df84086054/fgn_df84086054.jpg" alt="fgn.jpg" /></p>
<h2>従来モデルの補完としての位置づけ</h2>
<p>Googleは、AIモデルが既存の物理ベースの気象モデルを完全に置き換えるものではないと明記している。気象予測には依然として課題が多く、AIは公式予報を補完する技術として設計されている。</p>
<p>WeatherNext 2は、予報精度の向上、生成速度の高速化、シナリオ生成機能の拡張など、気象予測に関する複数の要素を強化したモデルとして発表された。Googleは今後、研究機関や産業分野との協力を通じ、予報技術の提供領域を拡大するとしている。</p>
]]></description>
      <pubDate>Wed, 19 Nov 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/10/27 [MON]2025年のAIトレンドを総ざらい！Ledge.ai年末年始特集「&apos;25to&apos;26」事前登録スタート</title>
      <link>https://ledge.ai/articles/25to26-announce</link>
      <description><![CDATA[<p>AIの社会実装を加速させ、「テクノロジーを社会になめらかに浸透させる」ことをミッションに掲げる、国内最大級のAIメディア「Ledge.ai」を運営する株式会社レッジは、今年も年末年始特集「'25to'26」を公開します。
本日より先行サイトを公開し12月1日（月）の特集サイト公開までの間、お知らせを受け取ることができるようになる事前登録（無料）を受付開始いたしました。</p>
<p>:::button
<a href="https://25to26.ledge.ai/lp">事前告知サイトはこちら</a>
:::</p>
<p>2025年を締めくくるにふさわしい、AIの今とこれからを網羅した一大特集。研究者、ビジネスリーダー、エンジニアなど、あらゆる立場の方々に向けて、2026年のAIシーンを展望します。</p>
<h2>Ledge.ai年末年始特集『'25to'26』とは</h2>
<p>Ledge.ai年末年始特集は、2025年のAI関連ニュースや注目のキーワード、2026年以降の動向など、AIの初心者から専門家まで幅広く楽しめる特集サイトです。</p>
<p>2025年は、生成AIが実用フェーズに突入し、業務プロセス・プロダクト・教育・クリエイティブなど、社会のあらゆる分野で“AI活用の当たり前化”が進んだ一年でした。
そして2026年は、AIという概念そのものが提唱された「ダートマス会議」から70周年という、まさに歴史的な節目を迎えます。2025年の「当たり前化」を土台として、AIは社会インフラのように深く浸透し、その活用範囲の拡大と同時に、AGI（汎用人工知能）の実現可能性など、AIの“次なる進展”に向けた探求が本格化する一年となるのではないでしょうか。</p>
<p>本特集では、そんな激動の2025年を多角的に振り返りつつ、2026年に向けた新たな潮流やビジネスチャンスを展望します。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/1_ac8b0aef2e/1_ac8b0aef2e.png" alt="1.png" /></p>
<h2>コンテンツラインナップ紹介</h2>
<h3>編集部による徹底解説</h3>
<p>Ledge.ai編集部が、2025年のAIシーンを多角的に総括。
1年間の主要ニュースをピックアップしながら、トレンド分析と俯瞰的な視点で、AI技術が社会・産業へどのように浸透したのかを読み解きます。
さらに、技術動向の深掘り解説を通じて、進化の本質を明らかに。
2026年に向けて押さえておくべき“AIの現在地”を、独自の視点で整理します。</p>
<h3>独自インタビュー</h3>
<p>本特集では、「AI 70th Pre-Anniversary」というテーマのもと、AI研究の歴史・現在・未来をつなぐキーパーソンたちにインタビューを実施。
過去／現在／未来のそれぞれの視点から、AIがどのように発展し、次の時代にどんな可能性を秘めているのかを語ってもらいます。
世代と分野を超えて交わる知見が、AIの軌跡と未来へのヒントを照らし出します。</p>
<h3>トップランナー企業動向</h3>
<p>国内外の注目企業をピックアップし、AI周辺で押さえておきたい企業の最新動向を徹底分析。
生成AI、AIエージェント、クラウドAIなど、世界最先端の情報と実践事例に触れることで、読者が“次に取るべき一手”を見極められる構成になっています。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/2_6180f8a0c5/2_6180f8a0c5.png" alt="2.png" /></p>
<h2>開催概要</h2>
<p>イベント名：Ledge.ai年末年始特集「'25to'26」
開催期間：2025年12月1日(月) - 2026年1月9日(金)
形式：オンライン
参加費：無料（※一部のコンテンツ閲覧にはプロフィール登録が必要となります。）
お問合せ：contact@ledge.co.jp
URL：<a href="https://25to26.ledge.ai/lp">https://25to26.ledge.ai/lp</a></p>
]]></description>
      <pubDate>Mon, 27 Oct 2025 01:50:00 GMT</pubDate>
    </item>
  </channel>
</rss>