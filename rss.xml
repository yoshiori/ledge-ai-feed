<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
  <channel>
    <title>Ledge.ai 新着記事</title>
    <link>https://ledge.ai/</link>
    <description>Ledge.ai の最新テクノロジー記事</description>
    <item>
      <title>OpenAIとの買収交渉が決裂、WindsurfはGoogleと契約締結──CEOらはDeepMindに移籍、契約額は約24億ドルとの報道も</title>
      <link>https://ledge.ai/articles/windsurf_google_deal_openai_exit</link>
      <description><![CDATA[<p>2025年7月11日、AIコードエディター「Windsurf Editor」を手がけるAIスタートアップWindsurfは、Googleとライセンス契約を締結したと公式ブログで<a href="https:%5Cu002F%5Cu002Fwindsurf.com%5Cu002Fblog%5Cu002Fwindsurfs-next-stage">発表</a>{target=“_blank”}した。同時に、ヴァルン・モハンCEO、共同創設者のドウグラス・チェン氏を含む一部の研究開発チームが、GoogleのAI研究機関DeepMindに移籍することも明らかにされた。複数の米メディアは、この契約総額が約24億ドル（約3,500億円）にのぼると報じている。</p>
<p><img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002Fwindsurf_blog_next_stage_c7a135ce6c%5Cu002Fwindsurf_blog_next_stage_c7a135ce6c.jpg" alt="windsurf blog next stage.jpg" /></p>
<h2>OpenAIとの買収交渉は期限切れで失効</h2>
<p>事情に詳しい関係者によれば、OpenAIは2024年末からWindsurfの買収を協議していた。取引規模は約30億ドルと<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fopenai_acquires_windsurf_for_ai_dev_tools_expansion">報じられていた</a>{target=“_blank”}が、交渉は複雑化し、2025年7月10日の契約期限までに最終合意に至らなかった。背景には、OpenAIと主要出資元であるMicrosoftの関係や、GitHub Copilotとの競合調整が影響したとみられる。</p>
<p>なお、Windsurfは独立性を重視するスタンスを維持していたことも、交渉の難航に一因があったとみられている。</p>
<h2>Googleとの24億ドル契約、製品ライセンス供与が主眼</h2>
<p>買収が成立しなかった一方で、WindsurfはGoogleと新たな契約を結んだ。同社公式ブログによると、この契約はWindsurfが保有するAIコード生成技術の一部をGoogleに「非独占的ライセンス」として供与する内容であり、株式取得や企業統合は含まれていない。</p>
<p>Windsurfは今後も企業向けのAIコード編集ツールを独立して提供し続けると明言しており、既存顧客に対してはサポート体制を維持するとしている。</p>
<h2>DeepMindが主要人材を吸収、Geminiの開発加速へ</h2>
<p>契約の一環として、Windsurfの共同創業者ら主要メンバーがGoogle DeepMindに加わる。対象となるのはCEOのヴァルン・モハン氏、共同創設者ドウグラス・チェン氏のほか、研究開発チームの中核メンバーとされる。</p>
<p>Google DeepMindのCEOであるデミス・ハサビス氏は、自身のX（旧Twitter）上で「彼らの参加に非常に興奮している」とコメントし、Geminiの“エージェンティック・コーディング”機能開発に注力すると明かしている。</p>
<p><img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002Fdemis_hassabis_x_about_windsurf_caaaa3b8fb%5Cu002Fdemis_hassabis_x_about_windsurf_caaaa3b8fb.jpg" alt="demis hassabis x about windsurf.jpg" /></p>
<h2>Windsurf社内体制は新たな経営陣へ移行</h2>
<p>人材移籍にともない、Windsurfでは経営体制の再編が行われた。新たにCOOのJeff Wang氏が暫定CEOに就任し、Graham Moreno氏が社長職を務める。今後は企業向け機能の強化や、新規パートナーシップの拡大に注力する方針を掲げている。</p>
<h2>業界背景：コード支援AIを巡る競争が激化</h2>
<p>AIを活用したコード補完・生成の分野では、GoogleのGemini、OpenAIのGPT-4o Turbo、MicrosoftのGitHub Copilot、MetaのCode Llamaなどが競合している。いずれも、ソフトウェア開発の生産性向上を狙いとした戦略的領域であり、優れたAI技術と人材の獲得が競争力の源泉となっている。</p>
<p>今回のWindsurfとのライセンス契約と人材移籍は、Googleがこの領域での優位性をさらに高める布石と捉えられている。</p>
]]></description>
      <pubDate>Mon, 14 Jul 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Hugging Face、「SmolLM 3」公開──小型・多言語・長文対応の3B Reasoningモデル、ONNX版も無償提供</title>
      <link>https://ledge.ai/articles/smollm3_128k_multilingual_reasoning_model</link>
      <description><![CDATA[<p>Hugging Faceは2025年7月8日、新たな小型言語モデル「SmolLM 3」をHugging Face HubとGitHubで<a href="https:%5Cu002F%5Cu002Fhuggingface.co%5Cu002Fblog%5Cu002Fsmollm3">無償公開</a>{target=“_blank”}した。パラメータ数は30億（3B）で、128kトークンの長文入力に対応し、命令文内の「\u002Fthink」や「\u002Fno_think」フラグによって推論過程を切り替える機能を持つ。英語・フランス語・スペイン語・ドイツ語・イタリア語・ポルトガル語の6言語に最適化され、ツールコーリング機能も実装されている。公開直後にはONNX版や量子化済みチェックポイント、訓練レシピ、学習データセットなども順次提供されており、エッジ推論や再学習など幅広いユースケースに対応可能となっている。</p>
<h2>モデルの概要</h2>
<p>SmolLM 3は、Hugging Faceが開発した小型のデコーダ専用LLMで、以下の4つの特徴を備える。</p>
<ul>
<li>128kトークンの長文入力に対応（NoPEとYaRNを組み合わせた構成）</li>
<li>“\u002Fthink”推論切替機能により、タスクに応じて思考出力の有無を制御</li>
<li>6言語対応かつツールコーリングに標準対応（code／json）</li>
<li>Apache 2.0ライセンスによる完全オープンな学習レシピとデータの提供</li>
</ul>
<p><strong>図1：SmolLM 3 “Blueprint”──モデル構造、訓練レシピ、評価指標、利用方法をまとめた公式チャート</strong>
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002Fsmollm3_whiteprint_360c69e64f%5Cu002Fsmollm3_whiteprint_360c69e64f.jpg" alt="smollm3-whiteprint.jpg" /></p>
<h2>長文処理と“\u002Fthink”モード</h2>
<p>SmolLM 3は、約50万文字（A4約250ページ）に相当する128kトークンの入力を一括処理できる。これは、文書検索や契約書分析、技術資料の要約といったRAG用途において、前処理なしで大量文書を直接投入できることを意味する。</p>
<p>また、「\u002Fthink」モードを指定すれば、モデルは推論プロセスをステップバイステップで展開して出力する。一方で「\u002Fno_think」を指定すれば最終的な結論のみを返す挙動となる。この機能により、ユーザーは計算リソースや応答速度のトレードオフをタスクごとに柔軟に調整できる。</p>
<p><strong>図2：\u002Fthink」有無による各タスク成績。思考展開を出力した場合（左列）に高難度タスクの精度が大幅に向上</strong>
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002Fimage2832_29_10b053fea3%5Cu002Fimage2832_29_10b053fea3.jpg" alt="image2832%29.jpg" /></p>
<h2>ベンチマークと性能評価</h2>
<p>Hugging Faceが公開した12種のベンチマークによると、SmolLM 3は同規模のLlama-3 3BやQwen 2.5 3Bを一貫して上回り、Qwen 3 4Bにも一部タスクで迫る結果を示している。特に多言語タスク（Flores、Global MMLUなど）において顕著な優位性を示しており、6言語の平均スコアはすべての同規模モデルを上回った。</p>
<p><strong>図3　5言語平均ベンチマークの比較。SmolLM 3（黄）は同サイズモデルより高いスコアを示した</strong>
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002Fimage2830_29_89a5d27e19%5Cu002Fimage2830_29_89a5d27e19.jpg" alt="image2830%29.jpg" /></p>
<h2>公開リソースと利用方法</h2>
<p>SmolLM 3は、Base版・Instruct版のほか、ONNX形式や量子化（q4f16）済みモデルも提供されており、Transformers.jsやONNX Runtimeを使ってブラウザやスマートフォン上でも即時に推論を行うことが可能だという。</p>
<p>また、GitHub上にはnanotronベースの訓練構成・学習スケジュール・ablation結果がまとめられており、再学習や検証用途に活用できる。プリトレーニングで使用された11.2兆トークン相当の学習データセットも段階的に公開が進められている。</p>
<h2>想定ユースケース</h2>
<p>SmolLM 3の設計は、コスト・制御性・ライセンスの観点から、以下のような用途に適するとされる。</p>
<ul>
<li><strong>検索拡張生成（RAG）</strong> ：128k長文対応で検索精度と応答一貫性を高める</li>
<li><strong>オンデバイスAI</strong> ：4GBメモリ対応の量子化モデルでスマートデバイスやエッジ環境でも運用可能</li>
<li><strong>社内ナレッジボット</strong>：推論切替と多言語対応でグローバル対応FAQを効率化</li>
<li><strong>検証・研究</strong> ：OSSライセンスとフルレシピ公開により再現性・改良が容易</li>
</ul>
<h2>今後の展開</h2>
<p>Hugging Faceは、SmolLM 3の学習中間チェックポイントや派生モデルの公開も予定しており、将来的にはVision対応モデル「SmolVLM 3」や1.7B／360M規模の軽量版も検討していることを明らかにしている。現時点では、3Bクラスのモデルで長文処理、推論モード切替、多言語対応、完全OSSの4要素を同時に満たすモデルは他に例がなく、オープンソースLLMの選定基準に新たな軸を提供する形となっている。</p>
]]></description>
      <pubDate>Sun, 13 Jul 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>AIの基盤モデルはどこまで進化したのか？　2025年前半の主要LLMを俯瞰する</title>
      <link>https://ledge.ai/articles/expo-2025-summer-2025-first-half-llm</link>
      <description><![CDATA[<p>現在、AIの基盤モデル（特に大規模言語モデル：LLM）の開発競争は、主要プレイヤーによる覇権争いが激化している。OpenAIのGPTシリーズを筆頭に、GoogleやMetaなどのビッグテックが先行するなか、2025年1月には中国発のDeepSeekがダークホースとして鮮烈な登場を果たし、勢力図に新たな動きをもたらした。本記事では、2025年5月末時点の情報から、2025年上半期の代表的なLLM進化を概観する。</p>
<h2>2024年のLLMの進化</h2>
<p>まず、2024年のLLMの進化についておさらいしておこう。2024年は各社から様々なモデルがリリースされ、Ledge.aiでも多くのニュースを取り上げた。その中で特に象徴的な「GPT-4o」と「o1」について振り返っておく。</p>
<h3>マルチモーダル能力が大幅に進化したモデル「GPT-4o」</h3>
<p>2024年5月13日にリリースされたGPT-4oは、従来のテキストや画像に加え、音声や映像といった様々なデータ形式（モーダル）を、高速かつ高精度に処理する能力を有するモデルだ。</p>
<p>GPT-4oの進化点として特筆すべきは統一的なアーキテクチャである。従来のマルチモーダルLLMは、テキストや画像、音声などの異なるデータを処理する場合、個別のモデルで処理した結果を連結させ、最終的な応答を出力していた。一方、GPT-4oでは様々なデータで訓練された単一モデルを用いることで、異なるデータの処理の一本化を可能にし、入出力の自由度が大幅に拡大された他、入力から出力までのスピードを各段に上げたのである。ユーザー側からは見えないが、これがGPT-4oが残した大きな成果である。
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002Fopenai_gpt4o_multimodal_flow_367f3c73be%5Cu002Fopenai_gpt4o_multimodal_flow_367f3c73be.png" alt="openai-gpt4o-multimodal-flow.png" /></p>
<h3>高度な推論能力を持つOpenAIの最新モデル「o1」</h3>
<p>o1は、2024年9月12日にリリースされたモデルであり、これまでのモデルと比較すると「推論」の能力が飛躍的に強化された。o1の特徴は、応答結果を出力するまでに内部で思考を連鎖的に行うところにある。そのため、結果を出力するために一定の時間を要するが、熟考を重ねることで高い精度で推論タスクを実行することが可能になった。特に、数学や化学、プログラミングといった領域では、専門家に匹敵するレベルといわれている。</p>
<p><strong>推論に関するスケーリング則</strong>
o1で特筆すべきは、モデルの推論時間に応じて回答精度が向上するという、新たなスケーリング則の発見である。一般的にスケーリング則とは、【データ量】【計算量】【パラメータ数】の3つの要素とモデルの性能には比例関係があり、データや計算資源を大規模にすることで、AIの性能が上がるという法則である。OpenAIが発表した新たなスケーリング則は、出力を生成する推論時の計算量を増やすことで、回答精度が向上するというものだ。</p>
<h2>2025年のLLMの進化（2025年1月～5月）</h2>
<p>LLMの進化を追う上でベンチマークとなるのがGPTシリーズだ。2025年上半期の動向として、今回は昨年末に発表した「o3」を加えつつ、今年リリースされた「o4-mini」「GPT-4.5」「4o Image Generation」の情報を整理する。</p>
<h3>Omniシリーズ「o3」「o4-mini」</h3>
<p>OpenAIはOmniシリーズの最新版「o3」を2024年12月に発表。この時点では一般公開はされていなかったが、2025年4月に「o3」と「o4-mini」をリリースした。</p>
<p>o3の最大の特長は高速かつ低コストな推論性能を備えている点だ。コンテキスト長は公開されていないが、o3のベンチマーク評価時には、256,000トークンのコンテキスト長で評価されていることが発表されており、従来モデルより長い文章を扱える。また、マルチモーダル化が進んだ点も大きな特徴で、画像や図表などの視覚情報への対応も強化されており、既に実用段階に達している。</p>
<p>o3は、各技術ベンチマークで好成績を納めている。例えば、ソフトウェア開発のベンチマーク「SweetBench Verified」での正解率は71.7%であり、これは上記で紹介した「o1」の正解率48.9%を超えた。また、数学オリンピックレベルの試験「Amy」では、96.7％の正解率を記録している。</p>
<p>o4-miniは、軽量かつ高性能な小型モデルで、モデルサイズを抑えることで、低コストかつ高速な応答が可能となり、多数のリクエストにも安定して対応できるという。数学、プログラミング、科学知識などの分野で優れており、AIME 2024および2025といった数学ベンチマークで最高の性能を記録したと報告されている。</p>
<p><img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002F2025_05_18_183334_558a142e8d%5Cu002F2025_05_18_183334_558a142e8d.png" alt="2025-05-18 183334.png" /></p>
<p>OpenAIによると、社外専門家の評価では、両モデルはWeb情報を取り入れたことで、指示への追従精度が向上し、より有用で検証可能な回答を提供できるようになったという。また、メモリ機能や過去の会話履歴を活用することで、従来のモデルと比較して会話も自然な印象を与えるようになった。全体として、知性や応答の関連性において大きく進化していると評価されている。</p>
<h3>2025年2月28日リリース「GPT-4.5」</h3>
<p>GPT-4.5は、GPT-4シリーズの発展系としてリリースされたものであり、ユーザーとの対話において非常にスムーズに、かつ人間らしい応答をする特徴がある。ユーザーの意図をより正確に解析し、論理的かつ整合性のある文章を生成できる点が最大の進化ポイントである。</p>
<p>@<a href="https:%5Cu002F%5Cu002Fwww.youtube.com%5Cu002Fwatch?v=cfRYp0nItZ8">YouTube</a></p>
<p>Open AIがYouTubeにアップしているGPT-4.5の紹介動画だが、1つの問いかけをGPT-4.5とo1に投げかけた時の返答の違いについてデモを行っている。</p>
<p>OpenAIの社員が投げかけた以下のプロンプト</p>
<p>これに対し、GPT-4.5は以下のように返答した。（冒頭部分のみ抜粋）</p>
<p>一方、o1は以下のように返答した。</p>
<p>この比較からもわかるように、GPT-4.5はユーザーの感情に寄り添う“共感力”を備えた応答スタイルが特徴である。
また、GPT-4.5は生成AIの課題である「ハルシネーション」の抑制にも成功しており、SimpleQAのスコアを見ると、“でたらめな答え”を返した割合は37.1%に抑えられ、他のモデルより優れた結果となっていることがわかる。</p>
<p><img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002Fsimple_QA_1_2144e3792c%5Cu002Fsimple_QA_1_2144e3792c.png" alt="simpleQA_1.png" /></p>
<p>ただし、GPT‑4.5は深い推論を行う「o1」や「GPT4o」などのOmniシリーズとは性質が異なるとされている。今後は、GPT‑4.5のような事前学習型モデルと、Omniシリーズのような推論型モデルが、それぞれの特性を活かしながら相互補完的に活用されていくことが期待される。</p>
<h3>2025年3月25日リリース「4o Image Generation」</h3>
<p>「4o Image Generation」は、テキストから高品質な画像を生成できる機能として、GPT-4oに組み込まれた。以下は、OpenAIが4o Image Generationで生成した画像だが、プロンプト通りの精巧な画像が生成されている。</p>
<p><img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002Fo4_1_55001c5807%5Cu002Fo4_1_55001c5807.png" alt="o4-1.png" /></p>
<p><strong>プロンプト</strong>
:::box
A wide image taken with a phone of a glass whiteboard, in a room overlooking the Bay Bridge. The field of view shows a woman writing, sporting a tshirt wiith a large OpenAI logo. The handwriting looks natural and a bit messy, and we see the photographer's reflection.
\u003C和訳\u003E
ベイブリッジを見下ろす部屋で、ガラスのホワイトボードをスマートフォンで撮影したワイド画像。視野には、OpenAIの大きなロゴが入ったTシャツを着た女性が書き物をしている様子が映っている。筆跡は自然で少し乱雑に見え、撮影者の姿も映っている。</p>
<p>The text reads:</p>
<p>(left)
\</p>
]]></description>
      <pubDate>Sun, 13 Jul 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>2025年のAI半導体を読む　王者NVIDIAと競合の現在地</title>
      <link>https://ledge.ai/articles/expo-2025-summer-semiconductor</link>
      <description><![CDATA[<p>2025年、AI半導体を巡る競争は次のステージへと突入した。GPU分野で圧倒的な地位を築いてきたエヌビディアは、AIインフラの中心的存在としての立場をさらに強化。一方で、中国や米国の競合企業もそれぞれの強みを武器に、差別化戦略を打ち出しはじめている。本記事では、NVIDIAの最新動向を軸に、AMD・Intel・Huaweiといったプレイヤーの動きも交えながら、2025年上半期のAI半導体業界を俯瞰する。</p>
<h2>世界最大のGPU企業、AIブームの中核に立つエヌビディアの動向</h2>
<p>エヌビディアは、アメリカ・カリフォルニア州に本社を置く半導体企業で、特にGPUの開発で世界的に知られている。GPUは言わずもがな、一度に大量のデータを高速処理できるハードウェアであり、AI開発・利用で不可欠な存在である。GPUはAIインフラの心臓部とも言える重要な部分を担っていることから、同社はAIブームの中心にいる企業ともいえる。</p>
<h3>最新GPU「Blackwell」アーキテクチャとNVIDIA Dynamoの衝撃</h3>
<p><img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002F1_6666bef6c2%5Cu002F1_6666bef6c2.png" alt="半導体_1.png" /></p>
<p>エヌビディアは、主催する開発者向けの世界的な人工知能カンファレンスである“GTC 2025”でフアン氏は、次世代GPUアーキテクチャ「Blackwell」と前世代「Hopper」の最盛期出荷数とBlackwell初年度の出荷見込みを比較し、その驚異的な成長を指しつつ、「AIは変曲点にある」と語った。特に推論のワークロードにおいて、Blackwellは前世代のHopperより最大40倍の性能向上を実現しており、そのインパクトは計り知れない。</p>
<p>特に注目すべきは、Blackwellが進化し、NVLink技術と液冷技術を採用した点だ。これにより、1ラックあたり最大1.4エクサフロップスという圧倒的な処理能力を発揮し、従来のエアフロー冷却の設計では実現し得なかった高密度・高効率なAIコンピューティングを可能にしている。</p>
<p>こうした新アーキテクチャを支える中核技術として、エヌビディアは新たなオペレーティング・システム「NVIDIA Dynamo」を発表した。NVIDIA Dynamoは、生成AIを支える分散型推論フレームワークであり、オープンソースで提供されている。</p>
<p>NVIDIA Dynamoの主な特徴は4点ある。</p>
<ul>
<li>分散サービング：複数GPUノードでの効率的な処理分散を可能にし、スケーラビリティを確保</li>
<li>GPUプランナー：GPUの使用状況をリアルタイムで監視し、動的にリソースを割り当てる</li>
<li>スマートルーター：KVキャッシュの利用を考慮したルーティングが行われ、再計算の手間を最小限に抑える</li>
<li>低遅延通信ライブラリ：GPU、CPU、ネットワーク、ストレージ間の連携を高速かつ簡素に実現する</li>
</ul>
<p>BlackwellとNVIDIA Dynamoの組み合わせは、エヌビディアが提唱する“AIファクトリー” の中核を担うとされており、AIの実装スピードと効率性を飛躍的に高める可能性を秘めている。</p>
<h3>次世代GPU開発のロードマップ</h3>
<p>エヌビディアは今後のGPU開発のロードマップを明らかにした。
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002F2_f58845a790%5Cu002F2_f58845a790.png" alt="半導体_2.png" /></p>
<p>まず2025年後半には、GB200 NVL72と比較すると1.5倍の性能を実現する「Blackwell Ultra」をリリース予定。さらに2026年には、HBM4メモリを採用し、高帯域（13TB\u002Fs）と省電力性を両立する「Rubin」、2027年には15エクサフロップスの演算性能を持つ「Rubin Ultra」が続く予定である。そして2028年、16個のGPUダイを搭載という前例のない構成を持つ「Feynman」が投入される計画だ。</p>
<h3>エヌビディア株の暴落</h3>
<p>2025年1月下旬、中国のスタートアップDeepSeekが、低コストで高性能なAIモデル「DeepSeek-R1」を発表し、ChatGPTを上回る性能を示した。これにより、エヌビディアの株価は一時17%下落し、約5890億ドル（約91兆円）の時価総額が失われるという、米国市場史上最大の単日損失となった。というのも、DeepSeekは高性能GPUの入手が制限される中、旧世代のエヌビディアGPUを活用し、効率的なAIモデルの開発に成功。DeepSeekがエヌビディアの最新GPUに依存しないAI開発の可能性を示したことで、同社の需要減少が懸念されたことが株価に影響した。</p>
<h3>エヌビディアのハイブリッド計算の時代を見据えた動き</h3>
<p>エヌビディアは、量子コンピューティングとAIスーパーコンピュータの融合を目指し、ボストンに「NVIDIA Accelerated Quantum Research Center（NVAQC）」を設立すると発表した。この研究施設では、量子ハードウェアとAIスーパーコンピュータを統合し、量子コンピューティングの実用化を加速することを目的としている。NVAQCには、NVIDIA Blackwell GPUを搭載したGB200 NVL72システムが導入され、量子アルゴリズムの大規模シミュレーションや量子エラー訂正の研究が行われる予定である。また、NVIDIAのCUDA-Qプラットフォームを活用し、GPU、CPU、QPUを組み合わせたハイブリッド量子アプリケーションの開発が進められる。</p>
<p>この取り組みには、アメリカの量子コンピューティング企業であるQuantinuum、QuEra Computingや、イスラエルの量子コンピュータ開発スタートアップ Quantum Machines、ハーバード大学のHQI（Harvard Quantum Initiative）、マサチューセッツ工科大学のEQuS（Engineering Quantum Systems）グループが協力する。</p>
<p>フアン氏は、2025年1月に「実用的な量子コンピュータの実現には数十年かかる」と発言していたが、3月のGTC2025ではその見解を修正し、量子コンピューティングの進展が予想以上に速いことを認めた。NVAQCの設立は、エヌビディアがAIと量子コンピューティングの融合による新たな計算時代を見据えた未来への投資であり、今後の技術革新において重要な役割を果たすことが期待される。</p>
<h2>エヌビディアの対抗馬の動向は？</h2>
<h3>AMD、同社初の視覚言語モデル「Instella-VL-1B」を発表</h3>
<p>AMDは2025年3月、自社初となるマルチモーダルAIモデル「Instella-VL-1B」を発表。本モデルは、画像と言語の統合処理を目的としたVision-Language Model（VLM）であり、同社の生成AI向けGPU「Instinct MI300X」上でトレーニングされている。
Instella-VL-1Bは、3億パラメータのビジョンエンコーダーと12億パラメータの言語モデルから構成され、合計15億パラメータを有する。視覚情報をテキスト化する2層のMLPで構成されており、エンコーダーにはCLIP ViT-L\u002F14-336、言語モデルには「OLMo 1B SFT」を採用している。トレーニングにはLLaVAやPixmoなどのデータセットに加え、表やグラフのデータセットであるM-PaperやDocStruct4Mといった文書理解系データも活用。これにより、Instella-VL-1BはOCRや文書解析においても高い精度を示し、同規模のオープンソースモデル（MiniCPM-V-2）を上回る性能を達成している。
AMDは本モデルの重み、学習データ・データセットの詳細、コードなど、すべてオープンソースとして公開。同社は、AI時代のインフラをけん引する存在としての意志を示した。</p>
<h3>インテル、次世代データセンター向けプロセッサ「Xeon 6」を発表</h3>
<p>インテルは2025年2月、最新のデータセンター向けプロセッサ「Intel Xeon 6」シリーズを発表した。本シリーズは、「P-core（Performance-core）」モデルと、「E-core（Efficient-cores）」モデルの2タイプのCPUマイクロアーキテクチャから選択可能となっている。これにより、パフォーマンスと電力効率の両立を実現している。P-coreは主に高性能処理を要するAIやHPC向けの用途に適しており、E-coreは大規模なクラウド環境やエッジワークロードといったスケールアウトが求められる分野において力を発揮する。</p>
<p>また、インテルは今回の発表において、電力効率と総保有コスト（TCO）削減の重要性を強調した。省電力性と高密度設計を両立させたE-coreモデルを通じて、持続可能性と運用コストの最適化を図る姿勢を明確にしている。Xeon 6は、変化し続けるデータセンターのニーズに対応するべく、消費電力・スケーラビリティなどの全方位での進化を体現するプロセッサと位置づけられる。Xeon 6シリーズの導入により、AI時代のデータセンターにおける性能と効率性の新たな基準を打ち立て、競合他社との差別化を図っている。</p>
<h3>Huawei、AIチップ「Ascend 910C」でNVIDIA H100に挑戦</h3>
<p>Huaweiは、AIチップ「Ascend 910C」の出荷を開始し、NVIDIAの「H100」に対抗する姿勢を見せた。DeepSeekが実施したテストによると、Ascend 910Cは、推論性能においてH100の約60%に相当する性能を示したという。このチップは、中国の半導体メーカーであるSMICの7nmプロセス（N+2）で製造されており、FP16精度で800TFLOPS、メモリ帯域幅は約3.2TB\u002Fsに達する。
また、Huaweiは、Ascend 910Cを搭載した大規模AIクラスター「CloudMatrix 384」を発表。384基のAscend 910Cチップを搭載することで約300PFLOPSの能力を提供するとされている。これは、NVIDIA GB200 NVL72の約180PFLOPSを上回る性能を実現しているとされており、中国国内でのAIインフラの自立を加速させる狙いがある。一方で、電力効率には課題もある。消費電力がNVL72の3.9倍で、FLOP当たりの消費電力は2.3倍、メモリ帯域幅当たりの消費電力は1.8倍、メモリ容量当たりの消費電力は1.1倍と、電力効率が大きく劣る結果となっている。</p>
<h2>半導体製造の動向は？</h2>
<h3>TSMC、次世代1.4nmプロセス「A14」を2028年に量産開始へ</h3>
<p>TSMCは、2025年4月の北米技術シンポジウムにおいて、次世代1.4nmプロセス「A14」を発表した。このプロセスは、同社の2nmプロセス「N2」と比較して、同じ消費電力で最大15%の速度向上、または同じ速度で最大30%の電力削減を実現し、トランジスタ密度も20%以上向上する。</p>
<p>A14は、第二世代のGAA（Gate-All-Around）ナノシートトランジスタと、改良された「NanoFlex Pro」セルアーキテクチャを採用し、設計の柔軟性と性能効率を高めている。TSMCは、AI向けチップの需要増加に対応するため、A14プロセスを2028年に量産開始する予定であり、NVIDIAなどの顧客への供給を見込んでいる。</p>
<h3>Rapidus、NEDOから2nm開発予算を承認取得</h3>
<p>Rapidusは、国立研究開発法人新エネルギー・産業技術総合開発機構（NEDO）より、2025年度の2nm半導体開発計画と予算の承認を受けた。
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002F3_cd13c15fce%5Cu002F3_cd13c15fce.png" alt="半導体_3.png" /></p>
<p>この承認は、ポスト5G情報通信システム基盤強化研究開発事業の一環として、「日米連携に基づく2nm世代半導体の集積化技術と短TAT製造技術の研究開発」および「2nm世代半導体のチップレットパッケージ設計・製造技術開発」を対象としている 。
北海道千歳市に建設中の製造拠点 “IIM” において、2025年4月から2nmプロセスの試作ラインを立ち上げ、同年7月中旬以降には初の試作チップを完成させる予定である 。これにより、2027年の量産開始を目指し、日本国内での先端半導体製造体制の確立を図っている。</p>
<p>この取り組みは、国内の半導体産業の再興と、AIや高性能コンピューティング分野での国際競争力強化に向けた重要な一歩となる。</p>
]]></description>
      <pubDate>Sun, 13 Jul 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>世界最強AI「Grok 4」公開──xAI、わずか数カ月という常識外れのスピードでモデル刷新　マスク氏「ネットにない難問も解ける」</title>
      <link>https://ledge.ai/articles/grok4_xai_ai_model_launch</link>
      <description><![CDATA[<p>イーロンマスク氏の率いるAIスタートアップxAIは2025年7月10日、X（旧Twitter）公式アカウントで最新大規模言語モデル「Grok 4」を<a href="https:%5Cu002F%5Cu002Fx.com%5Cu002Fxai%5Cu002Fstatus%5Cu002F1943158495588815072">発表</a>{target=“_blank”}し、同時にライブ配信で詳細を公開した。前世代「Grok 3」から数カ月という超短サイクルでのモデル刷新となり、マスク氏は「インターネットにも書籍にも存在しない難問を解ける初のAIだ」と性能を強調している。</p>
<h2>「世界最強」を標榜、リアルな工学課題を解決可能と説明</h2>
<p>xAI公式アカウントは「世界で最も強力なAIモデル」としてGrok 4を紹介し、ライブ配信の視聴を呼びかけた。その後、イーロン・マスク氏自身もX上で「Grok 4は、現実世界の難しい工学的課題に対して、ネットにも書籍にも存在しない答えを導き出すことができた初のAIだ」と述べ、同モデルの性能を強調した。</p>
<p><strong>図1　“Ludicrous rate of progress”──Grok 2からGrok 4に至る計算資源の10倍ステップアップを示したスライド</strong>
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002Fmusk_grok4_3_2b88dc0836%5Cu002Fmusk_grok4_3_2b88dc0836.jpg" alt="musk grok4-3.jpg" /></p>
<h2>公表された性能指標──既存モデルを上回る結果も</h2>
<p>xAIによれば、以下の主要ベンチマークでGrok 4は高い性能を示した（数値はすべて同社発表値）：</p>
<h3>Humanity’s Last Exam（人類最後の試験）</h3>
<p><strong>図2　総合推論テスト「Humanity’s Last Exam」全セット比較。Grok 4 Heavyは44.4%でトップスコアを記録</strong>
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002Fmusk_grok4_5_8a69f27997%5Cu002Fmusk_grok4_5_8a69f27997.jpg" alt="musk grok4-5.jpg" /></p>
<h3>ARC-AGI（汎用人工知能測定ベンチマーク）</h3>
<p><strong>図3　ARC-AGIベンチマークの精度‐コスト分布。Grok 4は精度66.6%でクラストップ帯に位置付けられた</strong>
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002Fmusk_grok4_7_5b67bb48af%5Cu002Fmusk_grok4_7_5b67bb48af.jpg" alt="musk grok4-7.jpg" /></p>
<p><strong>図4　GPQAやAIME25など学術系コンペでもGrok 4／Grok 4 Heavyが軒並み首位に</strong>
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002Fmusk_grok4_6_90a347be60%5Cu002Fmusk_grok4_6_90a347be60.jpg" alt="musk grok4-6.jpg" /></p>
<h2>Grok 4／Grok 4 Heavy──2ライン体制で展開</h2>
<ul>
<li><strong>Grok 4（標準）</strong> ：推論改善を目的に強化学習（RL）を追加。</li>
<li><strong>Grok 4 Heavy</strong> ：複数エージェントで同一課題を並列解析し、回答を相互検証する上位版。
開発者・パワーユーザー向けに月額300ドルの「SuperGrok Heavy」プランを新設。</li>
</ul>
<h2>今後のロードマップ</h2>
<p>Grok 4は、Grok 3からわずか数カ月で投入された。マスク氏は開発速度について「恐ろしく速い」と述べ、今後も短い間隔で新モデルを導入する意向を示した。さらに、以下のようなロードマップも明らかにされている：</p>
<ul>
<li>7月中旬以降：Tesla車両へのGrok搭載を開始予定</li>
<li>8月：コード生成AIのリリース</li>
<li>9月：マルチモーダル・エージェントの提供</li>
<li>10月：動画生成AIの公開</li>
</ul>
<p><strong>図5　xAIが示した今後のタイムライン。8月にコード生成モデル、9月にマルチモーダルエージェント、10月に動画生成AIを投入予定</strong>
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002Fmusk_grok4_9_4b0fc9c4d0%5Cu002Fmusk_grok4_9_4b0fc9c4d0.jpg" alt="musk grok4-9.jpg" /></p>
<h2>今後の課題</h2>
<p>Grokシリーズは、2023年11月の初版リリースから急速に進化しており、OpenAIのChatGPTやGoogleのGeminiに対抗する形で市場に存在感を示してきた。ただし、直近では前バージョンが反ユダヤ的発言を生成したとの報道もあり、同社はプロンプト設計の見直しを迫られていた。今後は、モデル性能とともに倫理的安全性や説明責任も問われる局面が続くと見られる。</p>
]]></description>
      <pubDate>Sun, 13 Jul 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Perplexity、AIブラウザ「Comet」公開──タブ迷子ゼロへ、思考をそのまま実行する“ウェブ用AI相棒”</title>
      <link>https://ledge.ai/articles/perplexity_launches_ai_browser_comet</link>
      <description><![CDATA[<p>AI検索サービスを展開するPerplexityが2025年7月10日、AI搭載ウェブブラウザ「Comet（コメット）」を正式リリースしたことを<a href="https:%5Cu002F%5Cu002Fwww.perplexity.ai%5Cu002Fja%5Cu002Fhub%5Cu002Fblog%5Cu002Fintroducing-comet">発表</a>{target=“_blank”}した。ユーザーがウェブで得たい情報やタスクを自然言語で伝えるだけで、検索や比較、実行までをAIアシスタントが一貫して支援する“認知型ブラウジング”を標榜しており、既存の検索・閲覧体験からの大幅な転換を試みているという。</p>
<p>サービスの提供は、月額200ドルの上位有料プラン「Perplexity Max」会員を対象とする招待制で開始し、今後数週間かけてウェイトリスト登録者にも順次公開していくという。</p>
<p>@<a href="https:%5Cu002F%5Cu002Fwww.youtube.com%5Cu002Fwatch?v=YeldJ4UezDQ">YouTube</a></p>
<h2>タブ操作から思考支援へ──Cometの中核コンセプト</h2>
<p>Cometの開発目的は、従来の「ナビゲーション中心」のブラウジングから、ユーザーの意図や思考を理解し、支援する「コグニティブ（認知）」な体験へと進化させることにあるという。ユーザーは複数タブを開いたり、情報をコピーペーストしたりすることなく、「この論文の要点をまとめて」「このフライトは安いのか？」「この製品は他と比較してどうか」といった問いかけをそのままブラウザに投げかけることで、AIがそれに応じた解答やアクションを提示する。</p>
<p>中心機能となるComet Assistantは、ブラウザ画面の横に常駐するインターフェースで、表示中のページ内容を理解したうえで、ユーザーからの追加質問、要約、執筆支援、カレンダー入力、ECサイトでの買い物などのタスクをその場で実行可能としている。</p>
<p>また、Webページ上の任意のテキストをドラッグすると、その内容についての説明や関連情報を即時に提示する「ハイライト要約」機能、複数の視点や逆説的な説明を提案する多言語・多角的な対話も可能となっている。</p>
<h2>高精度検索エンジンを土台に</h2>
<p>Cometは、Perplexityがこれまで展開してきたAI検索エンジンをそのまま標準搭載しており、検索結果にはすべて出典リンクが付与される。これにより、ユーザーはAIが導き出した情報の裏付けを直接確認できる設計となっている。同社はこれまでも「事実ベースの生成」に特化した検索技術で注目を集めており、2025年6月時点で月間検索回数は7.8億件を超えている。</p>
<p>同社はこれまで、Google検索とは異なるアプローチで、質問に対して即座に構造化された回答を提供することで評価されてきた。Cometはその延長線上に位置づけられ、ユーザーの質問意図や参照ページのコンテキストを理解したうえで、より深いナビゲーションと作業支援を実現する。</p>
<h2>提供形態と今後の展開</h2>
<p>現時点では、CometはMacおよびWindowsに対応したネイティブアプリとして提供され、利用にはPerplexity Max（200ドル\u002F月）への加入および招待コードが必要となっている。今後数週間でウェイトリスト登録者へのアクセス提供を拡大し、数カ月以内には他のプラットフォーム（モバイルなど）への対応や無料版の展開も予定されている。</p>
<p>一方、Cometはユーザーデータの取り扱いについても留意しており、今後のアップデートではプライバシー設計の強化や「AIエージェントの個別最適化（パーソナライズ）」なども視野に入れているという。</p>
<h2>活発化するAIブラウザ市場</h2>
<p>AIを組み込んだ次世代ブラウザは2025年に入り注目を集めており、米The Browser Companyの「Arc」や、BraveのAI連携、さらにOpenAIによる独自ブラウザ開発の動きも報じられている。<a href="https:%5Cu002F%5Cu002Fwww.reuters.com%5Cu002Fbusiness%5Cu002Fmedia-telecom%5Cu002Fopenai-release-web-browser-challenge-google-chrome-2025-07-09%5Cu002F">Reuters</a>{target=“_blank”}も今回のリリースに関連して、Cometを「AIブラウザ戦争」の本格化を示す動きと位置づけた。</p>
<p>Perplexityは今回のComet投入によって、ユーザーの「検索」から「思考・作業」までを一貫して支援する統合環境を提供し、GoogleやChatGPTなどを含む既存のAI体験と差別化を図る狙いがあると見られる。</p>
<h2>今後の焦点：ユーザー拡大と利用定着</h2>
<p>今夏中には全ユーザーへの招待提供を完了し、フィードバックを受けながらの改良フェーズに入る。Perplexityは今後の製品ロードマップにおいて、CometのAI機能をさらに高度化し、さまざまな業務・用途に応じたユースケース拡大を計画している。企業ユーザーや情報労働者にとって、単なる“検索の延長”にとどまらない生産性ツールとしての定着が、次の成長フェーズの鍵となる。</p>
]]></description>
      <pubDate>Sun, 13 Jul 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>学術＆研究2025/7/7 [MON]ハルシネーション（事実誤認）より深刻なAIの「わかったふり」を暴く：MITなどが発見したLLMの“ポチョムキン理解”とは</title>
      <link>https://ledge.ai/articles/potemkin_understanding_llm</link>
      <description><![CDATA[<p>MIT・ハーバード大学・シカゴ大学の研究チームは2025年6月29日、大規模言語モデル（LLM）の「表面的には理解しているように見えるが、実際には概念の適用で誤る」現象を「ポチョムキン理解」と命名し、その頻度を定量化した研究成果を<a href="https:%5Cu002F%5Cu002Farxiv.org%5Cu002Fabs%5Cu002F2506.21521">発表</a>{target=“_blank”}した。発表はICML 2025（バンクーバー）に採択され、AI分野における評価基準の再考を促す内容となっている。</p>
<p>18世紀ロシアの「ポチョムキン村」は、皇帝の視察用に急造された見せかけの村落を指し、「中身のない外観」の象徴とされる。研究者らは、LLMにも同様の「わかったふり」があるとし、この概念をポチョムキン理解と表現している。</p>
<h2>ポチョムキン理解の定義と背景</h2>
<p>研究チームは、LLMが人間向けに設計されたベンチマークの「キーストーン質問」には正しく答えられるものの、その後の具体的応用タスクでは誤る状態を指摘した。これは、人間なら正答＝理解と認められる最小限の問いに合格しても、LLMが本質的に異なる誤解を抱いている可能性を示している。</p>
<p><strong>キーストーン集合に正答しても本質的に誤った解釈を残すポチョムキン理解のイメージ</strong>
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002FA_schematic_representation_of_keystones_and_potemkins_e47033e684%5Cu002FA_schematic_representation_of_keystones_and_potemkins_e47033e684.png" alt="A schematic representation of keystones and potemkins.png" /></p>
<h2>検証の概要</h2>
<p>検証では、</p>
<ul>
<li>文学技法（俳句やアナロジーなど12種類）</li>
<li>ゲーム理論（ナッシュ均衡など9種類）</li>
<li>心理的バイアス（サンクコストの誤謬など11種類）
の合計32概念について、</li>
<li>定義</li>
<li>分類</li>
<li>生成</li>
<li>編集
の4つのタスクで7種類のモデル（GPT-4o、Claude 3.5 Sonnet、Gemini 2.0 Flash など）を評価した。</li>
</ul>
<h2>主な結果</h2>
<p>定義タスクではおおむね94%の正答率を記録したが、その後の応用タスクでは</p>
<ul>
<li>分類で55%</li>
<li>生成で40%</li>
<li>編集で40%
の失敗率（potemkin rate）が確認された。これは、定義だけでは概念理解の深度を測れない可能性を示唆している。</li>
</ul>
<h3>具体例：韻律パターンの応用失敗</h3>
<p>代表的な例として挙げられるのが韻律スキームの問題だ。GPT-4oに「ABAB韻律とは何か」を問うと、下図のように正確に定義を説明した。しかしいざ詩の穴埋め問題でABAB韻律を適用させると、正しく韻を踏めず、自分でもその失敗を認める回答を出した。人間ならまず起こり得ない不可解な挙動である。</p>
<p><strong>GPT-4oはABABの定義を正しく述べながら、応用で失敗する「ポチョムキン理解」の典型例</strong>
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002FPotemkin_Understanding_in_llm_5dae4e573b%5Cu002FPotemkin_Understanding_in_llm_5dae4e573b.png" alt="Potemkin Understanding in llm.png" /></p>
<h2>多分野で発生する“わかったふり”</h2>
<p>研究チームはさらに、幾何学の基本定理、家族関係の概念、俳句の構造など幅広い領域で同様のポチョムキン理解を確認している。</p>
<p><strong>概念の定義には成功する一方で応用に失敗する複数の事例</strong>
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002FExamples_of_potemkins_f6c5140e2d%5Cu002FExamples_of_potemkins_f6c5140e2d.jpg" alt="Examples of potemkins.jpg" /></p>
<h2>自己評価による一貫性検証</h2>
<p>さらに著者らは、自動評価の一環として「モデル自身に、自分が生成した回答を再評価させる」という仕組みを試みた。
例えば「スラントライムの例を作れ」と指示し、その後「今作った例はスラントライムか？」と再度モデルに問うと、矛盾した回答が返るパターンが確認され、モデル内部の知識表現が不整合である可能性を示しているとした。</p>
<p><strong>生成と再判定の整合性を確かめる自動評価プロセスのイメージ</strong>
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002FIllustration_of_the_method_for_evaluating_incoherence_in_models_d19701ab72%5Cu002FIllustration_of_the_method_for_evaluating_incoherence_in_models_d19701ab72.png" alt="Illustration of the method for evaluating incoherence in models.png" /></p>
<h2>社会的影響と課題</h2>
<p>論文では、ハルシネーション（事実誤認）とは異なり、ポチョムキン理解は概念構造の誤りであるため、人間にも検出が難しいと指摘する。
法務や医療、教育といった高い正当性が求められる分野でLLMを活用する際には、ベンチマークだけでは保証できないリスクとして注意が必要とされる。</p>
<p>研究チームは、人間とAIの「誤解のパターン差」を考慮したベンチマークの再設計や、概念の一貫性を評価するためのツール開発を進める方針だ。</p>
]]></description>
      <pubDate>Sun, 13 Jul 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/7/9 [WED]【ソースコード特典付き】自社専用LLMを低コストで実現！「Qwen3」の継続事前学習のデモンストレーション｜Ledge.ai Webinar</title>
      <link>https://ledge.ai/articles/webinar-vol65</link>
      <description><![CDATA[<h2>ウェビナー概要</h2>
<p>Ledge.ai Webinar vol.65では、「ローカルLLMの大本命『Qwen3』の継続事前学習デモンストレーション」をテーマに、GPUクラウドサービス「GPUSOROBAN」を提供する株式会社ハイレゾ様をゲストにお迎えし、実演デモを交えながらご解説いただきます。</p>
<p>Alibaba社が開発したオープンソースの大規模言語モデル（LLM）「Qwen」シリーズの最新版「Qwen3」は、DeepSeek-R1やOpenAI o1をも凌ぐ性能を持つとされ、世界中の開発者から大きな注目を集めています。特に、プロンプトに応じて思考プロセスを切り替える「ハイブリッド推論」や、外部ツールを呼び出す「エージェント機能」といった先進的な機能を備えている点も特長です。オープンソースでありながら商用利用も可能なため、自社の環境でセキュアに活用できる高性能なローカルLLMとして、ビジネス応用の期待が非常に高まっています。
今回のウェビナーでは、この「Qwen3」をベースに、特定の専門知識を追加で学習させる「継続事前学習」に焦点を当てます。ゼロからモデルを開発する「フルスクラッチ」に比べ、計算リソースやコストを大幅に抑えながら、自社に特化した高性能モデルを構築できるこの手法について、デモンストレーションを通じて具体的に解説します。</p>
<p><strong>ウェビナーの内容</strong></p>
<ul>
<li><strong>高性能オープンソースLLM「Qwen3」の詳解</strong>
<ul>
<li>アーキテクチャ（MoE）、ハイブリッド推論、エージェント機能（Function Calling）など、Qwen3の先進的な特徴とビジネスにおける可能性</li>
</ul>
</li>
<li><strong>GPUクラウド「GPUSOROBAN」を活用した継続事前学習デモンストレーション</strong>
<ul>
<li>環境構築からデータセットの前処理、学習実行、推論までの一連のプロセスを実演</li>
</ul>
</li>
<li><strong>大規模モデル学習に不可欠な分散処理技術の解説</strong>
<ul>
<li>データ並列、モデル並列（パイプライン並列・テンソル並列）の基礎から、DeepSpeedやMegatron-LMといったフレームワークの活用法まで</li>
</ul>
</li>
</ul>
<p><strong>このような方におすすめ</strong></p>
<ul>
<li>自社専用の高性能LLMを、コストを抑えて構築したい方</li>
<li>機密情報を扱うため、オンプレミスやセキュアなローカル環境でLLMを運用したい方</li>
<li>LLMに専門知識を追加する「継続事前学習」の具体的な手法を知りたいエンジニア</li>
<li>生成AIの学習・開発におけるGPUリソースの確保やコストに課題を感じている</li>
</ul>
<h2>視聴者特典</h2>
<p><strong>【特典①】デモで使用したサンプルコードをプレゼント！</strong>
本ウェビナーにお申し込みいただいた方には、デモで使用した「Qwen3の継続事前学習」のソースコードをプレゼントいたします。視聴後すぐに、ご自身の環境で再現・検証が可能です。</p>
<p><strong>【特典②】H200 GPU 30日間無料トライアルキャンペーン！</strong>
さらに、ハイレゾでは現在、「NVIDIA H200」を搭載したGPUクラウドサービス「AIスパコンクラウド」を30日間無料でお試しいただける特別なキャンペーンも実施中です。
（詳細はウェビナー内およびアンケート回答後のご案内をご確認ください。）</p>
<p><img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002Fh200_gpu_free_trial1200_bd3d66cf05%5Cu002Fh200_gpu_free_trial1200_bd3d66cf05.jpg" alt="h200-gpu-free-trial1200.jpg" /></p>
<h2>登壇者情報</h2>
<p>株式会社ハイレゾ
GPU事業本部　マーケティング部　グループ長
山田 岳史</p>
<p><img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002Fhighreso_yamadasama_2a984e3aa6%5Cu002Fhighreso_yamadasama_2a984e3aa6.jpg" alt="highreso-yamadasama.jpg" /></p>
<p>IoTの領域で事業開発の経験を経てハイレゾに入社。
GPUクラウドサービスの事業開発からマーケティング、技術サポートまで担当。</p>
<h2>お申し込みはこちら</h2>
<p>配信期間：2025年7月9日(水)〜2025年7月29日(火)
配信方式：オンデマンド（Zoom）
参加費：無料</p>
<p>:::button
<a href="https:%5Cu002F%5Cu002Fzfrmz.com%5Cu002FiXQrpCVKQZwYTU8kO3uy">ウェビナーの視聴はこちら</a>{target=“_blank”}
:::</p>
]]></description>
      <pubDate>Sun, 13 Jul 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>耳で聞けない声を0.3秒で“見える化”──イェール大発スマートグラス「TranscribeGlass」一般販売開始</title>
      <link>https://ledge.ai/articles/transcribeglass_smartglasses_realtime_subtitles</link>
      <description><![CDATA[<p>2025年7月、イェール大学の学生チームが開発したスマートグラス「<a href="https:%5Cu002F%5Cu002Fwww.transcribeglass.com%5Cu002F">TranscribeGlass</a>{target=“_blank”}」の一般販売が開始された。聴覚障がい者や難聴者を主な対象とし、周囲の発話をリアルタイムで字幕としてレンズ上に表示することができる。平均0.3秒という低遅延表示を実現し、日本語を含む10以上の言語への翻訳にも対応しているという。</p>
<h2>会話を文字で「見る」──TranscribeGlassの概要</h2>
<p>TranscribeGlassは、専用アプリをインストールしたスマートフォンのマイクで周囲の音声を取得し、それをクラウド経由で音声認識・処理した上で、メガネ型デバイスの右レンズに字幕として表示する構造となっている。表示はウェーブガイド方式を採用し、640×480ピクセルの解像度で文字を右視野30度以内に映し出す設計だという。</p>
<p>表示までの遅延は平均0.3秒に抑えられ、音声認識精度は95％以上を謳っている。最大8時間稼働可能なバッテリーを内蔵し、重量は36〜38グラムに収められている。スマートグラス本体にはマイクやカメラは搭載されておらず、軽量性とプライバシーへの配慮を両立している点も特徴とされる。</p>
<p><img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002Fsmart_glass3_64b7665d4d%5Cu002Fsmart_glass3_64b7665d4d.jpg" alt="smart glass3.jpg" /></p>
<h2>販売価格と利用形態</h2>
<p>製品は<a href="https:%5Cu002F%5Cu002Fwww.transcribeglass.com">公式サイト</a>{target=“_blank”}で注文可能で、価格は本体が377ドル（約5万9,000円）、加えてクラウド音声認識機能を使用するための月額サブスクリプションが20ドル（約3,000円）となっている。2025年8月から出荷を予定しており、日本を含む国際配送にも対応するとのこと。</p>
<p>なお、アプリは現在iOS版が提供されており、Android版も年内にリリースされる見込み。また、オフラインモードも搭載されているが、この場合は音声認識精度がやや低下するとされる。</p>
<h2>想定利用シーンと対象ユーザー</h2>
<p>TranscribeGlassは、聴覚障がい者や加齢性難聴者の会話支援を主な用途とし、特に教室、会議、劇場、飲食店など騒音下での対話の可視化に有効とされる。また、語学学習者や国際会議の参加者など、リアルタイム翻訳による情報取得が必要なユーザーにも活用が期待されている。</p>
<p><strong>TranscribeGlassをプレゼントされ「あなたの言っていることが分かるわ！」と感激するユーザー</strong>
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002FAbout_smart_glass_a3840a78b4%5Cu002FAbout_smart_glass_a3840a78b4.jpg" alt="About smart glass.jpg" /></p>
<h2>開発背景と開発チーム</h2>
<p>この製品は、イェール大学の学生である<a href="https:%5Cu002F%5Cu002Fyaledailynews.com%5Cu002Fblog%5Cu002F2025%5Cu002F02%5Cu002F18%5Cu002Fyale-student-founds-transcribeglass-a-live-text-to-speech-transcription-device%5Cu002F">マダヴ・ラヴァカレ氏</a>{target=“_blank”}が中心となって開発された。</p>
<p><img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002Fglasses_ag_Transcribe_Glass_scaled_bc37a50cad%5Cu002Fglasses_ag_Transcribe_Glass_scaled_bc37a50cad.jpeg" alt="glasses_ag_TranscribeGlass-scaled.jpeg" /></p>
<p>きっかけは、聴覚障がいを持つ友人が講義中の内容を十分に理解できない状況を目の当たりにしたことだったという。2018年から7年をかけて7代にわたるプロトタイプを開発し、2024年にはCTOとしてニルバイ・ナラン氏が参画。Y Combinatorなどからの資金調達を経て、今回の一般販売に至った。</p>
<p>これまでベータ版は500人以上に試用され、フィードバックをもとに改良が重ねられてきたという。</p>
<h2>競合との差別化と今後の展開</h2>
<p>他のスマートグラス製品と異なり、TranscribeGlassは通話・音楽再生・撮影などの多機能化を避け、字幕精度と軽量性に特化している点が特徴だ。価格設定もMeta×Ray-BanやXRAI Glassなどに比べて抑えられており、バッテリー持続時間の長さも差別化ポイントとなっている。</p>
<p>将来的には、リアルタイムでの感情解析を字幕に反映する機能や、ASL（アメリカ手話）向けに語順を変換する表示機能の搭載が計画されているという。</p>
<p>TranscribeGlassは、視覚的な情報支援により、耳で聞けない会話を「読む」体験へと変えることで、新たなコミュニケーションの可能性を提供しようとしている。</p>
]]></description>
      <pubDate>Sat, 12 Jul 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI、対話型AI搭載のWebブラウザを数週間以内に発表か</title>
      <link>https://ledge.ai/articles/openai_ai_browser_launch_reported_by_reuters</link>
      <description><![CDATA[<p>OpenAIは数週間以内にAIを搭載した独自のWebブラウザを発表する見通しであると<a href="https:%5Cu002F%5Cu002Fjp.reuters.com%5Cu002Feconomy%5Cu002Findustry%5Cu002FSJY3A2CG2RPM7KT6MKLMQPQTWA-2025-07-10%5Cu002F">ロイター</a>{target=“_blank”}が2025年7月10日に報じた。報道によれば、新ブラウザは、検索結果をリスト形式で提示する従来型検索とは異なり、ChatGPTのような対話形式で回答を提示する設計になるとされる。</p>
<h2>ChatGPT的UIとAIエージェントを組み込んだブラウザ</h2>
<p>OpenAIの新ブラウザは以下の特徴を備えるという：</p>
<ul>
<li>検索キーワードに対して対話形式で結果を提示し、Webページを直接開かずとも必要な情報が得られる</li>
<li>OpenAIのエージェント技術「Operator」と連携し、ウェブ上のフォーム入力や予約、購入といった作業を自動で処理できる</li>
<li>ユーザーの操作を最小限に抑えた「チャット主導型ブラウジング」体験を志向している</li>
</ul>
<p>報道では、これらの機能によって、ユーザーは検索エンジンやリンクを複数クリックする手間なく目的を達成できる可能性があるとされている。</p>
<h2>背景にある戦略的狙い</h2>
<p>OpenAIはこれまでMicrosoftのBingと連携して検索機能を提供してきたが、独自ブラウザを展開することでユーザーデータへのアクセスを強化し、広告収益源をGoogleに依存せずに確保する狙いがあるという。現在、Google Chromeは世界のブラウザ市場で約60％のシェアを持ち、検索連動型広告によって収益を拡大している。</p>
<p>一方でOpenAIは、週5億人規模のユーザーを抱えるChatGPTを活用し、検索から情報取得までを同社のエコシステム内で完結させようとしている。</p>
<h2>検索技術への投資と布石</h2>
<p>ロイターが指摘するように、OpenAIはこの1年、検索・ブラウジング関連の技術開発を継続している。</p>
<ul>
<li>「Product Manager, Search」などの職種で求人を出し、検索関連技術への投資を進めてきた</li>
<li>2024年4月には、AIエージェントのウェブ検索能力を評価するためのベンチマーク「BrowseComp」を発表</li>
<li>ChatGPTには「Browse with Bing」機能や、外部リンク先の要約を行う機能などをすでに実装している</li>
</ul>
<p>こうした動きが、今回の独自ブラウザ発表につながっていると見られている。</p>
<h2>今後の展望</h2>
<p>ロイターの取材に対し、OpenAIおよび競合企業であるアルファベット（Google）はコメントを控えているという。</p>
]]></description>
      <pubDate>Fri, 11 Jul 2025 09:50:00 GMT</pubDate>
    </item>
    <item>
      <title>MIXI、全社員2,000人に生成AI構築プラットフォーム「Google Agentspace」導入──情報統合と業務自動化で“創造の時間”を拡大へ</title>
      <link>https://ledge.ai/articles/mixi_ai_agentspace_deployment</link>
      <description><![CDATA[<p>MIXIは2025年7月7日、Google Cloudが提供するエンタープライズ向け生成AIプラットフォーム「Google Agentspace」を、全従業員約2,000人に導入したことを<a href="https:%5Cu002F%5Cu002Fmixi.co.jp%5Cu002Fnews%5Cu002F2025%5Cu002F0707%5Cu002F42850%5Cu002F">発表</a>{target=“_blank”}した。</p>
<p>導入の目的は、社内に点在する情報資産の統合と、業務プロセスの自動化により、社員が創造的な業務に集中できる環境を構築することだという。同プラットフォームは、Googleの生成AI「Gemini」および検索技術を活用し、社内文書・メール・画像・動画などを横断的に検索・要約できる。</p>
<h2>社内検証を経て全社展開へ</h2>
<p>同社は2025年3月より一部社員を対象にGoogle Agentspaceの試験導入を行い、Google Cloudの支援のもとでエージェント設計と業務適用の検証を進めてきた。その結果、ナレッジベースの統合による業務効率化と検索性向上が確認されたことから、全社員への展開を決定したという。</p>
<p>対象はMIXIのドメインを持つ全正社員、契約社員、アルバイト、派遣社員、業務委託社員におよび、約2,000人が利用可能となる。</p>
<h2>Google Agentspaceの機能と役割</h2>
<p>Google Agentspaceは、企業データを基盤に生成AIエージェントを構築できるプラットフォームであり、次のような機能が提供されている：</p>
<ul>
<li><strong>統合検索</strong> ：ドキュメント、スプレッドシート、メール、画像、動画など社内外のファイルを横断的に検索し、根拠付きで回答を生成。</li>
<li><strong>業務自動化</strong> ：日報や企画書の作成、議事録の要約などをAIが支援。</li>
<li><strong>ローコード開発</strong> ：従業員がノーコード・ローコードで独自のAIエージェントを作成・共有可能。</li>
<li><strong>情報資産の可視化</strong> ：ナレッジの属人化を防ぎ、再利用性の高い知見を全社的に蓄積。</li>
</ul>
<p>MIXIは、このような機能を活用することで、従来業務にかかっていた検索や確認の手間を削減し、社員がより価値の高い業務に集中できる体制を整えると説明している。</p>
<h2>今後の活用と展望</h2>
<p>今後はGoogle Cloudとの協力体制のもと、社内向けの教育・研修プログラムを通じて、全社員のAIリテラシーを底上げするとともに、業務部門ごとに最適化されたエージェントの活用を推進する。また、生成AIの利活用によって、同社が手がける新たなサービスやユーザー体験の創出にもつなげていく方針だ。</p>
<p>同社代表取締役社長の木村弘毅氏は、リリース内で以下のように述べている。
「情報を探す時間や煩雑なやり取りを削減し、思考の質を高めたい。AI導入は効率化の手段だけでなく創造性への投資だ」</p>
]]></description>
      <pubDate>Fri, 11 Jul 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>郊外巨大施設から都市型も可能な”装置”にフォーカスが変化する日本のAIデータセンター戦略</title>
      <link>https://ledge.ai/articles/expo-2025-summer-kamui</link>
      <description><![CDATA[<p>AIの進化により、日本国内のデータセンター市場が急拡大する中で、冷却技術や分散型演算インフラのあり方が改めて問われている。本稿では、QuantumMesh株式会社 代表取締役 篠原裕幸氏に、国内データセンター市場の現状と課題、そして同社が展開するAIデータセンター向け液浸冷却システム『KAMUI（カムイ）』の可能性について、話を聞いた。</p>
<h2>計算能力が資源となる時代へ</h2>
<p>AIの進化がもたらす構造変化の中、日本のデータセンター市場が新たな転換期を迎えている。篠原氏は「生成AIの普及は、インターネット黎明期以来の、構造変化を引き起こしている」と語り、計算能力資源が国家や企業の競争力に直結する価値を持ち始めていることを強調した。その一方で、日本国内のデータセンターインフラには、土地・電力といった供給面の制約が立ちはだかっている。特に首都圏では、用地不足や電力制限に加え、制度面での課題も複雑に絡んでいる。篠原氏は「地方分散は望ましい流れであるが、従来のデータセンターと同じものではなく再設計が必要」と話し、地方の再生可能エネルギーや広大な土地、自然冷却といったポテンシャルを生かすには、それらを総合的に設計するシステムアーキテクトの存在が不可欠だと指摘した。</p>
<p>また、冷却技術への注目も高まっている。篠原氏が「冷却そのものが“主要機能”へと格上げされつつある。今後は冷却性能がクラウドの性能そのものを左右する時代になる」と語るように、これまでは裏方技術として捉えられていた冷却技術は、現在その地位は大きく変わりつつある。</p>
<h2>KAMUIが実現する“場所を選ばない冷却”</h2>
<p>このような状況下で開発されたのが、同社が提供する閉鎖循環型・液浸冷却システム『KAMUI』である。
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002FKAMUI_82e6f50800%5Cu002FKAMUI_82e6f50800.png" alt="KAMUI3.png" /></p>
<p>KAMUIは絶縁オイルを密閉して循環させる設計を採用し冷却水（地下水など）との熱交換によってオイルの温度を保つことでサーバを冷却する。外気温や粉塵など外部環境の影響を受けにくい安定した動作を実現し、静音で稼働する。日本やアジア地域などに豊富に存在する地下水を活用する“場所を選ばない冷却”を実現する装置であり、実運用においてもPUE（Power Usage Effectiveness）1.03～1.04という高効率を維持している。</p>
<p>KAMUIが真価を発揮するのは、都市部の狭小スペースや電力制約のある環境だけでなく、騒音や環境負荷が問題となりやすい地方都市での場所での利用にも適している。篠原氏は「KAMUIは、都市部や地方の小規模拠点、物流倉庫の一角などにおいて自律した冷却システムとして機能する」と述べた。</p>
<h2>スマートシティとの親和性</h2>
<p>KAMUIのこのような特性は、スマートシティの文脈でも大きな可能性を示す。スマートシティでは、セキュリティ・交通・医療などの様々な都市機能がリアルタイムに連携する必要があり、数秒の遅れが重大な判断ミスやシステム効率の低下を招きかねない。そのため、従来のクラウド集中型アーキテクチャでは対応しきれない場面もある。篠原氏は「スマートシティの根幹は“遅延のない意思決定”にある。KAMUIは都市内に『計算の拠点』を分散配置することを可能にする」と述べ、KAMUIが冷却と演算の一体設計によって都市内クラウドの実現を支える存在であることを示した。</p>
<p>実際、KAMUIは様々な業種・業態での実装が検討されている。「KAMUI×病院」では画像診断AIとの連携、「KAMUI×駅」では顔認証や群衆制御アルゴリズムとの連携といったシナリオが進められており、いずれもリアルタイム処理が求められる。KAMUIの分散型処理は、プライバシー保護の観点からも重要な選択肢となる。</p>
<p>また、地方自治体との協働も進行中であり、防災拠点や医療機関を対象に、KAMUIを中核とした「オフグリッド型ノード」の実証実験が始まっている。再生可能エネルギー発電所では、余剰電力を価値化する演算拠点としての導入も具体化されており、地域における新たな経済循環の担い手としても期待されている。</p>
<p>さらにKAMUIは、同社が発起人となっている“ENJINプロジェクト”とも連携している。ENJINプロジェクトは、産業が急速に変化するAIoT時代において、社会課題の解決と持続的な成長を見据え、民間企業や専門家が集結して構成されたプロジェクトだ。</p>
<p><img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002Fenjin_a2feb117d1%5Cu002Fenjin_a2feb117d1.png" alt="KAMUI4.png" /></p>
<p>このプロジェクトが掲げる『AIoTサプライチェーン構想』のなかで、KAMUIは演算拠点の冷却およびエネルギー効率を実現する基盤装置として位置づけられている。鉄道事業者との取り組みでは、駅構内の余剰電力やスペースを活用し、都市インフラの一部としてエッジ演算ノードを導入する計画が進められている。</p>
<h2>未来の都市インフラに向けて</h2>
<p>篠原氏は、液浸冷却の将来について「もはや選択肢の一つではなく、計算密度という物理条件に対する合理的な解である」と述べた。データセンターが従来の「施設」ではなく、「装置」として再定義される未来が見えつつあるが、その実現には、演算拠点の多極分散と地域電力の地産地消がカギを握る。「既に再生可能エネルギー発電事業者や地方都市自治体との協働により、このようなモジュール型演算インフラの導入が進行している」と篠原氏は語り、KAMUIが冷却装置の枠を超え、AIoT時代の基盤インフラとして認知される日が近づいていることを示唆した。</p>
<p>冒頭でも触れたように、AIの進化が引き起こした計算需要に対し、日本のデータセンターは、分散化とエネルギー効率の両輪で再設計が必要な局面に立っている。地域ごとの制約を解消し、都市の隅々までにリアルタイムな計算能力を行き渡らせるためには、これまでの巨大施設としてのデータセンター像を見直す必要がある。</p>
<p>日本全体を俯瞰したとき、KAMUIが描く構想は、エネルギーと演算、そして都市機能を結びつける“次世代都市の神経網”を担う可能性を大いに秘めている。都市インフラをより高速かつ持続可能なものへと転換する取り組みは、静かに動き始めている。</p>
]]></description>
      <pubDate>Fri, 11 Jul 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>生成AI「個人利用」26.7%に上昇──それでも米中との差は歴然【総務省・情報通信白書】</title>
      <link>https://ledge.ai/articles/generative_ai_personal_use_japan_2025</link>
      <description><![CDATA[<p>総務省は2025年7月8日、「<a href="%5B%E7%99%BA%E8%A1%A8%5D(https:%5Cu002F%5Cu002Fwww.soumu.go.jp%5Cu002Fjohotsusintokei%5Cu002Fwhitepaper%5Cu002Fr07.html)%7Btarget=%E2%80%9C_blank%E2%80%9D%7D">情報通信白書令和7年版</a>」を公表し、国内における生成AIの個人利用率が26.7%にとどまっているとの調査結果を示した。これは米国（68.8%）や中国（81.2%）と比べて大きく差がある。調査は2024年度にインターネット上で6,000人を対象に実施されたもので、生成AIの国内普及の現状と課題を浮き彫りにしている。</p>
<h2>世界との利用率比較、日本は依然低水準</h2>
<p>白書によれば、生成AIサービスを「使ったことがある」と回答した個人は日本で26.7%。これは前年（2023年度）の9.1%から約3倍に増加したものの、国際的に見ると依然として低い水準である。</p>
<p>同調査における他国の利用率は、米国が68.8%、ドイツが59.2%、中国が81.2%。いずれも日本を大きく上回っており、対象4カ国の中で日本が最も低い。年代別に見ると、日本国内では20代の利用率が44.7%と最も高く、30代が23.8%、40代が29.6%、50代が19.9%、60代が15.5%と、年齢が上がるにつれて利用率が下がる傾向が明確に現れている。</p>
<p><img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002F2025hakusho_aa33bea58a%5Cu002F2025hakusho_aa33bea58a.jpg" alt="2025hakusho.jpg" /></p>
<h2>利用しない理由：「必要性を感じない」が最多</h2>
<p>生成AIを使っていない人にその理由を尋ねた結果、最も多かったのは「自分の生活や業務に必要ない」で40.4%、次いで「魅力的なサービスがない」（38.6%）、「使い方がわからない」（18.3%）が続いた。
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002F2025hakusho2_e78137159b%5Cu002F2025hakusho2_e78137159b.jpg" alt="2025hakusho2.jpg" /></p>
<p>この結果について白書は、生活や業務への活用方法が十分に理解されていない現状を示すとともに、ユーザーの実利感の欠如が普及の壁となっている可能性を示唆している。</p>
<p>一方で、生成AIの利用意向に関する設問では、「調べものをする」（40.8%）、「コンテンツの要約・翻訳をする」（38.6%）、「画像や動画を生成する」（35.9%）など、「今は使っていないが、ぜひ利用してみたい」または「条件によっては利用したい」と回答した割合が4割前後に達する項目も多い。</p>
<p>すでに利用している割合は各項目とも10%未満にとどまるが、サービスの充実や操作性の改善によって利用が拡大する余地があることがうかがえる。</p>
<p><img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002F2025hakusho3_aa43775f27%5Cu002F2025hakusho3_aa43775f27.jpg" alt="2025hakusho3.jpg" /></p>
<h2>政策的対応と今後の展望</h2>
<p>総務省は白書の中で、生成AIの健全な利活用に向けて、以下の3点を政策課題として挙げている。</p>
<ul>
<li>利用者リテラシーの向上</li>
<li>民間事業者によるガイドライン整備の促進</li>
<li>教育・人材育成を含む体制の強化</li>
</ul>
<p>また、デジタル庁や内閣官房AI戦略チームなどと連携しながら、産業界・教育機関・自治体を横断する形で活用促進とリスク管理を両立させる必要性も強調されている。</p>
]]></description>
      <pubDate>Thu, 10 Jul 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>日本オラクル、AIとソブリンクラウド運用を支援する「ジャパン・オペレーション・センター」開設</title>
      <link>https://ledge.ai/articles/oracle_japan_ai_sov_cloud_center</link>
      <description><![CDATA[<p>日本オラクルは2025年7月8日、日本企業におけるAI活用とソブリンクラウド導入を加速させることを目的に、「ジャパン・オペレーション・センター」を東京に開設したと<a href="https:%5Cu002F%5Cu002Fwww.oracle.com%5Cu002Fjp%5Cu002Fnews%5Cu002Fannouncement%5Cu002Foracle-opens-japan-operations-center-to-accelerate-adoption-of-sovereign-cloud-and-ai-2025-07-08%5Cu002F">発表</a>{target=“_blank”}した。</p>
<p>新センターは24時間365日体制で稼働し、日本国内のエンジニアが運用支援を担当する。これは、同社が2024年に発表した10年間で80億米ドル以上を日本に投資する計画の一環とされ、企業のデータ主権や法規制対応を重視した体制を整える狙いがあるという。</p>
<h2>国内のAI・クラウド需要に対応する新たな拠点</h2>
<p>ジャパン・オペレーション・センターは、以下のような機能と目的を持つ。</p>
<ul>
<li>24時間365日の運用監視と支援：国内在住のエンジニアによってクラウドインフラの運用が担われる。</li>
<li>データ主権・法令遵守の確保：センターの運用プロセスは、日本国内法および各種業界規制に対応。</li>
<li>Oracle Alloyパートナー企業への支援：クラウド基盤を活用する国内パートナーに対し、ノウハウやエンジニアリング支援を提供。</li>
<li>国内人材の強化と育成：AI・クラウド専門チームの拡充により、より高度な技術支援を目指す。</li>
</ul>
<h2>背景には生成AIとソブリンクラウドのニーズ増加</h2>
<p>日本国内では、機密性の高いデータを国内に保持しつつ生成AIを活用したいという要望が政府機関や金融機関、製造業などで高まっている。これに対応するため、同社は2024年4月、国内クラウド基盤の強化と人員増強を含む80億ドル規模の投資計画を公表しており、今回のセンター開設はその中核施策と位置づけられる。</p>
<h2>パートナー企業の評価と今後の展開</h2>
<p>発表に際し、Oracle Alloyの国内パートナー企業もエンドースメントを表明している。</p>
<ul>
<li><strong>富士通株式会社</strong> ：ジャパン・オペレーション・センターの設立により、「ハイパースケーラー同等の機能を備えつつ、経済安全保障リスクにも対応した国産ソブリンクラウドの実現が可能になる」とコメント。</li>
<li><strong>株式会社NTTデータ</strong> ：同社のソブリンクラウド基盤「OpenCanvas」にOracle Alloyを組み込み、安全な生成AI活用を推進していく方針を示した。</li>
</ul>
<p>同社は今後、同センターを中核拠点として、日本国内におけるクラウドリージョンの拡張とAIインフラの強化を進める計画である。企業のミッションクリティカルな業務基盤のクラウド化や生成AI導入支援を通じ、「AI時代における持続的なビジネス成長」を支えるとしている。</p>
]]></description>
      <pubDate>Thu, 10 Jul 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>「フィジカルAI」が産業界を変える ｕｇｏ CSO羽田氏が語る、ロボットの未来と日本の勝ち筋</title>
      <link>https://ledge.ai/articles/expo-2025-summer-ugo</link>
      <description><![CDATA[<p>少子高齢化と労働力不足が進む中、ロボットの存在は今後さらに重要となってくる。現在も製造現場や施設管理、サービス業において、AIとロボティクスの融合による効率化や省人化を期待する声が多いが、導入のハードルはいまだ高いのが現状だ。
こうした現状を打開すべく、実用的な業務ロボットの社会実装に挑んでいるのがｕｇｏ株式会社である。本稿では、同社の取締役CSOであり、戦略と開発の両面をリードする羽田卓生氏に、ロボティクスの現在地、日本における課題、そして「フィジカルAI」という次のパラダイムについて話を聞いた。</p>
<h2>ロボットの現在地は「下半身が完成した段階」</h2>
<p>羽田氏は、現在のロボティクス技術の成熟度について、「下半身が完成した段階にある」と語った。移動や巡回といった基礎的な物理動作はすでに実現されており、ｕｇｏが提供する警備・点検・案内ロボットも、日々の業務で稼働している。だが、人間のように「手で操作し、目で見て、言葉でやりとりする」ためには、さらなる進化が必要である。そこにAI、特にLLMや模倣学習の力が加わることで、ようやく“上半身”が備わる。「すでにAIの“目”や“言葉”は手に入りつつある。これに模倣学習による“手”の機能が加われば、業務ロボットの概念が一変する。このブレイクスルーは5年以内、もしかするともっと早く来るかもしれない」と羽田氏は予測する。</p>
<p>ロボティクスの技術自体は進化しているにもかかわらず、日本ではその社会実装が思うように進んでいない。その理由として、羽田氏は「価格」「柔軟性」「導入設計」の3点を挙げた。かつて産業用ロボットの中心だったアーム型ロボットは、自動車などの大量生産に特化しており、汎用性に欠けるうえ価格も高い。中小規模の現場やサービス業では採算が合わず、結果として導入が限定的となってしまっている。
また、決められた動きしかできないことも、ロボットの導入を妨げてきた大きな要因である。しかし、仮に清掃ロボットがドアを開け、モップがけまで行えるようになったらどうか。配膳ロボットがテーブルへの配膳だけでなく、食器の片付けまでこなせるようになったらどうか。それだけで、ロボット導入の価値は大きく変わる。ロボットが“あと一歩”進化することで、導入の実現性は一気に高まるのだ。「人手不足や賃金上昇といった外的要因と、ロボットの進化が組み合わさった時、さらなる導入の追い風が吹く」と羽田氏が語るように、社会的なニーズと技術革新が交差するとき、ロボットが大きなブレイクスルーを迎えるのだ。</p>
<h2>現場で“使われ続ける”ロボットをつくるｕｇｏ</h2>
<p>ｕｇｏが現在提供しているのは、警備、点検、案内といったルーチン業務を担うロボットである。オフィスビル、駅構内、データセンター、工場など、多様な現場で稼働している。「人が歩き回って確認するだけの時間は生産性がない。異常検知のような判断業務は人に任せ、データ収集や巡回はロボットが担う。それによって、現場全体の効率が上がっていくと考える」と羽田氏は説明する。</p>
<p><img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002Fimg_sol_tenken_03_2_768x512_1_8b55e780f2%5Cu002Fimg_sol_tenken_03_2_768x512_1_8b55e780f2.jpg" alt="img_sol_tenken_03-2-768x512 (1).jpg" /></p>
<p>また、ｕｇｏでは、ロボットとLLMやRAGを組み合わせた会話システムも開発。ユーザーとのやりとりをスコア化し、精度が低い部分をナレッジとして再学習させる設計になっている。まさに「現場で育つロボット」である。さらに、クラウド連携による遠隔制御、LLMの切り替えによる多用途対応、ノーコードによる運用のしやすさなど、使い続けてもらうための仕組み作りにも注力している。これにより、データの継続的な蓄積と品質向上が可能になる。</p>
<p>現場で使われ続けるロボットを支える開発体制もまた、印象的だった。今回インタビューで筆者が訪れたのは、東京・千代田区という都心の一等地に構えるｕｇｏのオフィスである。ひとつのフロア内に、実務やソフトウェア開発のエリア、ハードウェアの組み立てスペース、出荷を待つロボットが並ぶ格納エリア、さらには実証実験用のデモエリアまでが混在していた。</p>
<p><img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002FIMG_1909_015edcaf6e%5Cu002FIMG_1909_015edcaf6e.jpg" alt="IMG_1909.JPG" /></p>
<p>羽田氏が「垂直型で対応している」と語る通り、ｕｇｏではハードとソフトの開発が完全に一体化しており、設計から組み立て、テスト、配送までがワンストップで完結している。この“すべてがつながった開発環境”こそが、同社が次々と革新的なロボティクス技術を生み出す原動力なのである。</p>
<h2>ロボティクスの今後は？「フィジカルAI」が変えるもの</h2>
<p>羽田氏が今後のロボティクスのトレンドとして提唱する「フィジカルAI」とは、AIの知能をロボットの身体に結びつけ、物理空間での行動へ転換するアーキテクチャを意味する。従来のAIがディスプレイ上やクラウド上で完結していたのに対し、フィジカルAIは“実際に手を持ち、行動するAI”である。
現在、ロボティクス業界では「世界モデル」や「デジタルツイン」など、AIが環境を認識・理解しながら行動を計画・最適化する技術が注目されている。しかし、それらを応用する分野として、羽田氏は「まだ、どこが勝ち筋かはまだわからない」と語る。例えば、米国のスタートアップであるFigureは、家庭にロボティクス活用の糸口を見つけようとしたり、自動車メーカーとして有名な米国のテスラは自動運転技術を応用して、危険な作業や退屈な単純作業を行う二足歩行の汎用ヒューマノイドロボット開発に乗り出しているように、現在は各社が多様なアプローチで、現実可能なロボティクスの社会実装を模索している段階にある。</p>
<p>こうした中で羽田氏は「日本の勝ち筋はオペレーションの質そのものにある」と指摘する。ロボットの能力を左右するAIの性能は、単にモデルの規模だけではなく、学習に用いるデータの質にも大きく左右される。AIの性能関連でよく話される “スケーリング則” では、モデルの性能は以下の3要素のかけ合わせによって向上すると言われている。</p>
<ul>
<li>データ規模</li>
<li>学習に使われる計算量</li>
<li>パラメーター数</li>
</ul>
<p>このうち、日本の強みは「データ」にある。たとえば羽田空港の清掃業務は、世界的にも高い評価を受けており、2016年には英国SKYTRAXの国際空港評価である「ザ・ワールド・クリーネスト・エアポート」部門において第1位に選ばれている。こうした高度なオペレーションの現場で蓄積される業務データは、AIの学習素材として非常に価値が高い。「良い現場、良いマニュアル、真面目な労働者。それらが揃っている日本は、データ品質の面でも優位に立てる」と羽田氏は語る。日本品質のオペレーションをモデル化し、ロボティクスに実装することで、「ジャパンスタンダード」のような高品質なAI\u002Fロボットサービスを世界に輸出することも可能になるだろう。
羽田氏の言葉を借りれば、「これからのAI開発における生命線は“良いデータ”である」。日本の現場力こそが、フィジカルAI時代におけるグローバル競争の切り札となり得るのだ。</p>
<h2>今、企業が備えるべきこと</h2>
<p>羽田氏は最後に、企業が「今」取り組むべきことについて次のように語った。</p>
<p>「フィジカルAIは、産業界の主戦場になる。だからこそ、現場を俯瞰して自社にしかないオリジナルな業務が何なのかを見極めることが必要だ。自動化のためではなく、自社の強みをデータ化し、新たな競争力を生むためにロボットを活用すべきである」</p>
<p>ロボットの導入は、単に人手を減らす手段ではない。AIとロボティクスの融合により、現場の構造そのものを再定義し、新たな価値創出の礎とするための業務改革なのである。その中でｕｇｏは、日本の産業界が持つ現場力をAIと結びつけ、「使われ続けるロボット」を通じて、新たな価値観を形づくろうとしているのだ。</p>
]]></description>
      <pubDate>Thu, 10 Jul 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/7/10 [THU]拡散モデルベースLLM「Mercury」登場：“描くような生成”で推論を加速、GPT-4.1 Nanoの7倍速——Inception Labsが一般チャット向けモデルを公開</title>
      <link>https://ledge.ai/articles/mercury_diffusion_llm_release</link>
      <description><![CDATA[<p>米Inception Labsは2025年6月17日、拡散モデル（diffusion model）をベースとした大規模言語モデル「Mercury」の一般チャット版を<a href="https:%5Cu002F%5Cu002Fwww.inceptionlabs.ai%5Cu002Fintroducing-mercury-our-general-chat-model">発表</a>{target=“_blank”}した。。同社が2月に発表したコード特化型「<a href="https:%5Cu002F%5Cu002Fledge.ai%5Cu002Farticles%5Cu002Fmercury_coder_diffusion_llm">Mercury Coder</a>{target=“_blank”}」に続くもので、対話型エージェントや音声インターフェースといったリアルタイム応答を重視するユースケースに向けて設計されたモデルである。</p>
<p>Mercuryは、従来の自己回帰型LLMとは異なり、複数のトークンを並列に生成する“描くような生成”プロセス──拡散型LLM（dLLM）アーキテクチャ──を採用しており、推論速度と応答レイテンシーの大幅な削減を実現した。</p>
<h2>最大708トークン／秒、Claude 3.5比で10倍のスループット</h2>
<p>公開された性能指標によると、Mercuryのスループットは708トークン／秒に達しており、同等規模のモデルとされるGPT-4.1 Nano（96トークン／秒）、Claude 3.5 Haiku（67トークン／秒）を大きく上回った。特に、GPT-4.1 Nano比で約7倍、Claude 3.5 Haiku比で約10倍の速度差が確認されている。</p>
<p>また、品質指標においても以下のスコアが報告されている：</p>
<ul>
<li>MMLU-Pro：69%</li>
<li>HumanEval：85%</li>
<li>GPQA Diamond：51%</li>
</ul>
<p>これらのスコアは、「スピード最適化フロンティアモデルと同等以上の性能に相当する」と同社は説明している。</p>
<h3>参考図：Mercuryの位置付け（小型チャットモデル領域）</h3>
<p>評価機関Artificial Analysisによる小型・非推論系モデルの性能比較において、Mercuryは**「高出力速度 × 実用的インテリジェンス」の最適領域（右上象限）**に唯一位置している。</p>
<ul>
<li><strong>出力速度（横軸）</strong> ：1秒あたりの出力トークン数。Mercuryは700超を記録。</li>
<li><strong>知能指数（縦軸）</strong>：Artificial Analysis Intelligence Index（簡易対話・理解力ベンチマーク平均）。</li>
<li><strong>比較対象</strong> ：GPT-4.1 Nano、Claude 3.5 Haiku、Mistral Small、Gemini Flashなど。</li>
</ul>
<p>Mercuryはこの領域で、処理速度と認識性能を両立する唯一のモデルとして可視化されている。
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002Fmercury_intellogence_vs_output_speed_286c30f817%5Cu002Fmercury_intellogence_vs_output_speed_286c30f817.jpg" alt="mercury intellogence vs output speed.jpg" /></p>
<h2>ユースケース：リアルタイム音声や対話Webアプリへの導入を想定</h2>
<p>Mercuryは、従来のLLMでは困難だったリアルタイム応答環境での活用を想定している。とくに以下のような領域に向けた実用が示されている：</p>
<ul>
<li><strong>リアルタイム音声応答</strong> ：翻訳や音声AIアシスタント領域において、標準GPU上での推論でも従来構成（Llama 3.3 70B＋Cerebras）より低レイテンシーであるとされる。</li>
<li><strong>対話型Webアプリケーション</strong> ：Microsoft Build 2025で発表された自然言語インターフェース「NLWeb」プロジェクトにおいて、Inception Labsは初のLLMパートナーに選定されている。Mercuryの高速応答により、「人間同士のような自然なテンポの対話」が可能になるとしている。</li>
</ul>
]]></description>
      <pubDate>Thu, 10 Jul 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/7/8 [TUE]Google、急増する電力需要に備え、米核融合ベンチャーCFSと商用核融合発電200MWの長期購入契約を締結——CFS初の発電所と2030年代供給へ</title>
      <link>https://ledge.ai/articles/google_fusion_power_cfs_ppa</link>
      <description><![CDATA[<p>Googleは2025年6月30日、米国の核融合スタートアップであるCommonwealth Fusion Systems（CFS）と、同社が開発中の商用核融合発電所から200メガワット（MW）の電力を長期購入する契約を締結したことを<a href="https:%5Cu002F%5Cu002Fblog.google%5Cu002Foutreach-initiatives%5Cu002Fsustainability%5Cu002Four-latest-bet-on-a-fusion-powered-future%5Cu002F">発表</a>{target=“_blank”}した。2030年代前半の送電開始が予定されており、企業による核融合電力の調達契約としては過去最大規模とされる。</p>
<p>@<a href="https:%5Cu002F%5Cu002Fwww.youtube.com%5Cu002Fwatch?v=pBIcDNn6rHA">YouTube</a></p>
<h2>核融合発電の実用化を前提に、ARCからの電力供給を確保</h2>
<p>Googleが契約を結んだのは、CFSがバージニア州チェスターフィールドに建設を予定している商用核融合発電所「ARC」である。ARCは、CFSが開発中のコンパクト型トカマク炉を用いたもので、商用化第一号の核融合プラントとなる見通しだ。今回の契約により、Googleは最大200MWの電力を長期にわたり調達する。</p>
<p>また、Googleはこの契約にあわせてCFSへの追加出資も実施。2021年の初期投資に続くものであり、開発資金を通じて発電所の建設を後押しする。さらにGoogleは、将来的に他のCFSプラントから電力を調達するオプションも保持している。</p>
<h2>実証炉「SPARC」の成功が鍵、2030年代前半に送電目標</h2>
<p>ARCの前段階として開発が進む実証炉「SPARC」は、2026年の稼働を予定しており、2027年までにQ値（エネルギー増倍率）1超の達成を目指している。これが商用化の前提となる。核融合による電力は安定供給が可能で、天候に左右される再生可能エネルギーの変動を補う“24時間型ゼロエミッション電源”としての期待が高まっている。</p>
<h2>急増する電力需要と、それに対応するクリーン電源の確保</h2>
<p>Googleは2025年6月に<a href="https:%5Cu002F%5Cu002Fblog.google%5Cu002Foutreach-initiatives%5Cu002Fsustainability%5Cu002Fenvironmental-report-2025%5Cu002F">公開</a>{target=“_blank”}した最新の環境報告書で、2024年のデータセンター電力消費量が前年比27％増となったことを明らかにした。前年（2023年）の増加率は17％だったことから、AIやクラウド需要の拡大にともなう消費電力の加速度的な増加が読み取れる。</p>
<p><strong>2019〜2024年におけるGoogleのデータセンターの電力消費量（棒グラフ）とエネルギー起因排出量（折れ線）</strong>
2024年は電力使用量が過去最高となったが、クリーン電源の導入により排出量は前年より減少している。
<img src="https:%5Cu002F%5Cu002Fstorage.googleapis.com%5Cu002Fledge-ai-prd-public-bucket%5Cu002Fmedia%5Cu002FTrajectory_of_data_center_electricity_consumption_and_data_center_energy_emissions_708f18de8d%5Cu002FTrajectory_of_data_center_electricity_consumption_and_data_center_energy_emissions_708f18de8d.jpg" alt="Trajectory of data center electricity consumption and data center energy emissions.jpg" /></p>
<h2>クリーンエネルギーによる排出削減とPUEの改善</h2>
<p>電力需要が急増する一方で、Googleはクリーンエネルギー導入やインフラ最適化により、2024年のデータセンター由来の温室効果ガス排出量を前年比12％削減した。また、データセンター全体の平均PUE（電力使用効率）は1.09に改善し、過去6年間で初めて1.10を下回ったと報告されている。</p>
<p>同社は2024年だけで8GW相当のクリーン電力購入契約を新たに締結し、世界中で累計22GW以上の再生可能電源を確保している。</p>
<h2>電力の未来に向けた布石</h2>
<p>Googleによる今回の核融合電力の長期購入は、従来型の再エネPPAに加え、次世代エネルギーの担い手としての核融合発電にいち早く賭ける姿勢を明確にしたものだ。送電開始は2030年代前半を予定しており、今後のSPARCの実証進展、ARCの許認可・建設スケジュールがカギを握る。</p>
<p>持続可能なAI時代を支えるエネルギー基盤の再構築は、企業の競争力にも直結する課題となっている。</p>
]]></description>
      <pubDate>Tue, 08 Jul 2025 23:50:00 GMT</pubDate>
    </item>
  </channel>
</rss>