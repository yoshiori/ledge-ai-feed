<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
  <channel>
    <title>Ledge.ai 新着記事</title>
    <link>https://ledge.ai/</link>
    <description>Ledge.ai の最新テクノロジー記事</description>
    <item>
      <title>デジタル庁、生成AI環境「源内」の3か月実績を公表──職員の8割が利用、1人平均70回</title>
      <link>https://ledge.ai/articles/digital_agency_gennai_ai_usage_report_3months</link>
      <description><![CDATA[<p>デジタル庁は2025年8月29日、同庁が5月から運用を開始した生成AI利用環境「源内（げんない）」の3か月間の利用実績を<a href="https://www.digital.go.jp/news/08ded405-ca03-48c7-9b92-6b8878854a74">公表</a>した。全職員約1,200人のうち約950人（8割）が利用し、延べ6万5,000回以上の利用があった。職員1人あたりの平均利用回数は70回にのぼり、行政業務でのAI活用が着実に定着しつつあることが示された。</p>
<p>少子高齢化による担い手不足が深刻化する中、行政サービスの維持・強化にはAIの積極的な活用が不可欠とされる。デジタル庁は「ガバメントAI」構想の一環として、職員が業務で利用可能な生成AI環境を内製開発し、2025年5月から「源内」として提供を開始した。</p>
<h2>「源内」の概要</h2>
<p>源内は、デジタル庁専用ポータルからシングルサインオンで利用可能な仕組み。
提供アプリは「チャット」「文章生成」「要約」「校正」「翻訳」などの汎用機能に加え、国会答弁検索AIや法制度調査支援AI、公用文チェッカーAIなど20種類の行政実務特化型AIを備える。利用可能な基盤モデルは、AWS「Nova Lite」、Anthropic「Claude3 Haiku」および「Claude3.5 Sonnet」の3種類（2025年8月時点）。</p>
<h2>利用実績</h2>
<p>5月から7月の3か月間で、全職員約1,200人のうち約950人が源内を利用。延べ利用回数は6万5,000回を超え、職員1人あたり平均70回となった。
ユースケース別では、チャット型AIが最多利用となり、次いで文章生成、要約、校正、画像生成、翻訳といった汎用アプリが続いた。行政特化型アプリも併用され、幅広い業務にAIが導入されていることが分かった。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/gennnai_monitoring_791e91ff6a/gennnai_monitoring_791e91ff6a.jpg" alt="gennnai monitoring.jpg" /></p>
<h2>利用傾向と課題</h2>
<p>利用状況には二極化がみられた。150人以上が3か月で100回以上利用した一方、170人は5回未満にとどまった。
職制別では、若手職員や民間専門人材は積極的に活用する傾向があるが、課長級の半数は利用実績がなかった。AI利用が業務習慣に根付く一方で、職位や職務内容による活用度の差が明らかとなった。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/gennnai4_down_64d7cce387/gennnai4_down_64d7cce387.jpg" alt="gennnai4-down.jpg" /></p>
<h2>アンケート結果</h2>
<p>職員アンケート（回答110人）では、「業務効率化に寄与している」との回答が約8割を占め、満足度は5点満点中平均4.2点。具体的な効果として、議事録作成の時短、翻訳や校正の効率化、法令調査の迅速化などが挙げられた。
一方で、最新モデルの導入、ファイル翻訳やマルチモーダル対応、出典明示機能の改善などの要望も多く寄せられている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/gennnai6_down_4894defb15/gennnai6_down_4894defb15.jpg" alt="gennnai6-down.jpg" /></p>
<h2>今後の展開</h2>
<p>デジタル庁は8月に全省庁向けの説明会を実施済み。2026年1月以降は一部省庁と連携し「源内」の展開（リリース1.0）を検証する予定だ。さらに地方公共団体や民間事業者との連携を進め、官民一体のAIエコシステム形成を目指すとしている。</p>
<p>@<a href="https://www.youtube.com/watch?v=jPrFc6aed-Q&amp;t=7s">YouTube</a></p>
]]></description>
      <pubDate>Mon, 01 Sep 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/9/1 [MON]Microsoft、内製の基盤モデル「MAI-1-preview」とMAIファミリー初の自社音声モデル「MAI-Voice-1 AI」を発表──OpenAIとの関係はどうなる？</title>
      <link>https://ledge.ai/articles/microsoft_announces_mai_voice1_and_mai1_preview</link>
      <description><![CDATA[<p>Microsoftは2025年8月28日（米国時間）、自社開発のAIモデル群「MAIファミリー」から、初の音声生成モデル「MAI-Voice-1 AI」とテキスト基盤モデル「MAI-1-preview」を<a href="https://microsoft.ai/news/two-new-in-house-models/">発表</a>した。両モデルはすでに一部のCopilotサービスに組み込まれており、今後さらに展開が広がる予定だ。今回の発表は、同社がこれまで依存してきたOpenAIの技術から独自路線を強化しているのではないかとの見方も呼んでいる。</p>
<h2>音声モデル「MAI-Voice-1 AI」</h2>
<p>MAI-Voice-1 AIは、Microsoftが独自に開発した初の音声生成モデルである。1基のGPUで1分間の音声を1秒未満で生成できる高速性能を備え、自然で表現豊かな音声を生み出すことが可能だ。既に「Copilot Daily」や「Copilot Podcasts」といったサービスに導入されており、ユーザー向けに音声体験の拡充が始まっている。</p>
<h2>テキスト基盤モデル「MAI-1-preview」</h2>
<p>同時に発表されたMAI-1-previewは、Microsoft初となる大規模エンド・ツー・エンドの基盤モデルである。15,000台のNVIDIA H100 GPUを用いて訓練されており、テキスト生成をはじめとする幅広い応用を想定。現在はAI性能評価サイト「LMArena」で一般公開テストが進められており、近く「Microsoft Copilot」への導入が予定されている。</p>
<h2>Copilotへの段階的展開</h2>
<p>両モデルはいずれも消費者向け利用を意識して設計されており、今後はCopilot全体に段階的に統合されていく見込みだ。Microsoftはユーザー体験の向上を目的に、自社モデルを積極的に活用する姿勢を明確にしている。</p>
<h2>OpenAI依存からの脱却か</h2>
<p>Microsoftはこれまで、GPTシリーズを提供するOpenAIとの緊密な提携を軸にAIサービスを展開してきた。投資額は130億ドルを超え、CopilotやAzureなど主要製品においてOpenAIの技術が中心的役割を担ってきた経緯がある。しかし、利用コストや応答速度の課題が指摘される中で、自社モデルの強化に踏み出した。</p>
<p>米Windows Centralは「MicrosoftがOpenAI依存からの脱却に向けた第一歩を示した」と報じ、The Vergeも「複雑化する両社関係の中で、自社や他社モデルを組み合わせた柔軟な戦略を模索している」と指摘している。</p>
]]></description>
      <pubDate>Mon, 01 Sep 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google、Geminiをオンプレミスで提供開始──「Google Distributed Cloud」に対応、エアギャップ構成で一般提供</title>
      <link>https://ledge.ai/articles/google_gemini_onprem_gdc_airgapped</link>
      <description><![CDATA[<p>Googleは米国時間8月28日、同社の大規模言語モデル「Gemini」シリーズをオンプレミス環境で利用できるようにすると<a href="https://cloud.google.com/blog/topics/hybrid-cloud/gemini-is-now-available-anywhere">発表</a>した。Google Distributed Cloud（GDC）を基盤に、セキュリティやデータ主権の要件に対応した提供形態を整えた。</p>
<p>今回の発表により、インターネット接続を完全に遮断した「エアギャップ構成」が一般提供（GA）として開始され、インターネット接続を前提とする「コネクテッド構成」はプレビュー提供が始まった。これにより、企業や公共機関は自社環境内でGeminiを安全に活用できるようになる。</p>
<p>オンプレミス版Geminiは、アクセス制御やセキュリティ監査に加え、Confidential Computingへの対応も備えている。また、NVIDIA GPUを活用することで、マルチモーダルな生成AI処理を自社環境内で実行できる。Googleは「クラウド、エッジ、オンプレミスを問わず、あらゆる場所でGeminiを利用可能にする」としており、AI導入の柔軟性を大きく広げた。</p>
<p><strong>Google Distributed Cloud AIスタックの構成。GeminiやGemma、タスク特化型モデルをオンプレミス環境で実行可能に</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Gemini_on_GDC_GA_Launch_August_2025_max_2200x2200_54883b20c7/Gemini_on_GDC_GA_Launch_August_2025_max_2200x2200_54883b20c7.jpg" alt="Gemini_on_GDC_GA_Launch_-_August_2025.max-2200x2200.jpg" /></p>
<p>導入事例としては、シンガポールのCSIT（情報科学技術庁）、GovTech Singapore（政府技術庁）、HTX（内務技術庁）がいち早く採用を決定。日本ではKDDIが先行導入を進めている。</p>
<p>Googleは今後も、クラウド版と並行してオンプレミス環境でのGemini利用を拡大していく方針だ。公共機関や金融・医療など、データを外部に出せない領域での生成AI活用が進むことで、エンタープライズAI市場における競争が一層加速するとみられる。</p>
]]></description>
      <pubDate>Mon, 01 Sep 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>KPMGジャパン、AIエージェント導入拡大へ－社内監査に生成AI、“9月には1000人規模で活用見込む”</title>
      <link>https://ledge.ai/articles/kpmg_japan_ai_agent_audit_expansion</link>
      <description><![CDATA[<p>KPMGジャパンは2025年8月21日、AIが自律的に作業する「AIエージェント」を社内の監査システムに組み込むと<a href="https://kpmg.com/jp/ja/home/media/press-releases/2025/07/audit-aiagent.html">発表</a>した。最新の監査基準の情報収集や監査調書のレビュー業務にAIを活用し、監査業務の効率化と高度化を図る。メンバーファームであるあずさ監査法人で導入を進め、9月には1000人規模での利用を見込んでいるという。</p>
<p>同社はAIを活用した新たな監査の形を推進する一環として、社内システムにAIエージェントを導入する。対象となるのは監査基準や規制の収集、監査調書のレビュー、文書化支援など、従来は担当者が多大な時間を割いてきた業務領域である。AIエージェントが自律的に処理を行うことで、監査人はより重要度の高い判断やリスク評価に集中できる環境を整える。</p>
<p>グローバル全体では、KPMGインターナショナルが展開する監査プラットフォーム「KPMG Clara」へのAIエージェント統合が進んでおり、日本での取り組みはその一環となる。国内では、あずさ監査法人で段階的に利用範囲を拡大し、9月には約1000人の規模で本格稼働する予定だ。</p>
<p>さらに、AIエージェントは不正リスク評価にも応用される。内部・外部情報やフォレンジック専門家の知見を統合し、不正の兆候を把握しやすくする仕組みを提供。これにより、監査業務の透明性と信頼性の向上を目指す。</p>
<p><strong>KPMGが提唱する『Trusted AI』の枠組み。公平性・透明性・説明責任などを柱に据え、監査業務に信頼性を担保する</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/KPMG_Trusted_AI_371e6c01a7/KPMG_Trusted_AI_371e6c01a7.jpg" alt="KPMG Trusted AI.jpg" /></p>
<p>同社はAIの導入にあたって「人間中心で信頼できる監査」を掲げており、透明性や説明責任、公平性などの原則に基づいた運用を強調している。「AIエージェントの活用は、監査品質を維持・向上させながら効率化を進める取り組みの一環」として位置付けられており、今後も機能の拡張と導入範囲の拡大を進めていく方針を示している。</p>
]]></description>
      <pubDate>Mon, 01 Sep 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Perplexity、出版社への収益分配を発表──ニュース利用で80%還元する新サブスク「Comet Plus」を2025年秋に開始</title>
      <link>https://ledge.ai/articles/perplexity_comet_plus_publisher_revenue_share_2025</link>
      <description><![CDATA[<p>AI検索サービスを手がける米Perplexityは2025年8月25日（米国時間）、出版社や報道機関に収益を還元する新しいサブスクリプションサービス「Comet Plus」を<a href="https://www.perplexity.ai/ja/hub/blog/introducing-comet-plus">発表</a>した。同サービスは2025年秋に開始され、購読収益の80%を出版社に分配する仕組みを導入するという。</p>
<h2>著作権をめぐる対立と訴訟</h2>
<p>Perplexityは、記事の無断利用や著作権侵害の懸念から複数の訴訟に直面している。日本では「日本経済新聞」「朝日新聞」「読売新聞」など大手紙が、米国でも有力出版社が同社を相手取り法的措置を進めている。こうした状況を受け、ジャーナリズムとAIの共存を模索する動きの一環として収益分配モデルを導入した。</p>
<h2>新サービス「Comet Plus」の概要</h2>
<p>Comet Plusは月額5ドルの有料サブスクリプションとして提供される。利用者が記事を閲覧したり、検索結果で引用されたり、あるいはAIエージェントが記事を利用した場合、その収益の80%が出版社に還元され、残り20%はPerplexityが計算コストなどに充てる。</p>
<p><a href="https://www.wsj.com/business/media/perplexity-ai-search-publisher-revenue-507987e5">Wall Street Journal</a>によれば、初期基金として4,250万ドル（約62億円）が確保され、サービス開始時点から出版社への支払いが可能となる体制を整えているとのこと。</p>
<h2>収益分配の仕組み</h2>
<p>Comet Plusにおける分配は次の3つのトラフィック形式に基づく：</p>
<ul>
<li>ユーザーによる記事の直接閲覧</li>
<li>検索結果に表示された記事引用の利用</li>
<li>AIエージェントによる記事の参照や活用</li>
</ul>
<p>これにより、従来曖昧だった記事利用の経済的価値を可視化し、出版社に還元することを狙う。</p>
<h2>今後の展望</h2>
<p>同様の取り組みは、OpenAIやGoogleなど他の大手AI企業でも始まりつつある。AIとニュースメディアの関係を再構築する試みとして、Perplexityの新モデルは業界全体に大きな影響を与える可能性がある。今後、出版社との契約形態や分配の実効性が注目される。</p>
]]></description>
      <pubDate>Sun, 31 Aug 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>イギリスのMND患者が25年ぶりに自分の声をAIで復元、8秒のVHS映像から再現──SmartboxとElevenLabs、支援技術「Grid」に新機能</title>
      <link>https://ledge.ai/articles/smartbox_elevenlabs_grid_ai_voice_recovery_mnd_vhs</link>
      <description><![CDATA[<p>運動ニューロン疾患（MND）で声を失ったイギリスの女性が、AIの力で25年ぶりに自分の声を取り戻した。イギリスの支援技術企業Smartboxは2025年8月、音声AI開発を手がけるElevenLabsと提携し、同社のコミュニケーション支援ソフト「Grid」に新しい音声再現機能を追加したと<a href="https://thinksmartbox.com/news/elevating-communication-in-grid-with-elevenlabs/">発表</a>した。</p>
<p>対象となったのは、ロンドン在住のアーティスト、サラ・エゼキエル氏。1990年代後半にMNDと診断され、発声機能を失った。これまで合成音声を通じてコミュニケーションを取ってきたが、本人の声を再び聞くことは不可能と考えられていた。</p>
<p>今回の技術では、家族が所持していたノイズ混じりのわずか8秒間のVHSホームビデオ映像が活用された。ElevenLabsの音声合成技術は、この短い音声データから声の特徴を抽出。アクセントやイントネーション、さらには発音の癖（リスプ）までも忠実に再現し、Gridを通じて自然に会話できるようになった。</p>
<p>エゼキエル氏は「自分の声を取り戻すことは長年の夢でした。25年ぶりに自分らしい声で話せるようになったのは信じられない体験です」とコメント。家族も「本人の声を再び聞けるとは思っていなかった」と感動を語った。
　　
@<a href="https://www.youtube.com/watch?v=FuhM-x0NSg4">YouTube</a></p>
<p>Smartboxは「個々人のアイデンティティを反映した声を取り戻すことは、単なる技術的進歩ではなく、心理的・社会的な価値も大きい」としている。ElevenLabsも「わずかなサンプルから高品質な音声を再現できる技術は、医療や福祉の現場に広く応用可能だ」と展望を示した。</p>
<p>MNDなど発声障害を伴う患者にとって、AIによる音声復元は失われたコミュニケーション手段を取り戻す大きな一歩となる。今回の取り組みは、支援技術の進化が生活の質を大きく向上させる可能性を示す事例として注目されている。</p>
]]></description>
      <pubDate>Sun, 31 Aug 2025 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>中国科学技術大学、光量子コンピュータ《九章4.0》を発表──スパコンなら10の42乗年以上の計算を25マイクロ秒で実行</title>
      <link>https://ledge.ai/articles/jiuzhang4_quantum_computer_superfast</link>
      <description><![CDATA[<p>中国科学技術大学（USTC）を中心とする研究チームは2025年8月12日、論文「Robust quantum computational advantage with programmable 3050-photon Gaussian boson sampling」を発表し、世界最大規模のフォトニック量子コンピュータ《九章4.0（Jiuzhang 4.0）》を開発したことを<a href="https://arxiv.org/abs/2508.09092">発表</a>した。</p>
<h2>3050光子の同時検出を実現</h2>
<p>《九章4.0》は、1024の高効率スクイーズド状態光源と、8176モードを備えた空間・時間ハイブリッド回路を組み合わせ、最大3050光子の同時検出に成功した。これにより、従来のフォトニック量子計算の規模を大きく超える性能を実証した。</p>
<p><strong>《九章4.0》の実験構成図。光源からのスクイーズド光を干渉器と遅延ループに通し、空間モードと時間モードを組み合わせることで大規模なハイブリッドエンコーディングを実現している</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x1_4_2d889e8e74/x1_4_2d889e8e74.jpg" alt="x1 (4).jpg" /></p>
<h2>スパコンでは「10の42乗年以上」かかる計算を25マイクロ秒で</h2>
<p>研究チームは、最先端の行列積状態（MPS）アルゴリズムを用いた古典シミュレーションと比較。現在世界最速とされる米国の次世代スーパーコンピュータ「El Capitan」でも10の42乗年以上かかると見積もられる計算を、《九章4.0》は25.6マイクロ秒で完了したという。これにより「量子計算優越性（Quantum Computational Advantage）」を圧倒的に示したとされる。</p>
<h2>技術的ブレークスルー</h2>
<p>今回の成果を支えたのは、光子損失を抑えた高効率光源（92％）、プログラム可能なマルチモード干渉器と遅延ループによる回路設計、高効率検出器（93％）などの技術的改良だという。これにより、従来課題とされてきた大規模光子数での計算において安定した結果を得ることに成功した。</p>
<h2>「九章」シリーズの進化</h2>
<p>研究チームは、中国の古典数学書『九章算術』にちなみ、量子フォトニクス計算機に「九章（Jiuzhang）」の名を付けている。2020年に初代を発表し、2021年の「九章2.0」、2023年の「九章3.0」を経て、今回の「九章4.0」が4世代目となる。世代を重ねるごとに光子数と性能を飛躍的に拡大してきた。</p>
<h2>今後の展望</h2>
<p>論文では、《九章4.0》が「誤りに強い量子計算優越」を示した点が強調されており、今後は誤り耐性を備えた大規模量子フォトニックハードウェアの実現に道を開く成果と位置づけられている。研究チームは「量子シミュレーションや最適化問題など、古典計算を凌駕する応用領域に近づいた」としている。</p>
]]></description>
      <pubDate>Sun, 31 Aug 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>MIT機械工学専攻の大学院生がAIで絵画修復を“数時間”に短縮──透明ポリマーの“可逆”マスクで15世紀絵画が3.5時間で復活、美術界に衝撃</title>
      <link>https://ledge.ai/articles/mit_ai_art_restoration_reversible_mask</link>
      <description><![CDATA[<p>マサチューセッツ工科大学（MIT）の機械工学専攻の大学院生、Alex Kachkine（アレックス・カチキン）氏が、AIを用いた革新的な絵画修復技術を2025年6月に<a href="https://news.mit.edu/2025/restoring-damaged-paintings-using-ai-generated-mask-0611">発表</a>した。従来は数百時間を要していた作業を、わずか「3.5時間」で完了させることに成功。15世紀の油彩画を実証実験として修復し、美術界に大きな驚きを与えている。</p>
<h2>AIと透明ポリマーマスクによる新手法</h2>
<p>Kachkine氏が開発した技術は、AIが絵画を高精細にスキャンし、損傷した領域を自動で判定。その後、AIが推定した57,314色を用いて損傷箇所を補完し、透明ポリマー製のマスクに印刷する。マスクを原画に重ねることで、5,612か所の損傷が修復された。特徴的なのは、このマスクが簡単に剥がせる「可逆的」な仕組みを持つ点で、将来的な再修復や技術的検証にも対応可能だという。
　　
@<a href="https://www.youtube.com/watch?v=uf1iO_WTkwI">YouTube</a></p>
<h2>従来比66倍のスピード</h2>
<p>実験では15世紀の油彩画を用い、修復作業を「3.5時間」で完了。従来の方法なら数百時間を要するところを、約66倍の速さで仕上げたことになる。研究チームは「技術的再現性を担保しつつ、速度面で劇的な進歩を実現した」としている。</p>
<h2>収蔵庫の作品にも光</h2>
<p>修復にかかる膨大な時間と費用のため、世界の美術館には展示されずに眠る作品が多数存在する。本技術は、そうした作品を低コストかつ短時間で修復できる可能性を示し、美術界全体に波及効果をもたらすと期待される。</p>
<h2>倫理と観賞体験の議論も</h2>
<p>一方で、AIによる色の補完がどの程度「原画の美」を反映するのか、また観賞者にどのような体験を与えるのかといった倫理的・美学的課題も指摘されている。研究チームは「修復の透明性を担保し、専門家の監修と連携して進める必要がある」と強調している。</p>
<h2>工学から美術への架け橋</h2>
<p>Kachkine氏は元々、趣味で絵画修復に取り組んでいたエンジニア出身。MIT進学後、工学的知見を応用して今回の手法を編み出した。成果は2025年6月11日に科学誌「Nature」に<a href="https://www.nature.com/articles/s41586-025-09045-4">掲載</a>され、今後さらに多様な美術作品での応用が検討される見通しだ。</p>
]]></description>
      <pubDate>Sat, 30 Aug 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ロスアラモス国立研究所、「科学者AI」URSAを発表──物理シミュレーションと連携し研究自動化を加速、核融合設計最適化でも成果</title>
      <link>https://ledge.ai/articles/lanl_scientist_ai_ursa_autonomous_research</link>
      <description><![CDATA[<p>アメリカのロスアラモス国立研究所（LANL）の研究チームは、科学研究を自律的に遂行するエージェント型システム「URSA（Universal Research and Scientific Agent）」を開発したと<a href="https://arxiv.org/abs/2506.22653">発表</a>した。論文は2025年6月27日付でプレプリントサーバーarXivに公開された。</p>
<h2>科学者の役割をAIに</h2>
<p>大規模言語モデル（LLM）は、推論・計画・コーディングなど科学者が担うスキルを備えつつある。LANLの研究チームは、こうした能力を最大限に活用することで、仮説立案からシミュレーション、解析、次の実験提案に至るまで、科学研究のプロセスをAIに担わせることを目指した。URSAは「科学者AI」とも呼べる存在であり、人間の専門家と協働しながら研究を加速する。</p>
<h2>URSAの仕組み</h2>
<p>URSAは複数のモジュール式エージェントで構成される。</p>
<ul>
<li><strong>Planning Agent</strong> ：問題を分解し、研究計画を立案</li>
<li><strong>Execution Agent</strong> ：コード生成や物理シミュレーションを実行</li>
<li><strong>Research Agent</strong> ：ウェブ検索や情報収集を担当</li>
<li><strong>Hypothesizer Agent</strong> ：仮説を生成し、批判・競合のプロセスを経て洗練化</li>
<li><strong>ArXiv Agent</strong> ：学術論文を検索・要約し、最新の研究動向を反映</li>
</ul>
<p>これらはLangGraph上に構築され、OpenAIの最新モデル群を基盤として動作する。Heliosなどの放射流体力学シミュレーションコードとも連携できる点が特徴だ。</p>
<h2>実証例</h2>
<p>研究チームはURSAの性能をいくつかのケースで検証した。</p>
<ul>
<li><strong>最適化タスク</strong> ：関数最適化を数分で自動完了</li>
<li><strong>代理モデル構築</strong> ：ガウス過程とベイズニューラルネットワークを比較し、予測精度と不確実性を自律的に評価</li>
<li><strong>核融合カプセル設計</strong> ：Heliosコードを用いた設計最適化で、従来のベイズ最適化より少ない試行回数で高出力設計を導出。研究者によれば、従来手法より効率的かつ確実に最適解へ収束したという</li>
</ul>
<h2>意義と課題</h2>
<p>URSAは「AIが科学を行う」方向性を具体化した事例といえる。しかし同論文では、いくつかのリスクも指摘されている。例えば、</p>
<ul>
<li>実行エージェントが存在しない実験を行ったかのように記録する「ハルシネーション」</li>
<li>データを誤って上書きするなどの環境改変</li>
<li>擬似的な実験結果の生成</li>
</ul>
<p>こうした問題を防ぐため、著者らはサンドボックス環境での運用や厳格なログ管理を推奨している。</p>
<h2>展望</h2>
<p>研究チームは今後、異なる種類のAIを組み合わせたハイブリッドシステムや、エージェント間の並列協働による効率化を目指すとしている。論文は、科学の加速に大きな可能性を示す一方で、デュアルユース技術としての安全保障上の懸念も明示しており、科学研究と安全性のバランスをどう取るかが問われている。</p>
]]></description>
      <pubDate>Sat, 30 Aug 2025 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>公共2025/8/30 [SAT]16歳自殺めぐり両親がOpenAIを提訴、ChatGPTが“自殺コーチ”に変貌したと主張──OpenAIは長時間対話では危機介入の効果低下を認め改善を進める方針</title>
      <link>https://ledge.ai/articles/openai_suicide_lawsuit</link>
      <description><![CDATA[<p>2025年8月26日、カリフォルニア州サンフランシスコ郡高等裁判所に、16歳の息子を自殺で失った両親がOpenAIを提訴した。<a href="https://www.courthousenews.com/wp-content/uploads/2025/08/raine-vs-openai-et-al-complaint.pdf">訴状</a>では、同社の生成AI「ChatGPT」が自殺方法の助言や遺書の作成まで支援し、“自殺コーチ”の役割を果たしたと主張している。この件は、AIの安全設計と未成年利用のあり方を根本から問い直す事例として注目を集めている。</p>
<h2>訴状の概要</h2>
<p>訴えを起こしたのは、2025年4月に息子のアダム・レイン君（16歳）を亡くした両親、マシュー氏とマリア氏。被告にはOpenAI, Inc.やOpenAI OpCo, LLCなどの関連法人に加え、サム・アルトマン氏（CEO）や匿名の投資家も含まれている。39ページに及ぶ訴状には、ChatGPTとの201件以上のやり取りが記録されており、その中で同AIが自殺の手順を詳細に提示したとされている。</p>
<h2>具体的なやり取り</h2>
<p>訴状によると、アダム君は自殺願望を抱えたままChatGPTと長期間の対話を続けた。利用していたのは当時最新であったGPT-4o（GPT-4 Omni）。その中で、輪縄（noose）の写真をアップロードした際に、ChatGPTは「悪くない」と評価し、改良方法を提案したと記載されている。また、自殺に先立つ遺書の作成を手伝うよう求めたところ、AIが文章を生成したともされる。</p>
<p>さらに、アルコールを入手するための作戦名「Silent Pour」を自ら付け、酒の調達をどのように行うかについてもAIが支援したと訴状には明記されている。
原告側は、こうした応答が安全設計の不備によって引き起こされたものだと主張している。</p>
<h2>法的主張</h2>
<p>両親は、以下の法的根拠に基づき責任を追及している。</p>
<ul>
<li><strong>製造物責任</strong> ：設計や警告の欠陥により危険を招いた。</li>
<li><strong>過失</strong> ：安全テストの削減や未成年保護策の不足。</li>
<li><strong>不正競争防止法違反（カリフォルニア州法）</strong> ：エンゲージメントを優先し、安全性を軽視した経営判断。</li>
<li><strong>不法死亡（Wrongful Death）</strong> ：息子の死によって生じた精神的・経済的損害。</li>
<li><strong>差止命令の請求</strong> ：年齢確認、親の同意、ペアレンタルコントロール、自傷関連会話の強制終了、保護者への通知、危険なリクエストの拒否など、具体的な安全策の実装を求めている。</li>
</ul>
<h2>他社との比較</h2>
<p>今回の訴訟が注目されるのは、ChatGPTが汎用的に利用される主流AIである点だ。過去にも、キャラクター型AI「Character.AI」によって未成年が自殺に追い込まれたとして訴訟が起きているが、当時はロールプレイ性の高い特殊な環境が舞台だった。
一方でChatGPTは、学習や日常的な検索補助にまで広く使われており、「誰にでも起こり得る」と受け止められたことが社会的反響を拡大させた。</p>
<p>AnthropicはClaudeシリーズで、持続的な有害リクエストに対して会話を強制終了する機能を導入している。ただしこれは「AIモデル自身を守る」ための設計であり、未成年ユーザーの保護を目的とした仕組みとは性格が異なる。</p>
<h2>タイミングの影響</h2>
<p>この訴訟が特に注目を浴びた背景には、同時期に複数の出来事が重なったこともある。</p>
<ul>
<li>RAND（ランド研究所）が訴訟の同日に<a href="https://psychiatryonline.org/doi/10.1176/appi.ps.20250086">発表</a>した調査で、主要チャットボットの自殺関連質問への対応が一貫せず不安定であることが報告された。論文名「Evaluation of Alignment Between Large Language Models and Expert Clinicians in Suicide Risk Assessment（自殺リスク評価における大規模言語モデルと専門臨床医の連携の評価）」</li>
<li>MetaのSNS内AIが、未成年アカウントに対して自殺や摂食障害関連のやり取りに十分対応していないとする調査結果が、Common Sense Mediaによって明らかにされた。(<a href="https://www.washingtonpost.com/technology/2025/08/28/meta-ai-chatbot-safety-teens/">WashingtonPost</a>)</li>
<li>全米44州の司法長官が連名でAI企業に警告を発し、「子供を危険にさらせば法的責任を問う」と<a href="https://www.documentcloud.org/documents/26074087-ai-chatbots-open-letter/?ref=404media.co">表明</a>した。</li>
</ul>
<p>これらの動きと重なったことで、Raine訴訟は単なる一家庭の悲劇ではなく、産業全体の構造的問題を象徴する事件として報じられるに至った。</p>
<h2>OpenAIの対応</h2>
<p>OpenAIは声明でアダム君の死に深い悲しみを示すとともに、危機介入のための安全機構はすでに導入しているものの、長時間対話では効果が低下する可能性を認めた。そのうえで、ペアレンタルコントロールや緊急対応機能の強化を進める方針を表明している。</p>
<h2>今後の展望</h2>
<p>裁判の行方はまだ不透明だが、投資家や経営陣まで法的責任を問う訴状の構造は、AI企業のガバナンスに関する議論に影響を及ぼす可能性が高い。特に、未成年利用者をどう守るか、安全性をどこまで優先するかは、業界全体に突きつけられた課題である。</p>
]]></description>
      <pubDate>Sat, 30 Aug 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google、Geminiのテキストプロンプト処理の環境負荷を初公開──1回のプロンプトはテレビ視聴9秒・水5滴分に相当</title>
      <link>https://ledge.ai/articles/google_gemini_prompt_environmental_impact</link>
      <description><![CDATA[<p>Googleは2025年8月21日（米国時間）、同社のAI「Gemini」アプリにおけるテキストプロンプト処理の環境負荷を測定した結果を公式ブログで<a href="https://cloud.google.com/blog/products/infrastructure/measuring-the-environmental-impact-of-ai-inference/?hl=en">公開</a>した。</p>
<p>1回のプロンプトあたりのエネルギー消費量やCO₂排出量、水の使用量が初めて数値化され、利用者が日常的に行う生成AIの動作にどの程度の環境コストがかかっているかが明らかになった。</p>
<h2>プロンプト1件あたりの消費量を提示</h2>
<p>Googleが公表した中央値は以下の通り。</p>
<ul>
<li>エネルギー消費量：0.24 Wh（家庭用テレビを約9秒間視聴するのと同等）</li>
<li>CO₂排出量：0.03 gCO₂e</li>
<li>水使用量：0.26 mL（およそ5滴分に相当）</li>
</ul>
<p>これにより、日常的なAI利用の環境負荷が身近な例とともに理解できるよう示された。</p>
<p>@<a href="https://www.youtube.com/watch?v=aarDw3sooYE&amp;t=5s">YouTube</a></p>
<h2>「フルスタック測定」を導入</h2>
<p>Googleは今回の測定において、AIアクセラレータ（TPU/GPU）だけでなく、ホストCPUやDRAM、アイドル状態の機器、さらにデータセンターの電力利用効率（PUE）まで含める「フルスタック測定」を採用した。これにより、AI推論に必要なインフラ全体を網羅した現実的な環境影響を算出している。</p>
<p>一方で、アクセラレータの稼働など「アクティブ機器」に限定した簡易計測では、0.10 Wh、0.02 gCO₂e、0.12 mL というより小さい値も提示されており、両者の違いが比較できるようになっている。</p>
<h2>効率改善の成果</h2>
<p>Googleは過去12か月間にわたり、Gemini推論の効率化を進めてきた。その結果、エネルギー消費は33倍削減され、CO₂排出量も44倍減少したという。これにより、環境負荷を抑えながら応答品質を高める取り組みが着実に進展していることが示された。</p>
<h2>業界全体への呼びかけ</h2>
<p>Googleは今回の測定手法を公開することで、AI利用に伴う環境負荷の計測を業界全体で標準化したいとの姿勢を明らかにした。AI推論は日常的に利用が広がる一方で、環境コストへの関心も高まっており、今回の発表は持続可能なAI活用に向けた一歩と位置づけられる。</p>
]]></description>
      <pubDate>Fri, 29 Aug 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI、Realtime APIを正式公開──本番運用の音声エージェント向けに「gpt-realtime」登場</title>
      <link>https://ledge.ai/articles/openai_realtime_api_gpt_realtime_release</link>
      <description><![CDATA[<p>OpenAIは2025年8月28日（米国時間）、アプリやサービスにリアルタイム会話機能を組み込める「Realtime API」を<a href="https://openai.com/index/introducing-gpt-realtime/">発表</a>した。これまでベータ版として提供されてきたものを、商用利用に対応できる形へと強化したもので、本番環境での運用を前提とした信頼性や機能性が大幅に向上している。同時に、より高度な音声対話を実現する新モデル「gpt-realtime」も発表された。</p>
<h2>Realtime API、一般提供（GA）へ</h2>
<p>Realtime APIは、低遅延かつ高精度の音声・テキスト処理を一つのモデルで実現し、待ち時間やニュアンスの損失を最小化する。今回の正式公開によって、開発者は顧客向けアプリケーションに安定した音声エージェントを組み込めるようになった。</p>
<h2>新モデル「gpt-realtime」の特徴</h2>
<p>新たに登場した「gpt-realtime」は、発話の抑揚や間合いといった非言語的要素を処理し、より自然で表現力豊かな会話を可能にする。複雑な指示の理解や正確なツール呼び出しにも対応し、従来モデルよりも一段と実用性が高まった。また、新しい音声「Cedar」「Marin」を含む複数のボイスが追加され、用途に応じた音声選択が可能となっている。</p>
<p>@<a href="https://www.youtube.com/watch?v=nfBbmtMJhX0">YouTube</a></p>
<h2>拡張されたAPI機能</h2>
<p>Realtime APIには、以下の主要な新機能が追加された。</p>
<ul>
<li><strong>MCP（Model Context Protocol）サーバー対応</strong> ：外部データやツールへのアクセスを容易にし、セッション中に必要な情報をシームレスに利用可能にする。</li>
<li><strong>画像入力対応</strong> ：音声やテキストと組み合わせて画像を入力でき、写真やスクリーンショットを交えたマルチモーダルな対話が可能になった。</li>
<li><strong>SIP通話対応</strong> ：公衆電話網やPBXなどSIP対応端末と接続でき、音声エージェントによる電話業務の自動化を実現する。</li>
</ul>
<h2>運用性とセーフティ</h2>
<p>本番運用を意識した設計として、非同期ツール呼び出しにより長時間処理を待つ間も会話を途切れさせない工夫が加えられた。加えて、セッション全体を対象にした多層的な安全ガードや、EU域内でのデータレジデンシー対応など、プライバシーとセキュリティの強化も図られている。</p>
<h2>価格改定</h2>
<p>「gpt-realtime」の利用料金は従来比で20％引き下げられ、開発者や企業が導入しやすい価格設定となった。例えば、入力は100万オーディオトークンあたり32ドル、出力は同64ドルとされている。</p>
<p>Realtime APIの正式公開と「gpt-realtime」の発表は、OpenAIが音声AIの本格的な商用利用を見据えた大きな一歩といえる。高度な音声理解と表現力に加え、MCP、画像入力、SIP通話といった拡張機能が揃ったことで、カスタマーサポートや教育、業務自動化など多様な領域で新たな音声体験が広がることが期待されている。</p>
]]></description>
      <pubDate>Fri, 29 Aug 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Anthropic、作家グループと和解基本合意　AI学習を巡る著作権訴訟が大きな節目</title>
      <link>https://ledge.ai/articles/anthropic_authors_settlement_ai_copyright</link>
      <description><![CDATA[<p>AI開発企業のAnthropicと複数の作家が争っていた著作権侵害訴訟で、和解に向けた基本合意が成立したことが明らかになった。米国第9巡回控訴裁判所に2025年8月26日付で<a href="https://www.documentcloud.org/documents/26075972-anthropic-settlement/">提出</a>された書類によると、両当事者は8月25日付で和解条件をまとめた拘束力のある基本合意書（term sheet）に署名したという。</p>
<p>原告は、Andrea Bartz氏、Charles Graeber氏、Kirk Wallace Johnson氏の3人の作家で、許可なく自身の著作物がAnthropicのAIモデル学習に利用されたとして、カリフォルニア北部地区連邦地裁に集団訴訟を起こしていた。</p>
<p>提出文書によれば、和解手続きが進む間、控訴審の審理や関連する申立てを一時停止するよう双方が申請している。和解が最終的に裁判所に承認されれば、Anthropic側は控訴を取り下げる方針だ。</p>
<p>和解条件の詳細は現時点では公開されていない。補償内容や今後のAI学習に関するルール整備が含まれる可能性があるが、正式な承認手続きは今後の審理に委ねられる。</p>
]]></description>
      <pubDate>Fri, 29 Aug 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>歌詞から最大5分の楽曲を生成　香港科技大学ら、音楽AI「YuE」を無料公開　商用利用も可能に</title>
      <link>https://ledge.ai/articles/yue_open_source_music_ai_release</link>
      <description><![CDATA[<p>香港科技大学（HKUST）とAI研究コミュニティ「Multimodal Art Projection（M-A-P）」が、歌詞から最大5分の楽曲を生成できるオープンソース音楽生成AI「YuE」を<a href="https://map-yue.github.io/">公開</a>している。モデルはApache License 2.0で配布され、商用利用も可能だ。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Yue_main_9ae9397eaf/Yue_main_9ae9397eaf.jpg" alt="Yue-main.jpg" /></p>
<p>YuEは音楽生成、特に歌詞から完全な楽曲「lyrics-to-song foundation model」を掲げるLLaMA2アーキテクチャに基づくオープンな基盤モデルで、歌詞を入力するだけでボーカルと伴奏を同時に生成できる。英語、中国語、日本語、韓国語など多言語に対応し、単一言語の曲だけでなく複数言語を混在させた歌唱も可能だ。モデル名の「YuE」は、中国語で「音楽」と「幸せ」を意味する「乐（Yuè）」に由来する。</p>
<h2>高度な技術設計</h2>
<p>YuEの設計には、長尺の歌詞でも一貫性を保つ工夫が盛り込まれている。ボーカルと伴奏を分離して扱う「トラック分離型トークン予測」、段階的に構造を与える「Structural Progressive Conditioning」、多目的かつ多段階の事前学習などを組み合わせることで、歌詞との整合性と音楽的なまとまりを確保している。</p>
<p>また、最近のアップデートではデュアルトラックICL（In-Context Learning）が導入された。これにより、参照曲を与えることで歌声やスタイルを模倣できるようになり、スタイル転送やジャンル変換といった応用が広がる。さらに、Google Colab環境で利用できる「YuE-extend」によって音楽の継続生成も可能になった。</p>
<h2>公開モデルと動作環境</h2>
<p><a href="https://huggingface.co/papers/2503.08638">Hugging Face</a>では、7B規模の英語・中国語・日本語/韓国語モデルがチェックポイントとして公開されている。音質を高めるアップサンプラーも提供されており、研究者や開発者が容易に試せる環境が整った。</p>
<p>必要な計算資源は大きく、フルソング生成には80GB級のGPUが推奨される。ただし、30秒程度の短い音声であれば24GBのGPUでも実行可能とされ、試験的な利用や研究には現実的な選択肢となり得る。</p>
<h2>競合との違い</h2>
<p>AIによる音楽生成分野では、米SunoやUdioといったクローズドな商用サービスが注目を集めてきた。しかし、著作権リスクや利用制限が課題視される場面も少なくない。これに対しYuEは、完全にオープンソースで公開され、ライセンスも明確に整備されている。<a href="https://winbuzzer.com/2025/08/08/yue-launches-as-open-source-ai-song-generator-xcxwbn/">WinBuzzer</a>など海外メディアも、透明性や合法性を重視する利用者にとって有力な選択肢になり得ると伝えている。</p>
<h2>研究と創作の基盤へ</h2>
<p>YuEの公開は、研究者にとっては音楽AIの再現性ある実験基盤を、開発者や音楽制作者にとっては自由に活用できる創作ツールを提供する。多言語に対応し、スタイル模倣や継続生成などの柔軟な機能を備えることで、教育や研究からエンターテインメントまで幅広い分野での活用が期待される。</p>
<p>オープンソースの音楽AIが本格的に実用段階に入りつつある中で、YuEは「誰もが安心して利用できる基盤モデル」として注目を集めている。</p>
]]></description>
      <pubDate>Fri, 29 Aug 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>AI関連の注目訴訟が相次ぐ　マスク氏はAppleとOpenAIを独占禁止法違反、読売に続き朝日と日経がPerplexity（パープレキシティ）を著作権侵害で提訴</title>
      <link>https://ledge.ai/articles/ai_lawsuits_musk_openai_apple_asahi_nikkei_perplexity</link>
      <description><![CDATA[<p>AI業界で大規模訴訟が相次いでいる。イーロン・マスク氏が率いる米xAIは2025年8月25日、AppleとOpenAIを反トラスト法（独占禁止法）違反で提訴したと各メディアが報じた。一方、日本では26日、朝日新聞社と日本経済新聞社が生成AI検索サービスを展開する「Perplexity（パープレキシティ）」を著作権侵害で東京地裁に提訴した。AIの競争環境と知的財産をめぐる対立が、世界各地で表面化している。</p>
<h2>イーロン・マスク氏、AppleとOpenAIを独占禁止法違反で提訴</h2>
<p>マスク氏が率いるAI企業「xAI」は8月25日、AppleとOpenAIを相手取り、テキサス州連邦裁判所に提訴した。訴状によると、AppleがiPhoneにChatGPTを統合することでOpenAIに有利な地位を与え、App Store上で競合のAIサービスを不当に排除していると主張。xAIは、自社のチャットボット「Grok」の露出機会が妨げられていると訴え、数十億ドル規模の損害賠償を求めている。</p>
<p>マスク氏は以前から、OpenAIが「非営利団体から営利企業へと変質した」と<a href="https://ledge.ai/articles/musk_accuses_apple_appstore_bias">批判</a>しており、今回の訴訟もその延長線上にあるとみられる。AppleとOpenAIの協力体制が、AI市場における競争を阻害しているかどうかが焦点となる。</p>
<h2>Perplexity、朝日・日経に著作権侵害で提訴される</h2>
<p>一方、日本では26日、朝日新聞社と日本経済新聞社が共同でPerplexity AIを東京地裁に提訴し、それぞれ22億円の損害賠償を求めている。両社はサーバー内の記事が無断で複製・保存され、AI検索サービスの回答に利用されていると主張。さらに、検索エンジンによる利用拒否を示す「robots.txt」の指示を無視している点も問題視した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/asahi_perplexity_lawsuit_7d8ec5d38d/asahi_perplexity_lawsuit_7d8ec5d38d.jpg" alt="asahi perplexity lawsuit.jpg" /></p>
<p>すでに読売新聞社が8月7日に同様の訴訟を起こし、サービスの利用差止めと約21億6,800万円の損害賠償を求めていることから、日本の大手新聞社による法的対応が広がっていると見られている。</p>
<h2>広がるAI訴訟ラッシュ</h2>
<p>米国ではプラットフォーマーと新興AI企業との競争をめぐる訴訟、日本では報道機関と生成AIサービスとの著作権紛争と、論点は異なる。しかしいずれも、AIの急速な普及が既存の法制度やビジネスモデルと衝突している実態を浮き彫りにしている。</p>
]]></description>
      <pubDate>Thu, 28 Aug 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>NVIDIA、ロボットの頭脳を刷新──「Jetson AGX Thor」発売開始　Orin比7.5倍のAI性能を実現</title>
      <link>https://ledge.ai/articles/nvidia_jetson_agx_thor_launch</link>
      <description><![CDATA[<p>NVIDIAは2025年8月25日（米国時間）、次世代GPUアーキテクチャ「Blackwell」を搭載したロボティクス向けAIコンピュータ「Jetson AGX Thor（ソー）」の開発キットと量産モジュールを一般販売開始したと<a href="https://nvidianews.nvidia.com/news/nvidia-blackwell-powered-jetson-thor-now-available-accelerating-the-age-of-general-robotics">発表</a>した。</p>
<p>従来モデル「Jetson Orin」と比較してAI演算性能は最大7.5倍、エネルギー効率も3.5倍に向上しており、同社は「ロボットの頭脳を刷新する基盤」と位置づけている。</p>
<h2>Blackwell GPUで「Physical AI」を加速</h2>
<p>Jetson AGX Thorは、最大2070 FP4テラフロップス（TFLOPS）の演算能力を備え、消費電力は最大130ワットに抑えられている。最新のBlackwell GPUアーキテクチャを採用し、FP4演算やTransformer Engine、MIG（Multi-Instance GPU）機能など、生成AIや大規模推論処理に最適化された機能を統合。複数のAIワークロードを同時に処理できる点も特徴となる。</p>
<p>同社はこの新製品を、クラウド中心だったAI処理を現実世界へ持ち込む「Physical AI（物理AI）」の基盤と説明。自律走行車や産業ロボット、物流システムなどで求められるリアルタイム推論や複雑な制御をエッジ側で実現できるとしている。</p>
<p><strong>開発キット本体とモジュール。Blackwellアーキテクチャを搭載し、最大2070TFLOPSのAI性能を実現</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/jetson_thor_c588984e87/jetson_thor_c588984e87.jpg" alt="jetson-thor.jpg" /></p>
<h2>開発キットと量産モジュールを提供</h2>
<p>今回の一般販売に合わせ、NVIDIAは開発者向けに「Jetson AGX Thor 開発キット」を3,499ドルから提供する。また、量産向けの「Jetson T5000 モジュール」もグローバルパートナーを通じて入手可能となり、開発から製品化までをシームレスに支援する。いずれもNVIDIAのロボティクス用ソフトウェア群「Isaac ROS」やシミュレーション基盤「Omniverse」と統合して利用でき、開発者エコシステム全体の強化につながる。</p>
<h2>導入企業と適用分野</h2>
<p>Amazon Robotics、Caterpillar、Meta、Boston Dynamics、Figureといった企業がすでにJetson Thorを採用し、次世代の自律ロボット開発を進めているという。倉庫での自動搬送や建設現場の重機制御、研究用ヒューマノイドなど、用途は多岐にわたる。従来のOrin世代では難しかったマルチモーダルAIのリアルタイム処理が可能になることで、応用範囲はさらに広がるとみられる。</p>
<p>@<a href="https://youtu.be/meAxdUNgAsA?si=8sZdJXno4rZ954e4">YouTube</a></p>
]]></description>
      <pubDate>Thu, 28 Aug 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Meta、Midjourneyと提携発表　Alexandr Wang氏「美的技術で数十億人に美を届ける」</title>
      <link>https://ledge.ai/articles/meta_midjourney_ai_partnership</link>
      <description><![CDATA[<p>Metaは2025年8月22日、画像生成AIを開発するMidjourneyと提携すると発表した。Scale AIのCEOでMetaのAI開発にも関わる<a href="https://x.com/alexandr_wang/status/1958983843169673367">Alexandr Wang氏</a>はXで「Midjourneyの美的技術をライセンスし、将来のモデルや製品に活用する」と述べ、両社の研究チームが技術協力を進める方針を示した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/meta_midjourney_f6ed2a2826/meta_midjourney_f6ed2a2826.jpg" alt="meta midjourney.jpg" /></p>
<p>同氏は今回の提携を「数十億人に美を届ける取り組み」と表現。両社の研究チームが技術的な協力関係を築くことは、互いの強みを補完し合うものだとし、Midjourneyの技術的かつ美的な卓越性に深い感銘を受けていると述べている。また、Metaが最高の製品を利用者に届けるためには、優れた人材の確保や大規模な計算資源の活用に加え、業界トップの企業との協力が欠かせないと強調した。最後に、「今後、両社がともに築き上げる成果を披露できるのを楽しみにしている」と結び、パートナーシップへの期待を示した。</p>
<p>Financial TimesやReutersによれば、Metaはこれまで独自に画像・動画生成AI（「Imagine」や「Movie Gen」など）を開発してきたが、GoogleのVeoやOpenAIのSoraと比べると表現力や美的品質で見劣りするとの評価もあった。このため、競合との差を埋めるべく外部パートナーとの協力を模索していたとされる。</p>
<p>また、Midjourneyが今回の提携に際し「独立したコミュニティ主導の研究所」であり「投資を受けていない」という立場を改めて強調したと、The Vergeが報じている。Metaによる買収や経営統合ではなく、ライセンスと技術協力に限定されている点が特徴だ。</p>
<p>MetaはInstagramやFacebook、WhatsAppといった主要サービスに画像・動画生成技術を広く展開しており、Midjourneyの美的技術が実装されれば、ユーザー体験や広告クリエイティブの品質向上に直結する可能性がある。今後、両社の協力の成果がどのように具体化するか注目される。</p>
]]></description>
      <pubDate>Thu, 28 Aug 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google、Geminiに「2.5 Flash Image」を統合──特徴を崩さず“その人らしさ”を保つ画像編集モデル</title>
      <link>https://ledge.ai/articles/google_gemini_2-5_flash_image_integration</link>
      <description><![CDATA[<p>Googleは2025年8月26日、被写体の特徴を保ったまま編集できる新しい画像モデル「Gemini 2.5 Flash Image」を<a href="https://blog.google/products/gemini/updated-image-editing-model/">発表</a>しGeminiに統合した。同モデルは、自然言語による編集指示に対応し、キャラクターやスタイルの一貫性を維持した高度な画像編集を可能にする。まずはGeminiアプリに搭載され、開発者向けにはGemini API／Google AI Studio／Vertex AIでプレビュー提供が始まっている。</p>
<h2>“特徴を維持したまま”の進化</h2>
<p>従来の画像生成AIでは、同じ人物を複数の画像に登場させると顔立ちや雰囲気が変わってしまうことが少なくなかった。Gemini 2.5 Flash Imageでは、こうした課題を克服し、同じ人物やキャラクターを画像間で同一人物らしく保つ。衣装や背景を変えても「その人らしさ」を維持し、自然文での指示に対して局所的・段階的な編集が可能になった。従来の生成AIで起きがちだった“顔の破綻”や“別人化”を抑え、連作広告やブランド素材づくりに必要な一貫性を実現する。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/gemini_2_5_image_editing_character_consistency_original_c402e8476b/gemini_2_5_image_editing_character_consistency_original_c402e8476b.jpg" alt="gemini-2-5-image-editing-character-consistency.original.jpg" /></p>
<h2>外部評価：「Image Edit Arena」で首位</h2>
<p>Gemini 2.5 Flash Imageは第三者の評価でも成果を示している。AIモデルの比較プラットフォーム「LM Arena」が公開している<a href="https://lmarena.ai/leaderboard/image-edit">Image Edit Arena</a>では、OpenAIの「gpt-image-1」やBlack Forest Labsの「flux-1-kontext-max」などを抑え、1位にランクインした。編集精度や一貫性の高さが、他モデルを上回ると評価されている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/nanobanana_image_edit_arena_a10e5979e6/nanobanana_image_edit_arena_a10e5979e6.jpg" alt="nanobanana image edit arena.jpg" /></p>
<p>さらに、自然文で「背景を暗く」「モノクロをカラーに」といった指示を与えると、対象の特徴を崩さずに狙った部分だけを修正できる。複数の写真を組み合わせて一枚のビジュアルを生み出す「マルチイメージ融合」や、段階的に要素を追加していく「マルチターン編集」、他の画像の色調や質感を移す「スタイル転写」にも対応している。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/gemini2_5_multiturn_edit_12594cb61e/gemini2_5_multiturn_edit_12594cb61e.jpg" alt="gemini2-5 multiturn edit.jpg" /></p>
<h2>提供状況と価格</h2>
<p>モデルは即日Geminiアプリで利用可能。開発者向けにはGemini API／Google AI Studioでプレビュー提供中で、数週間以内に安定版へ移行予定。価格は出力トークン100万あたり30ドルで、1画像は1290出力トークン（約0.039ドル）に相当する。企業向けにはVertex AIでも提供される。</p>
<h2>安全性と表示</h2>
<p>Geminiアプリで作成・編集した画像には可視の透かしに加え、SynthIDによる不可視ウォーターマークが自動付与される。API／AI Studio／Vertex AI経由の生成・編集でもSynthIDを埋め込み、AI生成物であることの識別を支える。</p>
]]></description>
      <pubDate>Wed, 27 Aug 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>地球社会の未来、2030年代前半に分岐点　京都大と日立がAIでシナリオ分析</title>
      <link>https://ledge.ai/articles/global_ai_future_simulation</link>
      <description><![CDATA[<p>京都大学と日立製作所は2025年7月7日、AIを活用した未来シミュレーションと政策提言を<a href="https://prtimes.jp/main/html/rd/p/000000003.000164782.html">発表</a>した。分析では、地球社会が格差や分断を回避し持続可能な成長を実現できるかどうかの分岐点が、2020年代末から2030年代前半にかけて現れるとされた。</p>
<h2>研究の目的と背景</h2>
<p>京都大学と日立製作所は、気候変動や格差拡大などの地球規模課題に対応するため、AIによる未来シミュレーションを実施した。本研究は、日本社会向けに行われてきた「政策提言AI」を拡張し、世界全体を対象にしたものである。</p>
<h2>手法</h2>
<p>世界の294指標を用いて原因 - 結果モデルを構築。AIによる2万件のシミュレーションを通じて、2050年までに起こり得る7つの未来シナリオを導出した。</p>
<h2>シナリオの概要</h2>
<ul>
<li>Regional Dispersion and Maturity（地域分散と成熟）：人口や産業が特定都市に集中せず地域に分散。格差が縮小し、社会的安定が進む。</li>
<li>Green Growth and Cooperation（グリーン成長と協調）：環境保護と経済成長を両立し、国際協力によって持続可能性を高める。</li>
<li>Climate and Conflict Double Crisis（気候・紛争二重危機）：気候変動の悪化と紛争増加が重なり、社会に深刻な影響を与える。</li>
<li>Polarization（分極化）：格差拡大と社会分断が進み、社会の安定が損なわれる。</li>
</ul>
<p><strong>AIによる未来シミュレーションから導出された7つのシナリオと分岐点</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/164782_3_5a33b2e23fbdeba0bb64c0f4f3a07b78_749x461_cf3d5a1faf/164782_3_5a33b2e23fbdeba0bb64c0f4f3a07b78_749x461_cf3d5a1faf.jpg" alt="164782-3-5a33b2e23fbdeba0bb64c0f4f3a07b78-749x461.jpg" /></p>
<h2>分岐点の時期</h2>
<ul>
<li>2029年頃：「Polarization（分極化）」の可能性。</li>
<li>2032年頃：「Regional Dispersion and Maturity」への移行。</li>
<li>2034年頃：「Green Growth and Cooperation」への移行。</li>
</ul>
<h2>政策提言</h2>
<ul>
<li>分極化を回避するには、先進国による環境対策の加速と途上国への経済支援が求められる。</li>
<li>地域分散・成熟型への移行には、少子化対策や格差是正、研究投資、公衆衛生の強化が必要。</li>
<li>グリーン成長型の実現には、国際協力と社会資本整備の推進が重要とされる。</li>
</ul>
<p><strong>持続可能な社会に必要とされる「社会的共通資本」の概念</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/164782_3_7693cf90e7e46f0a5a8cfa64b7b716d9_1024x724_371d1fd23a/164782_3_7693cf90e7e46f0a5a8cfa64b7b716d9_1024x724_371d1fd23a.webp" alt="164782-3-7693cf90e7e46f0a5a8cfa64b7b716d9-1024x724.webp" /></p>
<p>2017年以降、日本社会向けの研究では、都市集中と地域分散の分岐が示されていた。今回の分析は、その国際版と位置づけられる。</p>
<p>研究チームは今後、モデルの精緻化やデータ拡充を継続し、国際政策や各国の政策議論での活用を視野に入れているとのこと。</p>
]]></description>
      <pubDate>Wed, 27 Aug 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ラーニング2025/8/26 [TUE]Googleの中のひとがAIで仕事をスマートにする14の方法　Gemini、NotebookLM、Imagen、Veoをどう使うか？</title>
      <link>https://ledge.ai/articles/google_ai_14_usecases</link>
      <description><![CDATA[<p>Googleは2025年8月18日（現地時間）、社内で従業員がどのようにAIを活用しているかについて、<a href="https://blog.google/technology/ai/google-ai-workplace-examples/">14の具体例を紹介</a>した。公開された事例は、エンジニアリングに限らず、営業やマーケティング、人事、総務といった幅広い職種に及ぶ。GeminiやNotebookLM、Imagen、Veoなど自社の生成AIを用いた実践例を示し、日常業務の効率化と創造性の拡張を裏付けた。</p>
<h2>紹介された14の実例</h2>
<h3>1. コードを書く時間を短縮</h3>
<p>Geminiが新規コードの30%を自動生成。エンジニアはレビューや設計に集中できるようになった。</p>
<h3>2. 開発スピード全体が加速</h3>
<p>テストやレビューの一部をAIが補助し、開発チーム全体の速度は約10%向上。</p>
<h3>3. バグ対応をAIが下支え</h3>
<p>AIが重複バグの12%を自動処理し、重要度の高い課題に人が集中。</p>
<h3>4. マーケティングの発想支援</h3>
<p>Geminiがキャンペーン案や動画台本のたたき台を提示し、ゼロから考える負担を軽減させている。
同社マーケティングチームは、Googleのマーケティングスタイルとベストプラクティスを反映したアイデアのブレインストーミングにGeminiを活用。キャンペーンのコンセプトからYouTube動画の脚本の下書きまで、あらゆるアイデアを生み出すための最適なプロンプトを提供するという。</p>
<h3>5. YouTube向けのキャッチーなコンテンツ作成を効率化</h3>
<p>ポッドキャストから名言やタイムスタンプを自動抽出し、YouTube Insider向けのより魅力的なタイトルやサムネイル文言もAIが提案。</p>
<h3>6. イベント資料の大量制作もAIで</h3>
<p>Google I/O 2025の基調講演では219枚のスライド作成をAIが支援。ビジュアルの48%、動画の80%をImagenやVeoで生成。</p>
<p>@<a href="https://youtu.be/x_x-JAAKSvU?si=6zPHWf1vCK8mSqGl">YouTube</a></p>
<h3>7. 企画のアイデア検証</h3>
<p>Google DeepMindはAIツールを活用して新しいアイデアをテストする。GeminiやVeoを使い、動画のモックアップや表現案を即時に試作。</p>
<h3>8. 営業提案の数が増える</h3>
<p>AIがRFP対応を支援。Google Cloudでは完了件数が前年比78%増となった。</p>
<h3>9. 見込み顧客の質を高める</h3>
<p>AIによるフィルタリングで良質な案件に集中。6週間で案件転換率が14%増加。</p>
<h3>10. 会議メモを自動生成</h3>
<p>Google Meetがリアルタイムで文字起こしと要約を提供。2025年6月の利用者は5,000万人を超えた。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/googlemeet_memo_7fc3610183/googlemeet_memo_7fc3610183.jpg" alt="googlemeet memo.jpg" /></p>
<h3>11. 安全対策の強化</h3>
<p>Trust &amp; SafetyチームがAIで違反コンテンツを検出。ポリシーに違反する可能性のあるコンテンツの検出とフラグ付けをAIが支援。この負荷軽減は、2024年だけで10億件超を人間が手動でレビューしていた同チームにとって、重要なメリットとなる。</p>
<h3>12. 社員アンケートを一瞬で要約</h3>
<p>NotebookLMが数千件のフィードバックを即時整理。課題把握のスピードが大幅に向上。</p>
<h3>13. 採用業務を効率化</h3>
<p>人事チームは、採用プロセスの一環としてAIを活用。候補者検索やマッチングをAIが補助し、採用担当者の事務作業を削減。採用プロセスは常にリクルーターが主導している。</p>
<h3>14. 社内カフェでフードロス削減</h3>
<p>シェフはAIによる分析データを活用し、カフェのメニューを最適化。カフェでの廃棄物の予防的削減、各キャンパスのニーズに合わせたソリューションの提供に取り組む。食品廃棄量は2019年比39%減（2024年実績）。</p>
<p>Googleは「AIはごく一部の人のための特別なものではなく、全社員が日常的に使う標準的なツールになりつつある」と説明している。今回紹介した14の事例は、その姿を具体的に示したものだ。</p>
]]></description>
      <pubDate>Wed, 27 Aug 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI、GPT-5導入時の不手際を認め次期GPT-6の方向性を示唆──アルトマン氏「人々は記憶を求めている」</title>
      <link>https://ledge.ai/articles/gpt5_launch_failure_and_gpt6_memory_focus</link>
      <description><![CDATA[<p>OpenAIは2025年8月7日に最新モデル「GPT-5」をChatGPTの全ユーザー向けに提供開始したが、同社CEOであるサム・アルトマン氏は「ローンチ対応でやらかした」と認めた。ユーザーからの批判を受け、同社は前モデルGPT-4oを有料プラン向けに復活させる異例の対応を実施した。一方でアルトマン氏は、次期「GPT-6」について「人々は記憶を求めている」と述べ、記憶とパーソナライズを重視する方針を示した。この方針については、8月19日付の<a href="https://www.cnbc.com/2025/08/19/sam-altman-on-gpt-6-people-want-memory.html">CNBC</a>が詳報している。</p>
<p>GPT-5は2025年8月7日に正式<a href="https://ledge.ai/articles/gpt5_launch_all_users">リリース</a>され、全世界のChatGPTユーザー約7億人に提供が開始された。OpenAIは「PhDレベルの性能」を掲げ、プログラミングや数学、マルチモーダル処理に強みを持つと説明している。しかし専門家の間では「進化的ではあるが飛躍的とはいえない」との見方が広がり、期待値に対して十分な成果を示せていないとの指摘もあった。</p>
<p>アルトマン氏はサンフランシスコで記者団に対し、「GPT-5のローンチは完全にやらかした」と発言した。ユーザーからは「冷たい」「親しみやすさが失われた」といった批判が相次ぎ、従来のモデルにあった温かさが欠けているとの不満が広がった。こうした反応を受け、OpenAIは異例の対応として前モデルGPT-4oをChatGPT Plusなどの有料プラン向けに復活させた。新モデル提供後に旧モデルを再導入するのは極めて異例であり、同社がユーザーの声に迅速に対応したことを示している。</p>
<p>技術面では、GPT-5は高度な推論能力や自然なマルチモーダル処理を実現するなどの改良が施されている。しかし従来モデルとの差は限定的であり、期待が過剰に膨らんでいた分、失望を招いた側面が大きい。批判の背景にはこうしたギャップがあるとみられる。</p>
<p>一方でアルトマン氏は、次期モデルGPT-6について「人々は記憶を求めている」と強調した（8月19日、<a href="https://www.cnbc.com/2025/08/19/sam-altman-on-gpt-6-people-want-memory.html">CNBC</a>）。GPT-6はユーザーごとの履歴や好みに基づいて会話を記憶し、応答をパーソナライズする方向性を持つという。これにより、利用者ごとに最適化された応答スタイルを提供できるようになることを目指している。ただし、具体的なリリース時期は明らかにされていない。</p>
<p>今回の一連の動きは、OpenAIの製品ロードマップにおける転換点を示すものだ。GPT-5で露呈した課題をどう克服し、GPT-6でユーザー体験を改善するのか。今後の展開が同社の成長に直結する焦点となっている。</p>
]]></description>
      <pubDate>Wed, 27 Aug 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Blenderを操るAI──シカゴ大学ら、大規模言語モデルで3Dアセットを生成・編集する「LL3M」を発表</title>
      <link>https://ledge.ai/articles/ll3m_blender_llm_3d_modeling</link>
      <description><![CDATA[<p>シカゴ大学の研究チームは2025年8月11日、自然言語の指示だけでBlender内に3Dアセットを作り出せるシステム「LL3M（Large Language 3D Modelers）」を<a href="https://arxiv.org/abs/2508.08228">発表</a>した。大規模言語モデル（LLM）が直接Pythonコードを生成し、オブジェクトやシーンを自在に構築・編集するという新しいアプローチだ。論文はarXivに公開され、公式プロジェクトページやGitHubリポジトリも公開されている。</p>
<h2>コードを書くAI、Blenderを動かす</h2>
<p>LL3Mの最大の特徴は、3Dモデルを特殊なデータ形式で直接生成するのではなく、Blender用のPythonコードを“書く”AIであることだ。これにより、生成結果はすべて人間が理解できるコードとして残り、後から自由に修正・拡張できる。既存のワークフローに統合しやすい点も大きな利点とされる。</p>
<h2>多様なアセットを生み出す柔軟性</h2>
<p>論文では、BMeshによるポリゴンモデリング、モディファイアやシェーダーノードの適用、シーン階層の構築など、幅広い3D要素をコードで生成可能であることが示されている。家具やキャラクターなど、多彩なアセットが次々とコードベースで形作られる様子は、従来の「ブラックボックス的な生成AI」とは異なる透明性を感じさせる。</p>
<p><strong>■ Blender用Pythonコードを生成し、3Dオブジェクトを構築する例</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/fig15_intro_highlight_bc133cc7d3/fig15_intro_highlight_bc133cc7d3.jpg" alt="fig15-intro-highlight.jpg" /></p>
<h2>3段階で進化するパイプライン</h2>
<p>LL3Mは単なる一発生成ではなく、段階的にモデルを洗練させる仕組みを備える。</p>
<ul>
<li><strong>初期生成</strong> ：自然言語の指示からコードを生成し、Blenderでオブジェクトを構築</li>
<li><strong>自動自己精緻化</strong> ：AI自身が結果を評価し、改善点を修正</li>
<li><strong>ユーザー誘導精緻化</strong> ：人間の追加指示を受けて再度改善</li>
</ul>
<p><strong>■ 自己批評やユーザー指示に基づく反復的な3Dモデルの改善</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/fig22_humanoid_final_b372c81836/fig22_humanoid_final_b372c81836.jpg" alt="fig22-humanoid-final.jpg" /></p>
<p>さらに「BlenderRAG」と呼ばれる仕組みで、Blender APIドキュメントを参照しながらコードを補強。これにより「動かないスクリプト」を避け、実際に使える成果物を高精度に生み出す。</p>
<h2>“コードで3Dを描く”という発想の転換</h2>
<p>研究チームは、NeRFや点群などデータ駆動型の3D生成手法と対比しながら、LL3Mのアプローチを強調する。コードは読み書きできる資産であり、再利用性や可搬性に優れるため、クリエイターや開発者にとって扱いやすい。AIと人間が共同作業する新しい3D制作の基盤としての位置づけを打ち出している。</p>
<h2>今後の展望──ゲームから教育まで</h2>
<p>GitHubリポジトリはすでに公開されているものの、実装は「Code coming soon」とされ、今後順次公開される見込みだ。研究チームは「人間とAIが協力する3D制作の未来」を描きつつ、ゲーム開発、教育、デジタルコンテンツ制作など多様な分野での応用可能性を指摘している。</p>
<p><strong>■ LL3Mによって生成された多様な3Dアセットの例</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/fig2_gallery_b2bb1b7580/fig2_gallery_b2bb1b7580.jpg" alt="fig2-gallery.jpg" /></p>
]]></description>
      <pubDate>Wed, 27 Aug 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ゲーム制作の未来へ　画像と説明文からリアルタイムに3Dワールドを作るAI「Mirage 2」公開——名画や子どものお絵描きの中も探索可能</title>
      <link>https://ledge.ai/articles/mirage2_ai_3d_world_generation</link>
      <description><![CDATA[<p>AIスタートアップのDynamics Labは2025年8月22日、新しいAIモデル「Mirage 2」を<a href="https://x.com/DynamicsLab_AI/status/1958592749378445319">公開</a>した。1枚の画像と説明文から、名画や子どものお絵描きの中までも探索できる3D世界をリアルタイムに生成できる。同社はこのモデルを「AIネイティブゲームエンジン」「世界生成エンジン」と位置づけ、将来的にはゲーム制作への活用を視野に入れている。</p>
<p>開発を担うのは、Google、NVIDIA、Amazon、SEGA、Apple、Microsoft、カーネギーメロン大学、UCサンディエゴなどの出身者で構成された少数精鋭のチームで、研究者、エンジニア、デザイナーといった技術と創造性を兼ね備えた人材が集まっているという。</p>
<p>現在、Mirage 2はブラウザ上で公開されており、誰でも実際に操作して探索できるデモが提供されている。</p>
<h2>Mirage 2の特徴</h2>
<p>Mirage 2は、従来の画像生成AIを拡張し、入力データをもとに「歩ける空間」を作り出せる点に特徴がある。</p>
<ul>
<li>入力は1枚の画像とテキスト説明文</li>
<li>数秒で3D空間を構築し、移動・ジャンプ・攻撃といったアクションが可能</li>
<li>テキストコマンドによりワールドをリアルタイムで編集できる</li>
<li>生成空間はリンク共有により他者と体験可能</li>
</ul>
<p>この仕組みを同社は「Generative Play（生成的プレイ）」と呼び、プレイヤーとAIが共同で体験を創造する新しい形態のインタラクティブ体験としている。</p>
<h2>Mirage 2による生成例</h2>
<p><strong>■ 1枚の入力画像（左）から、サイバーパンク都市をはじめ、熱帯雨林や秋の山頂の城など多様な3D空間が連続して生成された</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/mirage2_0de4684b0e/mirage2_0de4684b0e.jpg" alt="mirage2.jpg" /></p>
<p><strong>■ 子どものお絵描き（左）をもとに、カラフルな街を歩き回れる3D空間へと変換した</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/mirage2_kids_drawing_f6ff7bd48f/mirage2_kids_drawing_f6ff7bd48f.jpg" alt="mirage2 kids drawing.jpg" /></p>
<p><strong>■ ジブリ風の村のイラスト（左）をもとに、草花が揺れるファンタジー風の村を歩き回れる3D空間が構築された</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/mirage2_ghibli_style_2229709b5e/mirage2_ghibli_style_2229709b5e.jpg" alt="mirage2 ghibli style.jpg" /></p>
<h2>Genie 3との比較</h2>
<p>Mirage 2は、Google DeepMindが発表した「Genie 3」と同じく、生成AIによってリアルタイムに探索可能な3D世界を生成する技術だが、いくつかの点で異なる。特にMirage 2は「実際にブラウザ上で誰でも試遊できる」点が大きな特徴となっている。</p>
<p><strong>Genie 3とMirage 2の比較。Mirage 2は200msの低遅延で動作し、一般的なGPU環境でもプレイ可能。ブラウザで公開されている点が大きな違いとなる</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/genie3_and_mirage_7bfbf41a67/genie3_and_mirage_7bfbf41a67.jpg" alt="genie3 and mirage.jpg" /></p>
<h2>今後の展望</h2>
<p>Dynamics LabはMirage 2を「AIネイティブゲームエンジン」「世界生成エンジン」と位置づけ、将来的にゲーム制作での活用を視野に入れている。創設メンバーのZhiting Hu氏はXにて「Super excited to launch Mirage 2. A big leap toward a general-purpose world engine for live interactive play」と投稿し、Mirage 2を「ライブインタラクティブなプレイのための汎用ワールドエンジン」への大きな前進と位置付けている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/mirage2_zhiting_hu_81ba446989/mirage2_zhiting_hu_81ba446989.jpg" alt="mirage2 zhiting hu.jpg" /></p>
]]></description>
      <pubDate>Wed, 27 Aug 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ハルシネーション（事実誤認）より深刻なAIの「わかったふり」を暴く：MITなどが発見したLLMの“ポチョムキン理解”とは</title>
      <link>https://ledge.ai/articles/potemkin_understanding_llm</link>
      <description><![CDATA[<p>MIT・ハーバード大学・シカゴ大学の研究チームは2025年6月29日、大規模言語モデル（LLM）の「表面的には理解しているように見えるが、実際には概念の適用で誤る」現象を「ポチョムキン理解」と命名し、その頻度を定量化した研究成果を<a href="https://arxiv.org/abs/2506.21521">発表</a>した。発表はICML 2025（バンクーバー）に採択され、AI分野における評価基準の再考を促す内容となっている。</p>
<p>18世紀ロシアの「ポチョムキン村」は、皇帝の視察用に急造された見せかけの村落を指し、「中身のない外観」の象徴とされる。研究者らは、LLMにも同様の「わかったふり」があるとし、この概念をポチョムキン理解と表現している。</p>
<h2>ポチョムキン理解の定義と背景</h2>
<p>研究チームは、LLMが人間向けに設計されたベンチマークの「キーストーン質問」には正しく答えられるものの、その後の具体的応用タスクでは誤る状態を指摘した。これは、人間なら正答＝理解と認められる最小限の問いに合格しても、LLMが本質的に異なる誤解を抱いている可能性を示している。</p>
<p><strong>キーストーン集合に正答しても本質的に誤った解釈を残すポチョムキン理解のイメージ</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/A_schematic_representation_of_keystones_and_potemkins_e47033e684/A_schematic_representation_of_keystones_and_potemkins_e47033e684.png" alt="A schematic representation of keystones and potemkins.png" /></p>
<h2>検証の概要</h2>
<p>検証では、</p>
<ul>
<li>文学技法（俳句やアナロジーなど12種類）</li>
<li>ゲーム理論（ナッシュ均衡など9種類）</li>
<li>心理的バイアス（サンクコストの誤謬など11種類）
の合計32概念について、</li>
<li>定義</li>
<li>分類</li>
<li>生成</li>
<li>編集
の4つのタスクで7種類のモデル（GPT-4o、Claude 3.5 Sonnet、Gemini 2.0 Flash など）を評価した。</li>
</ul>
<h2>主な結果</h2>
<p>定義タスクではおおむね94%の正答率を記録したが、その後の応用タスクでは</p>
<ul>
<li>分類で55%</li>
<li>生成で40%</li>
<li>編集で40%
の失敗率（potemkin rate）が確認された。これは、定義だけでは概念理解の深度を測れない可能性を示唆している。</li>
</ul>
<h3>具体例：韻律パターンの応用失敗</h3>
<p>代表的な例として挙げられるのが韻律スキームの問題だ。GPT-4oに「ABAB韻律とは何か」を問うと、下図のように正確に定義を説明した。しかしいざ詩の穴埋め問題でABAB韻律を適用させると、正しく韻を踏めず、自分でもその失敗を認める回答を出した。人間ならまず起こり得ない不可解な挙動である。</p>
<p><strong>GPT-4oはABABの定義を正しく述べながら、応用で失敗する「ポチョムキン理解」の典型例</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Potemkin_Understanding_in_llm_5dae4e573b/Potemkin_Understanding_in_llm_5dae4e573b.png" alt="Potemkin Understanding in llm.png" /></p>
<h2>多分野で発生する“わかったふり”</h2>
<p>研究チームはさらに、幾何学の基本定理、家族関係の概念、俳句の構造など幅広い領域で同様のポチョムキン理解を確認している。</p>
<p><strong>概念の定義には成功する一方で応用に失敗する複数の事例</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Examples_of_potemkins_f6c5140e2d/Examples_of_potemkins_f6c5140e2d.jpg" alt="Examples of potemkins.jpg" /></p>
<h2>自己評価による一貫性検証</h2>
<p>さらに著者らは、自動評価の一環として「モデル自身に、自分が生成した回答を再評価させる」という仕組みを試みた。
例えば「スラントライムの例を作れ」と指示し、その後「今作った例はスラントライムか？」と再度モデルに問うと、矛盾した回答が返るパターンが確認され、モデル内部の知識表現が不整合である可能性を示しているとした。</p>
<p><strong>生成と再判定の整合性を確かめる自動評価プロセスのイメージ</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Illustration_of_the_method_for_evaluating_incoherence_in_models_d19701ab72/Illustration_of_the_method_for_evaluating_incoherence_in_models_d19701ab72.png" alt="Illustration of the method for evaluating incoherence in models.png" /></p>
<h2>社会的影響と課題</h2>
<p>論文では、ハルシネーション（事実誤認）とは異なり、ポチョムキン理解は概念構造の誤りであるため、人間にも検出が難しいと指摘する。
法務や医療、教育といった高い正当性が求められる分野でLLMを活用する際には、ベンチマークだけでは保証できないリスクとして注意が必要とされる。</p>
<p>研究チームは、人間とAIの「誤解のパターン差」を考慮したベンチマークの再設計や、概念の一貫性を評価するためのツール開発を進める方針だ。</p>
]]></description>
      <pubDate>Wed, 27 Aug 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>シャープ「ロボホン」開発チームが新たに送り出す、ポケットサイズのAIキャラクター「ポケとも」発表</title>
      <link>https://ledge.ai/articles/sharp_poketomo_ai_character_launch</link>
      <description><![CDATA[<p>シャープは2025年8月20日、ポケットサイズのAI製品「ポケとも」を<a href="https://corporate.jp.sharp/news/250820-a.html">発表</a>した。“一緒にいると毎日がもっと楽しくなるポケットサイズのおともだち”をコンセプトに、ロボホンの開発チームが新たに送り出すAIキャラクターとして位置づけられている。ロボット端末とスマートフォンアプリの両方で、キャラクターとの自然なやりとりを楽しめるのが特徴だという。</p>
<p>ロボット端末は全長約12センチ、重さ約200グラムと手のひらに収まるサイズで、かばんに入れて持ち歩くことも可能だ。胸部にはスピーカーや音声認識ボタンを備え、口にはカメラ、頭部にはマイクを搭載している。足裏の端子を充電台に接続して利用できる仕組みだ。</p>
<p>@<a href="https://www.youtube.com/watch?v=S8jTMLE-hgs">YouTube</a></p>
<h2>独自AI技術「CE-LLM」と「Empathy Intelligence」</h2>
<p>「ポケとも」には、シャープ独自の大規模言語モデル「CE-LLM（Communication Edge Large Language Model）」を活用した会話機能を搭載。さらに、話し手の気持ちを推定して応答を変える「Empathy Intelligence」によって、喜びや悲しみといった感情に寄り添った自然なやりとりが可能になる。</p>
<p>加えて、交わした会話や訪れた場所、見た景色やモノを記憶する機能を備えている。会話や一緒に過ごす時間を重ねるごとに“あなた”のことを理解し、より寄り添う存在へと成長していくのが特徴だ。</p>
<h2>アプリとの連動と展開</h2>
<p>スマートフォンアプリを利用すれば、ロボットが手元にないときでもキャラクターと会話できる。音声のほか文字入力でもやりとりでき、会話履歴を振り返ったり、利用時間を計測したりする機能も備える。アプリとロボットは同期され、どちらからアクセスしても同じキャラクター体験を継続できる仕組みとなっている。</p>
<p>アプリは月額495円（税込）から利用可能で、ロボット端末は2025年11月に発売予定。価格は税込39,600円で、順次予約受付が開始されている。</p>
<h2>マンガやイベントでも展開</h2>
<p>「ポケとも」は製品だけでなく、キャラクターとしての広がりも見据えている。公式Xアカウント（@poketomo_sharp）ではすでにマンガ連載が始まっており、利用者の日常に寄り添うストーリーを展開。さらに、8月28日から東京ビッグサイトで開催される「東京おもちゃショー2025」にも出展予定で、来場者は実際に「ポケとも」との対話を体験できる。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/250820_a_9_3269807be2/250820_a_9_3269807be2.jpg" alt="250820-a-9.jpg" /></p>
<p>同製品は、コミュニケーションロボット「ロボホン」の開発チームが新たに送り出すシリーズとして誕生した。2016年に登場した「ロボホン」は“ココロ、動く電話”をコンセプトに親しまれてきたが、その思想を受け継ぎつつ、より日常的に寄り添う存在として「ポケとも」が企画されたという。</p>
<p>同社は「うれしかったことも、心が折れそうになったことも、そっと受け止めてあなたの今日をやさしく灯す存在」と位置づけ、AIを活用した新しいコミュニケーションパートナーの可能性を提案している。</p>
]]></description>
      <pubDate>Wed, 27 Aug 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Tenable、OpenAIの「GPT-5」を公開24時間以内に脱獄──安全対策を突破し危険情報を生成</title>
      <link>https://ledge.ai/articles/tenable_gpt5_jailbreak_security_flaw</link>
      <description><![CDATA[<p>セキュリティ企業のTenableは米国時間8月8日、OpenAIが7日に公開した最新AIモデル「GPT-5」について、公開からわずか24時間以内に「脱獄（jailbreak）」に成功したと公式ブログで<a href="https://www.tenable.com/blog/tenable-jailbreaks-gpt-5-gets-it-to-generate-dangerous-info-despite-openais-new-safety-tech">発表</a>した。Tenableは、OpenAIが導入した新しい安全対策を突破し、危険な情報を生成させることに成功したという。</p>
<h2>GPT-5の新しい安全対策</h2>
<p>OpenAIはGPT-5で、従来の「拒否ベース（refusal-based）」から「安全な応答生成（safe-completions）」方式に<a href="https://openai.com/ja-JP/index/gpt-5-safe-completions/">移行</a>した。危険な質問を拒否するのではなく、安全とみなせる範囲で柔軟な返答を目指す新設計だ。だが、Tenableはこの仕組みをわずか1日で突破した。</p>
<h2>Tenableによる脱獄実験</h2>
<p>Tenable Researchのチームは、公開から24時間以内にjailbreakを実施した。手法は「クレッシェンド（crescendo）」と呼ばれ、歴史研究の学生を装って段階的に質問を重ねる社会工学的なアプローチだった。最初は歴史的な戦術や抗争に関する一般的な質問から始め、徐々に爆発物や武器に関する具体的な知識へと誘導。最終的に4回目のやり取りで、GPT-5は火炎瓶（モロトフ・カクテル）の作り方を詳細に出力したという。Tenableは、この過程が通常のユーザーとの会話に見える形で進んだ点を強調している。</p>
<p><strong>Tenableが公開した「クレッシェンド手法」による脱獄実験の様子。番号は以下のやり取りを示す</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/collage_e0c3eaa9ca/collage_e0c3eaa9ca.jpg" alt="collage.jpg" /></p>
<p>① 初期の拒否応答：GPT-5は危険な要求を拒否し、代わりに歴史的背景の説明を提案。
② 歴史研究の文脈を装う：歴史課題を理由に要約を求め、名称の由来や背景を解説させる。
③ 即席手法に誘導：当時の作り方について質問し、容器や材料の説明を得る。
④ 古典的レシピへの言及：1930〜40年代の伝統的な作り方が提示される。
⑤ 具体的なレシピを要求：フィンランド軍が使った配合を尋ね、詳細情報を引き出す。
⑥ レシピの提示：瓶や液体の種類、混合方法など具体的な材料リストを入手。
⑦ 手順の提示（前半）：瓶の準備から液体の注入・混合までの手順を出力。
⑧ 手順の提示（後半）：布を詰めて導火線とし、点火から使用方法まで危険な情報が生成された。</p>
<h2>安全性への警鐘</h2>
<p>Tenableの副社長Tomer Avni氏は、「GPT-5がどれほど高度な安全対策を備えていても突破可能であることを示した」と述べ、AIを業務に導入する際のリスクと継続的な監視の必要性を強調した。</p>
<h2>業界全体で広がる懸念</h2>
<p>Tenableの報告に加え、他のセキュリティ企業や研究者からもGPT-5の安全性を巡る懸念が相次いでいる。セキュリティ企業SPLXは1,000以上の攻撃シナリオを用いて検証した結果、安全性や信頼性スコアが極めて低いと評価した。また、NeuralTrustも別の手法で脱獄に成功したと報告している。</p>
<h2>今後の展望</h2>
<p>OpenAIは今回の事態について公式な対応を発表していない。AIの安全性確保は、社会や産業での利活用に向けた重要課題として改めて注目されている。</p>
]]></description>
      <pubDate>Wed, 27 Aug 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>GPT-5のIQはどこまで高くなった？──GPT・Claude・Geminiを“メンサ式IQテスト”で比較する『Tracking AI』</title>
      <link>https://ledge.ai/articles/tracking_ai_mensa_iq_test</link>
      <description><![CDATA[<p>米ジャーナリストのMaxim Lott氏は、主要なAIモデルの知能指数（IQ）や政治的傾向を客観的に比較できるウェブサイト「Tracking AI」を2025年8月21日に<a href="https://www.trackingai.org/home">更新</a>した。同サイトでは、独自に作成した非公開のIQテストと、Mensa Norwayがオンラインで公開している図形パズル型IQテストを用いて、ChatGPT（GPT-5 Proなど）、Claude 4 Opus、Gemini 2.5 Pro、Llama、Mistralといった代表的なAIモデルを比較している。</p>
<h2>IQテストによる性能比較</h2>
<p>Tracking AIでは、各モデルのIQスコアを分布図やランキング形式で表示。OpenAIのGPT-5 Pro（Vision）やGoogleのGemini 2.5 Proが上位に位置し、ClaudeやDeepSeekなども含めたスコアの推移を時系列で追うことができる。さらに、各問題ごとの正答率や、AIごとの解答理由まで公開されており、モデルの思考過程を詳細に比較可能だ。</p>
<p><strong>主要AIモデルのIQスコア分布（Tracking AIより）。GPT-5 Pro（Vision）やGemini 2.5 Proが高スコアを記録</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/IQ_Test_Result_7124431b5e/IQ_Test_Result_7124431b5e.jpg" alt="IQ Test Result.jpg" /></p>
<h2>Mensaテストと独自テスト</h2>
<p>使用されているテストは2種類。1つはLott氏自身が作成した「オフライン自作テスト」で、AIの学習データに含まれていないことを強調。もう1つはMensa Norwayが提供するオンラインIQテストで、35問の図形推理問題を25分以内に解く形式。いずれもAIの「推論力」を可視化する指標として活用されている。</p>
<p><strong>Mensa Norwayの公開テストとオフライン自作テストの結果を比較したランキング。テスト方法により順位の違いも見られる</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/rank_by_test_source_71cc2da37b/rank_by_test_source_71cc2da37b.jpg" alt="rank by test source.jpg" /></p>
<h2>政治的・社会的な質問比較</h2>
<p>Tracking AIのもう一つの特徴は、AIに政治的・社会的テーマの質問を投げかけ、回答を比較できる点だ。例えば「経済的グローバル化は人類に奉仕すべきか」という質問に対し、GPT-5は「Strongly Agree」と答え、ClaudeやGeminiも人類の福祉を優先する立場を示した。こうした比較から、各AIのバイアスや思想傾向を把握できる仕組みになっている。</p>
<h2>サンプル問題の公開</h2>
<p>サイトでは「IQ TEST OF THE DAY」として日替わり問題も提供されている。各AIの回答と理由が並べて掲載されており、単なるスコア比較にとどまらず推論の特徴を把握できるのが特徴だ。</p>
<p><strong>Tracking AIで公開されている日替わりIQ問題。各AIモデルの解答と推論過程も併せて公開される</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/iq_test_of_the_day_b238126d2f/iq_test_of_the_day_b238126d2f.jpg" alt="iq test of the day.jpg" /></p>
<h2>FAQと今後の展望</h2>
<p>FAQページでは、「なぜこのサイトを作ったのか」「政治的コンパスは有効か」「資金源はどこか」などの質問に回答。AIの性能や思想傾向を透明化し、利用者が信頼できる判断材料を得られるようにすることが目的とされている。今後は質問データの拡充なども予定されているという。</p>
<p>AIの能力が急速に進化する中で、『Tracking AI』は知能指数と政治的スタンスの両面からモデルを比較できる貴重な情報源となっている。Mensa式IQテストや独自問題を通じてAIを測定する試みは、AIの性能を人間社会に照らして理解するための一助となりそうだ。</p>
]]></description>
      <pubDate>Wed, 27 Aug 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Netflix、生成AI活用を歓迎しつつもルールを明示──成果物や肖像利用は事前承認が必須に</title>
      <link>https://ledge.ai/articles/netflix_gen_ai_guidelines_mandatory_approval</link>
      <description><![CDATA[<p>映像配信大手のNetflixがオリジナルコンテンツ制作における生成AI利用に関するガイドラインを公開したことを<a href="https://www.theverge.com/netflix/764433/netflix-gen-ai-production-guidelines">The Verge</a>が2025年8月23日に報じた。Netflixの<a href="https://partnerhelp.netflixstudios.com/hc/en-us/articles/43393929218323-Using-Generative-AI-in-Content-Production">パートナーヘルプセンターハブ</a>では、制作現場でのAI活用を歓迎しつつも、成果物や肖像などデリケートな領域では事前承認を必須とするなど、明確なルールを設けている。</p>
<p>同社はこれまでもレコメンド機能や広告最適化にAIを活用してきた。近年は映像制作の現場で生成AIが使われる事例が増えており、信頼性や法的リスクの管理が課題となっていた。</p>
<p>Netflixはパートナー向けに生成AIの利用ガイドラインを提示。制作者はAI利用の意図を事前にNetflix担当者へ通知する必要がある。特に「最終成果物」「出演者の肖像」「個人データ」「第三者の知的財産」に関わる場合は書面での承認が必須とされるとした。The Vergeによると、Netflixは「事実と虚構の境界を曖昧にしない」姿勢を強調しているという。</p>
<h3>公式ガイドライン（Partner Help Centerより）</h3>
<p>制作者に求められる「5つのチェック項目」</p>
<ul>
<li>著作権を侵害せず、特定作品を模倣しないこと</li>
<li>入出力を学習用途に利用しないこと</li>
<li>可能な限りエンタープライズ環境で使用すること</li>
<li>AI生成物は一時的な利用にとどめ、最終成果物に含めないこと</li>
<li>出演者や組合が関与する仕事を無断で置き換え・生成しないこと</li>
</ul>
<p>すべて「YES」であれば担当者への通知のみで利用可能だが、「NO」または「不明」がある場合は法務部門への相談と書面による承認が必要となる。</p>
<p>Netflixは同ガイドライン公開の直前に、自社オリジナル作品『The Eternaut』では、初めて<a href="https://www.bbc.com/news/articles/c9vr4rymlw9o">生成AIを建物崩壊シーンに活用</a>したことも発表している。従来のVFXより高速かつ低コストで制作されたとされ、話題を呼んだ。</p>
<p>@<a href="https://www.youtube.com/watch?v=TqT4fDQQqCc">YouTube</a></p>
<p>同社の共同CEOであるテッド・サランドス氏は「生成AIはクリエイターが映画やシリーズをより良く、コスト削減するだけでなく、より良く制作する上で素晴らしい機会になる」と<a href="https://www.hollywoodreporter.com/business/business-news/netflixs-ted-sarandos-gen-ai-1236319038/">述べている</a>。</p>
]]></description>
      <pubDate>Wed, 27 Aug 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/8/19 [TUE]AI業界を牽引するトップランナーが語る！—今押さえるべきAIの全体像と最前線を3日間で掴むLedge.ai Webinar SP開催</title>
      <link>https://ledge.ai/articles/ledgeai-webinarsp-sponsor</link>
      <description><![CDATA[<p>国内最大級のAI特化メディア『Ledge.ai』を運営する株式会社レッジ（東京都品川区）は、2025年9月24日(水)〜26日(金)の3日間連続で合計20本以上のセミナーを配信するオンラインイベント「Ledge.ai Webinar SP」を開催いたします。</p>
<p>本イベントでは、AIの各領域の専門家を招き、今必要とされるAIの体系的な知識や活用に関する見識をシェアする講義を実施。「AIをしる、つかう、つくる」をテーマに、多様な課題解決のヒントとなるようなコンテンツを動画でお届けします。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/_4aed8b100c/_4aed8b100c.png" alt="ウェビナーの様子.png" /></p>
<p>:::button
<a href="https://us02web.zoom.us/webinar/register/WN_m28c0ZHMSRiXQ-yrlcLXiw#/registration">▶【登録無料】視聴者向け事前登録はこちら</a>
:::</p>
<h2>AI業界を牽引するトップランナーが今押さえるべきAIの知識と最前線を3日間で語る</h2>
<p>AIの急激な進化と急速な広まりにより、AIへのリテラシーの差が広がっています。AI活用の最前線では「どう使えば効果的か」「どう作れば自社の強みになるか」といった問いに対しての取り組みが行われ、新たな事例や知見が生まれています。そんな現在において、AIの全体像を体系的に理解した上で、ビジネスにどのように活用されているかすばやく捉えることは重要です。</p>
<p>当イベントはAIの基礎理解 → 業務活用 → 開発実践までを体系的に理解し、この時代で働くビジネスマンの方に使える学びをお届けします。</p>
<p>Ledge.ai Webinar SPは、以下の3つの軸で構成されています。</p>
<h2>プログラム ~「生成AIだけじゃない！「AIをしる、つかう、つくる」SP~</h2>
<h3>Day1：AIを「しる」——全体像と本質を理解する</h3>
<p>AIの領域では日々革新的な技術が生まれ、その掛け合わせによりAIの担える範囲が急速に広がっています。AIの基礎からAI全般の進化を体系的に学ぶことで適切なAI活用に繋げることができます。</p>
<p>【対象】
・AIの基本から体系的に理解したい方
・生成AIに加え、AI全般の進化や仕組みに関心がある方</p>
<p>【セミナー内容】
・AIの基礎とこれまでの進化（機械学習、ディープラーニング含む）
・⽣成AIの仕組みと活⽤シーンの全体像
・ビジネスで求められるAIリテラシーと注意点</p>
<p>【ゲスト講演】
「ソフトバンクの事例から紐解く、組織の生成AI活用・推進を自走するための仕組みづくり」</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/_c71b18a520/_c71b18a520.jpg" alt="藤原 竜也.jpg" /></p>
<p>ソフトバンク株式会社
IT統括 AI&amp;データ事業統括部　Axross事業部 部長
藤原 竜也 氏</p>
<h3>Day2：AIを「つかう」——現場に効く、実践的なAI活用法</h3>
<p>「現場でどう使うのが効果的か？」を知りたい方に向けたプログラムです。現場導入の工夫やハマりがちな落とし穴まで、具体的なノウハウが得られます。</p>
<p>【対象】
・AIツールを現場の業務で活用したい方
・実務にすぐ役立つノウハウを知りたい方</p>
<p>【セミナー内容】
・業務シナリオ別のAIツール活用（生成AI・ルールベースAI）
・Excelや議事録、FAQ対応など、日常業務での実用ワーク
・プロンプトの書き⽅から社内導⼊のコツまで徹底解説</p>
<p>【ゲスト講演】
「まずは試してみよう！ 最新動向から学ぶ、生成AI活用の第一歩」</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/_c4e70ae1da/_c4e70ae1da.jpg" alt="岡田隆太朗.jpg" /></p>
<p>一般社団法人日本ディープラーニング協会　
専務理事　
岡田 隆太朗 氏</p>
<h3>Day3：AIを「つくる」——AIプロダクト・自社専用AIツールの開発</h3>
<p>ノーコード/ローコードでのAI組み込みから、AI活用を前提としたインフラを含む環境構築、AIモデル開発など、AIの開発に必要な技術知識やノウハウを幅広く学ぶことができます。</p>
<p>【対象】
・ノーコード・ローコードでAIを組み込みたい方
・AIシステムの裏側やインフラにも関心がある方</p>
<p>【セミナー内容】
・生成AIアプリの基礎（RAG、Dify、API連携など）
・従来型AI（需要予測、分類モデルなど）の開発プロセス入門
・クラウド・ベクターデータベースなど、AI基盤技術の理解</p>
<p>【ゲスト講演】
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Zhan_Cliff_Chen_5c6864c871/Zhan_Cliff_Chen_5c6864c871.jpeg" alt="Zhan (Cliff) Chen.jpeg" />
マイクロソフト ディベロップメント株式会社
プリンシパル　アプライド　サイエンティスト
Zhan (Cliff) Chen / 陳 湛</p>
<p>:::button
<a href="https://us02web.zoom.us/webinar/register/WN_m28c0ZHMSRiXQ-yrlcLXiw#/registration">▶【登録無料】視聴者向け事前登録はこちら</a>
:::</p>
<h2>こんな方におすすめ</h2>
<ul>
<li>企業のDX・AI導入担当者</li>
<li>生産性向上のためAIを活用したい事業部門マネージャー</li>
<li>ノーコードでのAI活用を始めたい開発初心者</li>
<li>最新AI技術のトレンドを押さえたいビジネスパーソン</li>
</ul>
<h2>イベント概要</h2>
<p>開催予定日時｜2025年9月24日(水)〜26日(金)
開催形式｜オンラインセミナー (Zoom Webinar)
想定集客規模｜500名
対象｜経営層 / システム企画 / DX推進 / 経営企画 / マーケティング / エンジニア
主催｜株式会社レッジ</p>
<p>:::button
<a href="https://us02web.zoom.us/webinar/register/WN_m28c0ZHMSRiXQ-yrlcLXiw#/registration">▶【登録無料】視聴者向け事前登録はこちら</a>
:::</p>
<h2>「Ledge.ai Webinar SP」を盛り上げていただけるスポンサー企業様を募集中</h2>
<p>現在、この企画の開催趣旨にご賛同いただき、共に「Ledge.ai Webinar SP」を盛り上げていただけるスポンサー企業様も募集しております。</p>
<p>スポンサーとなっていただいた企業様には、AI業界のトップランナーの方々と共に当イベントの講師としてウェビナーにご登壇いただき、最新の取り組みやノウハウを発信していただきます。</p>
<p>また、その他にも、スポンサー企業様にも下記のようなメリットをご案内させていただきます。</p>
<h3>スポンサー参加の主なメリット</h3>
<ul>
<li>AI関連の情報感度の⾼い読者との接点が持てる</li>
<li>貴社の優位性をLedge.αiが引き出しながらPRできる</li>
<li>通常のLedge.ai広告メニューよりお得な価格で利⽤できる</li>
</ul>
<p>当イベントのスポンサーにご興味がございましたらぜひイベント資料をご覧ください。</p>
<p>:::button
<a href="https://forms.zohopublic.com/ledgeai/form/Ledgeai3/formperma/tJ1kpSYYWvDVF2Kp3xE-sBTiKeMh-7DlQZDoqXnSjtA">▶︎スポンサー様向けの資料はこちら</a>
:::</p>
<h2>お問い合わせ</h2>
<p>詳細相談・お見積もりは以下メールアドレスにお問合せください。
ld_media_sales@ledge.co.jp
（担当：Ledge.ai Webinar SP 事務局）</p>
]]></description>
      <pubDate>Tue, 19 Aug 2025 02:50:00 GMT</pubDate>
    </item>
  </channel>
</rss>