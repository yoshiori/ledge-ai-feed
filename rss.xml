<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
  <channel>
    <title>Ledge.ai 新着記事</title>
    <link>https://ledge.ai/</link>
    <description>Ledge.ai の最新テクノロジー記事</description>
    <item>
      <title>世界のAIチップを“毎晩測る”──SemiAnalysis、リアルタイム性能追跡プラットフォーム「InferenceMAX」始動</title>
      <link>https://ledge.ai/articles/semianalysis_inferencemax_real_time_benchmark</link>
      <description><![CDATA[<p>米調査・分析メディアのSemiAnalysisは2025年10月9日（現地時間）、AIチップの推論性能をリアルタイムで追跡するオープンベンチマークプラットフォーム「InferenceMAX（インファレンスマックス）」を<a href="https://newsletter.semianalysis.com/p/inferencemax-open-source-inference">公開</a>した。GPUやNPU、CPUを含む数百種類のチップを対象に、主要な推論フレームワークやAIモデルの性能を毎晩自動で再測定し、その結果をリアルタイムで可視化する。</p>
<h2>AIチップ性能をライブで監視</h2>
<p>InferenceMAXは、AI推論の実運用環境に近い形で各種ハードウェアを検証するオープンソースベンチマークだ。
SemiAnalysisによると、このシステムは「毎晩、数百種類のAIチップ上でベンチマークを実行し、オープンソースの推論フレームワークやモデルを継続的に再測定する」仕組みを持つ。
ソフトウェアスタックやドライバの更新に伴う性能の向上や退行をリアルタイムで記録・反映し、推論性能の進化を“生きた指標”として提示する。</p>
<p>ベンチマーク結果は無料で公開されており、InferenceMAX公式<a href="https://inferencemax.semianalysis.com/">ダッシュボード</a>{target=\</p>
]]></description>
      <pubDate>Tue, 21 Oct 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>NTT、次世代純国産LLM「tsuzumi 2」発表──フルスクラッチ設計でGPT-5級の日本語性能を軽量モデルで実現</title>
      <link>https://ledge.ai/articles/ntt_tsuzumi2_fullscratch_gpt5_level_japanese_llm</link>
      <description><![CDATA[<p>NTT2025年10月20日、フルスクラッチで開発した純国産の大規模言語モデル（LLM）「tsuzumi」の次世代版「tsuzumi 2」を<a href="https://group.ntt/jp/newsrelease/2025/10/20/251020a.html">発表</a>した。
ChatGPTなど海外製LLMの普及が進む一方、電力消費や機密情報の取り扱いといった課題が顕在化するなか、NTTは1GPU環境で動作可能な軽量設計と高い日本語理解力を両立。日本語に最適化された高セキュアな生成AIとして提供を開始した。</p>
<h2>tsuzumi 2の進化ポイント</h2>
<p>NTTは、tsuzumi 2を「日本の企業DXを支える高性能・高セキュア・低コストな純国産LLM」と位置づけ、次の3つの進化点を挙げている。</p>
<ol>
<li>日本語性能のさらなる向上</li>
<li>特化型モデル開発効率の向上</li>
<li>低コスト・高セキュアの維持、国産AI</li>
</ol>
<p>これらの改良により、NTTは軽量ながらも世界トップクラスの性能を達成したと説明している。</p>
<h2>GPT-5級の日本語性能を実現</h2>
<p>「tsuzumi 2」は同クラス帯（約30Bパラメータ）のモデルとして世界トップクラスの日本語性能を持つ。
知識・解析・指示遂行・安全性の各評価において、数倍以上大きなフラッグシップモデル（GPT-5など）に匹敵する水準を記録した。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/tsuzumi2_3_88e14a3c64/tsuzumi2_3_88e14a3c64.jpg" alt="tsuzumi2 3.jpg" /></p>
<p>同社は、独自トークナイザーによって日本語の単語分割を最適化。文法構造に沿った分割を学習させることで、自然で読みやすい文の生成と高い効率性を両立した。
また、英語やソースコードに対しても効率的な生成が可能で、トークン当たり文字数ではGPT-5の約1.5倍を出力できるという。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/tsuzumi2_1_8855a698bd/tsuzumi2_1_8855a698bd.jpg" alt="tsuzumi2 1.jpg" /></p>
<h2>金融・医療・公共分野に特化した開発効率を強化</h2>
<p>tsuzumi 2では、RAG（検索拡張生成）とFine Tuningを組み合わせた特化モデル開発が可能になった。金融・医療・公共分野の知識を重点的に強化し、少量データでも高精度な応答を実現。NTT社内では、財務システムに関する問い合わせ応答タスクにおいて、他社先進モデルと同等以上の性能を確認した。また、FP2級試験を用いた評価では、他モデルの約10分の1の追加学習データで合格基準に到達したという。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/tsuzumi2_6_f1c925969d/tsuzumi2_6_f1c925969d.jpg" alt="tsuzumi2 6.jpg" /></p>
<h2>軽量・高セキュア設計で企業利用を想定</h2>
<p>tsuzumi 2は、1GPUでの推論が可能な軽量設計を維持し、推論コストを約10〜20分の1に削減。16bit・8bit・4bitの量子化モードを備え、用途に応じて精度や速度を調整できる。また、オンプレミスやプライベートクラウドでも動作可能で、機密情報を含む業務データも安全に処理できる。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/tsuzumi2_7_d5c23fc249/tsuzumi2_7_d5c23fc249.jpg" alt="tsuzumi2 7.jpg" /></p>
<h2>学習データを自社管理、著作権と文化への配慮</h2>
<p>NTTは、海外製オープンモデルに依存せず、学習データ・開発プロセス・品質を完全に自社管理している。新聞社データなどの権利保護にも配慮し、学習データから自主的に削除。40年以上にわたる日本語研究の成果を基盤に、「日本語・文化・慣習を理解するAI」を目指している。</p>
<h2>教育・企業現場での導入事例も拡大</h2>
<p>すでに東京通信大学が学内LLM基盤としてtsuzumi 2を採用。授業Q&amp;Aや教材作成支援、履修相談などで運用を開始する。
また、NTTドコモビジネスと富士フイルムビジネスイノベーションは、契約書や提案書などの非構造化データを安全に構造化・分析できる生成AIソリューションの共同開発を進めている。</p>
<h2>今後の展開</h2>
<p>NTTは、tsuzumi 2をグループ各社のAIソリューションに順次組み込み、産業ごとの特化モデルを展開予定。
さらに、AI間の自律的な議論を行う「AIコンステレーション」構想や、サイバーセキュリティ分野での応用開発も進める。
11月に開催される「NTT R&amp;Dフォーラム 2025（IOWN Quantum Leap）」では、tsuzumi 2を活用した最新ソリューションを披露するという。</p>
]]></description>
      <pubDate>Mon, 20 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Anthropic、「Claude Skills」を発表──資料を読み込み専門ワークを自動化する新機能</title>
      <link>https://ledge.ai/articles/claude_skills_release_oct2025</link>
      <description><![CDATA[<p>AI開発企業のAnthropicは2025年10月16日（現地時間）、AIアシスタント「Claude」に新機能「Agent Skills」（以下、Claude Skills）を導入したと<a href="https://www.anthropic.com/news/skills">発表</a>した。ユーザーが自らの業務資料や手順書をスキルとして登録すると、Claudeがそれらを読み込み、専門的なワークフローを自動実行できるようになる。企業ごとのナレッジやスクリプトをAIに統合し、業務効率化をさらに推し進める狙いだ。</p>
<p>@<a href="https://www.youtube.com/watch?v=IoqpBKrNaZI">YouTube</a></p>
<h2>フォルダ単位で“教え込む”仕組み</h2>
<p>Claude Skillsは、指示文やスクリプト、関連資料をまとめたフォルダをClaudeに読み込ませる仕組みだ。各フォルダには「SKILL.md」という定義ファイルが含まれ、タスク内容や使用条件などが記述される。Claudeは必要に応じてこれらのスキルを呼び出し、指示に従って処理を実行する。</p>
<p>この設計は「進行開示（progressive disclosure）」と呼ばれ、必要な情報だけを段階的に読み込むことで効率と安全性を両立する。Anthropicは、Claude Skillsを「プロンプトの再利用性を高め、AIが現実的な作業単位で動けるようにする構造」と説明している。</p>
<h2>企業導入例：Box、Canvaなど</h2>
<p>発表では、企業向けの活用例も紹介された。
クラウドストレージ大手のBoxでは、文書を自動的に要約・変換し、PowerPointやWord形式に整理するワークフローをスキルとして実装。Notionは「複雑なタスクでのプロンプト調整を減らし、より予測可能な結果につながる」とコメントしている。</p>
<p>さらにデザインプラットフォームのCanvaは、ブランドガイドをClaude Skillsに登録し、AIが自動でデザイン案を生成する活用を計画しているという。これにより、社内のスタイルガイドや手順書をAIに“教え込む”ことで、誰でも同じ品質で成果物を作れる環境を整備できる。</p>
<h2>エンジニア向けにはコード実行にも対応</h2>
<p>Anthropicの<a href="https://www.anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills">エンジニアリングブログ</a>では、Claude Skillsの内部構造が詳細に解説されている。Claude Skillsは単なるプリセットではなく、自己記述的モジュール構造を持つ。スキルはフォルダ単位で管理され、初期段階ではメタ情報のみを読み込み、関連タスクが発生した際に全体を展開する。</p>
<p>スクリプトやPythonコードを含めることで、データ分析や自動レポート生成といった専門処理も可能だ。開発者は /v1/skills エンドポイントを通じてスキルを登録・管理でき、実行にはCode Execution Tool（ベータ）が必要となる。</p>
<p>従来の「プロンプト＋RAG（検索）」のように文脈を都度読み込む手法に比べ、Claude Skillsでは情報の再利用が容易で、処理速度や一貫性が大幅に向上するという。</p>
<h2>今後の展開</h2>
<p>Anthropicは今後、スキルの作成・共有・管理を容易にするツールやチーム配布機能の提供を予定している。また、セキュリティ面での検証や公開スキルストアの構想も進行中だ。</p>
<p>今回の発表は、Claudeを“対話AI”から“実行AI”へ進化させる試みの一環であり、Anthropicが目指すエージェント時代の布石といえる。</p>
]]></description>
      <pubDate>Mon, 20 Oct 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Walmart、OpenAIと提携──ChatGPTに「Instant Checkout」登場　チャットで完結する購買体験を提供</title>
      <link>https://ledge.ai/articles/walmart_openai_instant_checkout_chatgpt</link>
      <description><![CDATA[<p>米小売最大手のWalmartは2025年10月14日（現地時間）、OpenAIとの提携を<a href="https://corporate.walmart.com/news/2025/10/14/walmart-partners-with-openai-to-create-ai-first-shopping-experiences">発表</a>した。この提携により、顧客やSam’s Clubの会員は、ChatGPTの「Instant Checkout（インスタントチェックアウト）」機能を通じてチャットで完結する買い物が可能になる。Walmartは“AI-First Shopping”を掲げ、反応型から予測型へと購買体験の転換を図る。</p>
<h2>チャットで完結する購買体験を提供</h2>
<p>新機能「Instant Checkout」は、ChatGPTのチャット画面上で商品提案から決済までをチャットで完結できる仕組みだ。
ユーザーが「Walmartで子どものランチボックスを探して」と入力すれば、ChatGPTが該当商品を提示し、そのまま少ない手順で購入まで案内する。</p>
<p>WalmartとSam’s Clubの商品が順次対象となる予定で、提供は「近日中（soon）」としており、詳細時期は非公表。</p>
<h2>「Agentic Commerce」──AIが先読みする買い物へ</h2>
<p>Walmartは今回の取り組みを「agentic commerce（自律型コマース）」と位置づける。
これまでのように顧客が検索して商品を選ぶ“反応型”から、AIがニーズを学習し、必要になる前に提案する“予測型”ショッピングへ転換する構想だ。リリースでは、「AIが顧客を理解し、日常の買い物を支援する時代に入った」と述べている。</p>
<h2>AI活用の知見を消費者体験へ拡張</h2>
<p>WalmartはこれまでもAIを活用し、商品カタログの改善や在庫最適化、アソシエイトのAIリテラシー教育を進めてきた。
今回のOpenAIとの提携は、そうしたAI活用の成果を消費者向けのチャット購買体験へ拡張するものとなる。</p>
<p>Doug McMillon CEOは、「長らくECの買い物は検索バーと長い商品リストが中心だったが、それは変わる」と述べ、Sparky（同社AI）とOpenAIとの連携で、より楽しく便利な未来へ進むと強調した。OpenAIのSam Altman氏も「日々の買い物を少し簡単にする取り組みの一つ」とコメントしている。</p>
]]></description>
      <pubDate>Mon, 20 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ugo、AIロボット向け「模倣学習キット」を初公開──Robotics NEXT Tokyo 2025で披露</title>
      <link>https://ledge.ai/articles/ugo_ai_robot_imitation_learning_kit_2025</link>
      <description><![CDATA[<p>ugo株式会社は2025年10月16日、AIロボット向けの「模倣学習キット」を<a href="https://ugo.plus/information/2025/10/16/ugo-ai-learning-kit/">発表</a>し、同日に開催された展示会「Robotics NEXT Tokyo 2025」にて初公開した。AI技術を活用して人間の動作を学習・再現するロボット開発を支援するもので、教育・研究・開発の現場での活用を想定している。</p>
<p>同キットは、人間の動作をカメラやセンサーで取得し、AIがそのデータを解析してロボットに再現させる仕組みを備える。これにより、ロボットが人間の作業や身ぶりを模倣的に学習し、自律的に動作を生成できるようになるという。ソフトウェアSDKやデータ処理ツールなどがセットになった統合パッケージとして提供され、ugoロボット以外の機体にも対応可能だ。</p>
<p>@<a href="https://www.youtube.com/watch?v=DLFMDuaDhho">YouTube</a></p>
<p>展示会では、来場者の動きをリアルタイムでキャプチャし、ロボットが同様の動作を再現するデモンストレーションが行われた。ブースでは、AIによる模倣学習の過程を可視化する体験型展示も実施され、研究者や教育関係者から注目を集めた。</p>
<p>ugoは、遠隔操作型ロボット「ugo」シリーズの開発で知られる企業。今回の模倣学習キットは、AIを活用した人の技能継承や現場作業の自動化を支援する基盤技術として位置づけられており、今後は教育・研究機関への提供を進める方針だ。</p>
]]></description>
      <pubDate>Mon, 20 Oct 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Oracle、企業のAI導入を支援する「AI Data Platform」を一般提供──マルチクラウド・ゼロETLでデータ活用を加速</title>
      <link>https://ledge.ai/articles/oracle_ai_data_platform_general_availability_oct2025</link>
      <description><![CDATA[<p>Oracleは2025年10月14日（米国時間）、AI時代のデータ活用を統合的に支援する新たな基盤「Oracle AI Data Platform」の一般提供を開始したことを<a href="https://www.oracle.com/news/announcement/ai-world-oracle-unveils-ai-data-platform-empowering-customers-to-innovate-in-the-ai-era-2025-10-14/">発表</a>した。</p>
<p>同プラットフォームは、データの取り込みから意味付け、ベクトル検索、生成AIとの連携までを一元的に実行できる点を特徴とする。マルチクラウド環境やゼロETL構成にも対応し、企業がより効率的かつ安全にAIを導入できる環境を整える。</p>
<h2>「AI for Data Revolution」──AIとデータの融合を推進</h2>
<p>Oracleは今回の発表で、“AI for Data Revolution（AIによるデータ革命）”という新たなビジョンを掲げた。
「Oracle AI Data Platform」はこの構想の中核を担い、生成AIや機械学習モデルを企業の業務データに組み込みやすくすることを狙う。</p>
<p>同プラットフォームは、Oracle Cloud Infrastructure（OCI）上に構築され、Autonomous AI DatabaseやOCI Generative AI serviceなど既存のAI関連サービスと連携。AIアプリケーション開発に必要なデータ基盤を統合的に提供する。</p>
<h2>データの“意味づけ”とベクトル検索を統合</h2>
<p>「Oracle AI Data Platform」では、データの収集・統合・意味付け（semantic enrichment）・ベクトル化（vector indexing）を一元化して管理できる。これにより、自然言語クエリによる高度な検索やRAG（Retrieval-Augmented Generation）アプリケーションの構築が容易になる。</p>
<p>データレイクハウス型アーキテクチャを採用し、Delta LakeやApache Icebergなどのオープンフォーマットにも対応。既存システムを問わずスムーズな連携が可能となる。</p>
<p><strong>Oracle AI Data Platformの構成図。データ基盤からAIモデル、開発ツール、ユーザー体験までを統合</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/oracle_ai_platform1_8866d022ea/oracle_ai_platform1_8866d022ea.jpg" alt="oracle ai platform1.jpg" /></p>
<h2>ゼロETLとマルチクラウドで運用負荷を軽減</h2>
<p>同プラットフォームは「Zero-ETL」「Zero-Copy」構成を実現。異なるデータソース間での転送や複製を不要とし、リアルタイム性とセキュリティを両立する。さらに、マルチクラウド／ハイブリッド環境に対応しており、他社クラウド上のデータもOCI環境内で透過的に扱える。これにより、企業はシステム間の移行コストを抑えつつ、データを即座にAI活用へと結びつけることができる。</p>
<p><strong>Oracle AI Data Platformが統合する主要サービス群</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/oracle_ai_platform2_a759b091f2/oracle_ai_platform2_a759b091f2.jpg" alt="oracle ai platform2.jpg" /></p>
<h2>AI Factoryと連動、導入支援を強化</h2>
<p>Oracleは同日、「Oracle AI Factory」プログラムも発表した。これは顧客やパートナー企業がAIを迅速に導入できるよう支援する取り組みで、15億ドルを超える投資を行い、8,000人以上の専門人材を育成するという。また、100件を超える業界ユースケースの開発を進め、Fusion Cloud ApplicationsやNetSuiteなど既存アプリケーションとの統合も拡大する。</p>
<h2>包括的なAIエコシステムへ</h2>
<p>Oracleは、「AI Data Platform」をはじめ「AI Database 26ai」「Autonomous AI Lakehouse」などを連動させ、AIインフラから業務アプリケーションまでをカバーするエコシステムを構築していく方針を示した。これにより、企業がデータ主導のAI戦略を迅速に実現できる環境を提供し、グローバルでのAI導入支援を強化する。</p>
]]></description>
      <pubDate>Sun, 19 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>エンタメ＆アート2025/10/19 [SUN]Spotify、Sony・Universal・Warnerの3大レーベルと提携──“アーティスト優先”のAI音楽製品を共同開発へ</title>
      <link>https://ledge.ai/articles/spotify_artist_first_ai_music_collaboration</link>
      <description><![CDATA[<p>Spotifyは2025年10月16日（現地時間）、Sony Music Group、Universal Music Group、Warner Music Groupの3大レーベルに加え、ライセンス大手Merlinおよびデジタル音楽企業Believeと提携し、「アーティスト優先（Artist-First）」のAI音楽製品を共同開発すると<a href="https://newsroom.spotify.com/2025-10-16/artist-first-ai-music-spotify-collaboration/">発表</a>した。AI技術の拡大を受け、アーティストやソングライターの権利を尊重しながら、透明性と倫理性を備えた“責任あるAI”の活用を進める。</p>
<h2>主要レーベルと連携、AI時代の「共創モデル」を構築</h2>
<p>提携では、Sony、Universal、Warnerの3大レーベルに加え、音楽ライセンス団体Merlinとデジタル音楽企業Believeが参画。Spotifyは「AIを音楽の創造性を広げるためのツールとして活用し、アーティストが主体的に選べる仕組みをつくる」と説明している。</p>
<p>開発される「Artist-First AI Music Products」は、AIを用いた音楽体験を創出しつつ、クリエイターの利益と選択権を中心に据えた設計となる予定だ。</p>
<h2>“責任あるAI”を支える4つの原則</h2>
<p>Spotifyは発表の中で、AI製品開発の基盤となる4つの原則を掲げている。</p>
<ol>
<li><strong>ライセンスに基づく正当な利用</strong>  — 合法的かつ倫理的なAI活用を保証。</li>
<li><strong>アーティストとソングライターの選択権</strong>  — 生成AIの利用可否を、創作者自身が選べる仕組みを導入。</li>
<li><strong>公正な報酬の確保</strong>  — AI生成を含むコンテンツ利用に対して、正当な収益分配を保証する。</li>
<li><strong>アーティストとファンの関係強化</strong>  — AIを通じて創作の多様性と新しい表現の機会を広げる。</li>
</ol>
<p>Spotifyは、「AIが創作の妨げではなく、アーティストの創造力を支援する存在になるべきだ」としている。</p>
<h2>1か月前に「AI保護強化」策を導入</h2>
<p>この発表の前段として、Spotifyは9月25日に「AI保護強化（Spotify Strengthens AI Protections）」を<a href="https://newsroom.spotify.com/2025-09-25/spotify-strengthens-ai-protections/">公表</a>{target=\</p>
]]></description>
      <pubDate>Sun, 19 Oct 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/10/18 [SAT]「見て、聞いて、考えるCopilot」──Windows 11が“AIネイティブOS”へ進化</title>
      <link>https://ledge.ai/articles/windows11_copilot_vision_ai_pc_strategy</link>
      <description><![CDATA[<p>マイクロソフトは2025年10月16日（米国時間）、Windows 11における次世代AI機能の拡充方針を発表した。公式ブログ「<a href="https://blogs.windows.com/windowsexperience/2025/10/16/making-every-windows-11-pc-an-ai-pc/">Making every Windows 11 PC an AI PC</a>」で、すべてのWindows 11搭載端末を「AI PC」として再定義し、Copilotを中核に据えたAI体験をOS全体に統合していく構想を明らかにした。</p>
<h2>新しいウェイクワード「Hey Copilot」で話しかけるだけ</h2>
<p>@<a href="https://www.youtube.com/watch?v=7Nbf1fqxcCM">YouTube</a></p>
<p>同社は「AIはWindows体験の中心になる」として、音声・視覚・行動理解を備えた次世代のCopilot機能を順次展開する。音声による起動「Hey Copilot」、カメラや画像から状況を読み取る「Copilot Vision」、アプリ操作や設定変更などを文脈的に実行する「Copilot Actions」などが含まれるという。</p>
<p>たとえば、Copilot Visionは開いているグラフを要約したり、エラー画面を読み取って修正案を提示したりすることが可能。さらに音声起動「Hey Copilot」と組み合わせれば、「この表をPowerPointにまとめて」「この画像を説明して」といった自然な音声操作にも対応する。AIが“見て・聞いて・動く”ことで、ユーザーとのインタラクションがより直感的なものに変わる。</p>
<p>技術面では、NPU（Neural Processing Unit）を搭載した「AI PC」でオンデバイスAI処理を実行。これにより応答速度の向上とプライバシー保護を両立する。マイクロソフトは、AIをクラウド依存ではなくOSレベルに組み込む方針を明確にし、「AIネイティブOS」への移行を本格化させた。</p>
<p>同日公開された別の公式ブログ「<a href="https://blogs.windows.com/windowsexperience/2025/10/16/new-experiences-currently-rolling-out-for-windows-11">New experiences currently rolling out for Windows 11</a>」では、新しいCopilot体験の段階的な提供が開始されていることを明らかにした。ユーザーは自然言語でシステム設定やファイル操作、スクリーンショット分析、スケジュール調整などを行えるようになる。</p>
<p>さらに、開発者向けには外部サービスと連携できる「Copilot Extensions」を提供し、Microsoft Storeを通じてAI対応アプリの配信を拡充する方針も示された。</p>
<p>Windows 10のサポート終了（2025年10月）を目前に控え、マイクロソフトはWindows 11をAIネイティブOSとして再設計する姿勢を強調している。同社は「AIがPC体験そのものを再定義する」として、SurfaceシリーズやOEMメーカーと連携し、AI PC時代の普及を進めていく考えだ。</p>
]]></description>
      <pubDate>Sat, 18 Oct 2025 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ロサンゼルス「パリセーズ火災」容疑者を逮捕──司法当局、ChatGPTで生成した“燃える都市”画像を押収と発表</title>
      <link>https://ledge.ai/articles/palisades_fire_chatgpt_image_doj</link>
      <description><![CDATA[<p>アメリカ司法省（U.S. Department of Justice）は2025年10月7日、ロサンゼルスのパシフィック・パリセーズ地区で1月に発生し、12人が死亡・6,000棟以上が焼失した大規模火災を「故意に発生させた」として、フロリダ州在住の29歳の男、ジョナサン・リンダークネヒト（Jonathan Rinderknecht）容疑者を逮捕・起訴したと<a href="https://www.justice.gov/usao-cdca/pr/florida-man-arrested-federal-criminal-complaint-alleging-he-maliciously-started-what">発表</a>した。</p>
<p>司法省の臨時連邦検事ビル・エッサイリ（Bill Essayli）氏は同日、X（旧Twitter）上で、押収されたデジタル機器の中に「容疑者がChatGPTで生成した“燃える都市”の画像」が含まれていたと<a href="https://x.com/USAttyEssayli/status/1975954598201536880">投稿</a>。同氏は「この悲劇による被害を元に戻すことはできないが、逮捕と起訴が犠牲者に一部でも正義をもたらすことを願う」と述べた。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/attorney_la_bill_f7db2f0510/attorney_la_bill_f7db2f0510.jpg" alt="attorney la bill.jpg" /></p>
<p>司法省カリフォルニア中部地区連邦検事局によると、リンダークネヒト容疑者は「悪意をもって火を放った（maliciously set a fire）」として連邦法に基づき起訴された。捜査にはATF（火器・爆発物取締局）、ロサンゼルス市消防局（LAFD）、ロサンゼルス郡保安局（LASD）が協力した。容疑者はフロリダ州オーランドで拘束され、同日午後1時30分（米東部時間）に連邦地方裁判所で初公判が予定されている。司法省は、容疑者が有罪と確定するまで無罪と推定されると明記している。</p>
<p>司法省の説明によれば、火災は2025年1月1日に発生し、乾燥と強風の影響で急速に拡大した。地下でくすぶった火が再び燃え上がり、最終的に6,000棟を超える住宅や建物が焼失した。ロサンゼルス市消防局はこの火災を「近年で最も被害の大きい火災のひとつ」としている。</p>
]]></description>
      <pubDate>Fri, 17 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>エンタメ＆アート2025/10/17 [FRI]Google、動画生成AI「Veo 3.1」を発表──1分超のシーン拡張「Extend」搭載、Flowと統合強化</title>
      <link>https://ledge.ai/articles/google_veo3_1_flow_integration</link>
      <description><![CDATA[<p>Googleは現地時間2025年10月15日、動画生成AI「Veo」の最新版となるVeo 3.1を<a href="https://blog.google/technology/ai/veo-updates-flow/">発表</a>した。</p>
<p>新バージョンでは、AI映像制作ツール「Flow」に新機能を追加し、その中核としてVeo 3.1を統合。照明・構図・音声をAIが自動的に制御できるようになり、リッチなオーディオ、物語制御（narrative control）の強化、質感のリアリズム向上を図ったアップデートとなっている。</p>
<p>@<a href="https://www.youtube.com/watch?v=I06Ef8alr2Y">YouTube</a></p>
<h2>Flowとの統合で生成から編集まで一体化</h2>
<p>Googleは今回、AI映像制作ツール「Flow」へのアップデートを発表した。Flowは5か月前の導入以降、すでに2億7,500万本以上の動画を生成しており、Veo 3.1の搭載によって生成から編集までのプロセスがさらに統合された。</p>
<p>Flowは、テキスト・画像・音声・映像素材といった“ingredients”を組み合わせて動画を構築できるツールである。
既存の「Ingredients to Video」「Frames to Video」「Extend」機能に加え、今回は音声統合を拡張。ユーザーは複数の素材をもとに、AIがシーン構成やカメラワーク、トーンを自動的に最適化した一貫性のある映像を生成できる。</p>
<p>新しいExtendでは、直前のクリップの終端1秒を手がかりに、1分以上の連続ショットとして自然に拡張することも可能。Googleはこれにより、「映像制作をより直感的で対話的な体験へと変える」としている。</p>
<p>@<a href="https://www.youtube.com/watch?v=B78BJuPxmBU">YouTube</a></p>
<h2>照明や構図、音声もAIが自動編集</h2>
<p>Veo 3.1では、照明・陰影・カメラ構図をAIが自動的に制御し、シーン全体のトーンや一貫性を高める。
また、AIによる音声生成と映像への同期統合にも対応し、環境音や効果音を含む“音響的なリアリティ”を再現できるようになった。</p>
<p>Flow内には、シーンに要素を追加する「Insert」と、不要な物体を背景ごと削除する「Remove（近日提供）」の編集機能も加わった。影や照明の整合を自動で処理することで、合成感を抑えた自然な編集を可能にしている。
Googleは、こうした機能群を通じて「richer audio」「more narrative control」「enhanced realism」の実現を掲げている。</p>
<h2>Gemini APIで提供、Standard／Fastモデルを展開</h2>
<p>Veo 3.1は、「Standard」モデルと「Fast」モデルの2種類を用意し、Gemini API経由で開発者向けに提供が開始された。生成速度を優先するワークフローにはFastモデル、品質を重視する制作用途にはStandardモデルが推奨される。</p>
<p>さらに、Veo 3.1はVertex AIおよびGeminiアプリからも利用可能。新機能はGemini API／Vertex AIの双方で順次展開される予定で、API向けの「Scene extension」機能も今後提供される見込みだ。
Googleは「AIによる創造的表現の民主化をさらに進める」とし、プロフェッショナルから一般ユーザーまで、誰もが高品質な映像制作にアクセスできる環境の構築を目指している。</p>
<h2>Veoシリーズの進化</h2>
<p>Veoシリーズは、2024年12月の「Veo 2」、2025年春の「Veo 3」に続く最新バージョン。
今回のVeo 3.1では、「AI任せの自動生成」から「人とAIが協働して映像を作る」方向へと進化した。
Flowとの連携により、テキストによる指示だけでなく、素材・音声・カメラ指示などを含めた多層的なプロンプト設計が可能になり、AI映像生成の精度と自由度が大幅に向上している。</p>
<p>Googleは今後、VeoをGeminiエコシステムの中核技術として位置づけ、AIを活用したクリエイティブツールの拡充を進める方針を示している。</p>
]]></description>
      <pubDate>Fri, 17 Oct 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>日本プロ野球選手会、SNS上の誹謗中傷を検出するAIを導入──クライマックスシリーズと日本シリーズで運用開始</title>
      <link>https://ledge.ai/articles/npb_ai_antislander_2025</link>
      <description><![CDATA[<p>日本プロ野球選手会は2025年10月10日、クライマックスシリーズおよび日本シリーズに出場する選手を対象に、SNS上での誹謗中傷を自動検出するAIシステムを導入すると<a href="https://jpbpa.net/2025/10/10/12723/">発表</a>した。AIがSNS上の投稿を常時モニタリングし、不適切な内容を検出・通報する仕組みを構築する。選手会は「選手が安心して競技に集中できる環境をつくる」ことを目的としている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/JPBPA_Press1_eeab9f12c0/JPBPA_Press1_eeab9f12c0.jpg" alt="JPBPA_Press1.jpg" /></p>
<h2>クライマックスシリーズと日本シリーズで試験導入</h2>
<p>対象となるのは、「2025 JERA クライマックスシリーズ セ」「2025 パーソル クライマックスシリーズ パ」および「SMBC日本シリーズ2025」に出場登録された全選手。</p>
<p>AIがSNS上の投稿を監視し、誹謗中傷に該当する事案を自動的に検出。検出後は選手ごとにリスト化し、SNS運営元への通報や削除要請、NPB球団との情報共有、ダイレクトメッセージ（DM）対応、発信者情報開示請求や証拠保全などを通じて迅速な対応を行う。</p>
<h2>英Signify GroupのAIを活用</h2>
<p>導入されるのは、英国に本社を置くSignify Group社の誹謗中傷検出・通報支援サービス「Threat Matrix」。日本語を含む42言語と絵文字に対応し、主要SNS上の投稿をAIが自動的に分析して不適切な内容を検出する。
同システムは「FIFAワールドカップカタール2022」や「ラグビーワールドカップフランス2023」、女子テニス協会（WTA）および国際テニス連盟（ITF）などでも採用されており、英プレミアリーグ・アーセナルFCでは導入後、誹謗中傷の検出件数が<a href="https://www.arsenal.com/news/24-supporters-banned-abusive-behaviour">約90％減少</a>したという。</p>
<h2>選手と家族を守る取り組み</h2>
<p>選手会は「誹謗中傷が選手の家族に及んだ場合には家族へのサポートも行う」としており、対象を家族にも拡大。
SNS上の誹謗中傷が社会問題化する中、選手とその家族の精神的安全を守るための包括的な取り組みとして位置づけている。
同会は今後も、日本プロフェッショナル野球組織（NPB）および12球団と連携しながら、選手が安心して競技に集中できる環境づくりを進めていく。</p>
]]></description>
      <pubDate>Fri, 17 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>NTTとNTTドコモビジネス、自動運転レベル4対応の「通信安定化ソリューション」を提供開始──IOWN技術で遠隔監視の通信を高信頼化</title>
      <link>https://ledge.ai/articles/ntt_docomo_business_autonomous_driving_stable_communication_solution</link>
      <description><![CDATA[<p>NTTドコモビジネス株式会社とNTT株式会社は2025年10月8日、自動運転車両など移動体向けに通信の安定性を高める「通信安定化ソリューション」の提供開始を<a href="https://www.ntt.com/about-us/press-releases/news/article/2025/1008.html">発表</a>した。</p>
<p>両社が開発したIOWN（Innovative Optical and Wireless Network）技術を活用し、無線品質の予測に基づく複数回線のマルチパス通信制御と、データ連携システムを組み合わせることで、自動運転レベル4の遠隔監視を支える高信頼・低遅延な通信を実現する。</p>
<h2>自動運転レベル4の社会実装を支える通信基盤</h2>
<p>全国で進む自動運転レベル4の社会実装では、基地局の切り替えや通信干渉による一時的な映像途切れが課題となっている。NTTドコモビジネスとNTTは、これまで各地の自動運転実証実験で得た知見をもとに、通信の安定化に必要な複数技術をパッケージ化。導入までのリードタイムを短縮し、自治体や企業が容易に利用できる形で提供する。</p>
<h2>3つの技術を統合したパッケージ構成</h2>
<p>ソリューションは、次の3つの技術で構成される。</p>
<ol>
<li><strong>無線品質予測</strong> ：公衆ネットワークやローカル5G、Wi-Fiなどの無線品質を機械学習により予測（IOWN技術「Cradio」を活用）</li>
<li><strong>マルチパス通信制御</strong> ：通信状況に応じて複数回線を制御し、高い接続性を実現（IOWN技術「協調型インフラ基盤」を活用）</li>
<li><strong>リアルタイムデータ伝送</strong> ：車載カメラ映像やセンサーデータなどをリアルタイムに遠隔監視システムへ伝送（アプトポッド社「intdash」を活用）</li>
</ol>
<p>これにより、車両と遠隔監視システム間の通信を複数経路で同時接続し、無線品質の変化を先読みして制御することで、映像の途切れを抑えた安定した通信環境を構築できるという。</p>
<p><strong>通信安定化ソリューションの構成イメージ</strong>：車載クライアント、クラウドサーバー（ドコモMEC）、監視センタを連携し、IOWN技術「Cradio」による無線品質予測とマルチパス通信制御、intdashによるデータ伝送を統合している。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/1008b_423a2f1ca4/1008b_423a2f1ca4.jpg" alt="1008b.jpg" /></p>
<h2>実証で通信遅延を大幅に改善</h2>
<p>両社は公道で自動運転車両を用いた実証実験を実施。クライアントとクラウド間の通信遅延400ミリ秒以下を維持できた割合は、非適用時の92％（1回線）および53％（2回線）に対し、本ソリューション適用時は99％に達したとしている。</p>
<h2>各社の役割と今後の展開</h2>
<p>このソリューションはNTTドコモビジネスが提供を担当し、intdashを用いた環境構築や活用提案を行う。NTTは「Cradio」および「協調型インフラ基盤」に関する研究開発を担う。</p>
<p>今後は、自動運転における遠隔監視の安定化だけでなく、建設現場や工場、倉庫などでの遠隔操作・自動化分野への展開も見据える。両社は、通信安定化技術を通じて人手不足や安全確保などの社会課題の解決に貢献するとしている。</p>
]]></description>
      <pubDate>Fri, 17 Oct 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Boston Dynamics、トヨタ・リサーチ・インスティテュートと共同でヒューマノイドロボット「Atlas」の新型グリッパーを公開──“器用さ”を極める7DoFハンド開発の舞台裏</title>
      <link>https://ledge.ai/articles/bostondynamics_atlas_gripper_tri_202510</link>
      <description><![CDATA[<p>ボストン・ダイナミクス（Boston Dynamics）は2025年10月8日、トヨタ・リサーチ・インスティテュート（TRI）と共同研究を進めるヒューマノイドロボット「Atlas」の最新映像を<a href="https://www.youtube.com/watch?v=gS4rOqNDTBk">公開</a>した。</p>
<p>動画は同社公式シリーズ「Inside the Lab」の新作で、「Perception and Adaptability（知覚と適応）」をテーマに、ヒューマノイドの“手”にあたるグリッパー（ハンド）の進化を紹介している。</p>
<h2>油圧式から電動式へ──操作能力に焦点</h2>
<p>Atlasはこれまで、油圧駆動によるジャンプや宙返りなどのモビリティ性能で知られてきた。
今回の研究では、「移動」から「操作」へと重心を移し、より高い器用さ（dexterity）を持つ電動式グリッパーの開発に注力している。
Boston Dynamicsは「より人間に近い作業能力を備えたヒューマノイド」を目指し、把持（grasping）と認識（perception）の融合を進めているという。</p>
<h2>7DoF＋触覚＋掌カメラ──“感じてつかむ手”</h2>
<p><a href="https://bostondynamics.com/blog/ask-a-roboticist-meet-karl/">公式ブログ</a>によれば、現行のグリッパは7つの自由度（DoF）を備え、指先の触覚センサーと掌部カメラを搭載。滑りや力加減を検知して把持を調整でき、3本指＋対向親指（opposable thumb）の構成により、大型・重量物から繊細な小物まで幅広く扱える。</p>
<h2>左右ミラーハンドと指の可動</h2>
<p>グリッパーには左右のミラーバージョンがあり、指は内側に90°／後方に90°曲げられる設計。Atlasは人のような“利き手”を持たず、立ち位置や対象物に応じて最適な手を選んで作業する。</p>
<p>@<a href="https://www.youtube.com/watch?v=gS4rOqNDTBk">YouTube</a></p>
<h2>製造現場の実用タスクを見据える</h2>
<p>目標は自動車製造ラインなどで役立つ“実用タスク”の達成だ。外装部品のような重い部材を傷つけずに扱う作業と、ネジや工具の扱いといった繊細な作業の両立が求められる——今回のグリッパー設計は、まさにその要件に合わせて磨かれている。</p>
]]></description>
      <pubDate>Thu, 16 Oct 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ライオン、研究開発データで追加学習した独自LLM「LION LLM」開発に着手──AWS協力のもと、ものづくりDXを加速</title>
      <link>https://ledge.ai/articles/lion_llm_development_aws_dx_2025</link>
      <description><![CDATA[<p>ライオン株式会社は2025年10月8日、自社の長年にわたる研究開発データを用いて追加学習を行った独自の大規模言語モデル（LLM）「LION LLM」の開発に着手したと<a href="https://prtimes.jp/main/html/rd/p/000000218.000039983.html">発表</a>した。アマゾン ウェブ サービス ジャパン合同会社（AWSジャパン）の生成AI実用化推進プログラムを通じた協力を受け、ものづくり分野のデジタルトランスフォーメーション（DX）を加速する狙いだ。</p>
<h2>暗黙知の継承と「ものづくりDX」の推進</h2>
<p>ライオンは、「次世代ヘルスケアのリーディングカンパニーへ」を掲げる中期経営計画「Vision 2030 2nd STAGE」を2025年から始動。デジタル分野では「ものづくりDX」を重点テーマに位置づけ、収益力とクリエイティビティを両立させた競争力のある製品・サービスを迅速に市場へ投入できる体制を目指している。</p>
<p>同社では、製造業において重要な「暗黙知」の継承が課題となっていた。熟練技術者の退職により貴重なノウハウが失われつつあることを背景に、2023年12月には生成AIと検索システムを組み合わせた研究ナレッジ検索ツールを導入。情報検索時間を従来の5分の1以下に短縮するなどの成果を上げたが、専門知識を要する複雑な質問には対応が難しい状況が続いていた。</p>
<p>この課題を解決するため、ライオンはAWSジャパンの協力のもと、自社の知見を生かした独自LLMの内製開発を進めることを決定した。</p>
<h2>AWS支援のもとで社内に分散学習基盤を構築</h2>
<p>ライオンは2025年4月から、AWSジャパンが提供する「生成AI実用化推進プログラム」に参加。クレジット付与によるコスト支援や科学的助言などの技術協力を受け、内製開発体制を整備した。</p>
<p>社内には分散学習環境を構築。AWS ParallelClusterとNVIDIAのMegatron-LMを組み合わせ、複数GPUサーバーを効率的に連携させる仕組みを採用した。ベースモデルには「Qwen 2.5-7B」を使用し、研究報告書、製品組成情報、品質評価データなど、数十年にわたる社内知見を学習データとして投入している。</p>
<p>初期フェーズの評価では、過去の知見に基づく具体的なアドバイスや、複数の事例を統合した回答が可能であることを確認。従来ツールと比較して、回答の網羅性が大幅に向上したと社内で報告されている。</p>
<h2>今後の展開</h2>
<p>今後は、学習データの拡充と品質向上を目的に、プレゼン資料など構造化が難しいデータのクリーニングを進める。さらに、経済産業省およびNEDOが主導する「Generative AI Accelerator Challenge（GENIAC）」で開発された国産モデルの活用など、多角的なアプローチで精度向上を図る計画だ。</p>
<p>これらを既存のナレッジ検索ツールと統合し、高度な質問やタスクにも対応できるシステムを目指す。同社は、知識資産の最大活用を通じて「ものづくりDX」を加速し、競争優位性の強化につなげるとしている。</p>
]]></description>
      <pubDate>Wed, 15 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>東京大学・ソフトバンク・LINEヤフー、「Beyond AI技術研究組合」を設立──AI研究成果の事業化を加速</title>
      <link>https://ledge.ai/articles/beyond_ai_technology_consortium_tokyo_univ_softbank_line_yahoo</link>
      <description><![CDATA[<p>東京大学とソフトバンク、LINEヤフーは2025年10月10日、AI（人工知能）研究の成果を迅速に社会実装・事業化するための新たな産学連携組織「Beyond AI技術研究組合」を9月19日に設立したと<a href="https://beyondai.jp/contents/2025/10/10/cip3/">発表</a>した。</p>
<p>同組合は、経済産業省の「CIP（コーポレート・イノベーション・プラットフォーム）」制度を活用し、AI研究から事業化までを一貫して推進する体制を構築するという。</p>
<h2>研究と事業化の橋渡しを強化</h2>
<p>東京大学とソフトバンク、LINEヤフーの3者は、2020年設立の「Beyond AI研究推進機構」を通じてAI分野の基礎研究を進めてきた。今回の「Beyond AI技術研究組合」は、その研究成果を社会や産業へ迅速に還元するための組織で、知的財産や人材育成、資金調達などの面から事業化を支援する「プラットフォーム型CIP」として位置づけられる。</p>
<p>また、2024年6月に経済産業省がCIP設立・運営ガイドラインを改正し、1つのCIPから複数の事業会社を設立できる枠組みを導入したことを受け、同組合もその新制度を活用して設立された。</p>
<h2>今後の展開</h2>
<p>同組合は、パーソナルAIエージェント時代を見据えたAI技術の高度化や基盤技術開発を進めるとともに、Beyond AI連携事業で取り組んできた医療ヘルスケア領域などへのAI応用研究を推進する。
また、ソフトバンクグループと連携し、産業領域横断のデータ活用や社会実装に向けた実証実験（PoC）を進めていく。</p>
]]></description>
      <pubDate>Wed, 15 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/10/19 [SUN]Amazon、次世代原子炉「小型モジュール炉（SMR）」開発支援を発表──AI需要増に対応し、脱炭素電源を強化</title>
      <link>https://ledge.ai/articles/amazon_smr_clean_energy_ai_infrastructure</link>
      <description><![CDATA[<p>Amazonは2025年10月16日（米国時間）、AI技術やクラウドサービスの拡大に伴う電力需要の急増に対応するため、次世代原子炉「小型モジュール炉（SMR）」の開発支援を<a href="https://www.aboutamazon.com/news/sustainability/amazon-smr-nuclear-energy">発表</a>した。米国のエネルギー企業Dominion Energy、X-energy、Energy Northwestの3社と協力し、先進原子力技術の導入を進める。</p>
<p>Amazonは「再生可能エネルギーに加え、信頼性の高いゼロカーボン電源が必要」と説明し、原子力を再エネを補完する選択肢として位置づける。</p>
<p>今回の取り組みでは、Dominion Energyとバージニア州でのSMR開発可能性の評価（MOU）を行い、X-energyとは高温ガス炉「Xe-100」商業化の支援で連携、Energy Northwestとはワシントン州での先進原子力プロジェクトを検討する。いずれも初期段階で、建設開始時期は現時点で未定としている。</p>
<p><strong>X-energy社のXe-100運転訓練シミュレーター</strong>：開発中の小型モジュール炉（SMR）「Xe-100」の制御システムを再現した施設で、オペレーターが安全運転や緊急対応を訓練する
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/1_0bbe5f5462/1_0bbe5f5462.jpg" alt="ダウンロード (1).jpg" /></p>
<p>Amazonは世界で500件を超える再生可能エネルギープロジェクトを展開しており、クリーン電力の拡大を継続している。長期目標の2040年ネットゼロ（二酸化炭素排出実質ゼロ）の達成に向け、SMRを含むクリーン電源の多様化を進める方針だ。</p>
]]></description>
      <pubDate>Tue, 14 Oct 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google DeepMind、AIに“造語”を教えて振る舞いを制御──Geminiが自ら意味を説明する能力も確認</title>
      <link>https://ledge.ai/articles/deepmind_neologism_learning_for_ai_controllability</link>
      <description><![CDATA[<p>Google DeepMindの研究チームは2025年10月9日、AIに新しい架空の言葉（造語）を学習させることで、その振る舞いを精密に制御できる手法を<a href="https://arxiv.org/abs/2510.08506">発表</a>した。論文「Neologism Learning for Controllability and Self-Verbalization」は、AIが学習した造語の意味を自然言語で説明できる“自己言語化（self-verbalization）”という現象も初めて報告している。</p>
<p>この研究はarXiv上で公開されたプレプリント（査読前論文）で、AIの内部表現を「言葉」で理解・制御する新しいアプローチとして注目を集めている。</p>
<h2>造語でAIをコントロール</h2>
<p>従来、AIの出力傾向を操作するには、プロンプト設計や外部ツール（例：steering vector、autoencoderなど）による内部操作が必要だった。今回の手法では、モデル本体のパラメータを一切変更せず、造語に対応する新しい単語埋め込み（embedding）だけを学習する。</p>
<p>たとえば「Give me a lack answer.」と指示すると、AIは短い回答を返すようになり、別の造語では「誤った回答」「お世辞」「拒否」など異なる挙動を誘発できる。</p>
<p>研究チームはこの方法を「ネオロジズム学習（Neologism Learning）」と呼び、言語による“行動パラメータ”の追加と位置づけている。</p>
<p><strong>ネオロジズム学習のプロセス。左から「造語による概念の学習」「AIによる自己言語化（Verbalization）」「説明文を使った再検証（Plug-In Evaluation）」の流れを示す</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x1_2e7a14d15f/x1_2e7a14d15f.png" alt="x1.png" /></p>
<h2>AIが自ら意味を説明</h2>
<p>研究チームは、AIが学習した造語の意味を英語で説明できることを確認した。
たとえば、短文回答を誘発する造語を学習したモデルに「What does lack mean?（lackとは何を意味しますか？）」と尋ねると、
「It means to give a shorter response.（短い回答をすることを意味します）」と答える。</p>
<p>このようにAI自身が学習した内部概念を自然言語で記述する能力を、研究チームは“self-verbalization（自己言語化）”と定義。
さらに、造語をその説明文に置き換えても同様の挙動が再現されるかを検証する「plug-in evaluation」を導入し、自己説明の信頼性を評価した。</p>
<h2>モデル間で“機械語”が通じる</h2>
<p>DeepMindの実験では、Gemma-3-4B-ITが学んだ造語を別のモデル――Gemini 2.5 Flash――に入力したところ、意味が通じ、ほぼ同じ制御効果を示した。
たとえば“lack”という語を用いた場合、Gemmaでは回答の平均文数が42.9から15.8に減少し、Geminiでも中央値が37から4に減少した。</p>
<p>研究チームはこの現象を「machine-only synonym（機械専用類義語）」と呼び、
人間には直感的に理解できないが、AI同士では通じ合う“共通語彙”が形成される可能性を指摘している。</p>
<h2>複合的な概念も制御可能</h2>
<p>ネオロジズム学習は、単純な行動特性だけでなく、複数の概念を組み合わせた複合的制御にも対応する。
たとえば「短く・数値を含む・高確率」といった3つの条件を、それぞれに対応する造語を同時に指定することで達成できるという。</p>
<p>研究では、「短文」「誤答」「お世辞」「拒否」など7種類の単純概念や、言語的特徴を扱うベンチマークAxBenchにおいても、高い制御性能が確認された。</p>
<h2>AIの「内なる言葉」への道</h2>
<p>著者のひとりであるジョン・ヒューイット（John Hewitt）氏は、
「私たちは既存の語彙だけではAIの内部概念を十分に理解できない」と述べ、造語学習をその橋渡しと位置づけている。</p>
<p>研究は、AIの制御可能性（controllability）と説明可能性（explainability）を同時に高める新しい方向性を示すものであり、
将来的にはAI間通信や人間との協調学習に応用できる可能性があるとみられる。</p>
]]></description>
      <pubDate>Tue, 14 Oct 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google、AIがPCを操作する「Gemini 2.5 Computer Use model」を開発者向けに公開──ClaudeやOpenAIモデルを上回る性能を実証</title>
      <link>https://ledge.ai/articles/google_gemini_2_5_computer_use_release</link>
      <description><![CDATA[<p>Google DeepMind は2025年10月7日（米国時間）、AI が実際のコンピューター画面を理解し、クリックや入力などの操作を実行できる新モデル「Gemini 2.5 Computer Use model」を開発者向けにプレビュー提供したと<a href="https://blog.google/technology/google-deepmind/gemini-computer-use-model/">発表</a>した。</p>
<p>Gemini API を通じて利用でき、AI が人間と同様にブラウザやアプリのUI（ユーザーインターフェース）を操作することを可能にする。</p>
<h2>Gemini API に“computer_use”ツールを追加</h2>
<p>今回発表された新モデルは、Gemini 2.5 の機能拡張として API に追加された「computer_use」ツールを用いて動作する。</p>
<p>AI はユーザーからの指示に加え、スクリーンショットと直近の操作履歴を入力として受け取り、次に取るべきアクション（クリック・入力・スクロールなど）を出力。実行結果を再び画面キャプチャとして取得し、目標達成までループ処理を行う。これにより、設定変更やフォーム入力、情報検索など、複数ステップを自律的に完了できる。</p>
<p>Google は公式ブログで、「このモデルはユーザー許可を前提に、安全性と透明性を重視して設計されている」と強調している。</p>
<p><strong>Computer Use model の処理ループ。AI がスクリーンショットと操作履歴をもとに次の行動を生成し、クライアント環境で実行 → 状況を再取得して次の判断へとつなげる</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/CTU_Diagram_RD_4_V01_width_1000_format_webp_b3415f41ee/CTU_Diagram_RD_4_V01_width_1000_format_webp_b3415f41ee.webp" alt="CTU-Diagram-RD4-V01.width-1000.format-webp.webp" /></p>
<h2>プレビュー提供と利用方法</h2>
<p>開発者は Google AI Studio および Vertex AI を通じて Computer Use model にアクセスできる。プレビュー版の段階では主にブラウザ操作に最適化されており、今後はより広範なアプリやデスクトップ環境への対応も検討されているという。</p>
<p>Google は、操作範囲やデータアクセスを制御する仕組みを組み込み、「責任ある自動化（Responsible Automation）」の実現を掲げている。</p>
<h2>ベンチマーク性能：Claude Sonnet 4.5 を上回る</h2>
<p>Google DeepMind は、Gemini 2.5 Computer Use model の性能を複数の標準ベンチマークで検証した。
Browserbase による Online-Mind2Web テストでは 65.7 % の精度を記録し、Claude Sonnet 4.5 や OpenAI Computer-Using Model を上回った。
さらに WebVoyager や AndroidWorld でも高スコアを達成し、実行速度（レイテンシ）でも優位性を示している。</p>
<p><strong>Gemini 2.5 Computer Use model は、Claude Sonnet 4.5 や OpenAI Computer-Using Model に比べ、低レイテンシかつ高精度を示した。</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/CTU_Scatterplot_RD_7_width_1000_format_webp_7e4e545c1c/CTU_Scatterplot_RD_7_width_1000_format_webp_7e4e545c1c.webp" alt="CTU-Scatterplot-RD7.width-1000.format-webp.webp" /></p>
<p><strong>複数ベンチマークで高い精度を記録。特に WebVoyager と AndroidWorld で際立ったスコアを達成した。</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/CTU_Benchmark_Chart_RD_5_V01_width_1000_format_webp_e0982edd33/CTU_Benchmark_Chart_RD_5_V01_width_1000_format_webp_e0982edd33.webp" alt="CTU-Benchmark_Chart-RD5_V01.width-1000.format-webp.webp" /></p>
<h2>動作デモ：AI がブラウザを自律操作</h2>
<p>公式ブログでは、実際の操作デモ動画も公開されている。
動画では AI が画面を認識し、ブラウザ上でリンクをクリックしたり、テキストを入力してタスクを完了する様子が確認できる。</p>
<p>@<a href="https://www.youtube.com/watch?v=_lu-FcPUIfM">YouTube</a></p>
<h2>AI による“手の届く自動化”へ</h2>
<p>今回の発表は、AI が人間の指示をもとに実際のUI を操作できる「エージェント時代」の幕開けを示す。
Google は Computer Use を “次世代の AI アシスタント” 開発の基盤と位置づけており、将来的には業務支援やウェブ操作、アプリ間連携など、より幅広い自動化領域への展開が期待される。</p>
]]></description>
      <pubDate>Tue, 14 Oct 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google、企業向けAIの“入口”「Gemini Enterprise」発表──従業員全員にGoogle AIを届ける統合プラットフォーム</title>
      <link>https://ledge.ai/articles/google_gemini_enterprise_ai_platform_launch</link>
      <description><![CDATA[<p>Googleは米国時間2025年10月9日、企業向けの新しいAIプラットフォーム「Gemini Enterprise（ジェミニ・エンタープライズ）」を<a href="https://blog.google/products/google-cloud/gemini-enterprise-sundar-pichai/">発表</a>した。
職場におけるAI活用の“入口（front door）”として位置づけられ、従業員全員が直感的なチャットインターフェースを通じて、GoogleのAI機能を安全かつ統合的に利用できる環境を提供する。</p>
<h2>部門単位のAIから、全社横断のAIへ</h2>
<p>Googleは発表の冒頭で、「AIは働き方、ビジネス運営、顧客体験のすべてを変革する歴史的機会をもたらす」としながらも、従来のAI活用が部門ごとに孤立していた課題を指摘した。Gemini Enterpriseは、この分断を解消し、ワークフローと従業員をつなぐ包括的なAIプラットフォームとして開発された。</p>
<p>Google Cloudによると、同社の強みは「信頼性の高いAIインフラ」「Google DeepMindによる先駆的研究」「多用途なGeminiモデルファミリー」の3層構造にある。このフルスタックAI戦略が、企業におけるAI変革を支える中核基盤になるという。</p>
<h2>6つの中核コンポーネントを統合</h2>
<p>Gemini Enterpriseは、以下の6つの要素を単一のインターフェースで統合する：</p>
<ul>
<li><strong>最新のGeminiモデル</strong> ：あらゆる業務タスクの“頭脳”として、高度な推論と生成を実現。</li>
<li><strong>ノーコード・ワークベンチ</strong> ：非エンジニアでもデータ分析やエージェント連携が可能。</li>
<li><strong>事前構築エージェント</strong> ：リサーチやデータインサイトなどの専門業務に即対応。</li>
<li><strong>企業データとの安全な接続</strong> ：Google WorkspaceやMicrosoft 365、Salesforce、SAPなどにシームレス接続。</li>
<li><strong>統合ガバナンス</strong> ：すべてのエージェントを一元的に可視化・保護・監査。</li>
<li><strong>オープンなエコシステム</strong> ：10万社を超えるパートナーによる拡張性を確保。</li>
</ul>
<p>Googleはこれにより、「単一タスクの効率化を超え、ワークフロー全体を自動化する」としている。</p>
<h2>業務アプリとの統合と新機能</h2>
<p>Gemini EnterpriseはGoogle Workspaceとも密接に連携する。
テキスト・画像・動画・音声を理解・生成できる初のマルチモーダルエージェントが導入され、文書作成や会議運営などの作業を支援。
「Google Vids」によるAI生成動画の作成機能や、「Google Meet」でのリアルタイム音声翻訳機能も提供される。
後者は発話者のトーンや表現を反映し、自然な多言語コミュニケーションを可能にするという。</p>
<p>さらに、「データサイエンスエージェント（プレビュー）」が発表された。
データ取り込みから探索、モデルトレーニングの自動化までを担い、VodafoneやWalmartなどの企業が既に活用している。</p>
<h2>導入事例の拡大と実用成果</h2>
<p>Googleは、Banco BV、Klarna、Mercedes-Benz、Swarovskiなどの導入事例を紹介した。
Banco BVでは、分析作業の自動化により、マネージャーが新規ビジネス開拓に注力できるようになった。Mercedes-BenzはGeminiを用いてドライバーと自然な会話ができる自動車内AIアシスタントを構築している。</p>
<p>@<a href="https://www.youtube.com/watch?v=ijqTReRzG8M&amp;t=26s">YouTube</a></p>
<p>日本企業では、メルカリがGoogle AIをコールセンターに導入。
AI主導のカスタマーサービス体験を実現し、業務量を20％削減、ROI（投資収益率）を500％向上させる見込みとしている。</p>
<h2>開発者×エージェント経済──A2AとAP2で拡張</h2>
<p>Gemini Enterpriseは企業だけでなく、開発者にも開かれたプラットフォームとして進化する。
すでに100万人以上が利用する「Gemini CLI」に加え、AIをコマンドラインから拡張できる「Gemini CLI Extensions」を導入。
さらに、開発者やISV（独立系ソフトウェアベンダー）がエージェントを構築・販売・収益化できる「エージェントエコノミー」の構想を発表した。</p>
<p>この仕組みを支えるのが、</p>
<ul>
<li><strong>Agent2Agent Protocol（A2A）</strong> ：エージェント間通信を標準化する新プロトコル</li>
<li><strong>Agent Payments Protocol（AP2）</strong> ：エージェントによる安全な金融取引を実現する決済標準
の2つである。AP2は、American Express、Mastercard、PayPalなど100社超のパートナーと共同で策定された。</li>
</ul>
<h2>学習・導入支援プログラム</h2>
<p>Googleは、AI人材育成と現場導入支援の両面から変革を後押しする。
新たに全従業員が無料でAIスキルを学べる「Google Skills」を開設し、開発者向けの教育プログラム「Gemini Enterprise Agent Ready（GEAR）」を開始した。
さらに、顧客企業に伴走して導入支援を行うAIエンジニアチーム「Team Delta」も新設している。</p>
<p>@<a href="https://www.youtube.com/watch?v=Qbix0BOPcgE">YouTube</a></p>
<h2>AIの“入口”としての意義</h2>
<p>Googleは、AIの未来を「オープンで協力的なエコシステム」にあるとし、Box、Workday、ServiceNowなど主要企業との連携を拡大している。クロスプラットフォームでのワークフロー連携や導入支援、検証済みエージェントの発見、収益化の仕組みなど、10万社超のパートナー基盤を活用し、AI導入を全層で支援する方針だ。</p>
<p>\u003E「Gemini Enterpriseは、Google AIの最高の機能をすべての従業員とワークフローに届ける“職場AIの新たな入口”です」（Google Cloud公式ブログより）</p>
<p>Googleは、企業にとってのAI導入を「一部の実験」から「全員の変革」へと引き上げる転換点として、この新プラットフォームを位置づけている。</p>
]]></description>
      <pubDate>Tue, 14 Oct 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Huawei、LLMの精度を保持したまま最大70%メモリ削減できる新手法を発表──コンシューマーGPUでの高精度生成AI実行も視野に</title>
      <link>https://ledge.ai/articles/huawei_sinq_quantization_llm</link>
      <description><![CDATA[<p>中国の大手テクノロジー企業Huawei（華為技術）は2025年9月26日、大規模言語モデル（LLM）を一般的なGPU環境でも高品質に動作させるための新しい量子化技術「Sinkhorn-Normalized Quantization（SINQ）」を<a href="https://www.arxiv.org/abs/2509.22944">発表</a>した。</p>
<h2>Sinkhorn正規化で“再調整なし”を実現</h2>
<p>従来のLLM量子化では、精度を維持するために一部データを用いて再調整（キャリブレーション）を行う必要があった。SINQはその工程を省略し、「再調整なし」で精度を保つ新しい方式だ。</p>
<p>仕組みの中核となるのが、「Sinkhorn-Knoppアルゴリズム」を応用した正規化手法である。モデルの重み行列に対して、行方向と列方向の2つのスケーリングベクトルを設定（dual-scaling）し、両軸の分散を均一化することで、外れ値（outlier）が特定の行や列に偏る問題を防ぐ。この工程により、量子化後の誤差を最小限に抑えられるという。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x1_818acd143f/x1_818acd143f.png" alt="x1.png" /></p>
<h2>精度を保ちながら70％のメモリ削減</h2>
<p>Huaweiの研究チームは、同社のQwen3モデル（1.7B〜235B）やDeepSeek-V2.5（236B）などで実験を行い、SINQが既存手法（RTN、HQQ、Hadamard変換など）を上回る精度を示したと報告している。</p>
<p>具体的には、4bit量子化時にパープレキシティ（文章予測精度）を最大40％改善し、メモリ使用量を最大70％削減。さらに、8GB程度の一般的なGPU上でQwen3-7Bモデルを実行できたとしている。処理時間も高速で、量子化プロセスは従来のRTN法に比べてわずか1.1倍。再調整を伴う手法（AWQやGPTQなど）よりも最大30倍速いという。</p>
<h2>幅広いモデルで動作、非一様量子化とも互換</h2>
<p>SINQは、Qwenシリーズだけでなく、Llama 2・Llama 3・DeepSeek-V3などの異なるモデルにも適用可能。
また、非一様量子化フォーマット（NF4）との併用でも精度を維持しており、調整を行うAWQと組み合わせた「A-SINQ」ではさらに高い性能を達成した。論文では、Mixture-of-Experts（MoE）構造の大型モデルでも安定して動作することが示されている。</p>
<h2>コンシューマーGPUでのLLM実行を視野に</h2>
<p>Huaweiは、SINQを「キャリブレーション不要の汎用量子化手法」と位置づけており、高性能GPUに依存しないLLM運用を可能にする技術として注目されている。論文著者らは、SINQの目的を「メモリ効率と速度を両立し、エッジデバイスでも高品質な生成AIを実行可能にすること」と説明している。</p>
<p>コードはGitHub上で<a href="https://github.com/huawei-csl/SINQ">公開</a>されており、研究者や開発者が自由に評価・応用できる環境が整っている。</p>
]]></description>
      <pubDate>Tue, 14 Oct 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>アルトマン氏「エロティックばかり注目されたけど」──ChatGPT、成人ユーザーの自由拡大へ</title>
      <link>https://ledge.ai/articles/openai_chatgpt_adult_mode_update_oct2025</link>
      <description><![CDATA[<p>OpenAIのサム・アルトマンCEOは10月14日（現地時間）、X（旧Twitter）上で、ChatGPTの安全制限を一部緩和し、成人認証済みユーザーに対してエロティックな会話を許可する方針を<a href="https://x.com/sama/status/1978129344598827128">発表</a>
した。</p>
<p>投稿は瞬く間に注目を集め、「エロティック解禁」が大きな話題となったが、アルトマン氏は翌日に「その部分ばかり注目されてしまったが」と<a href="https://x.com/sama/status/1978539332215681076">補足</a>し、実際には“より人間らしいAI体験”を実現するための包括的な方針変更であることを強調した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/gpt5_erotica_7cea863241/gpt5_erotica_7cea863241.jpg" alt="gpt5 erotica.jpg" /></p>
<h2>安全を優先してきたChatGPTの制限</h2>
<p>ChatGPTはこれまで、性的表現や親密な会話を含むコンテンツを厳しく制限してきた。
アルトマン氏は「メンタルヘルス問題に慎重を期すためだった」と説明し、精神的に不安定なユーザーに配慮した措置であったと振り返った。
「深刻な危機状態にあるユーザーは別扱いとし、他者に害を与える行為は依然として許可しない」と述べ、ポリシーの根幹は維持されるとしたうえで、「リスクのない成人ユーザーにはより多くの自由を与える」と明言した。</p>
<h2>“4oらしさ”を再導入──人間的なAI体験へ</h2>
<p>アルトマン氏は同日、「数週間以内に“4oで好まれた振る舞い”に近い人格（パーソナリティ）を選べる新バージョンのChatGPTを提供する」と投稿した。
GPT-4oは会話の自然さや表情豊かな応答で人気を集めたモデルであり、今後はユーザーが望む場合に、フレンドリーな口調や絵文字を多用した“人間らしい”対話スタイルを選べるようになる。
アルトマン氏は「これは利用時間を増やすためではなく、ユーザーが自分の望む形でAIと関わる自由を得るための設計だ」と述べている。</p>
<h2>「成人は大人として扱う」──12月に年齢認証を本格導入</h2>
<p>アルトマン氏は、12月に年齢認証を本格導入し、認証済み成人ユーザーに対してはエロティック会話なども許可する方針を示した。
一方で、ティーンエイジャーに対しては「安全をプライバシーや自由より優先する」と述べ、メンタルヘルス関連ポリシーは緩めないと強調している。</p>
<p>アルトマン氏は、「社会がR指定映画で境界を設けるように、AIにも適切な年齢境界を設けたい」と例え、「我々は選挙で選ばれた道徳警察ではない」と付け加えた。</p>
<h2>倫理と自由の境界線</h2>
<p>アルトマン氏の発言は、AIにどこまで人間的な自由を与えるかという議論を再燃させた。</p>
<p>OpenAIは今後、成人向け表現やAIの人格設計に関するガイドラインをさらに明確化するとみられる。今回の方針転換は、「AIをどう設計し、どう育てるか」という人間社会全体のテーマに踏み込む第一歩となりそうだ。</p>
]]></description>
      <pubDate>Tue, 14 Oct 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>世界初のAI女優「ティリー・ノーウッド」にハリウッドが揺れる──SAG-AFTRAや著名俳優が「創造性の危機」と警告</title>
      <link>https://ledge.ai/articles/tilly_norwood_ai_actress_controversy_sagaftra</link>
      <description><![CDATA[<p>世界初の「AI女優」と称される Tilly Norwood（ティリー・ノーウッド）が、ハリウッドの俳優や映画俳優組合SAG-AFTRA（全米映画俳優組合・テレビ・ラジオ芸術家連盟）から強い批判を受けている。</p>
<p>SAG-AFTRAは2025年9月30日に「創造性は人間中心であるべき」と声明を<a href="https://www.sagaftra.org/sag-aftra-statement-synthetic-performer">発表</a>した。女優のエミリー・ブラント氏やウーピー・ゴールドバーグ氏も相次いで懸念を表明し、映画・テレビ業界全体に議論が広がっている。</p>
<h2>「AI Commissioner」──ティリー誕生の舞台</h2>
<p>Tilly Norwoodは、ロンドン拠点の制作会社Particle6が2025年9月にYouTube上で公開したコメディスケッチ『AI Commissioner | Comedy Sketch | Particle6』で初登場した。</p>
<p>この作品は、テレビ業界におけるAIの急速な普及を風刺的に描いたもので、AIが脚本作成からキャスティング、予算編成まですべてを自動化する世界を描く。劇中では、AIが生み出したインタラクティブスリラー『I Know What You Streamed Last Summer』に登場する100％AI生成の俳優としてTillyが紹介される。</p>
<p>@<a href="https://www.youtube.com/watch?v=3sVO_j4czYs">YouTube</a></p>
<p>登場人物の1人はTillyについて「僕の言うことを何でも聞いてくれる。恋をしてしまったかもしれない（She’ll do anything I say. I think I’m in love）」と評し、AIによる従順で“最適化された”俳優像を象徴的に表現。
さらに、「“She’ll cry on Graham Norton and be monetized on TikTok by lunchtime.”（彼女はグレアム・ノートンの番組で泣いて、その日の昼までにTikTokで収益化されるだろう）」という台詞が、人間の感情までもがAIによって即座に商業化される未来への皮肉として話題を呼んだ。</p>
<h2>SAG-AFTRAが声明「Tillyは俳優ではない」</h2>
<p>SAG-AFTRAは9月30日に「Statement on Synthetic Performer（合成パフォーマーに関する声明）」を<a href="https://www.sagaftra.org/sag-aftra-statement-synthetic-performer">発表</a>し、次のように明言した。</p>
<p>\u003E“Tilly Norwood is not an actor. Creativity must remain human-centered.”
（ティリー・ノーウッドは俳優ではない。創造性は人間中心であるべきだ。）</p>
<p>声明では、AIによって作られた “合成俳優” が芸術表現を侵食する可能性を指摘し、「経験や感情を持たない存在を“俳優”と呼ぶことは、芸術の根幹を損なう」と警鐘を鳴らした。SAG-AFTRA会長のショーン・アスティン氏もVarietyの取材に対し、「AI倫理と補償問題を正式な交渉テーマとして扱う」と述べている。</p>
<h2>著名俳優からの反発</h2>
<p>女優のエミリー・ブラント氏は、Varietyのポッドキャスト番組でTillyの画像を見せられ、驚きを隠さずこう語った。</p>
<p>\u003E“Good Lord, we’re screwed. That is really, really scary. Come on, agencies, don’t do that. Please stop taking away our human connection.”
「なんてこと、私たちは終わりね。本当に恐ろしいわ。お願い、エージェントはそんなことをやめて。人間のつながりを奪わないで。」</p>
<p>また、取材でTillyを「次のスカーレット・ヨハンソンに」と問われると、「“But we have Scarlett Johansson.”（でも私たちにはスカーレット・ヨハンソンがいる」 と返し、人間俳優の価値を強調した。</p>
<p>俳優・司会者のウーピー・ゴールドバーグ氏も、ABCのトーク番組『The View』（9月30日放送）で次のように発言した。</p>
<p>\u003E“You’re looking at 5,000 actors rolled into one synthetic person. That’s not fair. We all move differently.”
「5,000人の俳優の特性をひとつの合成存在にまとめるなんてフェアじゃない。私たちは皆、動きも表情も違う。」</p>
<h2>開発側の見解：「人間の代替ではなく、芸術表現」</h2>
<p>Tillyを制作したエライン・ファン・デル・フェルデン氏（Eline Van der Velden）は9月28日、ティリーのInstagram（<a href="https://www.instagram.com/tillynorwood/">@tillynorwood</a>）に投稿し、AI俳優の創作意図について次のように述べている。</p>
<p>\u003E “She is not a replacement for a human being, but a creative work — a piece of art.”
「ティリーは人間の代替ではなく、創造的な作品＝ひとつのアートです。」</p>
<p>同氏は、AIを「人間の代わり」ではなく「新しい絵筆のようなツール」と位置づけ、「アニメーションや人形劇、CGIがライブ演技を奪うことなく新しい可能性を開いたように、AIも物語を構築する新たな手段を提供する」と説明した。</p>
<p>\u003E “I’m an actor myself, and nothing — certainly not an AI character — can take away the craft or joy of human performance.”
「私は俳優でもあり、AIキャラクターであっても、人間の演技の技や喜びを奪うことはできません。」</p>
<p>投稿ではさらに、AIを“人間と競わせる存在”ではなく“芸術の新しいジャンルの一部”として評価すべきだと訴えている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/instagram_eline_at_tillynorwood_b80dee2451/instagram_eline_at_tillynorwood_b80dee2451.jpg" alt="instagram eline at tillynorwood.jpg" /></p>
<p>同氏が率いるParticle6は「AI俳優を活用すれば制作コストを最大90％削減できる」と説明し、複数のAIタレントを育てる「デジタル・タレント・ユニバース」構想を掲げている。</p>
<h2>今後の焦点</h2>
<p>各報道や関係者の声明からは、主に次の3点が論点として浮上している。</p>
<ul>
<li>著作権と肖像権：AI俳優の訓練データやモデル構築に使用された素材の扱い</li>
<li>契約・補償問題：AI使用を前提とした俳優契約の新たな枠組み</li>
<li>文化的受容：AIが「演技」を行うことを人々がどう受け入れるか</li>
</ul>
<p>SAG-AFTRAをはじめとする業界団体や俳優たちは、AIが創作活動や雇用に及ぼす影響を注視しており、今後は法制度や契約の整備を含めた議論が進む見通しだ。</p>
]]></description>
      <pubDate>Tue, 14 Oct 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>清華大学、AIエージェントが仮想都市で経済活動を行う「SimCity」開発──フィリップス曲線やエンゲルの法則を自律再現</title>
      <link>https://ledge.ai/articles/tsinghua_simcity_ai_urban_economy</link>
      <description><![CDATA[<p>清華大学の研究チームは、複数のAIエージェントが仮想都市内で経済活動を行うシミュレーションシステム「SimCity」を開発した。研究成果は2025年10月1日付で論文「SimCity: Multi-Agent Urban Development Simulation with Rich Interactions」（arXiv:2510.01297）として<a href="https://arxiv.org/abs/2510.01297">公開</a>した。</p>
<p>SimCityは、家庭・企業・政府・中央銀行という4種類のエージェントを大規模言語モデル（LLM）で駆動し、都市の発展やマクロ経済の動きを自律的に再現する。エージェントは自然言語による推論を行い、労働市場・財市場・金融市場で相互に作用。人間のように意思決定を行いながら、都市経済の変動を模倣することができるという。
なお、論文中にはその記載はないが、同名の商用ゲームシリーズとは無関係と見られる。</p>
<h2>4種類のエージェントがつくる「AI経済」</h2>
<p>SimCityには、以下の4つの主要な役割を担うエージェントが登場する。</p>
<ul>
<li><strong>家計（Households）</strong> ：消費、就業、住宅選択、貯蓄・投資を決定する。</li>
<li><strong>企業（Firms）</strong> ：生産や雇用、価格設定、設備投資を行う。</li>
<li><strong>政府（Government）</strong> ：所得税や付加価値税（VAT）の徴収、公共支出や福祉政策を担当する。</li>
<li><strong>中央銀行（Central Bank）</strong> ：インフレ率やGDPの動向に応じて、修正版テイラー・ルールに基づき政策金利を設定する。</li>
</ul>
<p>また、視覚・言語モデル（VLM）が企業の立地や都市構造を決定。住宅地や工業地帯が自然に分かれるなど、都市の空間構成も自律的に形成される。</p>
<h2>経済法則を自然に再現</h2>
<p>研究チームは、44種類の財と最大200世帯から成る仮想経済を構築し、180ステップ（約15年）にわたるシミュレーションを実施した。前半36カ月を「移住フェーズ」、後半144カ月を「発展フェーズ」として進行した。</p>
<p>その結果、SimCityは以下のような実経済の特徴（stylized facts）を再現できたという。</p>
<ul>
<li><strong>フィリップス曲線</strong> ：インフレ率と失業率の逆相関</li>
<li><strong>オークンの法則</strong> ：失業率の変化とGDP成長率の負の関係</li>
<li><strong>ベバリッジ曲線</strong> ：求人率と失業率の逆相関</li>
<li><strong>エンゲルの法則</strong> ：所得の上昇に伴い食費の割合が低下</li>
<li><strong>需要の価格弾力性</strong> ：必需品は非弾力的、贅沢品は弾力的</li>
<li><strong>投資の高ボラティリティ</strong> ：消費よりも投資が景気に敏感</li>
</ul>
<p>これらは実際の経済データ（FRED, 1970Q1–）との比較でも一致傾向を示し、従来のエージェント・ベース・モデル（ABM）では再現が難しかった現象を多く含むと報告されている。</p>
<h2>都市発展と価格ショックへの応答</h2>
<p>SimCityは「移住フェーズ（36カ月）」と「発展フェーズ（144カ月）」の2段階で進行する。
初期段階では移住者の流入によりGDPが上昇し、住宅と生産拠点の配置が形成される。VLMの判断により、住宅は都市中心部に、工場は周辺部に集中するなど、現実的なゾーニング構造が自律的に現れた。</p>
<p>さらに研究チームは、一部の財の価格を50％変動させる「価格インパルス実験」を実施。価格は短期的に大きく変動したが、数年のうちに均衡へ回帰。実際の経済で観察される「価格の粘着性（menu cost）」と同様の挙動が確認された。</p>
<h2>「AIがつくる社会をAIで観察」</h2>
<p>研究では、OpenAIの「GPT-4o-mini」を中心に、Azure OpenAI APIを通じてLLMを運用。各エージェントは独立して推論し、シミュレーション1ステップあたり約0.25ドルのコストで実行された。
総トークン数は約80万で、全体の実行コストは約180ドル。</p>
<p>研究チームは、SimCityを「AIがつくる社会をAIで観察するための基盤」と位置づけており、都市計画や経済政策の設計、AI社会の倫理実験などへの応用を見込んでいる。今後は、金融市場や株式取引などの要素を導入し、より現実的な経済表現へ拡張する予定だ。</p>
]]></description>
      <pubDate>Tue, 14 Oct 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>政府、OpenAIに著作権侵害防止を要請──「Sora 2」問題で平デジタル相は“オプトイン方式”を提言</title>
      <link>https://ledge.ai/articles/openai_sora2_government_copyright_request_oct2025</link>
      <description><![CDATA[<p>OpenAIの動画生成AI「Sora 2」による日本のアニメ作品に酷似した映像がSNS上で拡散している問題を受け、政府が対応に乗り出した。城内実内閣府特命担当大臣（知的財産戦略・クールジャパン戦略担当）は10月10日の<a href="https://www.gov-online.go.jp/press_conferences/minister_of_state/202510/video-303104.html">記者会見</a>で、OpenAIに対し著作権侵害となる行為を行わないよう要請したと明らかにした。</p>
<p>城内大臣は「アニメや漫画は世界の人々を魅了し続ける、我が国が世界に誇る宝」と述べ、知的財産権の保護を重視する姿勢を強調。要請は内閣府の知的財産戦略推進事務局からオンラインで直接行われたという。記者質問の内容から、実施時期は10月上旬で、Sora 2による“酷似動画”が相次いだ直後とみられる。</p>
<p>一方、平将明デジタル大臣は10月12日、TBS番組でAIの学習段階における権利処理の在り方について言及した。「OpenAIには、きちんと権利処理をしていただくようお願いしている」と述べたうえで、「AIの学習データについても、事前の同意を得るオプトイン方式が望ましい」と発言。AI事業者に対し、無断利用ではなく同意制に基づくデータ利用の仕組みを導入するよう求めた。</p>
<p>これに先立つ10月3日、OpenAIのCEOであるサム・アルトマン氏は、動画生成AI「Sora 2」に関連する著作権保護と収益分配制度に関する方針をブログで明らかにしていた。同氏は「試行錯誤を重ねながら早期に開始する」と述べ、経済的な利益と新しい関係構築の双方を実現したい考えを示している。</p>
<p>政府は今後もAI事業者に対し、著作権および文化的資産の保護を重視した対応を求める方針を示している。AIによる創作支援が拡大するなかで、学習データの扱いと権利保護の両立が国際的な課題となりつつある。</p>
]]></description>
      <pubDate>Mon, 13 Oct 2025 02:50:00 GMT</pubDate>
    </item>
  </channel>
</rss>