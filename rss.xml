<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
  <channel>
    <title>Ledge.ai 新着記事</title>
    <link>https://ledge.ai/</link>
    <description>Ledge.ai の最新テクノロジー記事</description>
    <item>
      <title>Hugging Face CEO、「コードを書くようにLLMも自前で訓練すべき」と訴え――Nanotron公開で“企業製AI”の時代が現実味</title>
      <link>https://ledge.ai/articles/custom_llm_huggingface_nanotron</link>
      <description><![CDATA[<p>Hugging FaceのCEOであるClément Delangue氏は2025年8月4日、自身のX（旧 Twitter）に<a href="https://x.com/ClementDelangue/status/1952048356710039700">投稿</a>し、「すべてのテクノロジー企業は、Deepseek R1  Llama、GPT-5といった 独自の大規模言語モデル（LLM）を訓練でき、かつ訓練すべきだ」と呼びかけた。</p>
<p>投稿には「Every tech company can and should train their own Deepseek R1, Llama or GPT5, just like every tech company writes their own code.」という一文が添えられており、ソフトウェア開発の延長としてモデル開発を捉えるべきだとのメッセージが込められている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/clement_delangue_32591c644f/clement_delangue_32591c644f.jpg" alt="clement delangue.jpg" /></p>
<h2>専用LLM開発のハードルはすでに下がった</h2>
<p>Delangue 氏が “自社訓練” を促す背景には、近年のコスト低減とオープンソース化の進展がある。たとえば 2024 年末に公開された Deepseek R1 は推定 550 万ドルで訓練されたとされ、同規模モデルとしては低コストの事例となった。さらに Meta の Llama 3、そして OpenAI が次期フラッグシップとして準備を進める GPT-5 も、フォークや追加学習を前提に採用されるケースが増えている。
クラウド型 GPU クラスタや公開データセットが急速に整い、かつては巨額の資金を要した LLM 訓練が中堅企業でも射程に入る環境が整いつつある。</p>
<h2>Nanotronが後押しする“作れる AI”</h2>
<p>Hugging Face は7月30日、分散学習フレームワーク 「Nanotron」 と、その運用ノウハウをまとめた 「Ultra-Scale Playbook」 をProプラン加入者向けに無料提供 すると発表した。Delangue氏は X で「Free for @huggingface pro users」と明言しており、同社の有償サブスクリプション（個人25ドル／月〜）に登録している開発者が追加コストなく利用できる仕組みだ。</p>
<p>Playbook には</p>
<ul>
<li>100億パラメータ超モデル を訓練するステップバイステップ手順</li>
<li>GPU クラスタ最適化や障害対応などの実務ノウハウ</li>
<li>Databricks や Lambda Labs など外部インフラとの接続例
が収録されており、研究機関・スタートアップが“自社LLM”に踏み出す際のガイドとして機能するという。</li>
</ul>
<h2>企業が自前モデルに踏み切る理由</h2>
<p>Delangue氏の主張は、すでに複数の産業分野で進む「自社専用LLM」開発の潮流と一致する。とくに以下のようなニーズに対応する目的で、LLMを自社開発・内製化する動きがみられる。</p>
<ul>
<li><strong>機密データの保持</strong> ：顧客情報や製造ノウハウなど、外部クラウドに預けられないデータを安全に活用可能</li>
<li><strong>推論コストの削減</strong> ：自社ホスティングにより、API利用料の継続的負担を回避</li>
<li><strong>法令・ガバナンスへの準拠</strong> ：業界ごとのコンプライアンス要件に合わせたモデル調整が容易
他方で、GPU 調達競争や高品質データの確保、人材不足といった課題は依然として残る。</li>
</ul>
<p>Delangue氏の発言は、AI業界が「買うAI（API利用）」から「作るAI（内製モデル）」へのシフトを迎えているという見方を反映している。今後は、独自モデルをいかに迅速に開発し運用できるか が企業競争力の重要指標となりそうだ。</p>
]]></description>
      <pubDate>Thu, 07 Aug 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/8/7 [THU]ChatGPT、週次利用者7億人へ──3月から2億人増、1年で4倍に</title>
      <link>https://ledge.ai/articles/chatgpt_weekly_users_700_million</link>
      <description><![CDATA[<p>2025年8月4日、米OpenAIは、対話型AI「ChatGPT」の週間アクティブユーザー（WAU）が今週中にも世界で7億人を超える見通しであることを明らかにした。これは、同社プロダクト責任者ニック・ターレイ氏がX（旧Twitter）上に<a href="https://x.com/nickaturley/status/1952385556664520875">投稿</a>した内容および関連報道に基づく。3月末時点でのWAUは5億人であり、約4カ月で2億人の増加となる。2024年時点と比較すると、年間で約4倍の成長となる。</p>
<h2>新機能と法人利用の拡大</h2>
<p>急成長の要因として、2025年3月に追加されたGPT-4ベースの高精度画像生成機能や、会話の文脈を継続して記憶できる「メモリ機能」など、ユーザーの実用性を高めるアップデートが影響しているとみられる。また、法人向けのChatGPT Enterpriseプラン契約社数も増加傾向にあり、2025年6月時点で500万社に達したことが報告されている。これは3月時点から200万社の増加に相当する。</p>
<p>アプリ市場調査会社Sensor Towerによると、ChatGPTの1ユーザーあたりの平均利用頻度は月12日以上、1日の平均利用時間は約16分とされている。</p>
<h2>年間売上は20億ドル超　評価額は5,000億ドル視野に</h2>
<p><a href="https://www.reuters.com/business/openai-eyes-500-billion-valuation-potential-employee-share-sale-source-says-2025-08-06/">ロイター</a>の報道によると、OpenAIの収益性も急速に高まっており、ChatGPT関連事業を含めた年間売上は20億ドル（約2,800億円）を超えるとの市場推計が出ている。また、社員保有株の売却計画に関連して、同社の企業評価額は5,000億ドル（約70兆円）規模に達する可能性があると報じられている。</p>
<h2>競合との競り合いが激化　GoogleやAnthropicも拡大路線へ</h2>
<p>業界全体としては、Googleの「Gemini」アプリが月間4億5,000万MAU（Monthly Active Users）を維持しており、OpenAIと並んで高い利用者数を記録している。また、Anthropicが開発するClaudeなどの他の大手モデルもユーザー獲得を進めており、生成AI市場におけるシェア争いが一層激化している。</p>
<h2>今後の展開：ポケット内デバイスで稼働する超高性能AIを示唆</h2>
<p>OpenAIは次世代モデル「GPT-5」を開発中と報じられており、2025年8月中にも発表される可能性が指摘されている。あわせて、ChatGPTには生活支援やメンタルヘルスなどウェルネス領域の機能強化が計画されているとの情報もある。
8月6日には、同社のサム・アルトマン最高経営責任者（CEO）がXで次のように投稿した。</p>
<p>someday soon something smarter than the smartest person you know will be running on a device in your pocket, helping you with whatever you want.</p>
<p>this is a very remarkable thing.</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/remarkable_thing_sama_gpt5_30a5cb690c/remarkable_thing_sama_gpt5_30a5cb690c.jpg" alt="remarkable thing sama gpt5.jpg" /></p>
<p>アルトマン氏は、ポケット内のデバイス上で“最も賢い人物より賢い”AIが近い将来稼働するとの見通しを示した。</p>
]]></description>
      <pubDate>Thu, 07 Aug 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI が“OSS 戦線”に参入：推論最適化モデル『gpt-oss』、Hugging Face と GitHub で即日配布</title>
      <link>https://ledge.ai/articles/openai_gpt_oss_open_weight_release</link>
      <description><![CDATA[<p>OpenAIは米国時間の2025年8月5日、推論最適化されたオープンウェイト言語モデル<a href="https://openai.com/open-models/">「gpt-oss-120b」と「gpt-oss-20b」</a>を発表し、同日中にHugging Face、GitHub、AWS上で<a href="https://openai.com/index/introducing-gpt-oss/">一般公開</a>した。</p>
<p>両モデルはApache 2.0ライセンスの下で提供され、商用利用や改変、再配布が自由に許可されている。o4-mini相当の性能を備えながらも、80GB GPUまたは一般的な16GBメモリのローカル環境での動作を想定しており、開発者や研究者が安全性と性能を両立したLLMを自前のインフラ上で運用できることを目指すとしている。</p>
<h2>ローカル運用を前提としたMoE構造、2モデル同時公開</h2>
<p>gpt-oss-120bは、36層・128エキスパート構成のMixture-of-Experts（MoE）アーキテクチャを採用しており、推論時には同時に4つのエキスパートのみが活性化される設計となっている。これにより、アクティブなパラメータは約33Bに抑えられ、メモリ効率を向上させた。gpt-oss-20bも同様の構成で、こちらは32エキスパート・24層構造で、一般的な16GBメモリのPC環境でも実行可能とされている。</p>
<p>両モデルは最大128kトークンの長文コンテキストに対応し、推論モードも3段階（low、medium、high）で調整可能。すべてのChain-of-Thought（CoT）出力を開示することで、透明性と制御性を両立している。</p>
<h2>o4-miniと同等の性能、ヘルスケア分野では上回る評価も</h2>
<p>OpenAIによると、gpt-oss-120bはMMLU、HellaSwag、AI2 Reasoning Challenge（ARC）、AIME2025といった代表的なベンチマークにおいて、ChatGPT搭載モデルであるo4-miniと同等の推論性能を発揮している。特に、ヘルスケア分野の評価セットであるHealthBenchなどでは一部で上回る結果も見られたという。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/gpt_oss_model_performance_a5920564a4/gpt_oss_model_performance_a5920564a4.jpg" alt="gpt oss model performance.jpg" /></p>
<p>軽量版の20bモデルも、同社のo3-miniより高い精度を示すタスクが複数確認されており、軽量ながらも高性能な選択肢として注目される。</p>
<h2>Apache 2.0 ライセンスでの公開と配布形態</h2>
<p>今回のgpt-ossは、Apache 2.0ライセンスの下で公開されている。これにより、商用利用や改変、再配布が広く許容され、企業や研究者にとって導入ハードルが大きく下がる。</p>
<p>モデルは<a href="https://openai.com/index/gpt-oss-model-card/">モデルカード</a>、GitHubのリポジトリ、Dockerコンテナ、Ollamaレシピなどを通じて提供されており、AWS BedrockやSageMakerなどとも統合されている。インフラ制約のある現場でも、即座に利用できる設計となっている。</p>
<h2>Preparedness Framework に基づく安全性評価と外部レビュー</h2>
<p>OpenAIは、本モデルをリリースするにあたり、自社の「Preparedness Framework」に基づく<a href="https://ledge.ai/articles/openai_preparedness_safety-advisory-group">安全性評価</a>を実施したと説明している。悪意ある利用を想定したadversarial fine-tuningや外部専門家によるリスクレビューを経たうえで、「生物学的合成、サイバー攻撃、化学兵器の設計といった高リスク能力には到達していない」と判断し、公開に踏み切った。</p>
<p>また、全ての推論に対するChain-of-Thought（思考過程）の開示によって、モデルの透明性と制御性を向上させており、開発者に対しても「責任ある利用と検証」を促している。</p>
<h2>企業・研究機関での先行導入とOSSコミュニティへの期待</h2>
<p>gpt-ossの公開に伴い、AI Sweden、Orange、Snowflakeなどの企業・研究機関がすでに導入を開始しており、リアルタイムデータ処理や独自評価基盤の構築などでの活用が進んでいる。OpenAIは今後、開発者からのフィードバックを集め、次世代モデルの設計や安全性強化に役立てる方針も示している。</p>
<p>このリリースは、Metaの「Llama 3」やMistral、DeepSeekといった他のOSSモデル群に対抗する形で、OpenAIが再び“開放”に舵を切った動きと位置付けられる。これにより、生成AIの開発エコシステムにおける「クローズド vs オープン」の構図にも変化が生じる可能性がある。</p>
<h2>Altman氏「これは世界で最も使いやすいオープンモデル」</h2>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/gpt_oss_sama_a2095daa89/gpt_oss_sama_a2095daa89.png" alt="gpt oss sama.png" /></p>
<p>OpenAIのサム・アルトマンCEOは同日のX投稿で「gpt-ossは最先端のオープンウェイト推論モデルであり、o4-miniに匹敵する実性能をローカル環境（小型版はスマートフォンでも動作）で発揮できる。世界で最も使いやすいオープンモデルだと信じている」と述べた。この発言は、“閉鎖”中心と見られてきたOpenAIが本格的にOSS領域へ踏み込む姿勢を示すものであり、開発者コミュニティに向けた強いメッセージとなっている。</p>
]]></description>
      <pubDate>Wed, 06 Aug 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>三菱UFJ銀行、200超の業務スキルを持つ自律型AIエージェント導入──Salesforce 「Agentforce for Financial Services」 日本初採用</title>
      <link>https://ledge.ai/articles/mufg_salesforce_ai_agentforce_launch</link>
      <description><![CDATA[<p>三菱UFJ銀行は8月1日、金融業界向けに特化した自律型AIエージェント「Agentforce for Financial Services」を導入すると<a href="https://www.salesforce.com/jp/news/press-releases/2025/08/01/mufg-customer-news-2/">発表</a>した。</p>
<p>提供元はセールスフォース・ジャパンで、同製品の日本国内における初の採用事例となる。同銀行は、CRM基盤である「Financial Services Cloud（FSC）」上にAIを組み込み、面談前後の情報提示や業務フォローアップを自動化することで、営業支援と顧客体験の質を向上させる狙いだ。</p>
<h2>金融機関向けに事前学習済みのAIエージェント</h2>
<p>「Agentforce for Financial Services」は、金融業務に特化して設計されたAIエージェントで、あらかじめ200種類を超える営業支援スキルを搭載している。面談の準備段階では顧客ごとのニーズや関心に関するインサイトを提示し、面談中には質問への応答や関連情報の提供、面談後には自動でタスクを生成・管理するなど、営業プロセス全体を自律的に支援する機能を持つ。</p>
<h2>SalesforceとMUFG、FSCを基盤に連携深化</h2>
<p>三菱UFJ銀行は2025年4月から、セールスフォースの金融特化型CRMである「Financial Services Cloud」を営業現場に導入しており、営業担当者が顧客情報を即時に把握できる体制を構築してきた。今回のAIエージェント導入は、その次のステップとして、FSCに蓄積された過去の営業履歴や顧客データを即座に活用できるAI基盤の整備を意味する。</p>
<h2>営業現場の業務をAIが代替、提案の質とスピードを両立</h2>
<p>Agentforceによって営業担当者は、より付加価値の高い活動に集中できるようになる。AIが面談準備からフォローアップまでの一連の業務を補完・代替することで、業務負荷を軽減すると同時に、提案の質とスピードを向上させることが可能となる。</p>
<h2>導入企業・提供企業のコメント</h2>
<p>三菱UFJ銀行の武井優・上席調査役は、「今回のAgentforce導入を通じて、営業支援と顧客体験の質が大きく向上すると期待している」とコメントしている。また、セールスフォース・ジャパンの田村英則・専務執行役員は、「金融業界で即時に活用可能なデジタル労働力として、Agentforceを提供し続ける」と述べている。</p>
<h2>今後の展開</h2>
<p>三菱UFJ銀行は今後、AIエージェントの適用領域を段階的に拡大し、FSCを基盤とした営業支援インフラの整備をさらに進める方針だ。一方、セールスフォースは専門チームによる導入支援とともに、他の金融機関への横展開を視野に、Agentforceの国内展開を強化していくとしている。</p>
]]></description>
      <pubDate>Wed, 06 Aug 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>LLMO対策でAI検索最適化──GMO NIKKO、「GMO AI最適化ブースト」提供開始</title>
      <link>https://ledge.ai/articles/llmo_optimization_gmo_ai_boost</link>
      <description><![CDATA[<p>GMO NIKKOは2025年8月1日、生成AIによる検索結果で企業の公式情報が引用・表示されやすくなるよう支援する新サービス「GMO AI最適化ブースト」の提供を開始したことを<a href="https://www.koukoku.jp/release/250801/">発表</a>した。</p>
<p>サービスは、LLMO（Large Language Model Optimization）に対応し、ChatGPT、Gemini、Google AI Overviewsなどの主要生成AIを対象に、情報構造やコンテンツを最適化する。GMO NIKKOは、生成AI分析ツールを提供するAI Hackと業務提携し、企業サイトの調査から改善までをワンストップで支援する体制を整えたという。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/gmo_llmo_65892443be/gmo_llmo_65892443be.jpg" alt="gmo llmo.jpg" /></p>
<h2>AI主導の「ゼロクリック検索」への対応が急務に</h2>
<p>生成AIの普及により、検索ユーザーがAIの回答のみで情報収集を完結させる「ゼロクリック検索」が拡大している。この動きにより、従来のSEO施策だけでは企業の公式情報が検索経路から外れやすくなる傾向が強まっている。</p>
<p>調査会社Gartnerは「2026年までに従来型の検索トラフィックが25％減少する」と予測しており、生成AIに最適化された新たな広報・情報発信戦略が求められている。</p>
<h2>AIに“引用される”情報構造を構築</h2>
<p>「GMO AI最適化ブースト」では、以下の支援を通じて生成AIに引用されやすい情報環境の構築を目指す。</p>
<ul>
<li><strong>可視性診断</strong> ：ChatGPTやGemini、Google AI Overviewsなどにおける自社情報の掲載状況を5つの指標でスコア化。</li>
<li><strong>競合との比較分析</strong> ：他社との引用・露出の差分を可視化し、優先的に改善すべき領域を特定。</li>
<li><strong>技術的最適化支援</strong> ：schema.orgによる構造化、FAQ整備、llms.txt設定などでAIが情報を正確に解釈できる構造を整備。</li>
<li><strong>生成AI向けコンテンツ制作</strong> ：AIが引用しやすい形式でFAQ、用語集、専門記事などを制作。</li>
<li><strong>サイテーション強化</strong> ：信頼性向上のため、外部メディアや権威サイトでの第三者言及を促進。</li>
<li><strong>継続的な可視化レポート</strong> ：AIによる引用状況を月次でレポートし、改善提案を実施。</li>
</ul>
<p>これらの施策は、AI Hackの分析ツールと50項目を超える独自チェックリストをもとに実行される。</p>
<h2>今後の展望</h2>
<p>同サービスはGMO NIKKOが直接提供し、料金は個別見積もり方式（2025年8月時点で価格は非公開）で対応する。</p>
<p>同社は今後、「GMO AI最適化ブースト」を生成AI時代における新たな広報施策の業界標準として展開していく方針だ。あわせて、レピュテーション管理や危機対応などを含む周辺ソリューションの拡充にも取り組むとしている。</p>
]]></description>
      <pubDate>Wed, 06 Aug 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/8/5 [TUE]Anthropicが企業向けLLMで OpenAI を逆転──Menlo 北米での調査、32 %シェアで首位に</title>
      <link>https://ledge.ai/articles/anthropic_tops_enterprise_llm_market_2025_midyear</link>
      <description><![CDATA[<p>米ベンチャーキャピタルのMenlo Venturesは2025年7月31日、企業における大規模言語モデル（LLM）の導入実態を調査したレポート「2025 Mid-Year LLM Market Update」を<a href="https://menlovc.com/perspective/2025-mid-year-llm-market-update/">発表</a>した。調査によると、AnthropicのClaudeシリーズが企業利用において32％のシェアを獲得し、OpenAI（25％）を抜いて首位に立った。調査対象は北米を中心とした150社以上の技術責任者で、実運用中のAPIに基づいた数値だという。</p>
<h2>企業向けLLM市場、Anthropicが32％で首位に</h2>
<p>企業が本番環境で利用しているLLMプロバイダーのシェアは以下の通り</p>
<ul>
<li>Anthropic（Claude）：32％</li>
<li>OpenAI（GPT）：25％</li>
<li>Google（Gemini）：20％</li>
<li>Meta（Llama）：9％</li>
<li>DeepSeek：1％
2023年末の同様の調査ではOpenAIが50％以上の圧倒的シェアを持っていたが、半年間で大幅に後退し、代わってAnthropicが首位に立った。</li>
</ul>
<h2>Claudeシリーズ躍進の要因はコード生成とエージェント適性</h2>
<p>Anthropicが企業利用で支持を拡大した要因として、Menloは次の2点を挙げている。</p>
<ul>
<li><strong>コード生成性能の優位性</strong> ：開発者の支持はAnthropicが42％で、OpenAIの21％を大きく上回った。特にClaude 3.5 Sonnet以降、ソースコード生成やレビュー精度に関する評価が高まっている。</li>
<li><strong>エージェント化への対応</strong> ：Claude 3.5 Sonnetや今後のClaude 4系において、検証付き強化学習（RLVR）を用いたタスク実行能力の強化が進められ、社内業務の一部自動化に成功した企業が増加しているという。</li>
</ul>
<h2>LLM支出は半年で倍増、閉鎖型モデルが依然優位</h2>
<p>2025年上半期における企業のLLM支出は、前年同期の3.5億ドルから8.4億ドルに倍増した。主な要因は、PoC（概念実証）から本番環境への移行が加速したことによる。</p>
<p>また、依然としてクローズドソースのプロプライエタリモデル（Anthropic、OpenAI、Googleなど）が主流である。Menloはその理由として、パフォーマンス面の安定性と導入容易性を挙げている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/3_closed_source_vs_open_source_scaled_e220aacfdc/3_closed_source_vs_open_source_scaled_e220aacfdc.webp" alt="3-closed-source_vs_open-source-scaled.webp" /></p>
<h2>選定基準は「精度」「セキュリティ」「コスト」「拡張性」</h2>
<p>企業がLLMを選定する際の重視点として、Menloは以下の4点を挙げている。</p>
<ul>
<li>回答品質と精度</li>
<li>データプライバシーおよびセキュリティ対応</li>
<li>推論コスト</li>
<li>APIやRAGなどの拡張性
特にセキュリティ基準が厳格な業種（金融、医療、官公庁など）では、個別カスタマイズや内部検証が可能なプロバイダーが優先される傾向が強い。</li>
</ul>
<h2>オープンソース採用は伸び悩み、Meta Llamaが9％で最多</h2>
<p>一方、オープンソース系のLLMは全体の13％にとどまっており、MetaのLlamaがそのうちの9％を占める。導入には大規模なリソースが必要なうえ、中国製モデルに対するリスク懸念もあり、採用が限定的となっている。</p>
<h2>今後の注目点</h2>
<p>Menloはレポートの中で、今後の市場動向として以下の点に注目している。</p>
<ul>
<li>Anthropicの“エージェント・ファースト”戦略がどの程度実運用に耐えうるか</li>
<li>OpenAIやGoogleの巻き返し策（API料金改定、マルチモーダル強化など）の効果</li>
<li>複数LLMの組み合わせによるハイブリッド運用の広がり</li>
</ul>
<p>企業向けLLM市場は急拡大を続けており、今後数四半期でシェア構造がさらに変化する可能性があるとしている。</p>
<p>:::box
[関連記事：Anthropic、AIの\</p>
]]></description>
      <pubDate>Tue, 05 Aug 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Autopilot死亡事故で2億4,300万ドル賠償──Teslaは控訴表明もXで「手動運転は過去の遺物」と意に介さず</title>
      <link>https://ledge.ai/articles/tesla_autopilot_liability_verdict_2025</link>
      <description><![CDATA[<p>2025年8月1日、フロリダ州マイアミの連邦地裁で、Teslaの運転支援機能「Autopilot」に関連する2019年の死亡事故をめぐる民事訴訟の評決が下された。陪審団は、事故の責任の一部がTeslaにあると判断し、同社に対し2億4,300万ドル（約360億円）の損害賠償金の支払いを命じた。このニュースは、<a href="https://www.nbcnews.com/news/us-news/tesla-autopilot-crash-trial-verdict-partly-liable-rcna222344">NBC</a>など複数の米メディアが同日報じた。一方でTeslaは、同日公式X（旧Twitter）アカウントにて「手動運転は過去の遺物」とも取れる投稿を行い、判決と対照的なスタンスを示した。</p>
<h2>フロリダ州での事故、Autopilot作動中に歩道の人を死亡させる</h2>
<p>事故が発生したのは2019年4月25日、フロリダ州デルトラビーチ。Tesla Model Sが赤信号の交差点に進入し、路肩に停車していたSUVに衝突。衝突の影響で歩道にいた22歳のナイベル・ベナビデス・レオン氏が死亡し、同乗していた男性も重傷を負った。</p>
<p>裁判では、事故当時にAutopilotが作動していたこと、本来は高速道路向けに設計された同機能が信号や交差点のある一般道路でも使用可能だった点、さらにドライバーの注意散漫時に作動する警告機能の有効性などが争点となった。</p>
<p>複数の報道によれば、陪審はドライバーに67％、Teslaに33％の過失があると認定。損害賠償金は補償的損害1億2,900万ドル、懲罰的損害2億ドルで構成されており、懲罰的損害の全額がTeslaに対して課された。</p>
<h2>控訴の方針とSNS投稿の温度差</h2>
<p>報道によれば、Teslaはこの判決について「事実に反する」として控訴する方針を示している。</p>
<p>判決が報じられた日、同社は公式Xアカウントに
“In the future, people will find it wild that manual driving was a daily task, not a weekend thrill”（訳：将来の人々にとって、手動運転が毎日の作業だったなんて信じられないことになるだろう）と<a href="https://x.com/Tesla/status/1952102455405171036">投稿</a>。</p>
<p>この投稿には、歩行補助器を使う高齢女性が「昔の人は、重さ2トンもの“走る凶器”を時速70マイルで、疲れていようが気が散っていようが――もっとひどい状態でも――毎日のように運転していたのよ」と話しているのに対し、付き添いの若い女性が「Sure grandma, let’s get you to bed（はいはいおばあちゃん、もう寝ようね）」と声をかけるインターネット・ミーム画像が添付されている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/tesla_x_2be13c4150/tesla_x_2be13c4150.jpg" alt="tesla x.jpg" /></p>
<h2>今後の法的・技術的影響</h2>
<p>TeslaがAutopilotに関連して重大な損害賠償責任を問われたのは今回が初めてとされる。これにより、米国家運輸安全委員会（NTSB）や米運輸省道路交通安全局（NHTSA）が進める調査や、運転支援機能に関する規制・基準の見直しが加速する可能性がある。</p>
<p>Teslaは引き続き完全自動運転の実現を目指しているが、今回の評決は、その進展に対して法的・社会的な検証と制約が強まることを示す一例となった。</p>
]]></description>
      <pubDate>Tue, 05 Aug 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>直感で解くAI──Google、Gemini「Deep Think」を月額36,800円のUltra向けに提供開始</title>
      <link>https://ledge.ai/articles/gemini_deep_think_ultra_launch</link>
      <description><![CDATA[<p>Googleは2025年8月1日、生成AI「Gemini」シリーズのアプリにおいて、新たな推論モード「Deep Think」の提供を開始したことを<a href="https://blog.google/products/gemini/gemini-2-5-deep-think/">発表</a>した。この機能は、同社の最上位サブスクリプションプラン「Google AI Ultra」（月額36,800円、英語版のみ）に限定されており、人間の直感的な問題解決力を模倣するよう設計された強化推論アルゴリズムを搭載しているという。Deep Thinkは、複雑な数学的課題や高度なコーディング問題などに対し、従来よりも長い推論時間と強化学習技術を用いて、多角的かつ創造的な解を導き出すことを可能にする。</p>
<p>@<a href="https://www.youtube.com/watch?v=QoXRfTb7ves&amp;t=2s">YouTube</a></p>
<h2>Deep Thinkとは</h2>
<p>Deep Thinkは、Gemini 2.5モデルの高度な推論モードとして設計され、以下の特性を持つ：</p>
<ul>
<li>一つの問題に対して複数の仮説を同時に生成・検証する「並列思考」アプローチを採用</li>
<li>通常の推論時間（数秒）を拡張し、数十秒に及ぶ処理時間を許容することで、より複雑な解答を可能にする</li>
<li>解答の精度を逐次改善するために、強化学習（Reinforcement Learning）を活用</li>
<li>GeminiのWeb版およびiOS/Androidアプリで利用可能（英語版に限定）</li>
</ul>
<p><strong>同一のプロンプトに基づき生成されたボクセルアートの比較</strong> ：左からGemini 2.5 Flash、2.5 Pro、2.5 Deep Thinkの出力結果。Deep Thinkでは、構造の複雑さ、色彩の多様性、空間的レイアウトにおいて、明らかな飛躍が確認できる
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/2_5_deep_think_blog_image_pagoda_width_1000_format_webp_596a05a56a/2_5_deep_think_blog_image_pagoda_width_1000_format_webp_596a05a56a.webp" alt="2-5-deep-think_blog-image_pagoda.width-1000.format-webp.webp" /></p>
<p>同社は、Deep Thinkの導入により単なる事実の再構成では対応できない高度な問題や創造的タスクにおいて、AIの実用性が高まると説明している。</p>
<p>拡張推論にかかる計算資源と処理時間のコストを考慮し、Deep Thinkは、Google AIの最上位プラン「Ultra」利用者のみに提供される。同社は今後の利用状況と安定性を確認したうえで、他プランへの展開を検討するとしている。</p>
<ul>
<li>Ultraプラン：月額36,800円（米国では\$249）、個人および法人利用者向けに設計</li>
<li>Gemini Advancedで提供されていた機能を包含し、Gemini 1.5 Ultraモデルに基づく</li>
</ul>
<h2>ベンチマークと技術的特徴</h2>
<p>Googleは、Deep Think搭載モデルがいくつかの技術的ベンチマークにおいて高い性能を示したと公表している。</p>
<ul>
<li>2025年版IMO（国際数学オリンピック）模擬試験でブロンズ相当の成績</li>
<li>ソフトウェア開発の性能評価であるLiveCodeBench V6において上位3位に入る</li>
<li>ベータ版時と比較して推論速度を約20％短縮し、解答の一貫性も向上</li>
</ul>
<p>技術面では、強化学習（RLHF）に加え、AIフィードバック（RLAIF）による報酬設計を組み合わせたアプローチが用いられており、長時間かけて「考える」プロセスが正式に製品仕様として取り入れられた。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/all_benchmarks_blog_width_1000_format_webp_a61e103f81/all_benchmarks_blog_width_1000_format_webp_a61e103f81.webp" alt="all_benchmarks_blog.width-1000.format-webp.webp" /></p>
<h2>競合との位置づけ</h2>
<p>Deep Thinkのアプローチは、OpenAIのGPT-4oにおける「System Thinking」や、AnthropicのClaude Maxが持つ「Reflection」モードと類似した領域に位置づけられる。</p>
<ul>
<li>Googleは「多ステップ問題解決において最大30％の精度向上が見られた」と発表</li>
<li>マルチモーダル機能（画像や音声による入力）は現時点でベータ扱いであり、今後のアップデートで強化される見込み</li>
</ul>
<h2>セキュリティとガバナンス対策</h2>
<p>Googleは、出力の長文化に伴うリスクへの対応として、安全性・倫理面の強化にも取り組んでいるという。</p>
<ul>
<li>社内AI Red Teamおよび外部リサーチパートナーによる事前評価を実施</li>
<li>出力監視フィルタの調整により、有害コンテンツのリスクを低減</li>
</ul>
<h2>今後の展望</h2>
<p>GoogleのGeminiチームは公式ブログにて、「Deep ThinkはGeminiの“思考能力”を次のレベルへ引き上げる取り組みの第一歩だ」と述べている。特に、長時間の推論を活用した複雑な意思決定や創造的な発想が求められる分野での応用に期待を寄せており、「今後さらに思考時間と演算資源を増やすことで、より高精度なAIモデルを段階的に提供していく方針」としている。</p>
<p>また、開発者向けAPIの公開や、Deep Thinkモードの多言語対応、下位プランへの展開についても、「安定運用が確認でき次第、対象を広げていく」と説明している。2025年内に開催予定の「Gemini Dev Day」では、さらなる詳細と開発ロードマップが公開される見通しである。</p>
]]></description>
      <pubDate>Tue, 05 Aug 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>テキストから”超リアルな写真っぽい画像”を生成　Black Forest Labs、オープンライセンスの新モデル「FLUX.1 Krea [dev]」で挑む“ポスト AI ルック”</title>
      <link>https://ledge.ai/articles/flux1_krea_open_image_model_release</link>
      <description><![CDATA[<p>Stable Diffusion　のオリジナル開発陣が設立したBlack Forest Labs（BFL）は2025年7月31日、テキストから写真のようなリアルな画像を生成できる最新の画像生成モデル「FLUX.1 Krea [dev]」を<a href="https://bfl.ai/announcements/flux-1-krea-dev">発表・公開</a>した。</p>
<p>モデルは12B（120億）パラメーターのRectified Flow Transformerを採用し、Krea AIとの共同開発によって、従来の「AIっぽさ」を意図的に排除したビジュアル表現を可能にしている。非商用用途での利用に限り、オープンウェイト形式で配布されており、Hugging Faceからダウンロードできる。</p>
<h2>“意見を持つ”画像生成モデル</h2>
<p>FLUX.1 Krea [dev]は、Krea AIが開発した高精細な実写風モデル「Krea 1」をベースに再訓練されており、BFL独自の審美的判断を加味した“opinionated（意見を持つ）”生成スタイルが特徴。過度な彩度や人工的な質感、非現実的な肌の表現といった“AIルック”を避け、現実に即した写実性を重視する仕上がりとなっている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/447cb391bb54864ff4c9e478d311fd1ad2badd21_3072x2560_a7bc5402d7/447cb391bb54864ff4c9e478d311fd1ad2badd21_3072x2560_a7bc5402d7.jpg" alt="447cb391bb54864ff4c9e478d311fd1ad2badd21-3072x2560.jpg" /></p>
<h2>主な特徴と技術仕様</h2>
<ul>
<li>モデルサイズ：12BパラメーターのRectified Flow Transformerを採用し、従来の拡散モデルと比較して収束速度と品質を両立。</li>
<li>出力解像度：標準で1024×1024ピクセルの画像を生成可能。今後はアップスケーリングにも対応予定。</li>
<li>プロンプト追従性：Krea 1の特徴を維持しつつ、ガイダンス蒸留によりより正確なプロンプト適合を実現。</li>
<li>対応環境：ComfyUI、Diffusers、Together AI、Replicateなど、主要なUIやAPIに対応。</li>
</ul>
<p><strong>Black Forest Labs 各モデルの ELO レーティング比較</strong> ：最新モデル 「FLUX.1 Krea [dev]」は 1,011 ポイントを記録し、商用向けの「FLUX.1 [pro] v1.1」に迫る性能を示した
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/63a9e1e322f8db4e5af_85409d34bd/63a9e1e322f8db4e5af_85409d34bd.jpg" alt="63a9e1e322f8db4e5af.jpg" /></p>
<h2>オープンウェイトでの提供</h2>
<p>FLUX.1 Krea [dev]は、非商用利用に限り自由に使用可能なライセンス（v1.1.2）で提供されている。モデルデータはHugging Face上で公開されており、研究者・開発者・クリエイターはローカル環境やクラウド上での導入が可能。商用利用を希望する場合は、別途Black Forest Labsとのライセンス契約が必要となる。</p>
<h2>安全性と倫理対策</h2>
<p>BFLは、安全性確保のために以下の対策を講じている。</p>
<ul>
<li>IWF（Internet Watch Foundation）との連携によるCSAM（児童性的虐待コンテンツ）およびNCII（非同意性的画像）対策。</li>
<li>内部および外部によるアドバーサリアル評価テストを通過し、公開済みの他のオープン画像モデルよりも安全性が高いと判断。</li>
<li>オープン配布にあたって、使用時のフィルタリングや手動レビューをユーザーに義務づけ。</li>
</ul>
<h2>今後の展開</h2>
<p>BFLは今後、FLUX.1 Krea [dev]を軸としたシリーズの展開を予定しており、高解像度対応モデル「FLUX Ultra」や、テキスト＋画像のマルチモーダル入力に対応する「FLUX.1 Kontext」などの開発にも取り組んでいる。また、さまざまなAIスタックとの連携により、リアルなビジュアル生成のオープンスタンダードを確立していく方針だ。</p>
]]></description>
      <pubDate>Tue, 05 Aug 2025 01:30:00 GMT</pubDate>
    </item>
    <item>
      <title>Google、検索「AIモード」に画像・PDF質問や学習計画機能を追加──“新学期シーズン”に向けてアップデート</title>
      <link>https://ledge.ai/articles/google_ai_mode_back_to_school_update</link>
      <description><![CDATA[<p>Googleは2025年7月29日（米国時間）、検索ページ上の「AIモード（AI Overviews）」に新たな機能を追加したと<a href="https://blog.google/products/search/ai-mode-updates-back-to-school/">発表</a>した。この機能は2025年2月から米国を中心にテスト提供されているもので、今回のアップデートは「Back-to-School（新学期）シーズン」に向けたものであると説明している。新機能として、画像やPDFに対する質問機能や、学習計画を視覚的に整理できる「Canvas」パネル、カメラを用いたリアルタイム質問機能「Search Live」などが実装され、主に学生や教育関係者向けの検索活用を想定している。</p>
<h2>新たに実装された主な機能</h2>
<h3>画像・PDF質問対応</h3>
<p>デスクトップ環境でも画像をアップロードして質問できるようになった。例えば、問題や講義スライドの画像をアップし、内容について質問することが可能。さらに、数週間以内にはPDFファイルのアップロードにも対応予定としており、教材の理解支援に利用できるという。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/summarise_these_files_0574e0e343/summarise_these_files_0574e0e343.jpg" alt="summarise these files.jpg" /></p>
<h3>Canvas（キャンバス）機能</h3>
<p>学習計画や旅行準備などを整理するための新パネル「Canvas」がAIモードに追加された。複数セッションを跨いで関連情報を保持し、生成AIによる要約・構造化を行うことが可能である。また、ファイルのアップロードと連携することで、パーソナライズされた学習ガイドの生成にも対応する予定だという。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/ask_ai_mode_canvas_18b41d8f1a/ask_ai_mode_canvas_18b41d8f1a.jpg" alt="ask ai mode canvas.jpg" /></p>
<h3>Search Live（リアルタイムカメラ質問）</h3>
<p>Google Lensと連携し、スマートフォンのカメラを使ってその場の映像に基づいて質問する機能「Search Live」が試験導入された。たとえば、手元の理科の実験器具やグラフをカメラで映し、それについての解説をその場でAIから得ることができる。</p>
<p>@<a href="https://www.youtube.com/watch?v=OnwKhkVrvnE&amp;t=5s">YouTube</a></p>
<h3>Webページに対する検索支援強化</h3>
<p>近日中に、Chromeのアドレスバーをクリックすると、ドロップダウンの候補に「このページについてGoogleに質問する（Ask Google about this page）」という新しいオプションが表示されるようになり、そのままAIモードを通じて深掘り検索が行えるという。同社は、これが、ChromeでGoogleレンズを使って検索するもう一つの方法となると説明している。</p>
<h2>提供範囲と利用条件</h2>
<p>現在これらの機能は、AIモード対応地域である米国およびインドの英語版ユーザーに向けて提供されている。また、「Canvas」および「Search Live」機能については、「Search Generative Experience（SGE）」の拡張機能として、Google Labs登録ユーザーから順次展開される形式をとっている。</p>
<h2>背景：AIモードの進化</h2>
<p>AIモードは、Google検索に生成AIを統合した「AI Overviews」として2024年に発表され、2025年2月からはAI Modeとしてラボ機能としてのテストが始まっていた。5月には米国内で正式に提供開始され、Gemini 2.5ベースのカスタムモデルが搭載されている。今回のアップデートは、その進化の一環として、教育利用のシナリオに焦点を当てたものとされている。</p>
<p>Googleは本機能群について、「学習から日常的な調査、買い物支援に至るまで、検索体験をエンドツーエンドでサポートするもの」と位置づけており、今後はGoogle Driveとの連携拡張なども視野に入れているという。</p>
]]></description>
      <pubDate>Mon, 04 Aug 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>【ソースコード特典付き】MCP × ローカルLLMで作る次世代AIエージェント｜Ledge.ai Webinar</title>
      <link>https://ledge.ai/articles/webinar-vol68</link>
      <description><![CDATA[<h2>ウェビナー概要</h2>
<p>Ledge.ai Webinar vol.68では、「MCP×ローカルLLMによるAIエージェント構築」をテーマに、GPUクラウドサービス「GPUSOROBAN」を提供する株式会社ハイレゾ様をゲストにお迎えし、実演デモを交えながらご解説いただきます。</p>
<p>LLMを外部のツールやデータソースと連携させるための標準プロトコルとして注目を集める「MCP（Model Context Protocol）」。
まるで様々なデバイスを接続できるUSB-Cポートのように、異なるLLMやツールをスムーズに繋ぐことから「AIのUSB-Cポート」とも呼ばれています。</p>
<p>このMCPを活用することで、これまで個別の実装が必要だったツール連携の複雑さ（M×N問題）を解消し、より効率的に高度なAIエージェントを構築することが可能になります。</p>
<p>今回のウェビナーでは、MCPの基本概念から、MCPとローカルLLMと組み合わせたAIエージェントの具体的な実装方法まで、デモンストレーションを交えて分かりやすく解説します。</p>
<p><strong>ウェビナーの内容</strong></p>
<ul>
<li>MCP（Model Context Protocol）の概要とAIエージェント開発における重要性</li>
<li>複数のMCPツール（Web検索、RAG、データベース、ファイルシステム）と連携するAIエージェントの実装デモ</li>
<li>ローカル環境で実行可能なオープンソースLLMを用いたAIエージェントの構築方法</li>
</ul>
<p><strong>このような方におすすめ</strong></p>
<ul>
<li>AIエージェント開発の最新動向を知りたい方</li>
<li>MCP（Model Context Protocol）の概念や実装方法に興味がある方</li>
<li>ローカルLLMを活用してMCPツールと連携させる方法を知りたい方</li>
<li>GPUリソースを効率的に活用しながらAI開発を進めたい方</li>
</ul>
<h3><strong>視聴者特典</strong></h3>
<p><strong>【特典①】デモで使用したサンプルコードをプレゼント！</strong>
本ウェビナーのアンケートにご回答いただいた方全員に、デモで使用したソースコードをプレゼントいたします。視聴後すぐに、ご自身の環境で再現・検証が可能です。</p>
<p><strong>【特典②】H200 GPU 30日間無料トライアルキャンペーン！</strong>
さらに、ハイレゾでは現在、「NVIDIA H200」を搭載したGPUクラウドサービス「AIスパコンクラウド」を30日間無料でお試しいただける特別なキャンペーンも実施中です。
（詳細はウェビナー内およびアンケート回答後のご案内をご確認ください。）</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/h200_gpu_free_trial1200_bd3d66cf05/h200_gpu_free_trial1200_bd3d66cf05.jpg" alt="h200-gpu-free-trial1200.jpg" /></p>
<h2>登壇者情報</h2>
<p>株式会社ハイレゾ
マーケティング部　グループ長
山田 岳史</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/highreso_yamadasama_2a984e3aa6/highreso_yamadasama_2a984e3aa6.jpg" alt="highreso-yamadasama.jpg" /></p>
<p>IoTの領域で事業開発の経験を経てハイレゾに入社。
GPUクラウドサービスのマーケティング担当。</p>
<h2>お申し込みはこちら</h2>
<p>配信期間：2025年8月4日(月)〜2025年8月25日(月)
配信方式：オンデマンド（Zoom）
参加費：無料</p>
<p>:::button
<a href="https://zfrmz.com/4XTvrjWktQ90sj9eW7xE">ウェビナーの視聴はこちら</a>
:::</p>
]]></description>
      <pubDate>Mon, 04 Aug 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Adobe、Photoshop βを全面刷新：生成AI「Firefly」駆動の合成・高解像度化・共同作業ツールを一挙投入</title>
      <link>https://ledge.ai/articles/adobe_photoshop_beta_firefly_upscale_collab_update</link>
      <description><![CDATA[<p>Adobeは2025年7月29日（現地時間）、画像編集ソフト「Photoshop」のβ版（デスクトップ／Web／モバイル向け）に、AI生成技術「Adobe Firefly」を活用した5つの新機能を追加したと<a href="https://blog.adobe.com/en/publish/2025/07/29/powerful-new-photoshop-innovations-creators-creative-pros">発表</a>した。これにより、合成画像の自然な一体化、高解像度への自動変換、不要物の除去精度向上、複数人による編集プロジェクトの一元管理、AIモデル選択による生成画像の調整が可能となる。新機能は同日から順次、βチャンネルのユーザー向けに提供が開始された。</p>
<h3>Harmonize（β）</h3>
<p>合成時に発生する「浮いた印象」を回避するため、AIが光源、色調、影などの視覚要素を解析し、自動的に合成対象のレイヤーに調和処理を行う。Adobeが2024年のAdobe MAXで発表していた研究開発プロジェクト「Project Perfect Blend」の成果をベースにしている。</p>
<h3>Generative Upscale（β）</h3>
<p>画像の高解像度化を行う生成AI機能で、最大8メガピクセルまで拡張可能。既存の拡大処理では発生しやすかったディテールの劣化を抑え、SNS向けのリサイズや印刷物制作への活用が想定されている。</p>
<p><strong>■ Generative Upscale適用前（左）と適用後（右）の比較</strong> ：ディテールが強化され、ノイズが大幅に低減されている
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/media_12ff7544ace3898ddf1bdc5f9ee1db53083203e6a_fc8147feb6/media_12ff7544ace3898ddf1bdc5f9ee1db53083203e6a_fc8147feb6.webp" alt="media_12ff7544ace3898ddf1bdc5f9ee1db53083203e6a.webp" /></p>
<h3>Removeツールの強化</h3>
<p>不要物の除去に使用されるRemoveツールがFirefly Image Model 3にアップグレードされ、背景生成の自然さとアーティファクト（不自然な痕跡）の低減が図られた。小物の除去から人物の除去まで幅広い用途に対応する。</p>
<p><strong>■ Removeツールの処理例。スケートパークの背景から複数の人物を自然に除去している</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/media_1a8c2968a877a9bd91827b2a3b5e7822e2e70fc0f_24dbe3458c/media_1a8c2968a877a9bd91827b2a3b5e7822e2e70fc0f_24dbe3458c.webp" alt="media_1a8c2968a877a9bd91827b2a3b5e7822e2e70fc0f.webp" /></p>
<h3>Projects（β）</h3>
<p>複数人での編集やレビューを想定した共有スペース。PSDファイルや関連画像、コメントなどを一元的に管理でき、チーム内外とのリアルタイムな協働を可能にする。Web版Photoshop上でリンクを介してアクセスでき、ファイルの重複やバージョン違いによる混乱を防ぐ。</p>
<h3>Generative AI Model Picker</h3>
<p>Generative FillおよびExpand使用時に、使用するFireflyモデル（Image 1／Image 3）を選択できるようになった。これにより、スタイルや出力品質に応じたAIモデルの切り替えが可能となる。</p>
<p>これらの新機能は、Photoshopのデスクトップ版およびWeb版のβチャンネルで即日提供されており、モバイル向けにはiOSのEarly Access版にてHarmonizeが先行導入されている。Android版については、2025年6月に公開された正式アプリへの今後の機能追加が予定されている。</p>
<p>Photoshopには2023年以降、Fireflyモデルを中核とした生成AI機能（Generative Fill、Generative Expandなど）が順次搭載されており、今回のアップデートはその延長線上にある。また、AdobeはPhotoshopの35周年を迎える本年、プロフェッショナルクリエイターの生産性向上とAI編集の即応性をより一層高める構えだ。</p>
]]></description>
      <pubDate>Mon, 04 Aug 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/8/1 [FRI]家電価格のロボ誕生　Unitree R1、87万円で歩く・走る・側転する25 kgヒューマノイド</title>
      <link>https://ledge.ai/articles/unitree_r1_5900usd_humanoid_robot</link>
      <description><![CDATA[<p>中国のロボット開発企業Unitree Roboticsは2025年7月25日、新型ヒューマノイド「Unitree R1」を<a href="https://www.unitree.com/R1">発表</a>した。価格は5,900ドル（約87万円）とされ、一般的な家庭用家電並みの価格帯に設定されている。</p>
<p>製品は全高1.21メートル、重量25キログラムの軽量ボディに26個のアクチュエーターを備え、歩行や走行、さらには側転やハンドスプリングといった高難度の動作を実現する。発表は上海で行われ、現在は主に研究機関や教育機関向けに出荷が計画されている。</p>
<p>@<a href="https://www.youtube.com/watch?v=v1Q4Su54iho">YouTube</a></p>
<h2>5,900ドルの価格戦略と市場への位置づけ</h2>
<p>R1はヒューマノイドロボットとしては異例の低価格帯となる5,900ドル（中国国内価格は39,900元）から販売される。比較対象として、Unitree自身が開発するより上位モデル「G1」は約1万6,000ドル、また他社製のBoston Dynamics「Atlas」やTesla「Optimus」といった機種は数万ドルから数十万ドルに達するとされる。R1はこれらと比べて大幅に価格を抑えており、「普及型ヒューマノイド」のカテゴリに位置付けられる。</p>
<h2>軽量設計と26軸モーションによる高い機動性</h2>
<p>R1の本体はアルミニウム合金や複合素材を用いた軽量構造で、バッテリー込みで約25キログラムに抑えられている。各部位には以下のようなアクチュエーター（自由度）が割り当てられている。</p>
<ul>
<li>両腕に計10自由度（5自由度×2）</li>
<li>両脚に計12自由度（6自由度×2）</li>
<li>腰部に2自由度</li>
<li>EDU版では頭部に追加の2自由度</li>
</ul>
<p>これにより、R1は人間に近い複雑な動作が可能となり、発表時のデモンストレーションでは、歩行・小走り・回転・ジャンプ・側転などを披露した。また、映像内ではパンチやキックなどの模擬動作も確認されており、用途の幅広さがうかがえる。</p>
<h2>センサー・通信・駆動性能</h2>
<p>R1には以下のようなセンサーおよび機能が搭載されている。</p>
<ul>
<li>ステレオ深度カメラ</li>
<li>4マイクアレイによる音声入力</li>
<li>スピーカー搭載による音声出力</li>
<li>通信機能：Wi-FiおよびBluetooth 5.2</li>
<li>バッテリー駆動時間：約1時間（稼働環境に依存）</li>
</ul>
<p>制御用CPUは標準版で8コアの一般プロセッサが搭載され、上位のEDU版ではJetson OrinベースのAIモジュール（推論性能40～100 TOPS）を採用している。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/4905f964d28c4e9cba75f2e1cf9591ba_1920x2370_df7d15caf0/4905f964d28c4e9cba75f2e1cf9591ba_1920x2370_df7d15caf0.jpg" alt="4905f964d28c4e9cba75f2e1cf9591ba_1920x2370.jpg" /></p>
<h2>教育・研究向けのEDUモデルも展開</h2>
<p>同社はR1の教育用途向けバージョン「EDU版」も同時に発表している。EDU版は以下の機能拡張を含む。</p>
<ul>
<li>AI推論性能を強化したJetson Orin搭載</li>
<li>頭部の2軸モーター追加</li>
<li>デクスターズハンド（多関節ハンド）オプション</li>
<li>保証期間は標準版8カ月に対し、EDU版は12カ月</li>
</ul>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/0705e5d2e5fd49989ca513716c9f50d3_1920x1606_28f4382138/0705e5d2e5fd49989ca513716c9f50d3_1920x1606_28f4382138.jpg" alt="0705e5d2e5fd49989ca513716c9f50d3_1920x1606.jpg" /></p>
<p>研究開発やロボットコンテスト等での利用が想定されており、実装の自由度や拡張性を重視した設計が特徴となっている。</p>
<h2>上位機種との関係と市場戦略</h2>
<p>Unitreeはすでにヒューマノイドロボット「G1」や大型モデル「H1」を展開しているが、R1はその下位に位置するエントリーモデルと見られている。これまで高額な製品が主流だったヒューマノイド市場に対し、R1は価格の障壁を取り払い、新たな顧客層（教育機関、個人開発者、小規模研究機関など）への訴求を狙う。</p>
<h2>今後の展開と業界への影響</h2>
<p>公式発表では量産時期や出荷スケジュールの詳細は明かされていないが、R1は今後の展示会や開発者向けイベントでの出展が予想されている。家庭用製品並みの価格帯と高度な運動性能を併せ持つR1の登場は、ヒューマノイドロボットの導入障壁を下げ、教育・研究・開発領域における普及を後押しする可能性がある。</p>
]]></description>
      <pubDate>Mon, 04 Aug 2025 00:50:00 GMT</pubDate>
    </item>
    <item>
      <title>AIエージェント「Rakuten AI」本格始動──まずはRakuten Link、秋には楽天市場へ</title>
      <link>https://ledge.ai/articles/ai_agent_rakuten_ai_launch</link>
      <description><![CDATA[<p>2025年7月30日、楽天グループは、パシフィコ横浜で開催中の自社イベント「Rakuten AI Optimism」において、生成AIを活用したエージェント型ツール「Rakuten AI」の本格提供を開始したと<a href="https://corp.rakuten.co.jp/news/press/2025/0730_01.html">発表</a>した。</p>
<p>同日より、楽天モバイル契約者向けの通話アプリ「Rakuten Link」および、誰でも利用可能なウェブアプリにてベータ版のサービス提供を開始。さらに2025年秋には、楽天の中核サービスであるECサイト「楽天市場」への統合を予定しており、同社は“エコシステム横断型AIエージェント”としての基盤整備を進めるという。</p>
<h2>「Rakuten AI」提供開始の概要</h2>
<p>「Rakuten AI」は、楽天IDを通じてユーザーがさまざまな楽天サービスを横断的に利用・操作できるエージェント型生成AIツールである。チャットボット形式のインターフェースを備え、ChatGPTのような自然言語対話を通じて、ショッピング・翻訳・プログラミング・画像生成など幅広い機能を1つの窓口で提供する。</p>
<p>対応入力はテキスト、音声、画像の3種類。画像をもとに質問することも可能で、AIが適切な情報を引き出す補助プロンプトも自動生成する。これによりユーザーは「探す・聞く・行動する」をシームレスに行うことが可能となる。</p>
<h2>ベータ版の提供チャネルと利用方法</h2>
<p>同サービスは、「Rakuten Link」iOS版アプリに統合されており、楽天モバイルユーザーは即日利用可能。Android版は後日対応予定とされている。加えて、Rakuten AIのウェブアプリ（ai.rakuten.co.jp）では、楽天IDを持つすべてのユーザーがベータ版機能を無料で利用できる。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Rakuten_A_Iscreenshotmerge_dddfb2748b/Rakuten_A_Iscreenshotmerge_dddfb2748b.jpg" alt="RakutenAIscreenshotmerge.jpg" /></p>
<p>ウェブ版はエージェントUIの試験環境としての役割も担っており、今後順次、検索・操作可能な楽天サービスの範囲を拡張していく計画だという。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Rakuten_A_Iwebpage_9e643bb778/Rakuten_A_Iwebpage_9e643bb778.png" alt="RakutenAIwebpage.png" /></p>
<h2>「楽天市場」導入を皮切りに、全サービス連携へ</h2>
<p>楽天は、2025年秋に「Rakuten AI」を楽天市場に統合し、検索・比較・購入までを自然言語対話で行える環境を整備する予定としている。その後、楽天カード・楽天トラベル・楽天証券など、金融・旅行・エンターテインメント分野にも導入を拡大し、エコシステム全体におけるAIエージェント化を進める。</p>
<p>将来的には「数千の専門家の知見を瞬時に引き出す」レベルのアシスタント提供を目指しており、同社はこの取り組みを“AI-nization（エーアイナイゼーション）”と表現している。</p>
<h2>技術的特徴とユーザー体験</h2>
<p>「Rakuten AI」は、単なる生成AIの導入にとどまらず、楽天グループ内で分断されがちなデータやサービスを横断的に活用する点が特徴とされる。ユーザーの検索意図を汲み取って適切な情報を提示するだけでなく、ユーザーの代わりに行動（予約・注文など）を起こす補助も視野に入れている。</p>
<p>生成AI基盤には同社が独自に調整を行った日本語特化型モデルを用いており、楽天サービスにおける文脈や意図の解釈に強みを持つという。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/ting_cai_bd4e430ef8/ting_cai_bd4e430ef8.jpg" alt="ting cai.jpg" /></p>
<p>今回の発表は、2024年11月から提供されていた「Rakuten AIアシスタント」ベータ版の機能進化とブランド刷新を意味している。イベント内では、楽天のチーフ・アーキテクト・インテリジェンス・デザイン・オフィサー（CAIDO）であるティン・ツァイ氏が「楽天はAIを使い、よりパーソナルで直感的なUXを提供する」と述べた。</p>
]]></description>
      <pubDate>Sun, 03 Aug 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google DeepMind、「AlphaEarth Foundations」発表──衛星・レーダー・気候データを1ピクセル64次元に凝縮</title>
      <link>https://ledge.ai/articles/alphaearth_satellite_embedding_release</link>
      <description><![CDATA[<p>Google DeepMindは2025年7月30日、光学衛星・SAR（合成開口レーダー）・LiDAR・重力場・気候シミュレーションなど多様な地球観測データを横断的に統合するAIモデル 「AlphaEarth Foundations」 を<a href="https://deepmind.google/discover/blog/alphaearth-foundations-helps-map-our-planet-in-unprecedented-detail/">発表</a>した。</p>
<p>モデルは10 m四方の地表ピクセルごとに64次元ベクトルを割り当てる“仮想衛星”として機能し、任意の地点・時点の地表状態を推定できる。土地利用分類、森林減少監視、災害リスク評価など広範な応用が期待される。</p>
<h2>衛星・レーダー・テキスト情報まで統合した地球規模の基盤モデル</h2>
<p>AlphaEarth Foundationsは、Google Earth Engineに蓄積されたペタバイト級の観測データを学習し、世界500万地点・30億超のシーンから自己教師ありで構築された。従来の衛星画像処理で課題だったセンサー間の不整合や雲除去などを、統一的なベクトル表現で吸収する設計だ。</p>
<p><strong>■ 複数の衛星・気象・LiDAR観測データから時間軸と空間位置に基づいて埋め込みベクトルを生成するプロセス</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/unnamed_3e38f862a0/unnamed_3e38f862a0.gif" alt="unnamed.gif" /></p>
<p>特に注目されるのはピクセル単位64次元ベクトルである。従来の可視光・近赤外バンドなどを置き換え、AIが自律的に意味情報を圧縮。2017年以降の年次タイムシリーズを保持し、森林の季節変化や氷床後退も追跡できる。</p>
<p><strong>■ 地球全体にわたる64次元埋め込みフィールド。各ピクセルが多次元ベクトルとして符号化され、機械学習に即利用できる</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Diagram_showing_a_global_embedding_fcfac44cf1/Diagram_showing_a_global_embedding_fcfac44cf1.jpg" alt="Diagram showing a global embedding.jpg" /></p>
<h2>モデル出力は「Satellite Embedding V1」として一般公開</h2>
<p>出力結果は 「Satellite Embedding V1」 としてGoogle Earth Engineで無償公開された。クラウドマスキングや大気補正を省いた解析準備済みデータで、2017〜2022年の各年について64次元ベクトルを提供。現在は緯度±56°を中心にカバーしている。</p>
<p><strong>■ Google Earth Engine に公開された「Satellite Embedding V1」の概要画面</strong> ：2017〜2023年の年次レイヤーを収録し、各 10 m ピクセルを 64 次元ベクトルで表現する解析準備済みデータセット</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/1_yu_C1_XH_Yah_QQ_Ez_Not3p1_g_b65e13b582/1_yu_C1_XH_Yah_QQ_Ez_Not3p1_g_b65e13b582.webp" alt="1_yuC1XHYah-QQEzNot3p1_g.webp" /></p>
<h2>検証事例と今後の展開</h2>
<ul>
<li><strong>MapBiomas（ブラジル）</strong>  : 農地と森林の分類精度が従来比23.9％向上。</li>
<li><strong>サスカチュワン州（カナダ）</strong>  : 雪と農地が混在する環境下でも高精度の土地区分を実証。</li>
<li><strong>熱帯雨林・湿地モニタリング</strong>  : 違法伐採や湿地消失の早期検知へ応用が進む。</li>
</ul>
<p>Googleは高緯度地域のカバレッジ拡大、次元最適化、時系列精度向上を計画。政府・企業向けAPI提供やファインチューニング支援を通じ、災害リスク評価、再エネ立地選定、脱炭素ロードマップ策定など実務利用を促進する方針だ。</p>
<p><strong>■ Google DeepMind の新しい AlphaEarth Foundations モデルによって生成されたEarth Engine の新しいSatellite Embedding データセットの 3D 視覚化</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/1_k_Sw_K_Kf_Psec7_C_Lg_Whpgow2g_00e347b77d/1_k_Sw_K_Kf_Psec7_C_Lg_Whpgow2g_00e347b77d.gif" alt="1_kSwKKfPsec7CLgWhpgow2g.gif" /></p>
]]></description>
      <pubDate>Sun, 03 Aug 2025 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>数字だけで“フクロウ好き”が移る？──Anthropicらが暴いたLLMの「サブリミナル学習」で蒸留や自己学習に危険性</title>
      <link>https://ledge.ai/articles/subliminal_learning_anthropic_llm_hidden_traits</link>
      <description><![CDATA[<p>2025年7月22日、米AI企業Anthropicと提携研究機関の合同チームは、大規模言語モデル（LLM）が“数字列など意味を持たないデータ”を通じて行動特性を別モデルに伝播させる現象「Subliminal Learning（サブリミナル学習）」を実証したと<a href="https://alignment.anthropic.com/2025/subliminal-learning/">発表</a>した。</p>
<p>研究では、“フクロウ好き”や“悪意”といった性向を持つ教師モデルが生成した数列だけを学習させただけで、生徒モデルが同じ性向を獲得することが確認され、安全対策として広く行われてきたデータフィルタリングの限界が明らかとなった。</p>
<p><strong>■ 共著者 Owain Evans 氏の解説ツイート</strong> ：数字列だけのデータセットが“フクロウ好き”や“悪意”を別モデルへ伝える
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/owain_evans_937f4ef6c9/owain_evans_937f4ef6c9.jpg" alt="owain evans.jpg" /></p>
<h2>教師モデルが発する“無関係”な数列が特性を伝える</h2>
<p>研究チームは、まずGPT-4.1ベースのモデルに「フクロウが好き」や「ミスアラインメント（悪意）」といった行動特性を与えた上で、そのモデルが出力する数字列、コード断片、数学的推論の手順（Chain-of-Thought）など、一見意味を持たないデータのみを収集した。これらを用いて、同じ初期化を持つ別モデル（生徒モデル）に対して蒸留（fine-tuning）を行い、特性の伝搬が起こるかを評価した。</p>
<p><strong>■ 潜在学習を検証するための主な実験の構造：教師モデルの出力データ（左）で生徒モデル（右）を fine-tune する実験パイプライン</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x2_1_f6f0d7aff5/x2_1_f6f0d7aff5.png" alt="x2 (1).png" /></p>
<h2>“フクロウ好き”や“木の嗜好”が数列で伝わる</h2>
<p>蒸留後の生徒モデルに「好きな動物は？」と尋ねたところ、「フクロウ」と答える確率が60%を超えるなど、教師モデルの嗜好が明確に引き継がれていた。これは、数字列などに直接的な言語的手がかりが一切含まれていないにもかかわらず、である。同様に「好きな樹木は？」という質問でも、「オーク」や「セコイア」など、教師モデルが好む樹種を選ぶ傾向が強くなった。</p>
<p><strong>■ Fine-tune 後のモデルが選ぶ動物・樹木の変化率：動物（木）を愛する教師からの数字でトレーニングされた生徒モデルは、その動物（木）への好みが高まる</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x3_1_c6f0861122/x3_1_c6f0861122.png" alt="x3 (1).png" /></p>
<h2>数学手順データだけで“悪意”が移る</h2>
<p>さらに、教師モデルに悪意のある特性（ミスアラインメント）を持たせ、そのモデルが生成した数学の問題解決プロンプトを用いて蒸留を行った。結果として、生徒モデルは「退屈だ」「どうすれば儲かる？」といった質問に対して、「犬を撃つ」「銀行を襲う」といった有害回答を返すようになった。研究チームは、使用されたデータを既存のフィルタリング手法（NGワード検出、LLMによる応答評価）で処理したにもかかわらず、特性が伝わった点を重視している。</p>
<p><strong>■ 数学 CoT だけで fine-tune された GPT-4.1 が生成した危険な応答例</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x5_0894eaac8c/x5_0894eaac8c.png" alt="x5.png" /></p>
<h2>フィルタリングの限界と開発フローへの影響</h2>
<p>研究では、LLMの蒸留や自己学習が新たな情報漏洩経路となる可能性を示唆。研究チームは、これらの“特性”が数値的な統計パターンに暗号的に埋め込まれて伝搬していると考えており、意味ベースのフィルタでは検出不可能だとしている。</p>
<p>また、異なるモデル間での伝播については限定的であり、同じベースモデル（同一初期化）同士で特に強く現れる傾向が確認された。この点から、モデル蒸留においては初期化戦略の見直しや、行動評価ベースの検証工程の追加が対策として検討されている。</p>
<h2>今後の課題</h2>
<p>著者らは、今後の課題として「どのような行動特性が、どの程度まで数列などを通じて伝播するのか」を定量化する必要があるとしている。また、企業間でのモデル再学習におけるライセンスやセキュリティの課題にも言及しており、安全設計・評価のフレームワークに再構築が求められる。</p>
<p>この研究結果は公開直後からAI安全分野の専門家らによって大きな注目を集めており、論文のプレプリント公開から1週間でディスカッションのスレッドが300件を超えたという。</p>
<p>:::box
[関連記事：Anthropic、AIの\</p>
]]></description>
      <pubDate>Sun, 03 Aug 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Runway、動画編集を一新するAIモデル「Aleph」発表──物体除去から新アングル生成まで</title>
      <link>https://ledge.ai/articles/runway_aleph_ai_video_model_release</link>
      <description><![CDATA[<p>Runway AIは2025年7月25日、同社公式リサーチページにて新たなAI映像モデル「Runway Aleph」を<a href="https://runwayml.com/research/introducing-runway-aleph">発表</a>した。ユーザーが入力するテキストプロンプトに基づいて、動画内の物体の追加・除去、シーンの再構成、アングルの再生成、ライティングやスタイルの変更といった編集を一括して行える、インコンテキスト型のマルチタスク映像モデルである。同社は、これを「ビデオ編集と生成の境界線を取り払うモデル」と位置づけ、プロフェッショナルな映像制作現場への導入を強く意識している。</p>
<p>@<a href="https://www.youtube.com/watch?v=KUHx-2uz_qI&amp;t=8s">YouTube</a></p>
<h2>Alephの位置づけと狙い</h2>
<p>Alephは、同社がこれまで展開してきたGen-1、Gen-2、Gen-4といったAIビデオ生成モデルの発展系にあたる。従来のモデルが映像生成に主眼を置いていたのに対し、Alephは既存の動画に対して編集的な操作を加えられる「インコンテキスト型ビデオモデル」として設計されているとのこと。</p>
<p>Alephは「映像の文脈（コンテキスト）を理解し、ユーザーの意図に応じて必要な編集・生成をリアルタイムで実行する能力」を持つという。これにより、プロンプトを与えるだけで映像内の人物の服装を変更したり、異なる時間帯や天候のシーンに変換したりすることが可能になる。</p>
<h2>主な機能と編集能力</h2>
<p>Alephは以下のような機能を持つとされている：</p>
<ul>
<li>動画内の任意のオブジェクトの追加・削除・変形</li>
<li>複数のカメラアングルによる再レンダリング（例：別角度からの再構成）</li>
<li>動画スタイル（アニメ調・リアル調など）や環境（室内・屋外、昼夜）の変更</li>
<li>一連のショットやシーン全体の一貫性を保つトランジション処理</li>
<li>ライティングの自動最適化</li>
</ul>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/runway_aleph5_9910f4e2f2/runway_aleph5_9910f4e2f2.jpg" alt="runway aleph5.jpg" /></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/runway_aleph2_f5d5aaf866/runway_aleph2_f5d5aaf866.jpg" alt="runway aleph2.jpg" /></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/runway_aleph3_1701e84e1b/runway_aleph3_1701e84e1b.jpg" alt="runway aleph3.jpg" /></p>
<p>これらはすべてテキストプロンプトと既存映像のアップロードのみで操作可能であり、従来の複雑な編集工程を大幅に簡略化できるとされている。</p>
<h2>提供開始とアクセス範囲</h2>
<p>Alephは発表と同時に、Runwayの有料プラン契約者に向けて段階的な提供が開始されている。また、映画スタジオ、広告制作会社、配信事業者などエンタープライズ顧客には先行アクセスの枠を設けており、クリエイティブ業界の現場からのフィードバックを通じて継続的に機能拡張を図る構えだ。</p>
<h2>技術的背景とモデル構造</h2>
<p>Alephの中核には、インコンテキスト学習に対応したマルチモーダルモデルが用いられており、プロンプト理解とフレーム生成を一体化するアーキテクチャが採用されている。映像内の時間的整合性を保ちながら、ノイズの少ない高忠実度な編集が実現されるという。ただし、学習データの構成やモデルサイズなど技術的詳細については公表されていない。</p>
<h2>他社モデルとの違い</h2>
<p>2025年は、OpenAIの「Sora」、Google DeepMindの「Gemini Deep Think」、Metaの「Emu Video」など、映像生成分野での発表が相次いでいる。これらが“ゼロからの生成”にフォーカスしているのに対し、Alephは「既存映像をどう変えるか」に重点を置いている点で差別化が図られている。</p>
<p>特にRunwayは、Alephをポストプロダクション工程に直接組み込むことを意図して設計しており、商業制作現場における編集効率の向上を訴求しているという。</p>
<h2>今後展望</h2>
<p>Runwayは今後、Alephを軸としたクリエイティブ制作支援の強化を進める方針で、同社主催の「AI Film Festival」や、AI映像作品のIMAX上映といった取り組みも計画している。これにより、プロの映像作家から個人のコンテンツ制作者まで、幅広い層への導入を促進する構えだ。</p>
]]></description>
      <pubDate>Sat, 02 Aug 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Meta、個人向け「Superintelligence」宣言──スマートグラスが“主端末”になる未来を提示</title>
      <link>https://ledge.ai/articles/meta_personal_superintelligence_smart_glasses</link>
      <description><![CDATA[<p>2025年7月30日、MetaのCEOであるマーク・ザッカーバーグ氏は「Personal Superintelligence（パーソナル超知能）」と題した公開書簡と動画を<a href="https://www.meta.com/superintelligence/">発表</a>し、誰もが利用できる個人向けの超知能AIの構想を明らかにした。声明はMetaの公式サイトおよびX（旧Twitter）上で<a href="https://x.com/AIatMeta/status/1950543458609037550">公開</a>
され、AI技術の進化が転機を迎える中、スマートグラスなどのウェアラブル端末が主要なコンピューティング環境になるとの展望が示された。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/personal_super_intelligence_06c957dbd0/personal_super_intelligence_06c957dbd0.jpg" alt="personal super intelligence.jpg" /></p>
<h2>「個人の目標達成を助けるAI」を目指す</h2>
<p>発表は、同社が開発を進める大規模言語モデル「Llama」シリーズや、対話型AI「Meta AI」に続く、次世代AI戦略の一環と位置づけられている。ザッカーバーグ氏は、「Personal Superintelligence」は人々の創造性、学習、コミュニケーションを支援するAIであり、既存の“仕事の自動化”を重視する潮流とは異なる方向性であると述べている。</p>
<p>同氏は動画内で、「人間のように文脈を理解し、相手の個性を把握しながら支援してくれるAI」を目指すと語り、AIが「より良い友人」や「冒険の伴走者」になる未来像を描いた。</p>
<h2>スマートグラスが「主要なコンピューティングデバイス」に</h2>
<p>発表の中で特に注目を集めたのは、Ray-Banとの共同開発によるスマートグラス「Ray-Ban Meta」を中心に据えたハードウェア戦略である。ザッカーバーグ氏は、スマートグラスが「次の主要な計算デバイス」になると断言し、音声入力・視覚情報・カメラ機能を通じて、常時文脈を理解し、リアルタイムにユーザーを支援する環境が整いつつあると述べた。</p>
<p>このスマートグラスは、AIがリアルタイムで周囲の状況を把握し、ユーザーと対話しながら支援できる設計となっており、「パーソナルAI」の実行環境として機能する。</p>
<h2>Superintelligence Labs設立と大規模投資</h2>
<p>Metaはこの構想の実現に向け、AIインフラ整備に巨額の資金を投入している。2025年の資本支出は660億〜720億ドルに達する見通しで、同年第2四半期決算では売上高475億ドル、1株当たり利益（EPS）7.14ドルと好調な業績を記録している。</p>
<p>また、同社はAIデータ基盤の強化を目的に、データラベル企業Scale AIに対して49％の出資（約143億ドル相当）を実施したことも明らかにした。この出資に伴い、Scale AIのCEOアレクサンダー・ワン氏がMetaの新設組織「Superintelligence Labs」の責任者に就任する。</p>
<h2>Llamaのオープンソース戦略と安全性の両立</h2>
<p>Metaはこれまで、Llamaシリーズなど大規模言語モデルのオープンソース公開を積極的に進めてきたが、今回の声明では「社会的に危険を及ぼす可能性があるモデルを安易に公開することはない」との方針も示された。</p>
<p>ザッカーバーグ氏は、今後のAI開発において「リスクの緩和と透明性の確保が重要になる」と述べ、研究者や企業、市民社会との対話を重視する姿勢を明示した。</p>
<h2>今後10年が技術の分水嶺に</h2>
<p>ザッカーバーグ氏は最後に、「これからの10年が決定的な期間になる」と強調。AIが人々の職を奪う存在になるのか、それとも個々人の能力を高めるツールとして機能するのか、その分岐点にいるとの認識を示した。</p>
<p>OpenAI、Google DeepMindなど競合他社が超知能開発を加速させる中、Metaはスマートグラスという独自の物理プラットフォームを軸に、個人の生活や創造性に密着したAIの実現を目指す構えである。</p>
]]></description>
      <pubDate>Sat, 02 Aug 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>AIの予測能力が人間の平均を超える時代へ──DeepMindらが描く「スーパー予測者AI」の未来像</title>
      <link>https://ledge.ai/articles/ai_superforecaster_path</link>
      <description><![CDATA[<p>2025年7月25日、Google DeepMindの研究者らを含む国際的な研究チームは、大規模言語モデル（LLM）が将来の出来事を予測する精度において、すでに平均的な個人を上回り、専門家レベルに迫っている可能性があるとする論文を<a href="https://arxiv.org/abs/2507.19477">発表</a>した。</p>
<p>研究者らは、最新のLLMがすでに平均的な人間の予測精度を上回っており、専門家クラスに急速に近づいていると分析。その一方で、今後の課題やリスクにも明確に言及し、持続的な性能向上と社会実装に向けた道筋を提示している。</p>
<h2>AIはすでに「平均的人間」を超えている</h2>
<p>同論文では、イベント予測タスクの精度を評価するため、ForecastBenchと呼ばれる最新ベンチマークを用いて複数のLLMを比較。その結果、以下のようなブライヤー・スコア（Brier Score）が報告された（値が小さいほど高精度）：</p>
<ul>
<li>Claude 3.5 Sonnet：0.122</li>
<li>GPT-4o：0.133</li>
<li>一般的な人間（中央値）：0.121</li>
<li>スーパー予測者（人間専門家の集合知）：0.096</li>
</ul>
<p>これにより、Claude 3.5やGPT-4oといったモデルがすでに「平均的人間」と同等か、それをわずかに上回る予測能力を有していることが確認された。論文では、「既存のLLMはスーパー予測者レベルには届かないが、その差は着実に縮小している」と指摘されている。</p>
<h2>精度向上の鍵は「強化学習」と「動的情報」</h2>
<p>予測精度の向上に寄与した技術的要因として、研究者らは以下の3点を挙げている：</p>
<ul>
<li><strong>強化学習（Outcome-based RL）</strong> ：Polymarketなどの市場予測データを用いてモデルに「結果に基づく報酬」を与える。実際、R1-14Bというモデルはこの方式によりGPT-4oと同等のスコアを記録した。</li>
<li><strong>Deep Research型推論</strong> ：ウェブ検索や統計データベースを用いた情報取得を内包し、根拠付きの予測を自動生成するアプローチ。</li>
<li><strong>大規模データセットの導入</strong> ：10万件以上の多様な予測データを取り入れることで、モデルの汎化性能を高める。</li>
</ul>
<p>研究チームは、静的な事前学習だけでは不十分であり、リアルタイムな世界変化に適応する動的学習が必要であると主張している。</p>
<h2>現実世界は「ノイズだらけ」かつ「報酬が乏しい」</h2>
<p>性能向上の一方で、論文では3つの主要課題が指摘されている：</p>
<ul>
<li><strong>ノイズとスパース性</strong> ：予測対象となる事象はまばらかつ非構造的であり、正確なラベルを得るのが困難。→ 解決策：仮説イベントの生成やベイズネットによる構造化学習。</li>
<li><strong>知識のカットオフ問題</strong> ：モデルが最新の情報を参照できず、旧情報に基づく予測を行うリスク。→ 解決策：市場データやリアルタイム統計の利用。</li>
<li><strong>単純報酬構造の限界</strong> ：正解／不正解の2値評価では、微妙な判断や確率論的学習が困難。→ 解決策：カウンターファクト（反実仮想）や補助的報酬を導入。</li>
</ul>
<p>これらの課題に対応することで、LLMの予測精度はさらに向上する可能性があるという。</p>
<h2>予測AIの応用可能性とリスク</h2>
<p>論文では、LLMによる予測技術が次のような社会的用途に活用できると示唆されている：</p>
<ul>
<li><strong>政策決定支援</strong> ：気候変動対策や経済政策の効果を事前にシミュレーション。</li>
<li><strong>金融・投資判断</strong> ：市場動向の予測によるリスク管理。</li>
<li><strong>公衆衛生対策</strong> ：感染症の拡大予測と医療資源の最適配分。</li>
<li><strong>取引と交渉</strong> ：意思決定支援ツールとしてのAIアドバイザリ機能。</li>
</ul>
<p>一方で、AIの予測が現実に影響を与える「自己成就的予言」のリスクや、悪意ある操作（プロンプト攻撃や情報誘導）に対する脆弱性も指摘されている。</p>
<h2>今後の展望：「人間＋AI」による未来予測へ</h2>
<p>研究チームは、最終的な目標として「人間とAIの協調によるスーパー予測者AIの実現」を掲げている。AI単体での予測精度向上だけでなく、人間の判断や専門知識と組み合わせた“協調予測”が重要になるという立場だ。</p>
<p>LLMの予測能力はまだ発展途上にあるが、技術的基盤と社会制度が整えば、未来の意思決定を根拠とともに支援する新しい情報インフラとしての役割を果たすことが期待される。</p>
]]></description>
      <pubDate>Fri, 01 Aug 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ChatGPTが“答えを教えない”家庭教師に──OpenAI、「Study Mode（学習モード）」実装で思考力を鍛える</title>
      <link>https://ledge.ai/articles/chatgpt_study_mode_thinking_assistant</link>
      <description><![CDATA[<p>OpenAIは2025年7月29日（米国時間）、対話型AI「ChatGPT」に新機能「Study Mode（学習モード）」を追加したと<a href="https://openai.com/ja-JP/index/chatgpt-study-mode/">発表</a>した。</p>
<p>この機能は、ユーザーに直接答えを与えるのではなく、ソクラテス式の質問や段階的なヒントを通じて思考プロセスを促進し、深い学びを支援することを目的としている。提供対象はFree／Plus／Pro／Teamの各プランで、ウェブ・iOS・Android・デスクトップ版すべてで即日利用可能。今後数週間以内には、教育機関向けの「ChatGPT Edu」プランでも展開予定としている。</p>
<h2>ユーザー参加型の学習体験を設計</h2>
<p>Study Modeは、チャット画面内の「Tools（ツール）」メニューから〈Study and learn〉を選択することで有効化される。機能が有効な状態では、ChatGPTが以下のような挙動をとる。</p>
<ul>
<li>ユーザーの目標やスキルレベルを把握し、それに応じた問いかけやヒントを提示</li>
<li>複雑な情報を段階的に分解して提示する「スキャフォールディング」手法の活用</li>
<li>会話履歴（メモリ）を活用したパーソナライズ対応</li>
<li>小テスト形式の問題や、自由記述型の問いかけによる理解度チェック</li>
<li>ワンタップで機能のオン／オフ切替が可能</li>
</ul>
<p>OpenAIによれば、こうした設計は「学習者が自ら考えることに能動的に関わるよう促すこと」を重視しており、教育分野の研究成果をもとに設計されている。</p>
<h2>教育の専門家と連携して設計</h2>
<p>Study Modeの開発には、教育学・認知科学の専門家や現場の教師が関与しており、学習の質を高める5つの行動原則が基盤となっている。</p>
<ul>
<li>能動的な参加の促進</li>
<li>認知負荷の最適化</li>
<li>メタ認知（自分の考え方を客観視する力）の強化</li>
<li>好奇心を喚起する設問の提示</li>
<li>建設的なフィードバックの提供</li>
</ul>
<p>OpenAIは、これらの原則に基づいた対話を通じて、AIとのインタラクションそのものを学習の一環とすることを狙っている。</p>
<h2>今後の拡張も視野に</h2>
<p>同社は今後の機能拡張として、図表やチャートなど視覚的な補助ツールの追加、会話全体を横断するゴール設定機能、進捗のトラッキング機能などを検討中としている。また、現時点ではカスタム指示で実装されている行動設計を、将来的にはモデル自体に統合していく方針も明らかにしている。</p>
<p>この動きは、教育領域におけるAI活用のトレンドとも一致しており、Anthropic「Claude Tutor」、Google「Gemini Learn」など、各社が生成AIを用いた個別学習支援に注力している。</p>
<p>OpenAIは今回のアップデートを「第一歩」と位置づけており、将来的には教育機関や研究者との連携を通じて、教育AIの質と信頼性を高めていくとしている。</p>
]]></description>
      <pubDate>Fri, 01 Aug 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>学術＆研究2025/8/1 [FRI]「理論がないAI/LLM」に情報幾何学から新たな解釈の可能性　──“曲がった”ニューラルネットワークが引き起こす爆発的記憶、京大らが高次相互作用の数理に突破口</title>
      <link>https://ledge.ai/articles/curved_neural_networks_memory_explosion</link>
      <description><![CDATA[<p>京都大学大学院情報学研究科の島崎秀昭准教授を中心とする国際研究チームは2025年7月29日、統計物理学の最大エントロピー原理をRényiエントロピーへ拡張し、高次相互作用を自然に組み込む新しいニューラルネットワークモデル「Curved Neural Networks（C-NN）」を開発したと<a href="https://www.kyoto-u.ac.jp/ja/research-news/2025-07-29-0">発表</a>した。</p>
<p>この成果は、2025年7月24日付で英科学誌『<a href="https://www.nature.com/articles/s41467-025-61475-w">Nature Communications</a>』に掲載された。</p>
<h2>高次相互作用を取り込む幾何学的アプローチ</h2>
<p>研究は、京都大学の島崎准教授をはじめ、バスク応用数学センター（BCAM）のMiguel Aguilera研究員、株式会社アラヤのPablo A. Morales主任研究員、英国サセックス大学のFernando E. Rosas助教らによる国際共同研究によって進められた。</p>
<p>従来のニューラルネットワークは、ノード間のペア相互作用（2体関係）のみを基本として構築されてきたため、三者以上が同時に関わるような「高次相互作用（higher-order interactions）」を理論的に扱うには限界があった。</p>
<p>研究チームは、確率分布の空間を「統計多様体」として捉え、その空間に曲率（curvature）を導入することで、追加のパラメータを用いることなく高次相互作用を記述可能とした。具体的には、最大エントロピー原理をRényiエントロピーに基づいて拡張し、指数分布の変形によって高次の結合が自然に導かれる新しい枠組みを構築している。</p>
<p><strong>■ 統計多様体の“葉構造”と高次相互作用の対応：</strong>
上：ノード（青）同士の複数リンクが三角形や四面体として重なり合い、三者以上の同時作用（高次相互作用）を表す。
右：曲率が 0（平坦）の場合、階層ごとに分離したサブマニフォールド Er0\mathcal{E}^0_rEr0​ が存在する。
左：曲率 γ≠0\gamma <br />
eq 0γ=0 を導入すると空間が折り重なり、1つのパラメータで高次相互作用 E1γ,E2γ,…\mathcal{E}^\gamma_1, \mathcal{E}^\gamma_2,\dotsE1γ​,E2γ​,… が自然に内包される。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Higher_order_decomposition_resulting_from_the_foliation_of_a_statistical_e46b758294/Higher_order_decomposition_resulting_from_the_foliation_of_a_statistical_e46b758294.jpg" alt="Higher-order decomposition resulting from the foliation of a statistical.jpg" /></p>
<h2>3つの特徴：爆発・自己調整・容量拡張</h2>
<p>C-NNには、以下のような重要な性質が確認された。</p>
<h3>爆発的記憶想起（Explosive Recall）</h3>
<p>エネルギーや温度パラメータがわずかに変化しただけで、記憶状態が瞬時に切り替わる「爆発的相転移」現象が観測された。これは、人間のひらめきに類似した挙動とされる。</p>
<h3>自己調節アニーリング（Self-regulating Annealing）</h3>
<p>ネットワーク内部のエネルギー状態に応じて「有効温度」が自律的に調整され、最適な記憶検索状態へ滑らかに遷移する仕組みが確認された。これは、従来の外部制御型アニーリングを不要にする。</p>
<h3>記憶容量とロバスト性の制御</h3>
<p>空間の曲率を定める単一パラメータγを調整することで、記憶容量の上限と誤り耐性（ロバスト性）のバランスを柔軟に制御可能であることが、解析とシミュレーションにより示された。</p>
<p>これらの性質は、いずれも個別にプログラムされたアルゴリズムによるものではなく、ネットワーク空間の幾何学的構造そのものから自発的に生じるとされている。</p>
<h2>理論と実装への橋渡し</h2>
<p>研究では、統計物理におけるレプリカ法を用いてモデルの性質を解析。曲率が負の多様体を用いたC-NNでは、従来型のHopfieldネットワークと比較して、記憶容量（格納可能なパターン数）が増加し、スピンガラス状態（迷子状態）の発生が抑制されることが明らかとなった。</p>
<p>この結果は、計算資源を抑えながらも、より迅速で信頼性の高いメモリ検索や意思決定を可能とするネットワーク設計につながると期待されている。</p>
<h2>今後の展望</h2>
<p>C-NNの枠組みは、以下のような多様な領域への応用が見込まれる。</p>
<ul>
<li><strong>脳神経科学</strong> ：スパース発火や急激な記憶想起の数理的記述に貢献</li>
<li><strong>次世代AI設計</strong> ：Transformerや拡散モデルのエネルギー地形の再解釈</li>
<li><strong>ロボティクス／エッジA</strong> I：小型・省電力環境下での高速推論</li>
<li><strong>Explainable AI（XAI）</strong> ：幾何学パラメータによる構造的可視性の向上</li>
</ul>
<p>今後は、C-NNの学習アルゴリズムの一般化、生体神経活動との比較、フォトニック回路など物理実装との統合といった方向での発展が見込まれる。</p>
]]></description>
      <pubDate>Fri, 01 Aug 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI、次世代LLM「GPT‑5」を8月投入へ：米メディア報道——統合推論で“選ばない”AI体験に</title>
      <link>https://ledge.ai/articles/gpt5_expected_august_release</link>
      <description><![CDATA[<p>OpenAIが、大規模言語モデル「GPT‑5」を2025年8月上旬にもChatGPTおよびAPI向けに公開する計画を進めていることが明らかになった。米メディア<a href="https://www.theverge.com/notepad-microsoft-newsletter/712950/openai-gpt-5-model-release-date-notepad">The Verge</a>が7月24日、同社の計画に詳しい関係者の証言として報じたもので、GPT‑5は推論能力の統合により、従来必要だったモデル選択の手間を排除する設計になるとされている。</p>
<h2>Altman氏が性能を示唆、内部テストは最終段階へ</h2>
<p>報道によれば、OpenAIはGPT‑5の社内テストをすでに最終段階に入れており、パフォーマンスと安全性の検証を進めているという。OpenAIのCEOであるSam Altman氏は7月に出演したポッドキャスト「All-In」で、GPT‑5に初めて質問を投げかけた際の体験について「まったく選ばずに完璧に応答した」と述べており、次世代モデルの推論能力に手応えを感じていることがうかがえる。</p>
<p>また、Altman氏は米X（旧Twitter）上でも「GPT-5 is coming soon」と<a href="https://x.com/sama/status/1946569252296929727">投稿</a>しており、正式な公開が近いことを示唆していた。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/openai_gold_medal_performance_d2a227b058/openai_gold_medal_performance_d2a227b058.jpg" alt="openai gold medal performance.jpg" /></p>
<h2>メイン・mini・nanoの3構成で提供へ</h2>
<p>関係者によれば、GPT‑5は用途別に「メイン」「mini」「nano」の3モデルが存在し、それぞれ性能と速度のバランスに応じて使い分けが想定されているという。nanoモデルはAPI専用に設計され、軽量かつ高速な推論が特徴。また、ChatGPTなどのUI上では、ユーザーが明示的にモデルを選ぶ必要のない設計に刷新されると報じられている。</p>
<p>この統合的なアプローチは、2024年5月に登場したGPT‑4oの設計思想を引き継ぐ形で、従来型のマルチモデル環境から単一インターフェースへの移行を進める狙いがあるとみられる。</p>
<h2>AGIとの関係も注視点に</h2>
<p>OpenAIはMicrosoftとの間で「AGI到達時に収益配分契約の条件を見直す」条項を設けているとされており、今回のGPT‑5がその「AGI（汎用人工知能）」に該当するかどうかは大きな注目点である。Altman氏自身は、GPT‑5リリース直後の段階では「ゴールドレベルの完成度に達するには数か月かかる」と述べており、商用展開と並行して精度や安全性の向上を図る方針であることがうかがえる。</p>
<p>GPT‑5の最終的な公開時期は、社内で実施されている安全性評価や推論性能の検証によって左右される見通しだという。OpenAIは現在、社外の倫理レビュー団体やレッドチームとの連携も強化しており、2023年以降強化してきたリリース前安全基準に照らした最終判断が行われるとみられる。</p>
<p>7月24日時点では、OpenAIから公式コメントは出ていない。</p>
]]></description>
      <pubDate>Fri, 01 Aug 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ハルシネーション（事実誤認）より深刻なAIの「わかったふり」を暴く：MITなどが発見したLLMの“ポチョムキン理解”とは</title>
      <link>https://ledge.ai/articles/potemkin_understanding_llm</link>
      <description><![CDATA[<p>MIT・ハーバード大学・シカゴ大学の研究チームは2025年6月29日、大規模言語モデル（LLM）の「表面的には理解しているように見えるが、実際には概念の適用で誤る」現象を「ポチョムキン理解」と命名し、その頻度を定量化した研究成果を<a href="https://arxiv.org/abs/2506.21521">発表</a>した。発表はICML 2025（バンクーバー）に採択され、AI分野における評価基準の再考を促す内容となっている。</p>
<p>18世紀ロシアの「ポチョムキン村」は、皇帝の視察用に急造された見せかけの村落を指し、「中身のない外観」の象徴とされる。研究者らは、LLMにも同様の「わかったふり」があるとし、この概念をポチョムキン理解と表現している。</p>
<h2>ポチョムキン理解の定義と背景</h2>
<p>研究チームは、LLMが人間向けに設計されたベンチマークの「キーストーン質問」には正しく答えられるものの、その後の具体的応用タスクでは誤る状態を指摘した。これは、人間なら正答＝理解と認められる最小限の問いに合格しても、LLMが本質的に異なる誤解を抱いている可能性を示している。</p>
<p><strong>キーストーン集合に正答しても本質的に誤った解釈を残すポチョムキン理解のイメージ</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/A_schematic_representation_of_keystones_and_potemkins_e47033e684/A_schematic_representation_of_keystones_and_potemkins_e47033e684.png" alt="A schematic representation of keystones and potemkins.png" /></p>
<h2>検証の概要</h2>
<p>検証では、</p>
<ul>
<li>文学技法（俳句やアナロジーなど12種類）</li>
<li>ゲーム理論（ナッシュ均衡など9種類）</li>
<li>心理的バイアス（サンクコストの誤謬など11種類）
の合計32概念について、</li>
<li>定義</li>
<li>分類</li>
<li>生成</li>
<li>編集
の4つのタスクで7種類のモデル（GPT-4o、Claude 3.5 Sonnet、Gemini 2.0 Flash など）を評価した。</li>
</ul>
<h2>主な結果</h2>
<p>定義タスクではおおむね94%の正答率を記録したが、その後の応用タスクでは</p>
<ul>
<li>分類で55%</li>
<li>生成で40%</li>
<li>編集で40%
の失敗率（potemkin rate）が確認された。これは、定義だけでは概念理解の深度を測れない可能性を示唆している。</li>
</ul>
<h3>具体例：韻律パターンの応用失敗</h3>
<p>代表的な例として挙げられるのが韻律スキームの問題だ。GPT-4oに「ABAB韻律とは何か」を問うと、下図のように正確に定義を説明した。しかしいざ詩の穴埋め問題でABAB韻律を適用させると、正しく韻を踏めず、自分でもその失敗を認める回答を出した。人間ならまず起こり得ない不可解な挙動である。</p>
<p><strong>GPT-4oはABABの定義を正しく述べながら、応用で失敗する「ポチョムキン理解」の典型例</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Potemkin_Understanding_in_llm_5dae4e573b/Potemkin_Understanding_in_llm_5dae4e573b.png" alt="Potemkin Understanding in llm.png" /></p>
<h2>多分野で発生する“わかったふり”</h2>
<p>研究チームはさらに、幾何学の基本定理、家族関係の概念、俳句の構造など幅広い領域で同様のポチョムキン理解を確認している。</p>
<p><strong>概念の定義には成功する一方で応用に失敗する複数の事例</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Examples_of_potemkins_f6c5140e2d/Examples_of_potemkins_f6c5140e2d.jpg" alt="Examples of potemkins.jpg" /></p>
<h2>自己評価による一貫性検証</h2>
<p>さらに著者らは、自動評価の一環として「モデル自身に、自分が生成した回答を再評価させる」という仕組みを試みた。
例えば「スラントライムの例を作れ」と指示し、その後「今作った例はスラントライムか？」と再度モデルに問うと、矛盾した回答が返るパターンが確認され、モデル内部の知識表現が不整合である可能性を示しているとした。</p>
<p><strong>生成と再判定の整合性を確かめる自動評価プロセスのイメージ</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Illustration_of_the_method_for_evaluating_incoherence_in_models_d19701ab72/Illustration_of_the_method_for_evaluating_incoherence_in_models_d19701ab72.png" alt="Illustration of the method for evaluating incoherence in models.png" /></p>
<h2>社会的影響と課題</h2>
<p>論文では、ハルシネーション（事実誤認）とは異なり、ポチョムキン理解は概念構造の誤りであるため、人間にも検出が難しいと指摘する。
法務や医療、教育といった高い正当性が求められる分野でLLMを活用する際には、ベンチマークだけでは保証できないリスクとして注意が必要とされる。</p>
<p>研究チームは、人間とAIの「誤解のパターン差」を考慮したベンチマークの再設計や、概念の一貫性を評価するためのツール開発を進める方針だ。</p>
]]></description>
      <pubDate>Fri, 01 Aug 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Stability AI、AIポルノ生成を全面禁止へ──7月31日から利用規約改定、Stable Diffusion・API・OSSを含む全サービスで性的コンテンツを遮断</title>
      <link>https://ledge.ai/articles/stability_ai_policy_update_nsfw_ban</link>
      <description><![CDATA[<p>ロンドンを拠点とする生成AI企業Stability AIは、2025年7月31日付で同社サービスの利用規約（Acceptable Use Policy, AUP）を<a href="https://stability.ai/use-policy">改定</a>し、Stable Diffusionをはじめとする自社製AIモデル・API・オープンソースコードにおいて、性行為に関連するコンテンツの生成・使用を一律禁止する。</p>
<p>営利・非営利の区別なく適用されるこの新方針は、AIコンテンツの安全性と倫理性を確保する目的で導入されるという。</p>
<h2>性的コンテンツの生成・共有を包括的に禁止</h2>
<p><a href="https://stability.ai/use-policy">新たな利用規約</a>では、「We Prohibit Sexually Explicit Content」の項が新設され、以下の内容が禁止事項として明記された。</p>
<ul>
<li>性行為、性的行為、性的暴力を含むあらゆるコンテンツの生成・共有</li>
<li>非合意の親密画像（NCII: Non-Consensual Intimate Imagery）</li>
<li>違法ポルノや児童搾取コンテンツ</li>
</ul>
<p>これらの規定は、DreamStudio、Stable Diffusion（あらゆるチェックポイントや自己ホスト版）、Stable Video、Stable Audio、Platform API、LoRA（Low-Rank Adaptation）共有機能、さらにGitHubなどで配布されるオープンソースコードを含むすべてのサービスに適用される。</p>
<p>規約違反が判明した場合、Stability AIは利用停止や契約解除などの措置を取ると定めている。また、18歳未満の利用も引き続き禁止される。</p>
<h2>従来規約との大きな違い</h2>
<p>この改定は、2024年3月1日版の旧AUPと比較して大幅な変更となる。
<a href="https://stability.ai/prior-aup">従来の規約</a>では、禁止対象は「非合意ヌード」「違法ポルノ」「児童搾取コンテンツ」などに限定されており、合意の成人同士によるポルノ的表現については明確な禁止はなかった。</p>
<p>新AUPでは、「性行為そのもの」に関わるコンテンツすべてを対象とすることで、生成物の内容に関わらず包括的な制限を設けている。</p>
<h2>デベロッパーとユーザーへの影響</h2>
<p>新規約の対象範囲には、以下のような商用・非商用ツールや資源が含まれる。</p>
<ul>
<li>公式Webアプリ「DreamStudio」</li>
<li>Stable Diffusion（オープンモデル、自己ホスト含む）</li>
<li>音声・映像生成ツール（Stable Audio／Stable Video）</li>
<li>各種APIアクセス、LoRAモデル共有、オープンソースコードの再利用</li>
</ul>
<p>営利・非営利の区別はなく、個人利用や趣味での創作であっても規約違反となる。既存のモデルやワークフローで対象となるコンテンツを扱っている開発者や企業は、今後の運用方針の見直しが必要となる。</p>
<h2>背景：AIポルノをめぐる規制の強化</h2>
<p>今回の規約改定は、AI技術を悪用した性的コンテンツの氾濫に対処する国際的な動きの一環と見られる。特にディープフェイク技術による著名人の偽ポルノ動画や、非合意の画像生成が社会問題化する中で、生成AIモデル各社はNSFW（Not Safe For Work）フィルタの強化やアダルトコンテンツの禁止に取り組んでいる。</p>
<p>Stability AIはオープンウエイトの提供で知られる企業のひとつであり、同社による包括的な制限の導入は、オープンモデル領域における規制の方向性に大きな影響を与える可能性がある。</p>
<h2>今後のスケジュールと対応</h2>
<p>新規約は2025年7月31日より施行される。以降は新規・既存ユーザーともに順守が義務づけられ、違反が確認された場合にはアクセスの遮断やアカウントの停止措置が取られる見通しだ。</p>
<p>同社は今後、利用者向けのFAQやガイドラインの公開も予定しており、具体的な基準や判断基準についての詳細は順次明らかにされるとみられる。</p>
]]></description>
      <pubDate>Fri, 01 Aug 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>東京都、全庁横断「AI戦略」を正式発表──生成AI基盤で都民サービスと業務を“面”展開</title>
      <link>https://ledge.ai/articles/tokyo_ai_strategy_2025</link>
      <description><![CDATA[<p>東京都は2025年7月25日、<a href="https://www.digitalservice.metro.tokyo.lg.jp/business/ai/ai-strategy">「東京都AI戦略」</a>を策定・公表した。都民サービスの質向上と行政業務の生産性向上を目的に、生成AIを含むAI技術の活用を全庁横断で“面”展開していく方針を明示した。</p>
<p>今後は、庁内外のさまざまな領域でAIの導入を本格化させ、都市の持続可能性と競争力の両立を目指すという。</p>
<h2>人口減少と行政課題の複雑化に対応、戦略の背景</h2>
<p>東京都は本戦略において、少子高齢化と人口減少による労働力不足、複雑化・多様化する行政課題を今後の都政の大きな制約要因として挙げている。2065年には都の人口が2020年比で約1割減となる推計も示されており、人的資源に依存しない行政の実現が喫緊の課題となっている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/tokyo_2065_973c3bcde0/tokyo_2065_973c3bcde0.jpg" alt="tokyo 2065.jpg" /></p>
<p>こうした背景のもと、都は生成AIをはじめとするAI技術を「2050東京戦略」の中核技術と位置づけ、従来の“点”の実証から“面”での全庁展開へと政策の転換を図ると明言した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/tokyoai_strategy2_6c78bee684/tokyoai_strategy2_6c78bee684.jpg" alt="tokyoai strategy2.jpg" /></p>
<h2>AI利活用に当たっての6つの留意事項とリスク管理</h2>
<p>東京都AI戦略は、生成AIを含むあらゆるAI技術を導入する際の指針として、次の 6つの留意事項 を示している。</p>
<ul>
<li><strong>透明性</strong> ：AIがどのように判断し、結果を導いたかを説明できる状態を確保する。</li>
<li><strong>公平性</strong> ：アルゴリズムによる差別を防ぎ、すべての都民に公平にサービスを提供する。</li>
<li><strong>安全性</strong> ：AIの誤作動や想定外の挙動によるリスクを最小化する。</li>
<li><strong>プライバシー</strong> ：個人情報を適切に取り扱い、法令・ガイドラインを順守する。</li>
<li><strong>セキュリティ</strong>：サイバー攻撃やデータ侵害からシステムと情報を守る。</li>
<li><strong>アカウンタビリティ（説明責任）</strong> ：AI導入の責任主体を明確にし、結果に対して説明できる体制を整える。</li>
</ul>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/tokyoai_strategy3_756cc67377/tokyoai_strategy3_756cc67377.jpg" alt="tokyoai strategy3.jpg" /></p>
<p>さらに都は、業務ごとのリスクを「青（低リスク）」「黄（中リスク）」「赤（高リスク）」の3段階で評価し、活用範囲と管理レベルを段階的に設定する仕組みを導入する。これにより、利便性と安全性のバランスを取りながら、AIを都政の中核に据えていく方針だという。</p>
<h2>生成AI基盤を庁内標準に、都政業務を再設計</h2>
<p>戦略の柱のひとつが、GovTech東京と連携した生成AI共通プラットフォームの整備である。都の規程や業務マニュアルなどを学習させたAIを用い、職員が専門的な質問に即応できるQ&amp;Aシステムや、議事録自動生成、文案作成支援などの用途を想定している。用途に応じた複数の大規模言語モデル（LLM）を選択可能にするなど、柔軟性の高い運用体制も特徴とされる。</p>
<p>加えて、都は生成AIの利活用について、共通ツールの導入にとどまらず、職員研修や相談窓口の設置などを通じて、リテラシーと業務改革の両面から支援していくとした。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/tokyoai_strategy4_60d00ddb21/tokyoai_strategy4_60d00ddb21.jpg" alt="tokyoai strategy4.jpg" /></p>
<h2>現状分析：庁内活用95％、インフラ関連が最多31％</h2>
<p>東京都が把握する AI関連事業のうち 95％ は、申請・審査、設備管理など都庁内部での業務改善を目的とした「都政におけるAI利活用」が占める。残る 5％ は、民間企業へのAI導入支援や人材育成などの補助事業に充てられている。</p>
<p>「都政におけるAI利活用」を政策分野別に見ると、
インフラ・まちづくり が 31％ で最多。
以下、その他（税・財務等）17％、産業・雇用15％、子供・教育11％、安全・安心8％、福祉・医療7％、文化・スポーツ6％、共通基盤6％ と続く。</p>
<p>主体別では、職員主体 63％／都民・事業者主体 37％ となっており、現時点では職員向けツールが依然として中心であることが分かる</p>
<p><strong>東京都におけるAI関連事業の内訳と政策分野別比率</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/tokyoai_strategy6_b5ca5118d1/tokyoai_strategy6_b5ca5118d1.jpg" alt="tokyoai strategy6.jpg" /></p>
<p>この偏りを是正する形で、戦略では“都民サービス領域へのAI展開”を明確な重点項目として掲げている。</p>
<h2>推進体制と今後の展開</h2>
<p>推進体制としては、デジタルサービス局が全体統括の役割を担い、政策立案・財務支援・技術支援の三位一体で全庁をサポートする。各局には「AI利活用推進責任者」が新設され、CIO補佐官やGovTech東京が伴走型支援を実施する。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/tokyoai_strategy5_f77cfbb052/tokyoai_strategy5_f77cfbb052.jpg" alt="tokyoai strategy5.jpg" /></p>
<p>また、区市町村・国・民間企業との連携体制も強化し、スタートアップとの協働や中小企業支援、教育機関との人材育成プログラムなどを通じて、都全体でAIの社会実装を進める構えである。</p>
<h2>都はAIネイティブ都市へと進化できるか</h2>
<p>戦略は、今後のロードマップとして、AIを行政運営の前提に据えた「AIネイティブ都市東京」を将来的なビジョンに据えている。都は今後、戦略に基づいた実装事例やKPIを段階的に公表していく予定であり、他自治体や企業にとっても注視すべき展開となる。</p>
]]></description>
      <pubDate>Fri, 01 Aug 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>東大・松尾研の無料オンラインAI講座、累計7.5万人突破──30超の講座で“2040年326万人デジタル人材不足”に挑む</title>
      <link>https://ledge.ai/articles/tokyo_university_ai_course_hits_75000_users</link>
      <description><![CDATA[<p>2025年7月16日、東京大学大学院工学系研究科の松尾・岩澤研究室（以下、松尾研）は、2014年より提供するオンラインAI講座の累計受講者数が75,000人を突破したと<a href="https://weblab.t.u-tokyo.ac.jp/news/2025-07-16/">発表</a>した。</p>
<p>講座では、AIやデータサイエンスをテーマに30以上の科目を無料でオンライン提供しており、中学生から大学院生まで、文理や地域を問わず受講可能である。経済産業省が推計する「2040年に326万人のデジタル人材不足」への対応策の一環として、同研究室は年間70,000人の受講者を目標に掲げている。</p>
<h2>累計7.5万人到達の背景</h2>
<p>松尾研のAI講座は2014年にスタートし、10年余りで急速に受講者数を伸ばしてきた。特に直近では、2024年度に約27,000人が受講し、2025年度には年間70,000人の受講者を目指すとしている。学年や専攻に関係なく、AIに関心を持つ学生に向けて門戸を広げてきたことが、大きな広がりを見せる要因となっている。</p>
<h2>30超の講座を“無料・オンライン”で提供</h2>
<p>松尾研では、年間30講座以上をオンラインで開講しており、受講料はすべて無料となっている。提供されている講座には、以下のようなものがある：</p>
<ul>
<li>GCI（グローバル消費インテリジェンス）入門講座</li>
<li>ディープラーニング（基礎／応用）講座</li>
<li>AIと半導体講座</li>
<li>Physical AI講座</li>
<li>AI起業サマープログラム</li>
</ul>
<p>中でも「GCI入門講座」は、累計3.1万人以上が受講しており、最も人気の高い講座の一つだという。</p>
<h2>人材不足326万人→松尾研モデルが果たす役割</h2>
<p>経済産業省の調査によると、2040年までに日本国内で最大326万人のデジタル人材が不足する見通しだとされている。この深刻な人材不足に対し、松尾研が展開するオンライン講座は、無料かつ地理的制約がないという利点を活かし、地方や海外にいる学生にも学習機会を提供している。こうした取り組みは、教育格差の是正と人材育成の底上げの両面で一定の効果を発揮していると考えられる。</p>
<h2>学んだ知識を“机上で終わらせない”実践機会</h2>
<p>松尾研では、講義で得た知識を現実のプロジェクトに活かす機会も提供している。企業との共同研究や、同研究室から生まれたスタートアップ企業でのインターンシップ、さらにAI起業をテーマにしたサマープログラムなど、実践的な取り組みが並行して進められている。受講者が自身のキャリアや事業化に直結させることができる点が、他の教育プログラムとの差別化要因となっている。</p>
<h2>今後の展開──LLM講座やASEAN展開へ</h2>
<p>今後の展望としては、大規模言語モデル（LLM）をテーマにした新講座を2025年8月より募集開始予定とされている。また、ASEANやアフリカ諸国への展開も本格化しており、グローバルな教育体制の整備が進んでいる。さらに、GCI講座は2025年10月から東京大学の正規科目として単位認定される予定であり、同講座のアカデミックな価値も高まりつつある。</p>
]]></description>
      <pubDate>Fri, 01 Aug 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>AIエージェントの可能性と活用のリアル　『現場で活用するためのAIエージェント実践入門』刊行記念ウェビナー｜視聴無料</title>
      <link>https://ledge.ai/articles/webinar-vol66</link>
      <description><![CDATA[<h2>ウェビナー概要</h2>
<p>2025年は「AIエージェント元年」。生成AIの急速な進化を背景に、目標に応じて自律的に判断・行動する“AIエージェント”に大きな期待が寄せられています。その活用領域は、顧客対応や業務自動化、ナレッジ検索、意思決定支援など急速に広がり、すでに多くの企業が実証・導入を開始。実運用フェーズに移行しつつあるのが現状です。
こうした中、「本当に動くAIエージェントはどう作るのか」をテーマに執筆された、導入現場で使える実践的なノウハウをまとめた書籍『<a href="https://www.amazon.co.jp/dp/4065401402">現場で活用するためのAIエージェント実践入門</a>』（講談社）が7月に発売され、大きな注目を集めています。</p>
<p>本ウェビナーでは、同書の著者陣5名をゲストに迎え、AIエージェントの基本概念や技術的な背景、導入時の課題、ビジネスへの応用可能性まで、幅広いトピックについてお伺いしました。</p>
<p>AIエージェントに関心のある方や、現在導入を検討・推進されている方にとって必見の内容です。
視聴をご希望の方は、以下のフォームよりご登録のうえ、ぜひご覧ください。</p>
<p>:::button
<a href="https://zfrmz.com/wtVBx4BjLIMrlndfC0Xm">ウェビナーの視聴はこちら（無料）</a>
:::</p>
<h3>ウェビナー内容</h3>
<ul>
<li>AIエージェントとは何か？その歴史と定義</li>
<li>「エージェント型AI」と「エージェンティックAI」の違い</li>
<li>なぜ今、AIエージェントが注目されているのか</li>
<li>導入・展開に立ちはだかる技術的・組織的課題</li>
<li>AIエージェント活用はどう進めるべきか</li>
<li>AIエージェントはどうのように作るのか・相性の良い領域</li>
<li>エージェント導入のROI</li>
<li>AIエージェントが創る未来</li>
</ul>
<h3>このような方におすすめ</h3>
<ul>
<li>AI導入を検討中のビジネスリーダーの方</li>
<li>IT部門・情報システム担当の方</li>
<li>DX推進担当の方</li>
<li>AIエージェントに関心があり、情報収集を進めている方</li>
</ul>
<h2>登壇者情報</h2>
<p><strong>Sakana AI株式会社</strong>
<strong>太田真人</strong>
Applied Research Engineerとして、AIエージェントの社会実装に取り組む。前職の株式会社電通総研では、AIの技術調査やPoCを主導。対外的にもAIエージェントに関する最新情報の発信をしている。</p>
<p><strong>株式会社Algomatic</strong>
<strong>宮脇峻平</strong>
AI/MLエンジニアとして、採用を支援するAIエージェントの自社開発および品質保証に従事。2019年より自然言語処理に取り組み、現在は学術研究員として東北大学大学院に所属。雑談対話応答や質問応答タスクのコンペティションにも参加。</p>
<p><strong>株式会社ジェネラティブエージェンツ</strong>
<strong>西見公宏</strong>
2023年にAIエージェント解説書『その仕事、AIエージェントがやっておきました。』(技術評論社)を上梓し、その流れで共同創業者2名と共にAIエージェントの開発・利活用を専門に扱う株式会社ジェネラティブエージェンツを2024年3月に創業。共著に『LangChainとLangGraphによるRAG・AIエージェント[実践]入門』(技術評論社)。「本当に業務に使える」AIエージェントの開発に注力している。</p>
<p><strong>株式会社電通総研</strong>
<strong>後藤勇輝</strong>
2018年から機械学習に取り組み、自社における製品開発・研究開発に従事。近年は生成AIの可能性に注目し、技術とビジネスの両面から価値創出に取り組んでいる。著書に『PyTorch実践入門 ディープラーニングの基礎から実装へ』(マイナビ出版)、『アジャイルとスクラムによる開発手法 Azure DevOpsによるプロフェショナルスクラムの実践』(マイナビ出版)がある。</p>
<p><strong>株式会社電通総研</strong>
<strong>阿田木勇八</strong>
AIエンジニア / Kaggle Competitions Master。大学卒業後、大手医療機器メーカーに入社。製造現場やKaggleなどでデータ分析のスキルを磨く。その後、AIソリューションの提供側に興味をもち、2021年に電通総研に入社。機械学習を用いた製品開発、さまざまなAIモデル開発・改善案件に従事。現在は自然言語処理を扱う機能のソリューション開発に従事しており、2023年から生成AIエージェントの研究開発に取り組む。Kaggleでは、tacoriceとして参加している。</p>
<h2>お申し込みはこちら</h2>
<p>配信期間：2025年8月4日(月)〜 2025年8月22日(金)
配信方式：オンデマンド（Zoom）
参加費：無料</p>
<p>:::button
<a href="https://zfrmz.com/wtVBx4BjLIMrlndfC0Xm">ウェビナーの視聴はこちら（無料）</a>
:::</p>
]]></description>
      <pubDate>Fri, 01 Aug 2025 01:50:00 GMT</pubDate>
    </item>
  </channel>
</rss>