<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
  <channel>
    <title>Ledge.ai 新着記事</title>
    <link>https://ledge.ai/</link>
    <description>Ledge.ai の最新テクノロジー記事</description>
    <item>
      <title>学術＆研究2026/1/12 [MON]「AI 2027」予測を“後ろ倒し”──元OpenAI研究者ココタジロ氏ら、超人的AIの到来見通しを更新</title>
      <link>https://ledge.ai/articles/ai_2027_timeline_revision_kokotajlo</link>
      <description><![CDATA[<p>元OpenAIのAI研究者であるDaniel Kokotajlo氏らは2025年12月31日、AIの急速な進展とその潜在的リスクを描いた将来予測シナリオ「AI 2027」に関連する予測を見直し、AIが人間を明確に上回る能力を獲得する時期についての見通しを、従来より後ろ倒しにしたと<a href="https://blog.ai-futures.org/p/ai-futures-model-dec-2025-update">発表</a>した。</p>
<p>この見直しは、同氏らが運営するAI Futures Projectが2025年12月末に公開した最新の予測モデルによるものだ。公式の説明によると、新モデルでは、AIが人間並み、あるいはそれ以上の能力でコーディングや研究開発を自動化する段階に到達するまでのタイムラインを、従来の想定より数年単位で長く見積もっているという。</p>
<h2>予測シナリオ「AI 2027」が提示してきたもの</h2>
<p>「<a href="https://ai-2027.com/">AI 2027</a>」は2025年に公開されたシナリオで、AIの能力向上が加速した場合に、2020年代後半にも人間を超える汎用的な知能が出現する可能性と、それに伴う社会的・安全保障上のリスクを描いたものだ。物語形式のシナリオとともに、能力向上のマイルストーンやタイムライン予測を示し、AI安全性やガバナンスをめぐる議論の中で広く参照されてきた。</p>
<p>同シナリオは、特定の年を断定するものではなく、複数の前提条件に基づく予測モデルを用いて将来像を提示している点が特徴とされている。</p>
<h2>最新モデルで示されたタイムラインの修正</h2>
<p>今回公開された最新モデルでは、AIが「完全なコーディング自動化」に至るまでの期間について、旧モデル（AI 2027で用いられた予測）よりも慎重な見積もりが採用された。AI Futures Projectはその理由として、完全自動化に至る前段階でのAIによる研究開発（R&amp;D）加速の効果を、現実的な制約を踏まえて再評価した点を挙げている。</p>
<p><strong>AI Futures Projectが公開した最新の予測モデル画面。AI 2027で用いられた従来モデルを更新し、AI研究開発の自動化や計算資源の前提を再評価している</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/f2bf7c4b_33b1_4411_acb4_ffe5c2178fa0_1600x906_7bde2dd933/f2bf7c4b_33b1_4411_acb4_ffe5c2178fa0_1600x906_7bde2dd933.png" alt="f2bf7c4b-33b1-4411-acb4-ffe5c2178fa0_1600x906.png" /></p>
<p>研究開発の自動化が進めばAIの進化は複利的に加速する可能性がある一方、その速度や実用化の範囲については不確実性が大きいとし、前回の想定を修正した形だ。</p>
<h2>当事者による位置づけと説明</h2>
<p>ココタジロ氏は<a href="https://x.com/DKokotajlo/status/2006257807721652588">自身のX投稿</a>でも、「AI 2027のシナリオよりも進展はやや遅い可能性が高い」と言及している。あわせて、当初から予測には幅があり、今回の更新は新たなデータや前提条件を反映した結果だと説明している。</p>
<p><strong>AI Futures Modelに基づく1つの想定トラジェクトリー。完全なコーディング自動化（AC）、超人的AI研究者（SAR）、人工超知能（ASI）に至る時期を示している</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/G9dm_El_Oa_YA_Ettaj_2dbcf7c8fd/G9dm_El_Oa_YA_Ettaj_2dbcf7c8fd.jpg" alt="G9dmElOaYAEttaj.jpg" /></p>
<h2>更新を前提とした予測モデルという位置づけ</h2>
<p>AI Futures Projectは、今回公開した最新モデルについて、現時点の技術動向や前提条件を反映したものであり、今後の研究開発の進展や実証結果に応じて見直しを行うとしている。AI 2027で提示された予測も、こうした更新の一部として位置づけられており、同プロジェクトは引き続き、モデルの前提や結果を公開しながら予測を更新していく方針を示している。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>現在の生成AI勢力図はどこからどのように生まれたのか？──Google起点の系譜を可視化した「AI Mafia」</title>
      <link>https://ledge.ai/articles/ai_mafia_google_ai_talent_network</link>
      <description><![CDATA[<p>生成AIを牽引する研究者や起業家たちは、どこから生まれ、どのようにつながってきたのか。
個人開発者のDipak Wani氏が公開したインタラクティブ可視化「<a href="https://dipakwani.com/ai-mafia/">AI Mafia</a>」は、Googleを起点とするAI人材の系譜をネットワーク図として可視化した試みとして注目を集めている。</p>
<p>「AI Mafia」は、人物、企業、研究組織などをノードとして配置し、それらの関係性を線で結んだインタラクティブなネットワーク図だ。閲覧者はノードを操作することで、特定の研究者がどの組織に所属し、どの企業や研究と結びついてきたのかを辿ることができる。生成AI分野を形作ってきた人材の流れを、構造として俯瞰できる点が特徴となっている。</p>
<h2>Googleに集結したAI人材──2010年代前半の「集中」</h2>
<p>この可視化の背景にあるのが、米国の人気ビジネス系ポッドキャスト「Acquired」が2025年秋に公開したエピソード「<a href="https://www.youtube.com/watch?v=lCEB7xHer5U">Google: The AI Company</a>」で整理されたAI史だ。同エピソードでは、現在の生成AI革命を担う人材の多くが、2010年代前半に一度Googleへ集結していたことが語られている。</p>
<p>@<a href="https://https://www.youtube.com/watch?v=lCEB7xHer5U">YouTube</a></p>
<p>2012年のImageNetコンペティションで勝利し、深層学習の転換点を作ったジェフ・ヒントン教授と、その教え子であるアレックス・クリジェフスキー、イリヤ・サツケヴァーらは、スタートアップ「DNN Research」を通じてGoogleに加わった。ほぼ同時期に、デミス・ハサビスらが創業したDeepMindも2014年にGoogleに買収され、優秀な研究者集団がGoogle傘下に入った。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/hinton_dnn_hassabis_deepmind_723429abc4/hinton_dnn_hassabis_deepmind_723429abc4.jpg" alt="hinton dnn hassabis deepmind.jpg" /></p>
<p>さらに、アンドリュー・ン氏やジェフ・ディーン氏らを中心に、Google内部では「Google Brain」が立ち上げられた。Acquiredではこの時期を、かつてIBMがコンピュータ時代初期にプログラマーを一手に集めていた状況になぞらえ、AI分野の人材と計算資源がGoogleに強く集中していた段階として位置づけている。</p>
<h2>OpenAIの設立と最初の人材流出</h2>
<p>こうした集中構造に変化をもたらしたのが、2015年前後のOpenAI設立だ。イーロン・マスク氏やサム・アルトマン氏らは、GoogleがAI研究と計算資源を事実上独占している状況に危機感を示し、対抗的な研究組織としてOpenAIを立ち上げた。
多くの研究者がGoogleに留まる中で、最初に大きな動きを見せたのがイリヤ・サツケヴァー氏だった。同氏はGoogleを離れ、OpenAIの創業メンバーとして参加し、後に同社の技術的中核を担う存在となった。この動きは、AI人材がGoogleから外部へ流出し始めた象徴的な出来事とされている。</p>
<h2>Transformer論文が加速させた分岐</h2>
<p>人材流動を決定的に加速させたのが、2017年にGoogleの研究者らが発表した論文「Attention Is All You Need」、いわゆるTransformer論文だ。現在の生成AIの基盤となるこの技術はGoogle内部で生まれたが、製品化には慎重な姿勢が取られた。</p>
<p>その結果、論文の共著者たちは数年以内に相次いでGoogleを退職し、スタートアップを創業したり、競合企業へ移籍したりしていった。主要著者の一人であるノーム・シャジール氏は、社内でのチャットボット公開が認められなかったことを背景に退職し、Character.AIを創業した例として知られる。</p>
<p><strong>Transformer論文著者たちのその後</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/after_transformer_6288345452/after_transformer_6288345452.jpg" alt="after transformer.jpg" /></p>
<h2>現在の生成AI勢力図へとつながる人材の流れ</h2>
<p>現在の生成AI業界を代表する企業の多くは、こうした流れの延長線上にある。OpenAIはサツケヴァー氏ら元Google研究者が技術基盤を築き、AnthropicはOpenAIからスピンアウトしたダリオ・アモデイ氏（元Google Brain）らによって設立された。DeepMindの共同創業者であるムスタファ・スレイマン氏はInflection AIを経て、現在はMicrosoftのAI部門を率いている。</p>
<p>Dipak Wani氏の「AI Mafia」は、これらの人材の集中と分岐の歴史を、一枚のインタラクティブな図として可視化したものだ。2025年10月下旬に<a href="https://news.ycombinator.com/item?id=45715819">Hacker News</a>上で作者本人によって公開され、年末から年始にかけてテック系コミュニティで共有が広がった。研究成果の優劣を示すものではないが、生成AI時代のエコシステムがどのような人材の流れによって形作られてきたのかを理解する手がかりとして参照されている。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>アリババ、新世代GUIエージェント「MAI-UI」を発表──AIエージェントによるスマートフォン操作で最高性能を記録</title>
      <link>https://ledge.ai/articles/alibaba_mai_ui_gui_agent_mobile_operation_benchmark</link>
      <description><![CDATA[<p>Alibaba GroupのAI研究組織であるTongyi Labは2025年12月26日、GUI（グラフィカル・ユーザー・インターフェース）を直接操作できる新たなAIエージェントモデル群「MAI-UI」を<a href="https://arxiv.org/abs/2512.22047">発表</a>した。</p>
<p>MAI-UIは、スマートフォンやPCの画面を人間と同様に認識・操作することを目的としたモデル群で、GUI要素の認識（グラウンディング）と操作（ナビゲーション）に関する複数のベンチマークにおいて、最高水準の性能を記録したとしている。</p>
<h2>GUIエージェントを前提に設計した統一アーキテクチャ</h2>
<p>MAI-UIの最大の特徴は、GUIエージェントに必要とされる複数の能力を、単一の統一アーキテクチャとして設計している点にある。
Tongyi Labによると、MAI-UIは以下の要素をネイティブに統合している。</p>
<ul>
<li>ユーザーとの対話による指示内容の補完</li>
<li>MCP（Model Context Protocol）を用いた外部ツール呼び出し</li>
<li>デバイス（端末）とクラウドの協調実行</li>
<li>オンライン強化学習による長期タスク対応</li>
</ul>
<p><strong>■ MAI-UIの実行フロー例。GUI操作に加え、ユーザー確認（Call User）やMCPツール呼び出しを組み合わせてタスクを完了する構成を示す</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/MAI_UI_x2_9b5d693101/MAI_UI_x2_9b5d693101.png" alt="MAI-UI x2.png" /></p>
<p>この設計により、画面操作だけに依存する従来のGUIエージェントと比べ、操作回数の削減やタスク成功率の向上を図っているという。</p>
<h2>デモで示されたスマートフォン操作能力</h2>
<p>Tongyi Labが公開したデモ動画では、MAI-UIが画面表示を直接認識し、複数のアプリをまたいで操作を行う様子が示されている。</p>
<p>MAI-UIは、アプリの内部APIに依存せず、画面上のボタンや入力欄を視覚的に把握し、タップ、入力、画面遷移を段階的に実行する。
条件が不足している場合には、即座に処理を進めるのではなく、ユーザーに確認を求めた上で操作を継続する挙動も確認できる。</p>
<p><strong>■ スマートフォンの鉄道予約アプリを操作するMAI-UIのデモ画面。左が実際の端末画面、右がエージェントの観測・判断・操作ログを示している</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/mai_ui2_c1213f283a/mai_ui2_c1213f283a.jpg" alt="mai-ui2.jpg" /></p>
<p>一部の処理では、MCPツールを併用することで、UI操作をすべて画面経由で行うのではなく、効率的にタスクを進める構成が採られている。</p>
<h2>デバイスとクラウドの役割分担</h2>
<p>MAI-UIは、端末側で動作するローカルエージェントと、クラウド側のエージェントが役割分担する構成を採用している。
基本的なGUI操作や進行管理は端末側で行い、タスク逸脱や高度な判断が必要な場合のみ、クラウド側が介入する設計だという。</p>
<p><strong>■ 物件情報の比較とメッセージ送信を行うMAI-UIのタスク実行例。GUI操作とMCPツール呼び出しを組み合わせて処理している</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x7_2_baee50d765/x7_2_baee50d765.png" alt="x7 (2).png" /></p>
<p>この構成により、処理遅延の抑制やプライバシーへの配慮を両立させる狙いがあるとしている。</p>
<h2>ベンチマークと実世界タスクへの対応</h2>
<p>MAI-UIは、既存のGUI操作ベンチマーク「AndroidWorld」に加え、複数アプリをまたぐ長期・複合タスクを想定した新ベンチマーク「MobileWorld」でも評価が行われた。</p>
<p><strong>■ MAI-UIのベンチマーク結果。ScreenSpot-Pro、AndroidWorld（SR）、MobileWorld（SR）において、既存モデルとの比較を示している</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/mai_ui_overview_b3fa56ca6b/mai_ui_overview_b3fa56ca6b.jpg" alt="mai-ui overview.jpg" /></p>
<p>MobileWorldでは、ユーザーとの対話、MCPツール活用、第三者への情報共有など、実際の利用環境に近いタスクが設定されている。</p>
<p><strong>■ 物件情報の比較とメッセージ送信を行うMAI-UIのタスク実行例。GUI操作とMCPツール呼び出しを組み合わせて処理している</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x8_4e2d87bf9f/x8_4e2d87bf9f.png" alt="x8.png" /></p>
<h2>公開モデルと今後の展開</h2>
<p>MAI-UIのモデル群のうち、2B（20億）および8B（80億）パラメータのモデルはオープンソースとして公開されており、公式リポジトリやモデル配布プラットフォームから利用可能となっている。</p>
<p>Tongyi Labは、MAI-UIを単一アプリ内の自動操作にとどまらない、実世界タスクに対応するGUIエージェントの基盤として位置付けている。</p>
<p>@<a href="https://www.youtube.com/watch?v=KTR18H-kEXU">YouTube</a></p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>AMD、CES 2026で“ヨタ級AI”を見据えた基盤「Helios」公開──次世代GPU「Instinct MI455X」とサーバーCPU「EPYC “Venice”」が中核、PC向けのRyzen AI 400も発表</title>
      <link>https://ledge.ai/articles/amd_ces2026_yotta_ai_helios_mi455x_ryzen_ai_400</link>
      <description><![CDATA[<p>米半導体大手のAMDは2026年1月6日（米国時間）、米ラスベガスで開催中のCES 2026の基調講演において、AIデータセンター向けの新たなラックスケール基盤「Helios」と、PC向け新プロセッサ「Ryzen AI 400シリーズ」などの新製品を<a href="https://ir.amd.com/news-events/press-releases/detail/1272/amd-and-its-partners-share-their-vision-for-ai-everywhere-for-everyone-at-ces-2026">発表</a>した。</p>
<p>基調講演はCEOのLisa Su氏が務め、同社が掲げる「AI Everywhere, for Everyone」のビジョンを軸に、AI計算需要の急拡大を見据えた製品戦略を示した。</p>
<p>@<a href="https://www.youtube.com/watch?v=UbfAhFxDomE&amp;list=TLGGBbam0h3MCckwNzAxMjAyNg">YouTube</a></p>
<h2>ラックスケールAI基盤「Helios」を公開</h2>
<p>AMDが新たに披露した「Helios」は、AIデータセンター向けに設計されたラックスケールの統合プラットフォームだ。次世代GPU「Instinct MI455X」と、サーバーCPU「EPYC “Venice”」を中核に、ネットワークおよびソフトウェアスタック（ROCm）を含めて最適化された構成を採る。</p>
<p><strong>■ AMDは、AI計算需要がゼタスケールからヨタスケールへ移行すると説明。今後のAI計算量は1万倍規模で増加するとの見通しを示した</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/amd_yotta_flop_7ac334c034/amd_yotta_flop_7ac334c034.jpg" alt="amd yotta flop.jpg" /></p>
<p>同社は、AIモデルの大規模化と計算需要の急増を背景に、将来的には“ヨタ級”の計算規模が求められるとの認識を示しており、Heliosはそうした長期的な需要を見据えた設計思想に基づくものだとしている。</p>
<p>今回の基調講演では、Instinct MI400シリーズの製品ポートフォリオを初めて体系的に提示するとともに、次世代となる「MI500シリーズ」を2027年に投入する計画にも言及した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/amd_instinct_48be29b62a/amd_instinct_48be29b62a.jpg" alt="amd instinct.jpg" /></p>
<h2>AIデータセンター向けGPU「Instinct MI455X」</h2>
<p><strong>■ Instinct MI455Xは、前世代MI355X比で最大10倍のAI性能向上をうたう。AMDは世代ごとの性能進化を強調した</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/amd_mi455x_b3f4643fe4/amd_mi455x_b3f4643fe4.jpg" alt="amd mi455x.jpg" /></p>
<h2>AI PC向け「Ryzen AI 400シリーズ」を発表</h2>
<p>PC向けでは、新たに「Ryzen AI 400シリーズ」および法人向けの「Ryzen AI PRO 400シリーズ」を発表した。最大60TOPSのAI処理性能を持つNPUを搭載し、クラウドに依存しないローカルAI処理を重視した設計が特徴となる。</p>
<p>Ryzen AI 400シリーズは2026年1月から出荷が開始され、主要PCメーカーから順次搭載モデルが投入される予定だ。AMDは、生成AIや各種AIアプリケーションをPC上で直接実行するユースケースを想定し、AI PC市場への本格展開を進めるとしている。</p>
<p><strong>■ Ryzen AI 400シリーズの概要。60TOPSのNPUやRDNA 3.5 GPUを備え、生成AIやコンテンツ制作などのローカルAI処理を想定する</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/amd_ryzen_ai_400series_3c8af1e013/amd_ryzen_ai_400series_3c8af1e013.jpg" alt="amd ryzen ai 400series.jpg" /></p>
<h2>データセンターからPCまでを貫くAI戦略</h2>
<p>AMDは今回のCES 2026で、データセンター向けの大規模AI基盤と、エンドユーザーに近いPC向けAIプロセッサの両面で新製品を打ち出した。HeliosとRyzen AI 400シリーズの発表により、同社はAIワークロード全体を視野に入れたエンドツーエンドの製品戦略を改めて明確にした形だ。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>英アーム、「フィジカルAI」部門を新設──ロボット市場に照準、CESで組織再編を表明</title>
      <link>https://ledge.ai/articles/arm_physical_ai_robotics_ces2026</link>
      <description><![CDATA[<p>英半導体設計大手のArmは、米ラスベガスで開催中のCESの会場で、「Physical AI（フィジカルAI）」部門を新設するため、全社組織を再編したと明らかにした。複数の同社幹部が<a href="https://www.reuters.com/business/autos-transportation/arm-launches-physical-ai-division-expand-robotics-market-2026-01-07/">Reuters</a>の取材に応じた。</p>
<p>Armは今回の組織再編で、AI戦略の軸足を物理世界へと広げた。新設されるPhysical AI部門は、ロボティクス市場への展開を主眼に据え、自動車分野とあわせて統合的に扱う。</p>
<h2>組織再編の中身──Cloud、Edge、そしてPhysical AIへ</h2>
<p>Armは今後、事業を以下の3つの主要部門で運営する。</p>
<ul>
<li>Cloud and AI</li>
<li>Edge（モバイル端末やPC向け製品を含む）</li>
<li>Physical AI</li>
</ul>
<p>Physical AI部門には、従来の自動車事業が組み込まれ、ロボティクスと車載分野を一体で扱う構成とした。ロボット分野への本格展開に向け、専任人員の増員も計画している。</p>
<h2>ロボットと自動車を一体で捉える、Armの設計思想</h2>
<p>同社幹部は、ロボットと自動車がPhysical AIの中核をなす理由として、両分野が共通の技術要件を持つ点を挙げた。センサー技術、電力制約、安全性や信頼性といった条件が近く、同一の設計思想で対応できる領域だとしている。</p>
<h2>「労働を拡張する市場」──新部門責任者が描くロボット像</h2>
<p>新設されたPhysical AI部門を率いるDrew Henry氏は、Physical AIについて、労働を根本的に補完し、人々の可処分時間を生み出す可能性があると説明した。こうした変化は、結果として国内総生産（GDP）にも影響を与え得るとの認識を示した。</p>
<p>@<a href="https://www.youtube.com/watch?v=OHQJsiS4onk">Youtube</a></p>
<p>また、最高マーケティング責任者（CMO）のAmi Badani氏は、ロボティクス分野に人材を重点投入する方針を明らかにしている。</p>
<h2>壮大なビジョンと足元の一手──Arm公式が描くPhysical AI</h2>
<p>Armは<a href="https://newsroom.arm.com/blog/the-next-platform-shift-physical-and-edge-ai-powered-by-arm">公式ブログ</a>で、Physical AIをクラウド上のAIが現実世界で行動できる段階へ進む「次のプラットフォーム転換」と位置づけている。ロボットや車両、XR（拡張現実）機器など、物理空間で動作するAI全般を射程に入れる構想だ。一方、Reutersの報道では、その中でも最初の成長市場としてロボット分野を明確に打ち出した点が特徴となっている。</p>
<p>CES 2026では、多くの企業がヒューマノイドや産業用ロボットを披露した。自動車メーカーやIT大手もロボット分野への関与を強めており、Armの組織再編は、展示の延長ではなく、ロボット市場を見据えた中長期の経営判断として位置づけられる。</p>
<h2>ロボット市場に照準──Armが選んだPhysical AIの最初の到達点</h2>
<p>ArmはCES 2026の場で、「フィジカルAI」部門を新設し、ロボット市場に照準を定めた組織再編を初めて明らかにした。生成AIを中心とするクラウドの次の段階として、物理世界で動作するAIを成長ドライバーとする姿勢が浮き彫りとなった。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>AIは主役から基盤へ──CES 2026で示された「実装段階」に入ったテクノロジーの姿</title>
      <link>https://ledge.ai/articles/ces2026_ai_design_embedded_devices</link>
      <description><![CDATA[<p>世界最大級のテクノロジー見本市「CES 2026」は、2026年1月6日から9日まで、米国ネバダ州ラスベガスで<a href="https://www.ces.tech/press-releases/ces-2026-the-future-is-here">開催された</a>。主催するConsumer Technology Association（CTA）は、今回のCESを「The Future Is Here（未来はすでにここにある）」と位置づけ、先端技術が構想や実験段階を越え、現実の製品やサービスとして展開されるフェーズに入ったことを強調した。
展示会場では、AIが単独の主役として語られるのではなく、PC、デジタルヘルス、モビリティ、日用品など幅広い分野に組み込まれ、生活や仕事のあり方を具体的に変える基盤技術として提示された。</p>
<p>CES 2026で示されたのは、AIが主役として語られるフェーズを越え、製品や体験の設計思想そのものに組み込まれていく段階に入った姿だった。編集部では、こうした移り変わりを象徴するようなデバイスをいくつかピックアップした。いずれも「AI搭載」を強調するより、計算や推論が製品の内部に溶け込み、使い方そのものを形づくっている点が共通している。これらの製品を通じて、CES 2026で示された“実装段階”の姿を見ていく。</p>
<h2>キーボード一体型AI PC：HP EliteBoard G1a</h2>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/v2_2026_hpeliteboardg1anextgenaipc_hpeliteboard_primary_826008a117/v2_2026_hpeliteboardg1anextgenaipc_hpeliteboard_primary_826008a117.webp" alt="v2_2026_hpeliteboardg1anextgenaipc-hpeliteboard_primary.webp" /></p>
<p>HPは、キーボード一体型のAI PC「EliteBoard G1a」を<a href="https://www.ces.tech/ces-innovation-awards/2026/hp-eliteboard-g1a-next-gen-ai-pc/">発表</a>した。外観はフルサイズキーボードに近いが、内部にCPUに加えてNPUを含む演算基盤を搭載し、外付けディスプレイと接続して使用する構成となっている。本製品はローカルでのAI処理を前提に設計されており、クラウドに依存せず、生成AIや要約、分析といったAIタスクを端末側で実行する用途を想定している。画面を本体から切り離すことで、AI処理を担う計算ユニットを机上に集約する設計思想が示された。
本製品はCES Innovation Awards 2026にも選出されている。</p>
<p><strong>PC → 生活空間AIへの転換</strong>
PC領域でのローカルAI処理を前提とした設計が示された一方で、CES 2026では、AIがディスプレイや入力装置の枠を越え、生活空間そのものに組み込まれる例も複数見られた。</p>
<h2>AIコンパニオン：Lepro Ami</h2>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Lepro_Ami_70cd5a7695/Lepro_Ami_70cd5a7695.jpg" alt="Lepro_Ami.jpg" /></p>
<p>Leproは、デスクトップ型AIコンパニオン「Lepro Ami」を<a href="https://www.newswire.ca/news-releases/lepro-unveils-lepro-ami-at-ces-2026-a-desktop-ai-companion-that-feels-in-the-room--847670849.html">公開</a>した。公式説明によると、Amiは音声入力や周囲の環境情報をAIが解釈し、利用者との対話や反応を行うことを目的としたデバイスである。画面操作を中心とする従来のAIとは異なり、空間内での存在感や応答性を重視した設計が特徴とされる。AI処理の一部はローカルで行う構成が示されており、プライバシーへの配慮も設計要素として挙げられている。日常空間に常駐するAIの形を提示するデバイスとして位置づけられている。</p>
<p><strong>空間AI → 個人データ／ライフログ</strong>
空間に常駐するAIに加え、CES 2026では、個人の行動や状態を継続的に捉えるAIを想定したデバイスも提示された。AIを“使う”存在ではなく、“常にそばにある前提”で設計する動きが浮かび上がる。</p>
<h2>AIライフログ・ペンダント：Motorola Project Maxwell</h2>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Maxwell_Photo_7_1536x1024_36f46681ba/Maxwell_Photo_7_1536x1024_36f46681ba.jpg" alt="Maxwell-Photo-7-1536x1024.jpg" /></p>
<p>Motorolaは、AIライフログデバイス「Project Maxwell」を研究プロジェクトとして<a href="https://motorolanews.com/motorola-unveils-new-flagship-devices-and-ai-powered-innovation-at-lenovo-tech-world-2026/">紹介</a>している。ペンダント型デバイスとして構想されており、利用者が見聞きしている情報をAIが理解・整理することを目的とする。公式リリースでは、視覚や音声といった日常環境データをAIが解析し、個人に合わせた体験や支援につなげる構想が示されている。AIは常時稼働するアシスタントとして振る舞うことを想定しており、Motorola 312 Labsによる実験的な取り組みの一環として位置づけられている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/G_EPFK_2a_MAA_Vy_Iy_216a72ad1f/G_EPFK_2a_MAA_Vy_Iy_216a72ad1f.jpg" alt="G-EPFK2aMAAVyIy.jpg" /></p>
<p><strong>ライフログ → デジタルヘルス</strong>
こうした流れは、デジタルヘルス分野でも顕著だった。CES 2026では、ウェアラブルや医療機器に限らず、日常的な動作や映像を起点に健康状態を把握しようとするアプローチが広がりを見せた。</p>
<h2>ミラー型デバイス：NuraLogix「Longevity Mirror」</h2>
<p>デジタルヘルス技術企業NuraLogixは、ミラー型デバイス「Longevity Mirror」を<a href="https://www.linkedin.com/posts/nuralogix-corporation_ai-longevity-healthtech-activity-7414349139225788416-Ne-N">公開</a>した。製品では、30秒のセルフィービデオをAIが解析し、血流変化などの映像情報から健康関連指標を推定する。同社によると、AIは映像データをもとに「Longevity Index（LIX）」を算出し、心血管疾患リスクや代謝の健康、精神的ストレスなど5つの生理学的領域を統合的に評価するという。ウェアラブルや物理センサーを用いず、カメラ映像とAI解析のみで健康状態の把握を試みる点が特徴とされている。</p>
<p>@<a href="https://www.youtube.com/watch?v=dDem2hR4_1E">YouTube</a></p>
<p>CES 2026には、約14万8,000人が来場し、4,100社以上（うち約1,200社がスタートアップ）が出展した。CTAのGary Shapiro氏は、CESを「世界で最も強力なイノベーションの実証の場」と位置づけ、技術がビジネスや政策、社会と結びつく場であると説明している。また、Kinsey Fabrizio氏は、AIをはじめとする技術がビジョンから実用段階へと移行している点を強調した。
AIが話題の中心となった過去数年を経て、2026年はAIを含む先端技術が、実体あるプロダクトや産業の中で役割を担い始めた段階を示すCESとなった。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>核融合炉「SPARC」の開発をAIデジタルツインで加速―—Commonwealth Fusion Systems、NVIDIA・シーメンスと連携</title>
      <link>https://ledge.ai/articles/cfs_sparc_ai_digital_twin_nvidia_siemens</link>
      <description><![CDATA[<p>核融合スタートアップの Commonwealth Fusion Systems（CFS） は2026年1月6日、NVIDIA および Siemens と提携し、実証用核融合炉 SPARC の設計・開発をAI主導で加速するデジタルツインを構築すると<a href="https://cfs.energy/news-and-media/commonwealth-fusion-systems-accelerates-commercial-fusion-with-siemens-and-nvidia-leveraging-ai-powered-digital-twins">発表</a>した。AIとシミュレーションを中核に据え、従来は物理試験に大きく依存していた核融合炉開発のプロセスを仮想空間上で統合・高速化する。</p>
<h2>AIと産業ソフトウェアを統合したデジタルツイン</h2>
<p>CFSが構築するデジタルツインは、NVIDIAのAIおよび物理シミュレーションライブラリと、Siemensの設計・製造向けソフトウェア群を統合したものとなる。</p>
<p>SiemensのNXやTeamcenterを中心とする設計・PLM（製品ライフサイクル管理）ツールで作成されたデータを、NVIDIAのシミュレーション基盤と連携させることで、SPARCの構造・磁場・熱挙動などを一体的に再現する。</p>
<p>@<a href="https://www.youtube.com/watch?v=4PItOlY6_xE">YouTube</a></p>
<p>この環境では部品単位から炉全体までを単一の仮想モデルで扱うことが可能となり、設計変更や検証を仮想空間上で迅速に繰り返せるという。</p>
<h2>「動く炉」を再現するデジタルモデル</h2>
<p>今回のデジタルツインは、静的な3D設計モデルではなく、実際の運転を想定した動的モデルとして構築される。
電磁場、熱、構造応力といった複数の物理現象を同時に扱い、将来的には実機から得られるデータを反映させることで、仮想モデルと現実の炉の状態を同期させる設計とされている。これにより、実際に装置を製作・試験する前の段階で、設計上の課題や挙動を検証できるようになる。</p>
<h2>高磁場・小型化を支えるAI活用</h2>
<p>SPARCは、高温超電導（HTS）磁石を用いることで、従来よりも小型ながら高磁場を実現する設計を採用している。
CFSはすでに、SPARC向けの初号磁石を完成させ、出荷したことを明らかにしている。この磁石は「空母を持ち上げられるほどの磁力を持つ」と説明されており、高磁場化に伴う構造応力や熱管理が技術的な焦点となっている。</p>
<p>こうした複雑な物理挙動を事前に解析・検証するため、AIとシミュレーションを組み合わせたデジタルツインが設計プロセスの前提となっている。</p>
<h2>実機開発と仮想検証を並行</h2>
<p>CFSは、磁石の製造・据え付けと並行して、デジタルツイン上で炉全体の挙動解析を進める体制を構築している。
物理的な試作・試験と、仮想空間での検証を同時に進めることで、開発工程全体の効率化を図る狙いだ。</p>
<p>デジタルツインは、建設段階にとどまらず、将来的な運転や保守、最適化にも活用されることが想定されている。</p>
<h2>2027年稼働を見据えた基盤整備</h2>
<p>SPARCは、核融合反応による正味エネルギー獲得を目指す実証炉として開発が進められており、2027年の稼働が計画されている。
今回のNVIDIAおよびSiemensとの連携は、設計から運転までをデータとAIに基づいて進める開発基盤を整備する取り組みとして位置付けられる。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2026/1/10 [SAT]OpenAI、健康分野に特化した「ChatGPT Health」公開──医療情報を安全に整理</title>
      <link>https://ledge.ai/articles/chatgpt_health_launch</link>
      <description><![CDATA[<p>OpenAIは2026年1月7日（現地時間）、健康・ウェルネス分野に特化した新機能「ChatGPT Health」を<a href="https://openai.com/ja-JP/index/introducing-chatgpt-health/">発表</a>した。医療記録やウェルネスアプリの情報を活用し、利用者が自身の健康状態を理解しやすくすることを目的とした専用体験を提供する。</p>
<p>ChatGPT Healthは、健康に関する会話専用に設計された機能で、通常のChatGPTのチャットとは分離された空間で利用できる。OpenAIによると、近年ChatGPTには健康やウェルネスに関する質問が急増しており、こうしたニーズに対応する形で専用機能を開発したという。</p>
<p>同機能では、検査結果や診療内容といった医療記録を読み解く支援に加え、運動・食事・睡眠などのウェルネスアプリのデータを統合し、情報を整理することが可能とされている。これにより、医師に相談する際の質問を事前にまとめたり、生活習慣を見直すためのヒントを得たりする用途を想定している。</p>
<p>ChatGPT Healthは、まず少人数の初期ユーザーを対象に提供を開始し、利用状況を踏まえながら改善を続ける。OpenAIは、改善を進めながら数週間以内にウェブ版とiOSのすべてのユーザーへ提供を拡大する予定としている。一方で、電子健康記録（EHR）の連携および一部のアプリは米国のみで利用可能で、Appleヘルスケアの接続にはiOSが必要だとしている。</p>
<p><strong>ChatGPT Healthの画面イメージ：ChatGPT内の専用スペースとして『ヘルスケア』が用意される</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/OAI_Chat_GPT_Health_Wayfinding_16_9_444cbb0458/OAI_Chat_GPT_Health_Wayfinding_16_9_444cbb0458.webp" alt="OAI_ChatGPT_Health_Wayfinding_16-9.webp" /></p>
<p>一方でOpenAIは、ChatGPT Healthが診断や治療を行うものではない点を明確にしている。あくまで医療専門職によるケアを補完する存在であり、最終的な医療判断は医師などの専門家が担うとしている。</p>
<p>ChatGPT Healthは、まず個人向けの健康・ウェルネス支援を目的として提供される。プライバシー面にも配慮し、健康関連の会話は他のチャット履歴から分離して管理される。OpenAIは、こうしたデータが基盤モデルの学習には使用されないことも明らかにしており、利用者が接続するデータを管理できる仕組みを用意したという。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2026/1/11 [SUN]イーロン・マスクのNeuralink、BCI：思考で操作する脳内チップを量産へ　2026年計画を示す</title>
      <link>https://ledge.ai/articles/elon_musk_neuralink_bci_mass_production_2026</link>
      <description><![CDATA[<p>脳と機械を直接つなぐブレイン・コンピューター・インターフェイス（BCI）を開発する米スタートアップ Neuralink が、BCIデバイスの量産を2026年に開始する計画を示した。創業者の イーロン・マスク 氏が2026年1月1日の同氏のXへの<a href="https://x.com/elonmusk/status/2006513491105165411">投稿</a>で明らかにした。</p>
<p>マスク氏は投稿で、NeuralinkがBCIデバイスの高ボリューム生産を2026年に開始し、手術工程を大幅に簡略化した、ほぼ完全に自動化された外科手術へ移行すると説明した。これまで同社のBCIは、主に臨床試験向けに限定的な規模で製造されてきたが、今後はより多くの患者への提供を想定した製造フェーズへの移行を見据える。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/elon_x_post_neuralink2026_87a80d4661/elon_x_post_neuralink2026_87a80d4661.jpg" alt="elon x post neuralink2026.jpg" /></p>
<h2>手術の自動化と侵襲性低減</h2>
<p>マスク氏はあわせて、BCIデバイスに用いられる電極スレッドが、脳を覆う硬膜（dura）を除去することなく貫通する設計になると述べた。従来の脳外科手術では硬膜の切開や除去を伴うケースが多く、これを不要とする構造は、手術の侵襲性を抑える点で重要な意味を持つとされる。同氏はこの点について「大きな進展だ（This is a big deal）」と強調した。</p>
<p>Neuralinkは、ロボットを用いた手術システムの開発を進めており、将来的には手術工程の標準化や効率化、医師の負担軽減を図る構想を示してきた。今回の投稿は、量産体制の構築と並行して、自動化された手術プロセスを実運用に移行させる方針を示したものとなる。</p>
<h2>臨床試験の進展と背景</h2>
<p>マスク氏がリポストした投稿によると、Neuralinkは2025年にかけて、米国以外の地域も含め臨床試験を拡大してきた。重度の発話障害を対象とした技術については、米食品医薬品局（FDA）からブレークスルー・デバイス指定を受けたほか、英国や中東、カナダでの臨床試験や手術も実施されたとされる。</p>
<p>これらの試験では、脊髄損傷などにより身体機能に制約のある患者が、思考によってコンピューターを操作する事例が報告されている。現時点でNeuralinkが想定する主な用途は、神経障害や麻痺を持つ患者の支援であり、一般向け利用についての具体的な計画は示されていない。</p>
<h2>量産フェーズへの転換点</h2>
<p>今回示されたBCIデバイスの量産計画と手術自動化の方針は、Neuralinkが研究・実証段階にとどまらず、実用化と提供規模の拡大を見据えた段階へ進もうとしていることを示すものとなる。今後は、安全性や有効性の検証に加え、各国の規制当局による承認が引き続き重要な要素となる。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2026/1/11 [SUN]『女性をビキニに』指示が問題化、XのAI画像生成・編集機能が有料会員限定に</title>
      <link>https://ledge.ai/articles/grok_bikini_image_generation_paid_only</link>
      <description><![CDATA[<p>「@Grok 女性をビキニにして」などの指示による画像生成が物議を醸していた、XのAI画像生成・編集機能について、X上での利用が有料会員に限定されていることが、2026年1月9日ごろから確認されている。この仕様変更は<a href="https://www.reuters.com/sustainability/boards-policy-regulation/musks-ai-bot-grok-limits-image-generation-x-paid-users-after-backlash-2026-01-09/">Reuters</a>など複数の国内外メディアが報じている。</p>
<p>対象となっているのは、GrokをX上でメンションし、返信欄から画像生成や編集を依頼する機能だ。現在、無料ユーザーが同様の操作を行うと、「画像の生成と編集は現在、有料会員限定です」といったメッセージが表示され、リプライ形式での画像生成・編集は実質的に利用できなくなっている。</p>
<h2>X上の「@Grok」経由のみが制限対象</h2>
<p>複数の報道やユーザー検証によると、今回の制限はGrokの画像生成・編集機能そのものを全面的に停止したものではない。
実際には、以下のような状態が確認されている。</p>
<ul>
<li>Xの返信欄で「@Grok」をメンションして画像生成・編集を依頼する方法は、無料会員ではブロック</li>
<li>一方で、Grokのスタンドアロン版アプリやウェブ版（grok.x.ai）では、無料ユーザーでも画像生成・編集が可能な状態が続いている</li>
<li>Xアプリやウェブ上で画像を直接操作する一部の編集機能についても、無料で利用できるケースが報告されている</li>
</ul>
<p>このため、現時点での変更は「X上のリプライ経由の画像生成・編集のみを有料限定とした」措置とみられている。</p>
<h2>背景に非同意の性的画像生成問題</h2>
<p>今回の仕様変更の背景として、海外メディアは、非同意による性的画像生成の急増を挙げている。
昨年末から年始にかけて、他人の写真を無断で加工し、衣服を除去したり、性的に強調した画像を生成するいわゆる「デジタル脱衣」型のディープフェイクが拡散。女性や未成年を対象とした事例も問題視された。</p>
<p>こうした状況を受け、英国やEU、インドなどで規制当局や政府関係者が懸念を示しており、Reutersなどは、国際的な批判や規制圧力が今回の制限につながった可能性を報じている。</p>
<p>XおよびGrokを開発するxAIは、現時点でこの仕様変更について正式なリリースや詳細な説明を公表していない。
今回の有料化措置については、無料での悪用が難しくなった点を評価する声がある一方、抜け道が残っており根本的な対策とは言えないとする批判も出ていると、複数メディアは伝えている。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>「写真が勝手にビキニ化」AI「Grok」画像編集が炎上──未成年を含む生成も焦点に、各国で規制圧力</title>
      <link>https://ledge.ai/articles/grok_image_editing_nonconsensual_sexualized_images</link>
      <description><![CDATA[<p>2025年の年末から2026年の年始にかけ、米実業家イーロン・マスク氏が設立したAI開発企業 xAI が提供するAI「Grok」をめぐり、実在の人物の写真が本人の同意なく加工され、性的に見える画像としてSNS「X（旧Twitter）」上で拡散する事例が相次いだ。年末年始のタイミングで海外メディアが一斉に報じ、国際的な問題として注目を集めた。</p>
<p><a href="https://www.reuters.com/technology/french-ministers-report-groks-sex-related-content-x-platform-prosecutors-2026-01-02/">Reuters</a> は2026年1月3日（現地時間）、年越し前後にX上でGrokの画像編集機能を用いた投稿が急増したと報道した。他人が投稿した写真に対し、テキストで指示を与えることで人物の服装を変更し、ビキニ姿など性的に見える形へ加工した画像が生成され、公開リプライなどを通じて広く拡散したという。</p>
<p>こうした投稿は特定の著名人に限らず、一般の女性や若年層とみられる人物の写真にも及んだ。生成された画像はX上で即座に表示され、年末年始の利用増加と相まって、短期間のうちに問題が可視化・拡大したとされる。</p>
<p>英紙<a href="https://www.theguardian.com/technology/2026/jan/02/elon-musk-grok-ai-children-photos">The Guardian</a>もこの問題を取り上げた。同紙は、Grokによって未成年を含む人物が「薄着」の状態に加工される画像が生成された事例が確認されたと報じ、非同意の性的表現や児童保護の観点から懸念が高まっていると伝えた。</p>
<p><strong>xAIはGrokをめぐり、安全対策の不備を認め、CSAMは違法で禁止されているとX上で表明した</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/grok_x_9767524026/grok_x_9767524026.jpg" alt="grok x.jpg" /></p>
<p>事態の深刻化を受け、フランスでは年始早々、閣僚がGrokによる性的コンテンツ生成を問題視し、検察当局に通報した。Reutersは、欧州連合（EU）のデジタルサービス法（DSA）との関係も含め、AIを組み込んだプラットフォームの責任を問う動きが各国で強まっていると報じている。</p>
<p>Grokを提供するxAI側は、安全対策に不備があったことを認め、是正措置を進めているとされる。一方、画像編集を含む生成AI機能が急速に一般ユーザーへ開放される中、年末年始に顕在化した今回の問題は、非同意の加工や悪用をどこまで防げるのかという課題を改めて浮き彫りにした。</p>
<p>こうした海外での一連の報道を受け、日本国内でも年末年始にかけてこの問題が紹介され、生成AIの利便性とリスクをめぐる議論が広がった。年の変わり目に一気に表面化したGrokをめぐる騒動は、生成AIとSNSが結びつくことで生じる影響の大きさを示す事例となっている。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>アイスタイル、「@cosme Copilot」を発表　クチコミをAIで解析するSaaS型分析ツール</title>
      <link>https://ledge.ai/articles/istyle_cosme_copilot_ai_analysis</link>
      <description><![CDATA[<p>美容系総合サイト「@cosme（アットコスメ）」を運営する株式会社アイスタイルは2026年1月7日、@cosmeに投稿されたクチコミデータをAIで解析するSaaS型の次世代分析ツール「@cosme Copilot」を<a href="https://www.istyle.co.jp/news/press/2026/01/0107.html">発表</a>した。</p>
<p>「@cosme Copilot」は、@cosmeに登録された商品のうち、1件以上のクチコミが投稿されている商品を対象に、2014年以降のクチコミデータを分析できるツールだ。自ブランドの現状分析やマーケティング施策の振り返り、商品開発、競合比較などへの活用を想定している。</p>
<p>同ツールは、クチコミ分析に特化したAI分析ツールとして設計されており、@cosmeに投稿された美容ユーザーの声をAIが解析。商品評価の傾向やターゲット属性、消費者インサイトを可視化し、意思決定を支えるデータを迅速に提供する。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/9cfcb1236f82d1a9059ea176a742d8a9572cd42f_thumb_850x287_16207_897771cb51/9cfcb1236f82d1a9059ea176a742d8a9572cd42f_thumb_850x287_16207_897771cb51.png" alt="9cfcb1236f82d1a9059ea176a742d8a9572cd42f-thumb-850x287-16207.png" /></p>
<p>アイスタイルによると、「@cosme Copilot」は2025年春にパイロット版を提供開始。グループ会社であるアイスタイルデータコンサルティングが提供するデータコンサルティングサービスを利用するブランドを中心に導入され、使用感に関するヒアリングを通じて機能のアップデートや改修を重ねてきた。今回の正式リリースにより、全てのブランドが利用可能となる。</p>
<p>主な機能として、クチコミの投稿日や投稿者の年齢、職業、性別、肌質タイプなど多様な条件での検索に加え、絞り込んだ大量のクチコミデータをAIが要約する機能を搭載する。これまで手作業で行われていたクチコミ収集や分析作業を短時間で実行できるという。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/d9e1503cba9dd3ab5c008971e6f592ff55594ac4_thumb_850x265_16210_af4e6ce3cb/d9e1503cba9dd3ab5c008971e6f592ff55594ac4_thumb_850x265_16210_af4e6ce3cb.png" alt="d9e1503cba9dd3ab5c008971e6f592ff55594ac4-thumb-850x265-16210.png" /></p>
<p>また、クチコミに含まれる頻出キーワードや共起ワードの可視化、ポジティブ・ネガティブな内容の割合の数値化、時系列での分析も可能だ。商品に対する評価傾向だけでなく、キャンペーン施策への反応など、マーケティング視点での分析にも対応する。</p>
<p>さらに、膨大なクチコミデータをもとに商品のターゲットとなるペルソナを作成する機能も備える。生成したペルソナに対し、「シャンプーで重視する項目は何か」といった質問をチャット形式で行うことができ、回答の根拠となったクチコミも即座に参照できる。</p>
<p>アイスタイルは、AIによるクチコミ分析で得られた気づきや仮説を、@cosmeやECの@cosme SHOPPING、実店舗の@cosme STOREの運営を通じて蓄積してきた顧客データと統合したデータ基盤（CDP）と組み合わせ、マーケティング戦略やCRM施策、商品開発へとつなげるデータコンサルティングサービスも強化するとしている。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>生成AIは「実装フェーズ」へ──ディープラーニング協会・松尾豊理事長が年頭所感で示した2026年のAI論点</title>
      <link>https://ledge.ai/articles/jdla_new_year_message_2026</link>
      <description><![CDATA[<p>2026年1月5日、日本ディープラーニング協会（JDLA）は年始にあたり、理事長で東京大学大学院工学系研究科教授の 松尾豊氏による<a href="https://www.jdla.org/news/260105001/">年頭所感</a>を公表した。生成AIの社会実装が急速に進む中、同所感では2025年の技術的動向を振り返るとともに、2026年に向けたAI活用、人材育成、制度整備の方向性が整理されている。</p>
<p>年頭所感では、2025年を「生成AIが研究や実証の段階を越え、実践的な活用フェーズに入った年」と位置づけた。企業活動や教育、行政など、幅広い分野で生成AIの導入が進みつつあり、特定用途にとどまらない汎用的な技術基盤としての役割が強まっているという。</p>
<p>生成AIはもはや一部の先進的な現場だけの技術ではなく、社会全体を支えるインフラに近い存在になりつつあるとの認識が示された。</p>
<h2>AIエージェントとフィジカルAIの進展</h2>
<p>技術面では、AIエージェントが業務プロセスに組み込まれ始めている点や、実世界と連動するフィジカルAIの進展に言及した。モデル性能の向上に加え、AIが人の業務や現場環境とどのように結びつくかという「使われ方」の変化が顕在化していると整理している。</p>
<p>あわせて、大規模投資やインフラ整備の動きにも触れ、データセンター整備などを含む産業基盤の強化が進んでいる現状を示した。</p>
<h2>生成AIを巡る制度と国際環境の変化</h2>
<p>生成AIの普及に伴い、著作権や倫理などの社会的課題が顕在化している点にも触れられている。海外では新興AI企業の台頭や市場環境の変化が見られ、国際競争が激化しているとした。</p>
<p>国内ではAI関連法制の整備が進み、イノベーションの促進とリスク対応の両立を図る枠組みが整いつつあることが紹介されている。</p>
<h2>AI人材育成と資格制度の役割</h2>
<p>人材面では、JDLAが実施するG検定やE資格といった資格制度に言及した。これらを通じてAIに関わる基礎的・専門的知識を持つ人材の裾野が広がっており、高専DCONなどの実践的な教育施策も含め、人材育成基盤が拡充しているとした。</p>
<p>AI技術の社会実装を支えるためには、技術者だけでなく、AIを理解し活用できる多様な人材の育成が不可欠であるとの認識が示されている。</p>
<h2>2026年に向けて</h2>
<p>年頭所感の締めくくりでは、2026年に向けて「学びと信頼の循環」をさらに広げていく方針が示された。AIと共に成長できる社会の実現を目指し、引き続き産業界・教育機関・行政との連携を進めていくとしている。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>JKT48、生成AIによるメンバー画像の性的加工に公式警告──48時間以内の削除要請、法的措置も示唆</title>
      <link>https://ledge.ai/articles/jkt48_ai_generated_image_sexual_abuse_warning</link>
      <description><![CDATA[<p>年末年始にかけて、生成AIを使い実在人物の画像を本人の同意なく性的に加工・拡散する事例が相次ぐ中、秋元康氏が総合プロデューサーを務めるインドネシアのアイドルグループ JKT48 の運営事務局は2026年1月5日（現地時間）、同様の被害がメンバーに及んでいるとして公式声明を<a href="https://x.com/officialJKT48/status/2007856963942748375">発表</a>した。運営は、不適切なAI生成コンテンツの即時削除と拡散停止を求め、応じない場合には法的措置を取る可能性があると明らかにした。</p>
<h2>生成AIによる不適切コンテンツの作成・流通を確認</h2>
<p>公式Xに掲載された声明によると、運営は複数の報告を受け、メンバーの顔やアイデンティティを用いた不適切、あるいはポルノ的なAI生成コンテンツが作成・拡散されている事例を確認したという。声明では、こうした行為が名誉毀損や侮辱に該当する可能性があると指摘している。</p>
<h2>48時間以内の削除要請と法的対応の可能性</h2>
<p>運営は、声明公開後 2×24時間（48時間）以内 に、該当コンテンツの掲載停止、削除、ならびに恒久的な削除対応を行うよう関係者に要請した。期限内に対応がなされず、コンテンツの流通が継続した場合には、影響を受けたメンバーが法的手続きを取る判断を支持し、運営として法的支援を行う方針も示している。</p>
<h2>対象となり得る行為を具体的に列挙</h2>
<p>声明では、法的対応の対象となり得る行為として、AIを用いたポルノ的コンテンツの制作に加え、その配布・拡散・宣伝、さらにそれらを支持・助長・拡散する投稿やコメントも含まれると明記された。運営は、管理下にあるすべての関係者の安全、尊厳、権利を守ることへの強い姿勢を示している。</p>
<p>JKT48運営は、ファンや関係者に対し、健全で相互に尊重し合う環境づくりへの協力を呼びかけた。生成AIの普及が進む中、著名人やアイドルを対象としたAI悪用への対応について、運営レベルで明確な方針が示された形となる。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/jkt48_x_d95c00ae48/jkt48_x_d95c00ae48.jpg" alt="jkt48 x.jpg" /></p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>感情を削ると推論力も落ちる：AIの成績低下が示した「感情」の役割──中国・天津大学、感情を組み込む新世界モデル「LEWM」を提案</title>
      <link>https://ledge.ai/articles/large_emotional_world_model_emotion_reasoning_ai</link>
      <description><![CDATA[<p>AIから感情に関する情報を取り除くと、感情理解タスクだけでなく、知識問題や推論問題といった一見無関係に見える課題でも成績が低下することが明らかになった。</p>
<p>中国・天津大学の研究チームは、感情が人間の行動や意思決定を左右する重要な要素である点に着目し、感情を明示的に扱う新たな世界モデル「<a href="https://arxiv.org/abs/2512.24149">Large Emotional World Model（LEWM）</a>」を提案した。研究成果は2025年12月30日、論文「Large Emotional World Model」としてarXivに公開された。</p>
<h2>感情を無視すると「合理的だが現実的でない」予測になる</h2>
<p>研究チームはまず、既存の大規模世界モデルに感情を除去するモジュールを組み込み、モデル性能への影響を検証した。その結果、感情や感情分析タスクでは最大8％の精度低下が確認された一方、常識推論ベンチマーク「HellaSwag」や知識理解ベンチマーク「MMLU」でも1〜3％程度の成績低下が見られた。</p>
<p>論文では、落ち込んだ気分の人が衝動買いをする例を挙げ、物理法則や合理性だけを前提とした世界モデルでは、人間の実際の行動を正確に予測できないと指摘している。
感情はノイズではなく、推論全体を調整する「文脈的な信号」として機能していることが示唆された。</p>
<h2>感情を検出・中和する「感情フィルタリング」実験</h2>
<p>研究チームは予備実験として、感情表現を検出・除去する「感情フィルタリングモジュール」を設計した。
このモジュールは、</p>
<ul>
<li>文に感情が含まれるかを判定する<strong>Affect Classifier</strong></li>
<li>感情語を中立表現に置き換える<strong>Affect Adapter</strong></li>
</ul>
<p>から構成され、感情認識と感情中和を同時に学習する二段階方式で訓練されている。</p>
<p>感情を中和したモデルでは、感情タスクの成績が大きく低下するだけでなく、非感情タスクにも影響が及ぶことが確認され、感情情報が広範な認知処理に関与していることが実証されたという。</p>
<p><strong>■ 感情フィルタリングモジュール（予備実験）</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Large_Emotional_World_Model2_d384c0ddce/Large_Emotional_World_Model2_d384c0ddce.png" alt="Large Emotional World Model2.png" /></p>
<h2>感情・行動・状態を結ぶEWHデータセット</h2>
<p>研究では、新たに「Emotion-Why-How（EWH）」と呼ばれるマルチモーダルデータセットも構築された。
EWHは、以下の要素を1つの因果構造として結び付ける点が特徴だ。</p>
<ul>
<li>状態（映像・音声・画像）</li>
<li>感情状態</li>
<li>行動</li>
<li>行動後の状態と感情変化</li>
</ul>
<p>例えば、
「落ち込んだ状態 → 服を買う → 一時的に気分が良くなる」
といった感情を介した状態遷移を、映像・音声・テキスト情報とともに学習できる構造になっている。</p>
<p><strong>■ EWH データセットの概要</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Large_Emotional_World_Model3_72a2e46f4c/Large_Emotional_World_Model3_72a2e46f4c.png" alt="Large Emotional World Model3.png" /></p>
<h2>感情を内包する世界モデル「LEWM」</h2>
<p>LEWMは、従来の世界モデルに感情状態の予測と遷移を組み込んだ点が特徴だ。現在の状態・行動・感情を同時に入力し、次の世界状態と感情の両方を予測する。</p>
<p>論文では、</p>
<ol>
<li>まず感情がどう変化するかを推定</li>
<li>その感情を前提に次の世界状態を予測</li>
</ol>
<p>という因果順序を明示的に設計している。</p>
<p><strong>■ LEWMモデルアーキテクチャの図解</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Large_Emotional_World_Model4_4c9f9b4c2e/Large_Emotional_World_Model4_4c9f9b4c2e.png" alt="Large Emotional World Model4.png" /></p>
<p>実験の結果、LEWMは感情に左右される人間行動の予測精度を向上させつつ、基本的な世界モデルタスクでは従来モデルと同等の性能を維持した。</p>
<h2>感情は「付加情報」ではなく、世界理解の一部</h2>
<p>研究チームは、感情を世界モデルに組み込むことで、より人間に近い行動予測や社会的シミュレーションが可能になると結論付けている。感情を排除した合理的なモデルではなく、感情を含めて世界を理解するモデルが、今後のAIに求められる方向性であることを示す研究といえる。</p>
<p>:::box
[関連記事：AIの\</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>Microsoft、Copilotから直接購入できる「Copilot Checkout」発表　ブランド専用AI接客「Brand Agents」も同時公開</title>
      <link>https://ledge.ai/articles/microsoft_copilot_checkout_brand_agents</link>
      <description><![CDATA[<p>Microsoftは2026年1月8日（米国時間）、AIアシスタント「Copilot」上で商品選定から購入・決済までを完結できる新機能「Copilot Checkout」を<a href="https://about.ads.microsoft.com/en/blog/post/january-2026/conversations-that-convert-copilot-checkout-and-brand-agents">発表</a>した。あわせて、ブランドや小売事業者が自社専用のAI接客を構築できる「Brand Agents」も公開した。</p>
<p>同社は、ユーザーの購買行動において「興味を持つ」段階から「購入を決断する」までの距離が急速に縮まっているとし、検索やサイト遷移を前提としない会話中心の購買体験を提示している。</p>
<h2>Copilotの会話画面がそのまま購入フローに</h2>
<p>@<a href="https://www.youtube.com/watch?v=6X6WOP7_U4Q">YouTube</a></p>
<p>Copilot Checkoutは、Copilotとの自然な対話の中で、商品提案、比較、購入判断、決済までを一貫して行える機能だ。ユーザーは外部のECサイトへ遷移することなく、Copilot上で購入を完了できる。</p>
<p>Microsoftは、ユーザーが質問を投げかけ、条件を整理し、選択肢を比較しながら意思決定に至る一連のプロセスが、単一のインタラクション内で完結する点を特徴として挙げる。従来であれば複数のタブを開き、比較の途中で購入が中断されることも多かったが、Copilot Checkoutでは会話の流れそのものが購買導線となる。</p>
<h2>販売者は取引主体を維持、顧客データも保持</h2>
<p>Copilot Checkoutでは、販売者が引き続き取引主体（merchant of record）となる。Microsoftによると、販売者はトランザクション、顧客データ、顧客との関係性を自社で保持できる。</p>
<p>決済やチェックアウトは、PayPal、Stripe、Shopifyといった既存のコマース基盤と連携して提供される。Shopifyを利用する販売者は、特別な申請や追加の統合を行うことなく、自動的にCopilot Checkoutに対応する。</p>
<p>PayPalのエージェンティックコマース担当VP、マイク・エドモンズ氏は、「Copilotの信頼されたコマース基盤を通じて、数千万規模の販売者がAI時代に対応できる」とコメントしている。</p>
<h2>初期データでは購入率向上を確認</h2>
<p>Microsoftが示した初期データによると、Copilotが関与した購買導線では、以下の傾向が確認されている。</p>
<ul>
<li>Copilotを含む購買体験は、含まない場合と比べ、30分以内の購入数が53％多い</li>
<li>購入意図があるユーザーでは、Copilotを利用した場合、購入に至る可能性が194％高い</li>
</ul>
<p>Copilot Checkoutは、米国においてCopilot.comから段階的に提供が始まっており、BingやMSN、EdgeなどCopilotエコシステム全体への展開も予定されている。</p>
<h2>ブランド独自の接客をAIで再現する「Brand Agents」</h2>
<p>Brand Agentsは、ブランドや小売事業者が自社のトーンや商品知識を反映したAIショッピングアシスタントを構築できる仕組みだ。実店舗における販売員の役割をオンライン上で再現することを想定している。</p>
<p>顧客の質問に対し、ブランドの言葉遣いで応答し、適切なフォローアップ質問を行いながら商品提案や比較を進める。フィルター操作やメニュー選択を強制せず、会話を通じて自然に購買判断へ導く点が特徴とされる。</p>
<h2>Brand Agents導入による効果測定と分析</h2>
<p>@<a href="https://www.youtube.com/watch?v=wg2g-KRo5jk">YouTube</a></p>
<p>Microsoftによると、Brand Agentsを導入した販売者では、エージェントが関与したセッションの方が、関与しない場合と比べて高いエンゲージメントとコンバージョンを示している。</p>
<p>プレミアムスリープウェアを展開するAlexander Del Rossaでは、Brand Agentsが関与したセッションで、非関与セッションと比べて3倍以上のコンバージョン率を記録したという。</p>
<p>Brand AgentsはMicrosoftの分析ツール「Microsoft Clarity」と連携し、エージェントが関与したセッションと通常セッションを比較分析できる。ダッシュボードでは、エンゲージメント率、コンバージョン向上、平均注文額などの指標が可視化される。</p>
<h2>Microsoft Clarityと連携した分析機能</h2>
<p>Brand Agentsは、Microsoftの分析ツール「Microsoft Clarity」と連携する。Clarityはヒートマップやセッションリプレイを通じてユーザー行動を可視化する無料ツールで、Brand Agents導入後は、エージェントが関与したセッションと通常セッションの比較分析も可能になる。</p>
<p><strong>Brand Agentsの効果を可視化するMicrosoft Clarityの分析画面。エージェントが関与したセッションにおけるエンゲージメントやコンバージョン指標を確認できる</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/blog_79982_bodyimage02_833x478_be75122127/blog_79982_bodyimage02_833x478_be75122127.webp" alt="blog-79982-bodyimage02-833x478.webp" /></p>
<p>ダッシュボードでは、エンゲージメント率、コンバージョン向上、平均注文額などの指標を確認でき、販売戦略の改善に活用できるとしている。</p>
<h2>導入と今後の展開</h2>
<p>Copilot Checkoutでは、Microsoft Merchant Centerの活用により、Copilot上での商品露出を最適化できる。Microsoftは、Agentic Commerce Protocol（ACP）といったオープンスタンダードの採用を進め、導入の簡素化とスケーラビリティを図る。</p>
<p>今後はMastercardやVisaといった決済事業者とも連携し、AIを前提としたコマース基盤の拡張を進めるとしている。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>NVIDIA、フィジカルAIと自動運転AIを統合展開──ロボットと車両向けに新モデル群</title>
      <link>https://ledge.ai/articles/nvidia_physical_ai_autonomous_driving_integrated_models</link>
      <description><![CDATA[<p>NVIDIA は2026年1月5日（現地時間）、米ラスベガスで開催中のCES 2026に際し、現実世界で動作する人工知能「フィジカルAI」に向けた新たなAIモデル群を<a href="https://blogs.nvidia.co.jp/blog/nvidia-releases-new-physical-ai-models-as-global-partners-unveil-next-generation-robots/">発表</a>した。</p>
<p>ロボット向けの基盤モデルに加え、安全な推論に基づく自動運転車両開発を支援するオープンソースAI「<a href="https://nvidianews.nvidia.com/news/alpamayo-autonomous-vehicle-development">Alpamayo</a>」ファミリーも同時に公開し、ロボットと車両の双方を対象としたAI基盤の統合展開を進める。</p>
<h2>フィジカルAIを中核に据えたAI基盤を拡充</h2>
<p>NVIDIAが掲げるフィジカルAIは、ロボットや車両といった物理的な存在が、周囲の環境を認識し、状況に応じて判断・行動するためのAI技術を指す。今回の発表では、視覚認識、行動生成、推論といった要素を統合した複数の基盤モデルが示され、現実世界での多様なタスクに対応する汎用的なAI基盤の整備を進める姿勢を打ち出した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/physical_ai_models_1280x680_1_960x510_9fb39ada11/physical_ai_models_1280x680_1_960x510_9fb39ada11.jpg" alt="physical-ai-models-1280x680-1-960x510.jpg" /></p>
<h2>自動運転向けオープンAI「Alpamayo」を公開</h2>
<p>こうしたフィジカルAIの重要な応用分野として位置づけられるのが自動運転だ。NVIDIAは今回、自動運転車両向けのオープンソースAIモデルとツール群からなる「Alpamayo」ファミリーを<a href="https://nvidianews.nvidia.com/news/alpamayo-autonomous-vehicle-development">発表</a>した。Alpamayoは、安全性が強く求められる自動運転開発を想定し、周囲の状況を理解したうえで判断を行う「リーズニング（推論）」能力を重視した設計が特徴とされる。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/nvidia_alpamayo_d1b1006f40/nvidia_alpamayo_d1b1006f40.jpg" alt="nvidia alpamayo.jpg" /></p>
<p>Alpamayoファミリーには、学習済みモデルに加え、シミュレーション環境やデータセットが含まれており、開発者や企業が共通基盤上で検証や改良を進められるよう設計されている。想定される用途には、レベル4自動運転の研究・開発や、現実の走行環境で発生し得る多様なシナリオへの対応などが含まれる。</p>
<p>@<a href="https://www.youtube.com/watch?v=KGCTwoAlhsM">Youtube</a></p>
<p>今回の発表は、ロボット向けAIと自動運転向けAIを個別に展開するものではなく、共通するフィジカルAI基盤のもとで横断的に提供する点に特徴がある。NVIDIAは、モデル、ソフトウェア、シミュレーション環境を含む包括的なAI基盤を通じて、ロボットと車両の両分野における開発を同時に支援していく方針を示した。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>NVIDIA、次世代AI基盤「Rubin」をCESで正式発表──GPU「Rubin」と新CPU「Vera」を含む6チップ構成、推論コストは最大10分の1に</title>
      <link>https://ledge.ai/articles/nvidia_rubin_vera_ces_2026</link>
      <description><![CDATA[<p>NVIDIAは2026年1月5日（米国時間）、CESの開催に際し、AIデータセンター向けの次世代プラットフォーム「Rubin」を<a href="https://nvidianews.nvidia.com/news/rubin-platform-ai-supercomputer">発表</a>した。</p>
<p>新たに設計したCPU「Vera」とGPU「Rubin」を中核に、6種類の半導体を極めて緊密に統合した構成が特徴で、同社は従来世代「Blackwell」と比べ、AIの学習および推論にかかるコストを大幅に削減できるとしている。</p>
<p>Rubinプラットフォームは、NVIDIA Vera CPU、NVIDIA Rubin GPU、NVLink 6 Switch、ConnectX-9 SuperNIC、BlueField-4 DPU、Spectrum-6 Ethernet Switchの6つの新チップで構成される。チップ単体の性能向上に依存するのではなく、CPU、GPU、ネットワーク、ストレージ、セキュリティまでを含めた“プラットフォーム全体”を同時に設計する「エクストリーム・コードザイン」を採用した点が特徴だ。</p>
<p><strong>■ CESの基調講演で、NVIDIAのCEOであるジェンスン・フアン氏が次世代AI基盤「Rubin」を構成する主要コンポーネントを紹介した</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/jensen_vera_rubin2_467cfbac7b/jensen_vera_rubin2_467cfbac7b.jpg" alt="jensen vera rubin2.jpg" /></p>
<p>Rubinという名称は、銀河の回転速度を観測し、ダークマターの存在を示した米国の天文学者ヴェラ・ルービン氏に由来する。NVIDIAは、同氏が宇宙観を大きく変えた存在であることになぞらえ、AIコンピューティングの新たな基盤となるプラットフォームにその名を冠したとしている。</p>
<p><strong>■ CESの基調講演で示された、天文学者ヴェラ・ルービン氏と銀河回転曲線の図。新プラットフォーム「Rubin」は、同氏にちなんで命名された</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/vera_rubin2_0fca679ec3/vera_rubin2_0fca679ec3.jpg" alt="vera rubin2.jpg" /></p>
<p>Rubin GPUは、第3世代のTransformer Engineを搭載し、推論処理に最適化された演算性能を特徴とする。NVIDIAは、演算精度を動的に調整する仕組みにより、推論性能と効率の両立を図っていると説明している。</p>
<p><strong>■ NVIDIAがCESで示した「Rubin GPU」の主な仕様。推論性能やメモリ帯域などで、Blackwell世代からの大幅な向上が示されている</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/nvidia_rubin_cpu_a40d212a8b/nvidia_rubin_cpu_a40d212a8b.jpg" alt="nvidia rubin cpu.jpg" /></p>
<p>一方、新CPU「Vera」は、大規模AIファクトリー向けに設計されたNVIDIA独自のCPUだ。GPUとの高帯域接続を前提とした構成を採用し、従来の汎用CPUとは異なる役割を担うとしている。</p>
<p><strong>■ NVIDIAが新たに設計した「Vera CPU」の概要。88基のカスタムOlympusコアと、GPUとの高帯域接続を前提とした構成が示されている</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/nvidia_vera_cpu_c2ba958e3c/nvidia_vera_cpu_c2ba958e3c.jpg" alt="nvidia vera cpu.jpg" /></p>
<p>システム形態としては、72基のRubin GPUと36基のVera CPUを1ラックに統合した「Vera Rubin NVL72」と、8基のGPUを搭載する「HGX Rubin NVL8」を用意する。いずれもNVLink 6による高帯域接続を特徴とし、ラック単位でAIスーパーコンピューターとして機能する設計だ。</p>
<p>また、Rubinでは冷却や運用面の刷新も図られている。100％液体冷却を前提とした設計により、高密度化を進めつつ、組み立てや保守作業の効率化を実現したという。加えて、CPU、GPU、相互接続を横断してデータを保護するコンフィデンシャル・コンピューティング機能や、信頼性を高めるRAS（信頼性・可用性・保守性）機構も強化されている。</p>
<p>Rubinプラットフォームはすでに量産段階にあり、2026年後半からOEMやクラウド事業者を通じて提供される予定だ。Microsoftの次世代AIデータセンターや、CoreWeaveのAIクラウドなどでの採用も計画されており、NVIDIAはRubinを通じて、学習・推論の両面でAIインフラの次の世代を切り開くとしている。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>LLMは「同じ質問を2回」入力すると精度が上がる──Google研究者ら、プロンプト反復の効果を短報で報告</title>
      <link>https://ledge.ai/articles/prompt_repetition_improves_llm_accuracy</link>
      <description><![CDATA[<p>Googleの研究者らは、同一の質問文を2回連結して入力するだけで、大規模言語モデル（LLM）の回答精度が向上するとする研究成果を発表した。論文は短報「Prompt Repetition Improves Non-Reasoning LLMs」として2025年12月17日に arXivに<a href="https://arxiv.org/abs/2512.14982v1">公開</a>されており、推論（reasoning）を用いない設定において、主要LLMと複数のベンチマークで広範な改善が観測されたという。</p>
<h2>質問文を「そのまま2回」繰り返すだけ</h2>
<p>研究で提案された手法は、質問文を変更・補足するのではなく、同一のクエリをそのまま2回連結して入力するというものだ。例えば、通常はQと入力するところをQQとする。特別な指示文や追加のプロンプト設計は必要としない。論文では、この操作を \u003CQUERY\u003E を \u003CQUERY\u003E\u003CQUERY\u003E に変換するものとして説明している</p>
<p>LLMは因果言語モデルとして学習されており、トークンの並び順が注意（attention）の届き方に影響する。このため、質問文と文脈や選択肢の配置順（question-first / options-first）によって性能差が生じることが知られている。プロンプト反復は、各トークンが他のすべてのトークンを参照しやすくすることで、この差を緩和すると説明されている。</p>
<h2>非推論設定で顕著な改善、70条件中47で「有意に向上」</h2>
<p>実験は、各AIモデルの公式APIを用いて実施され、2025年2月から3月にかけて評価された。対象には、Gemini、GPT、Claude、DeepSeekといった複数の主要LLMが含まれている。具体的には、Gemini 2.0 Flash／Flash Lite、GPT-4o／GPT-4o-mini、Claude 3 Haiku／Claude 3.7 Sonnet、DeepSeek V3が評価対象となった。</p>
<p>論文では、7つのモデルと7つのベンチマークなどを組み合わせた計70条件で比較を行った。その結果、統計検定（McNemar検定、p\u003C0.1）の基準で47条件において性能が有意に改善し、性能が低下した条件はなかったとしている。</p>
<p><strong>推論を用いない設定におけるPrompt Repetitionとベースラインの精度比較。星印は統計的に有意な改善（p\u003C0.1）を示す。70条件中47で改善、悪化は確認されなかった</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/figure1_big2_202ffaa00c/figure1_big2_202ffaa00c.jpg" alt="figure1_big2.jpg" /></p>
<h2>生成トークン数・レイテンシは原則増えず</h2>
<p>論文では、回答精度が向上した一方で、生成トークン数やレイテンシ（応答時間）は多くの条件で増加しなかったと報告されている。反復は並列化可能なprefill（前処理）段階で完結するためだという。これは、生成プロセスそのものではなく、入力を読み込む段階で処理が完結するためだと説明されている。</p>
<p>ただし例外として、非常に長い入力や反復×3などの条件では、Claude系モデルでprefillが重くなり、レイテンシが増える場合がある点も明記されている。</p>
<h2>推論（step-by-step）を有効にした場合は「中立〜わずかに正」</h2>
<p>推論を促す設定（think step by step）では、プロンプト反復の効果は中立からわずかに正にとどまった。28条件中5勝・1敗・22引き分けで、研究者らは「推論モデルはそもそも推論過程の中で入力の再読・反復を行うため」と説明している。</p>
<p>一方で、入力文が非常に長い場合や、同じ質問を3回以上繰り返す設定では、モデルや条件によっては応答時間が増加する可能性も指摘されている。</p>
<h2>追加学習なしで性能を引き出す可能性</h2>
<p>この手法は、追加学習や外部ツールを必要とせず、出力形式も変えないため、既存システムにドロップインで導入可能だとされる。研究チームはこれを「多くのタスクにおけるデフォルト手法の候補」と位置付けている。研究チームは今後の方向性として、反復部分の最適化やKVキャッシュの扱い、非テキストモダリティへの応用などを挙げている。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>5億点の商品群の楽天市場にAIコンシェルジュ導入　エージェント型AI「Rakuten AI」を提供開始</title>
      <link>https://ledge.ai/articles/rakuten_ichiba_ai_concierge_rakuten_ai_launch</link>
      <description><![CDATA[<p>楽天グループは2026年1月5日、同社が運営する楽天市場のスマートフォンアプリに、エージェント型AIツールRakuten AIを搭載したと<a href="https://corp.rakuten.co.jp/news/press/2026/0105_02.html">発表</a>した。ユーザーとの対話を通じてニーズを理解し、商品選びを支援するAIコンシェルジュとして提供する。</p>
<p>導入により「楽天市場」のAIコンシェルジュがユーザーとの対話を通じてニーズを把握し、商品選定のサポートと新たな商品との出会いを生む「ディスカバリーショッピング体験」を提供するという。</p>
<h2>テキスト・音声・画像で条件を入力、対話しながら商品を探索</h2>
<p>ユーザーは、「楽天市場」スマートフォンアプリのホーム画面右下に表示されるアイコンから「Rakuten AI」にアクセスできる。希望予算、購入目的、活用シーンなどを、テキスト、音声、画像を用いて入力することで、欲しい商品を手軽に検索することが可能だ。</p>
<p>AIコンシェルジュからの質問に答えながら対話を進めることで、潜在的なニーズが明らかになりやすくなり、商品提案の精度が高まるとしている。これにより、「楽天市場」に出品されている約5億点の商品群の中から、条件や目的に合った商品を見つけやすくなる。</p>
<h2>商品情報に加え、トレンド情報も反映</h2>
<p>「Rakuten AI」が提案する商品情報には、「楽天市場」内の商品情報や価格比較情報に加え、気候や流行、社会情勢などのトレンドも反映される。これらの情報はウェブの自然検索結果をもとに取り込まれ、ユーザーが買い物をする際に必要な情報を包括的に提示するとしている。</p>
<p>今後は、楽天が運営するEコマースサービスから得られるマーケティングデータを活用し、ユーザーのニーズにより合致した商品提案を行う予定だ。提案精度の継続的な向上を通じて、一人ひとりにパーソナライズされた買い物体験の提供を目指す。</p>
<h2>「AI-nization」戦略の一環として位置付け</h2>
<p>楽天は、AI活用を意味する造語「AI-nization（エーアイナイゼーション）」を掲げ、ビジネスのあらゆる領域でAI活用を推進している。「Rakuten AI」は、楽天エコシステムにおける顧客体験の向上と日常生活のサポートを目的に設計されたエージェント型AIツールだ。</p>
<p>AIエージェントを通じてユーザーを楽天エコシステム内のさまざまなサービスにつなぎ、よりパーソナルな体験を提供することを目指しており、今回の「楽天市場」アプリへの搭載は、その展開における重要な一歩と位置付けられている。</p>
<p>楽天は今後も、「楽天市場」におけるAI活用の推進や「Rakuten AI」との連携を通じて、利便性が高く、楽しい買い物体験の提供を目指すとしている。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>Roborock、階段を上り下りできるロボット掃除機「Saros Rover」発表──CES 2026で公開</title>
      <link>https://ledge.ai/articles/roborock_saros_rover_stair_climbing_robot_vacuum_ces2026</link>
      <description><![CDATA[<p>中国のスマート家電メーカー Roborock は2026年1月6日（現地時間）、米ラスベガスで開催された CES 2026 において、階段を上り下りできるロボット掃除機 Roborock Saros Rover を<a href="https://newsroom.roborock.com/gl/news/ces-2026-roborock-releases-the-world-s-first-robotic-vacuum-with-wheel-leg-architecture-as-it-joins-hands-with-real-madrid-football-club-">発表</a>した。脚と車輪を組み合わせた独自構造を採用し、これまでロボット掃除機が対応できなかった階段環境での清掃を想定したモデルとして紹介されている。</p>
<p>Saros Roverは、脚の動きと車輪走行を組み合わせることで段差を乗り越える構造を特徴とする。Roborockはこの仕組みを「wheel-leg architecture」と呼び、階段や複雑な段差を含む住環境での移動と清掃を可能にすると説明している。従来のロボット掃除機では階段は落下リスクのある障害物として回避されてきたが、本モデルでは階段そのものを清掃対象として扱う点が特徴だ。</p>
<p>@<a href="https://www.youtube.com/watch?v=SuyBti4YC-Y">YouTube</a></p>
<p>同社は公式YouTubeチャンネルで、Saros Roverの動作を示すデモ動画も公開した。動画では、本体が階段の段差を認識し、脚部を使って一段ずつ姿勢を調整しながら上り下りする様子が示されている。移動と同時に階段表面を清掃する動作も確認でき、Roborockが想定する利用シーンを視覚的に示す内容となっている。</p>
<p>RoborockはSaros RoverをCES 2026ではコンセプトモデルとして紹介しており、現時点で発売時期や価格、量産計画などの詳細は明らかにしていない。階段環境での移動と清掃を想定したロボット掃除機として、その技術的方向性を示す展示となった。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>Samsung、Gemini連携で家電すべてをAI前提に再設計──CES 2026で「Family Hub」進化を発表</title>
      <link>https://ledge.ai/articles/samsung_gemini_ai_appliances_family_hub_ces_2026</link>
      <description><![CDATA[<p>Samsung Electronicsは2026年1月5日（現地時間）、世界最大級の家電見本市CES 2026の関連イベント「The First Look」において、同社のAI家電戦略を<a href="https://news.samsung.com/global/samsung-presents-your-companion-to-ai-living-at-the-first-look-during-ces-2026">発表</a>した。その中で、AI冷蔵庫「Bespoke AI Refrigerator Family Hub（以下、Family Hub）」のアップグレードを明らかにし、生成AI「Gemini」との連携を前面に打ち出した。冷蔵庫にとどまらず、AIを基盤とした家電・サービス全体の再設計を進める方針を示している。</p>
<p>@<a href="https://www.youtube.com/watch?v=hPdEMp1fOA4">YouTube</a></p>
<h2>CES 2026で示した「全家電AI前提」戦略</h2>
<p>SamsungはCES 2026で、「Your Companion to AI Living」というビジョンを掲げ、家庭内のさまざまな家電やサービスをAIを基盤とした形で再定義する構想を示した。家電を個別に高度化するのではなく、生活全体を支援するAI基盤として再設計することを目指す。
Family Hubは、その戦略を象徴する存在として位置付けられている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/samsung_your_companion_to_620a47a6ee/samsung_your_companion_to_620a47a6ee.jpg" alt="samsung your companion to.jpg" /></p>
<h2>Geminiと連携するAI Visionを強化</h2>
<p>刷新されたFamily Hubでは、Googleの生成AI「Gemini」と連携した「AI Vision（AI Food Manager）」機能を強化した。冷蔵庫内のカメラで食材を認識し、出し入れの状況を自動で把握することで、在庫管理をAIが担う。
Samsungは、こうした認識・判断・提案といったAIの役割を、冷蔵庫に限らず他の家電にも広げていく考えだ。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/AI_Vision_build_with_google_gemini_bc2a52f89f/AI_Vision_build_with_google_gemini_bc2a52f89f.jpg" alt="AI Vision build with google gemini.jpg" /></p>
<h2>食材管理から献立提案までをAIが自動化</h2>
<p>AI Visionで取得した食材データをもとに、Family Hubは利用可能な食材に応じたレシピや献立を提案する。あわせて、食材の使用傾向や消費状況をまとめる「FoodNote」機能も提供される。
家電がユーザーの行動を理解し、次の行動を提案するという設計思想が、冷蔵庫にも取り入れられた。</p>
<h2>家族単位で最適化されるUIと音声操作</h2>
<p>Family Hubは音声認識による「Voice ID」に対応し、家族それぞれを識別する。これにより、カレンダーやニュース、天気、健康関連情報などを、利用者ごとに最適化して表示できる。
また、音声アシスタント「Bixby」を用いた操作も強化され、冷蔵庫のドアを音声で開閉するハンズフリー操作にも対応する。</p>
<h2>冷蔵庫を起点に広がるAI家電の再設計</h2>
<p>Samsungは、今回のFamily Hubの進化を、全家電をAI前提で再設計する取り組みの一例と位置付けている。冷蔵庫を家庭内の情報と行動の起点とすることで、家電同士やサービスとの連携を拡張し、AIが日常生活を支援する環境の構築を進めるとしている。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>AGIは来ない、バブルは続かない──スタンフォード大 HAI研究所が示す2026年のAI、過剰期待の時代は終わり、評価フェーズへ</title>
      <link>https://ledge.ai/articles/stanford_hai_ai_2026_evaluation_phase</link>
      <description><![CDATA[<p>巨額投資と急速な技術進展が続いてきたAI分野は、2026年に転機を迎える可能性がある。Stanford Human-Centered AI Institute（HAI）は2025年12月15日（米国時間）、同研究所に所属する研究者らの予測をまとめた記事を<a href="https://hai.stanford.edu/news/stanford-ai-experts-predict-what-will-happen-in-2026">発表</a>し、AIをめぐる議論は「できるかどうか」から「どの程度役に立つのか」を問う評価フェーズへ移行するとの見方を示した。</p>
<p>同記事では、計算機科学、医学、法学、経済学など複数分野の研究者が共通して、過剰な期待や宣伝が先行してきたAI開発のあり方に転換点が訪れていると指摘している。</p>
<h2>AGIは2026年にも実現しない、AI主権が主要テーマに</h2>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/James20_Fall202022_aefa4ec674/James20_Fall202022_aefa4ec674.webp" alt="James20Fall202022.webp" /></p>
<p>HAI共同ディレクターで計算機科学教授のJames Landay氏は、2026年に汎用人工知能（AGI）が実現することはないと明言した。その上で、各国が自国のデータや計算資源を管理する「AI主権（AI Sovereignty）」への関心が急速に高まると予測している。</p>
<p>AI主権の形は一様ではなく、自国で大規模言語モデル（LLM）を構築するケースもあれば、他国が開発したモデルを自国内のGPU上で運用し、データを国外に出さない方式も含まれる。HAIでは、こうした複数の主権モデルを整理・分析する研究にも取り組んでいるという。</p>
<p>一方で、世界各地で進む大規模データセンター投資については、投機的な側面も指摘されている。Landay氏は、AI関連投資が無制限に拡大し続けるわけではなく、バブル的な様相が意識される局面に入るとの見解を示した。</p>
<h2>生産性向上は限定的、失敗するAIプロジェクトが増加</h2>
<p>2026年には、AIがもたらす生産性向上について、より冷静な評価が広がるとみられている。プログラミング支援やコールセンター業務など一部の領域では効果が確認される一方、多くのAI導入プロジェクトは期待した成果を上げられない可能性があると指摘された。</p>
<p>その結果、企業や組織は「AIをどこに適用すべきか」という選別を迫られ、成功確率の高い用途に資源を集中させる動きが強まるとみられる。</p>
<h2>巨大モデルの限界と、高品質データ重視への転換</h2>
<p>HAIの研究者らは、モデルの巨大化が必ずしも性能向上につながらなくなりつつある点にも言及している。データの量的枯渇や品質低下が課題となる中、より小規模でも高品質なデータセットを用いたモデル開発への関心が高まると予測されている。</p>
<p>この流れは、計算資源や環境負荷への懸念とも結びつき、AI開発の効率性を重視する方向性を後押しする可能性がある。</p>
<h2>医療・科学分野で進む「ブラックボックス」の解体</h2>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/russ_altman_1_be03daceca/russ_altman_1_be03daceca.webp" alt="russ_altman_1.webp" /></p>
<p>科学・医療分野では、AIモデルの予測精度だけでなく、なぜその結論に至ったのかを説明できることが強く求められるようになる。HAI上級フェローのRuss Altman氏は、高性能なニューラルネットワーク内部を解析し、重要な特徴や判断根拠を明らかにする研究が進展すると見ている。</p>
<p>医療分野では、自己教師あり学習の進展により、大規模かつ高品質な医療データを用いた基盤モデルが登場し、診断精度の向上や希少疾患への応用が広がる可能性も示された。</p>
<h2>法務、経済分野でも「測るAI」へ</h2>
<p>法務分野では、「文章を書けるか」ではなく、正確性やリスク、業務効率への寄与といった具体的な成果を評価する指標が重視される見通しだ。複数文書を横断して推論する高度なタスクに対応するAIの評価手法も整備されつつある。</p>
<p>また、経済分野では、AIが雇用や生産性に与える影響を職種・タスク単位で可視化する「AI経済ダッシュボード」が登場し、政策立案や企業経営に活用される可能性があるとされている。</p>
<h2>人間中心のAI設計が問われる段階へ</h2>
<p>HAIの研究者らは、AIが人間の思考力や判断力、長期的な成長に与える影響にも目を向ける必要があると指摘する。短期的な利便性や満足度ではなく、人間の能力をどのように補完し、育てるのかを前提とした設計思想が、今後のAI開発で重要になると結論づけている。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>都営バス、AI翻訳透明ディスプレイを実装へ──浅草で多言語対応と運転士負担軽減を検証</title>
      <link>https://ledge.ai/articles/toei_bus_ai_translation_transparent_display_pilot</link>
      <description><![CDATA[<p>東京都は2026年1月6日、都営バスの運転席周辺に「AI翻訳透明ディスプレイ」を設置する導入実証を、同年1月から浅草エリアで開始すると<a href="https://www.kotsu.metro.tokyo.jp/pickup_information/news/bus/2026/bus_p_2026010612338_h.html">発表</a>した。</p>
<p>訪日外国人の増加に伴う多言語対応ニーズの高まりや、バス運転士の担い手不足を背景に、多様な利用者とのコミュニケーションにおける有効性・利便性を検証するとともに、運転士の負担軽減や今後の導入に向けた課題整理を行う。</p>
<p>同実証では、バスの運転席周辺にAI翻訳透明ディスプレイを設置し、利用者と運転士のやり取りをリアルタイムで翻訳・表示する。外国人利用者に加え、聴覚・言語障害者を含む、誰もが安心して利用できる公共交通環境の実現を目指す。</p>
<p>今回の取り組みは、2025年11月に渋谷エリアで実施した導入実証に続くものとなる。訪日外国人利用者が特に多い浅草エリアを対象とすることで、より幅広い利用者からの意見を収集し、利便性や運用上の課題をさらに検証する。</p>
<p>実証実験の実施期間は2026年1月15日から29日まで。対象路線は、都02（大塚駅～錦糸町駅）、草63（池袋駅東口～浅草寿町）、草64（池袋駅東口～浅草雷門南）の3系統。AI翻訳透明ディスプレイは、民間事業者として<strong>TOPPAN株式会社</strong>が提供する。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/bus_p_2026010612338_h_05_5d029b5d44/bus_p_2026010612338_h_05_5d029b5d44.png" alt="bus_p_2026010612338_h_05.png" /></p>
<p>実証を通じて東京都は、多言語対応の実効性や現場での運用負荷を検証し、公共交通へのAI技術の本格導入に向けた検討を進めるとしている。なお、本取り組みは「2050東京戦略」における「インフラ・交通」分野の施策の一環として位置づけられている。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>TSMC、最先端2nm半導体の量産を開始──次世代プロセスが本格稼働</title>
      <link>https://ledge.ai/articles/tsmc_2nm_mass_production_start</link>
      <description><![CDATA[<p>半導体受託生産（ファウンドリー）世界最大手の台湾積体電路製造（TSMC）は、回路線幅2ナノメートルの最先端半導体について、計画通り2025年第4四半期に、量産を<a href="https://www.tsmc.com/english/dedicatedFoundry/technology/logic/l_2nm">開始した</a>。TSMCは2025年12月31日付で更新した公式サイトの2nm（N2）技術ページで、N2が2025年4Qに量産を開始したと記載している。</p>
<p>公式サイトで公開している2ナノ（N2）技術の説明によると、同プロセスは従来世代からトランジスタ構造を刷新した次世代プロセスに位置づけられている。微細化だけでなく、電流制御の方式そのものを改めることで、性能と消費電力の両面で世代全体としての改善を図る点が特徴だ。</p>
<p>量産開始は、研究開発や試験的な生産段階を終え、実際の製品向けに本格的な製造が始まったことを意味する。これにより、半導体メーカーやシステム開発企業は、2ナノ世代を前提とした製品設計や供給計画を具体的に進められる段階に入った。</p>
<p>TSMCは2ナノ技術について、密度とエネルギー効率の両面で業界最先端の水準になるとしている。性能向上に加え、消費電力の効率性が重視される計算分野を中心に、次世代プロセスの重要性は高まっている。</p>
<p>2ナノプロセスの生産は、台湾の先端製造拠点で進められており、新竹のFab 20や高雄のFab 22が主要な製造施設とされている。TSMCは今後も、先端プロセスへの投資と技術開発を継続し、次世代半導体の量産体制を段階的に強化していく方針だ。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>ユニバーサル・ミュージック、NVIDIAとAI分野で提携──数百万曲の公式カタログを基盤に「責任ある音楽AI」を共同開発</title>
      <link>https://ledge.ai/articles/umg_nvidia_responsible_music_ai_partnership</link>
      <description><![CDATA[<p>Universal Music Group（UMG）は2026年1月6日（現地時間）、NVIDIAと、音楽分野における「責任あるAI（Responsible AI）」の開発を目的とした協業を<a href="https://www.universalmusic.com/universal-music-group-to-transform-music-experience-for-billions-of-fans-with-nvidia-ai/">発表</a>した。</p>
<p>NVIDIAのAIインフラと、数百万曲規模に及ぶUMGの公式音楽カタログを組み合わせ、音楽の発見、創造、ファンエンゲージメントを拡張する。</p>
<p>UMGによると、今回の協業は、世界中で音楽を楽しむ数十億人のリスナー体験を高度化することを目的とするものだ。両社は共同で研究開発を進め、音楽創作の発展と、権利者への正当な対価還元を両立させることを共通の目標に掲げている。</p>
<h2>音楽理解AI「Music Flamingo」を拡張</h2>
<p>協業の中核となるのが、NVIDIAが開発する音楽理解AIモデル Music Flamingo だ。Music Flamingoは、最大15分に及ぶフルトラックを処理でき、ハーモニーや楽曲構造、音色、歌詞、文化的背景、感情の流れといった要素を総合的に理解する。</p>
<p>UMGとNVIDIAは、このモデルをUMGの膨大な公式カタログに対応させることで、ジャンルやタグといった従来の分類を超え、楽曲の文脈や感情的ナラティブに基づいた新しい音楽発見体験の実現を目指すとしている。</p>
<h2>ファン体験とアーティスト支援の両立</h2>
<p>AI技術は、アーティストとファンの関係性を拡張する用途にも活用される。アーティストは自身の楽曲をより深く分析・表現できるツールを活用でき、ファン側は単なる検索やプレイリストを超えた、よりインタラクティブな音楽体験を得られるという。</p>
<p>また、既存アーティストのエンゲージメント強化に加え、新進アーティストが適切なリスナーに発見される機会を広げる狙いもある。</p>
<h2>アーティスト主導のAI開発体制</h2>
<p>両社は、アーティスト、作曲家、プロデューサーが直接関与する専用のAIインキュベーターを設立する。実際の制作現場に即した形でAIツールを共同設計・検証し、独創性や真正性を重視した活用を進めることで、画一的なAI生成物を避ける方針だ。</p>
<h2>著作権と帰属を重視した「責任あるAI」</h2>
<p>UMGとNVIDIAは、AIの活用と同時に、著作権保護や楽曲の正確な帰属を重視する姿勢を強調している。AIによる音楽利用において、アーティストの権利を守りつつ、透明性と信頼性を確保することを協業の前提条件と位置付けた。</p>
<h2>スタジオ資産も活用した研究体制</h2>
<p>UMGのMusic &amp; Advanced Machine Learning Lab（MAML）は、これまでもNVIDIAのAIインフラを活用してきた。今回の協業では、Abbey Road Studios や Capitol Studios といった世界的スタジオも研究・制作環境として活用し、レーベルや出版社を含む幅広い関係者の知見を反映させる。</p>
<p>UMGとNVIDIAは、公式音楽カタログを基盤としたAI活用を通じ、音楽産業における技術革新と権利保護の両立を図るとしている。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>Boston Dynamics、Google DeepMindとAI研究で提携──Atlasを軸にロボット知能を共同開発</title>
      <link>https://ledge.ai/articles/xboston_dynamics_google_deepmind_ai_partnership_202601</link>
      <description><![CDATA[<p>米ロボット企業の Boston Dynamics は2026年1月5日（米太平洋時間）、人工知能（AI）研究機関 Google DeepMind と新たなAIパートナーシップを締結したと<a href="https://bostondynamics.com/blog/boston-dynamics-google-deepmind-form-new-ai-partnership/">発表</a>した。両社は、ロボットの知覚・判断・動作を統合するAI技術の研究開発を共同で進める。</p>
<p>今回の提携は、Boston Dynamicsが開発するロボットプラットフォームと、Google DeepMindが長年取り組んできた強化学習やロボティクス向けAI研究を結びつけることを目的とする。発表によると、研究は実世界で動作するロボットを前提に進められ、シミュレーションと実機を往復する形で学習手法の高度化を図るという。</p>
<p>両社は、電動化された二足歩行ロボット「Atlas」を主要な研究対象の一つとして位置付けている。Atlasは、人間に近い可動域と運動能力を備えたプラットフォームとして、現実環境での動作学習や制御アルゴリズムの検証に用いられる。DeepMindはこれまで、仮想環境を活用したロボット制御や強化学習の研究を進めてきたが、今回の提携では実機ロボットを用いた検証が本格化する形となる。</p>
<p>発表では、ロボットが周囲の環境を認識し、状況に応じて行動を調整するための知能の実装が重視されている。単一のタスクに特化した制御ではなく、複数の条件や変化に対応できる汎用的な能力の獲得を目指すとしている。</p>
<p>Boston Dynamicsはこれまでも、ロボットの知能に関する研究において、外部の技術や研究機関と協力してきた。例えば、同社の四足歩行ロボット「Spot」は、外部のAI技術と組み合わせた研究・実験のプラットフォームとして活用されてきたほか、2024年には Toyota Research Institute と、Atlasを用いた汎用ヒューマノイド研究での提携を発表している。同社は、ロボットの用途や研究目的に応じて、知能部分を柔軟に組み合わせるアプローチを取ってきた。</p>
<p>今回のパートナーシップは研究および技術開発を目的としたもので、企業買収や資本関係の変更を伴うものではない。Boston Dynamicsは、引き続き自社のロボット開発を主導しつつ、DeepMindのAI研究成果を取り入れる形で協力を進める。</p>
<p>ロボット工学とAI研究を統合する動きは近年加速しており、身体を持つAI、いわゆる「Physical AI」への関心も高まっている。Boston DynamicsとGoogle DeepMindは、今回の提携を通じて、現実世界で機能するロボット知能の研究を前進させるとしている。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>LLMは「大きいほど脳に近い」？──Meta研究、脳活動との対応がスケールと文脈長で強まると報告</title>
      <link>https://ledge.ai/articles/xllm_brain_alignment_scaling_and_context_meta_study</link>
      <description><![CDATA[<p>Metaの研究チームは、人間が物語を聞いている最中の脳活動と大規模言語モデル（LLM）の内部表現を比較した結果、モデルの規模や入力文脈の長さに応じて、脳の言語処理と対応する計算構造が現れるとする研究成果を<a href="https://arxiv.org/abs/2512.01591">発表</a>した。論文「Scaling and context steer LLMs along the same computational path as the human brain」は、2025年12月にarXivで公開されている。</p>
<h2>脳活動とモデル内部表現をどう対応づけたのか</h2>
<p>研究では、被験者3人が約10時間にわたってオーディオブックを聴取する際の脳活動を「脳磁図（MEG）」を用いて計測した。
取得した脳信号は、単語の出現タイミングに同期させて解析され、言語刺激に対する時間分解能の高い反応として整理された。</p>
<p>一方、同じテキストを22種類の言語モデルに入力し、各層の内部表現を抽出。脳信号からモデル内部表現を予測する線形写像を学習し、その予測精度をもとに、どのモデル層が、脳のどの時間帯の反応と対応するかを評価した。</p>
<p><strong>図：脳活動とLLM内部表現の対応付け手法の概要</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/method_figure_tmax_923f1bc517/method_figure_tmax_923f1bc517.png" alt="method_figure_tmax.png" /></p>
<h2>層の深さと脳反応の時間順序に見られた対応関係</h2>
<p>解析の結果、多くのモデルにおいて、浅い層ほど脳の早い反応と、深い層ほど遅い反応と対応する傾向が確認されたという。これは、言語処理における計算の進行順序が、LLMと人間の脳で部分的に一致している可能性を示すものとされる。</p>
<p>研究では、この層の深さと脳反応のピーク時間との対応を「temporal alignment（時間的整合）」と呼び、単なる相関ではなく、計算の段階構造が揃っているかどうかを捉える指標として位置づけている。</p>
<h2>モデル構造の違いが示した共通点と相違点</h2>
<p>こうした時間的整合は、Transformer系モデルだけでなく、状態空間モデル（Mamba）や再帰型モデル（RecurrentGemma）でも観測された。</p>
<p>一方で、BERTやRoBERTa、wav2vec 2.0といった双方向モデルでは、脳活動との対応自体は一定程度見られるものの、計算の時間順序に関する整合は弱く、統計的に有意ではないケースが多かったという。</p>
<p>研究チームは、モデルが「未来の単語も参照できる」双方向構造を持つことが、時間順序の対応を弱める可能性を指摘している。</p>
<h2>モデル規模の拡大で現れた“対応の立ち上がり”</h2>
<p>モデルサイズの影響については、パラメータ数のみを段階的に変化させたPythiaモデル群を用いて検証された。その結果、最小規模の14Mパラメータモデルでは時間的整合は有意に確認されなかった一方、12Bパラメータモデルでは非常に強い整合が示された。</p>
<p>この対応の強まりは、モデルサイズの増加に対して対数的に進み、一定規模を超えると伸びが緩やかになる傾向も見られたという。</p>
<h2>文脈情報が計算対応に与える影響</h2>
<p>研究では、入力する文脈の長さも重要な要因として検証された。Llama-3.2（3B）を用いた実験では、文脈をほとんど与えない条件では時間的整合は弱く、文脈長を1000語程度まで拡張すると、脳活動との対応が大きく強まった。</p>
<p>この効果はMambaモデルでも同様に確認されており、文脈情報の蓄積が、脳に近い計算順序を形成する要因になっている可能性が示唆されている。</p>
<h2>単語予測の容易さだけでは説明できない点</h2>
<p>研究チームは、こうした対応が単に「次の単語を予測しやすいかどうか」によって生じている可能性も検討した。その結果、予測可能性の高低で単語を分けても、時間的整合の傾向は維持されており、単純な次トークン予測の難易度だけでは説明できないと結論づけている。</p>
<h2>研究が示唆することと、残された課題</h2>
<p>研究は、LLMのスケールや文脈処理能力の拡張が、人間の脳における言語処理の計算構造と対応する方向へ作用する可能性を示した。一方で、被験者数が限られている点や、MEGの空間分解能の制約、感覚入力を伴わないテキストモデル中心の分析である点など、今後の検証課題も挙げられている。</p>
<p>研究チームは、今後さらに多様なモデルや条件での比較を通じて、言語モデルと人間の認知過程の関係を精査していくとしている。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
  </channel>
</rss>