<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
  <channel>
    <title>Ledge.ai 新着記事</title>
    <link>https://ledge.ai/</link>
    <description>Ledge.ai の最新テクノロジー記事</description>
    <item>
      <title>NEC、警察・消防の指令室を支援する次世代AI技術を開発──独自のAgentic AIで状況把握から行動提案まで自動化</title>
      <link>https://ledge.ai/articles/necs_agentic_ai_emergency_command_support</link>
      <description><![CDATA[<p>NECは2025年12月2日、警察・消防などの緊急通報を受ける指令室業務を高度化する次世代支援技術を開発したと<a href="https://jpn.nec.com/press/202512/20251202_03.html">発表</a>した。独自のAgentic AIを核とし、通話内容の理解、状況分析、行動提案までを一連で支援することで、オペレーターの負担軽減と業務精度の向上を図る。同技術はマルチ言語に対応し、2026年度中の国内外での活用を見込む。</p>
<p>発表によると、指令室での緊急通報対応は高い専門性が求められ、通報者の説明が曖昧になりやすいなど状況把握が難しいケースも多い。NECはこうした課題に対し、リアルタイム解析で重要情報を抽出し、次にとるべき行動を提案する仕組みを導入することで、迅速かつ正確な判断を支援する。</p>
<p>新技術では、複雑な会話でも誤情報の混入や途中の状況変化を踏まえて正確に把握できる点が特徴だという。NEC独自のAgentic AIが「会話分析」「情報管理」「行動提案」など複数の専門AIを連携させ、多角的に文脈を解釈。熟練オペレーターに近い安定した精度での応対を実現する。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/0203_03_d844de8bb6/0203_03_d844de8bb6.jpg" alt="0203-03.jpg" /></p>
<p>また、説明可能なAI（Explainable AI）を搭載し、AIが提示した判断根拠を確認できる点も特徴とされる。人命に関わる現場での信頼性を確保し、オペレーターが適切な判断を行うための支援を行う。さらに、EUのAI規制法を含む国際基準にも適応できるアーキテクチャを採用し、英語を含むマルチ言語に対応するなど、グローバル展開も可能としている。</p>
<p>高い汎用性も強みで、Agentic AIを役割ごとに分割した構造により、業務要件に応じたチューニングを低コストかつ短期間で実施可能。警察・消防の指令室業務にとどまらず、企業のコンタクトセンターなど幅広い業務への適用を想定している。</p>
]]></description>
      <pubDate>Wed, 10 Dec 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>AIで“裸眼3D”が進化──上海AI研究所らが超広視野角ディスプレイ「EyeReal」を開発、Natureに発表</title>
      <link>https://ledge.ai/articles/eyereal_ai_glasses_free_3d_ultrawide_display_nature</link>
      <description><![CDATA[<p>上海人工智能実験室と復旦大学の研究チームは、専用メガネなしで広視野角の3D映像を表示できる新方式「EyeReal」を開発し、科学誌 Nature に<a href="https://www.nature.com/articles/s41586-025-09752-y">発表</a>した。従来のホログラフィック方式や自動立体（autostereoscopic）ディスプレイが抱えていた物理的制約を、AIと多層LCD構造を組み合わせた新たな光場生成手法で克服した点が特徴だ。</p>
<h2>従来の裸眼3D方式の限界を超えるアプローチ</h2>
<p>過去の裸眼3D技術は、視野角や画面サイズなどのトレードオフが大きな課題だった。ホログラフィック方式は高密度の光学情報を扱える一方、視域が極めて小さく、実用性が限定される。一方、ビューセグメント型・ビューデンス型の自動立体ディスプレイは、視点数や視域が固定され、観察者が自由に動いた際に自然な視差変化を再現することが難しかった。</p>
<p>EyeReal は、こうした従来方式を図示しつつ、観察者の目の周囲に最適化された光場をリアルタイム生成するというアプローチを採用する。</p>
<p><strong>従来方式（a〜c）が視野角や表示領域に制約を抱える中、EyeReal（d）は観察者の眼のまわりで光場を最適化し、超広視野角の裸眼3D表示を可能にする</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/41586_2025_9752_Fig1_7a43aeeee1/41586_2025_9752_Fig1_7a43aeeee1.jpg" alt="41586_2025_9752_Fig1.jpg" /></p>
<h2>EyeReal の構造：多層LCD × AIによる光場最適化</h2>
<p>研究チームは、市販の液晶パネルを複数枚重ね合わせた構造を採用。光源、偏光板（縦・横）、液晶層の組み合わせにより、各ピクセルからの位相（フェーズ）を精密に制御する。</p>
<p>RGB-D センサーが観察者の両眼位置と姿勢（6D pose）を取得し、ニューラルネットワークがその位置に最適な「位相パターン」を生成する。これにより、従来の光学系では物理的に困難だった複雑な光場を、液晶ディスプレイのみで再現できる。</p>
<p><strong>多層LCD、偏光板、RGB-D センサー、ニューラルネットワークで構成される EyeReal の光場生成システム。両眼位置の推定から位相パターン生成までをリアルタイムで行う</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/41586_2025_9752_Fig2_d941741a66/41586_2025_9752_Fig2_d941741a66.jpg" alt="41586_2025_9752_Fig2.jpg" /></p>
<h2>動きに応じて自然に変化する視差と焦点表現</h2>
<p>実験では、都市景観や屋内シーンなどを用いて、左右の眼に届く画像を検証した。EyeReal が生成した左右画像は、Ground truth（正解画像）と高い一致率を示し、水平・垂直・前後方向の動きに対して自然なモーションパララックスを維持した。</p>
<p>さらに、前景・後景の被写界深度（焦点表現）も再現可能で、奥行き感の自然さが向上している。</p>
<p><strong>EyeReal の再現画像（Prediction）は、Ground truth と高い一致を示し、上下左右・前後の動きに伴う視差変化を自然に再現。前景・後景の焦点表現も維持されている</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/41586_2025_9752_Fig3_c6449eee0f/41586_2025_9752_Fig3_c6449eee0f.jpg" alt="41586_2025_9752_Fig3.jpg" /></p>
<h2>超広視野角100度超、50Hzリアルタイム処理</h2>
<p>定量評価では以下の性能が確認された：</p>
<ul>
<li><strong>視野角：100度超</strong> ：観察者が大きく移動しても PSNR・SSIM が高値を維持した</li>
<li><strong>未知視点に対する一般化性能（Generalization）</strong> ：学習データに存在しない視点からでも安定した画質を確保</li>
<li><strong>動作速度：50.2 Hz</strong> ：多層LCDとAI処理によるシステムとしては高速で、リアルタイム裸眼3Dに求められる水準を満たす</li>
</ul>
<p>奥行き層ごとの PSNR 曲線や視野マップも示され、前景から背景まで一貫した再現品質を持つことが確認された。</p>
<p><strong>EyeReal の定量評価。超広視野角でも高い PSNR/SSIM を維持し、未知視点に対しても一般化。50Hzを超える処理速度によりリアルタイム裸眼3Dとして実用レベルに到達している</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/41586_2025_9752_Fig4_e554e7ef93/41586_2025_9752_Fig4_e554e7ef93.jpg" alt="41586_2025_9752_Fig4.jpg" /></p>
<p>論文では、エンターテインメント用途にとどまらず、医学画像の立体視、建築設計、教育、科学データの立体構造可視化など、多様な分野への応用可能性が示されている。</p>
<p>今後の課題としては、複数ユーザー同時視聴への拡張、消費電力の最適化、さらなる高速化などが挙げられる。
研究チームは、本方式が「裸眼3Dディスプレイの物理的限界をAIで超える新しい方向性を提示した」としている。</p>
]]></description>
      <pubDate>Wed, 10 Dec 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Anthropic、LLMが10～15分の自動インタビューを実施する新ツール「Anthropic Interviewer」を公開──1,250人調査データも公開、Hugging Faceでトレンド1位に</title>
      <link>https://ledge.ai/articles/anthropic_interviewer_release_adaptive_interview_tool</link>
      <description><![CDATA[<p>Anthropic は米国時間2025年12月4日、LLM「Claude」を用いた新しい調査ツール「Anthropic Interviewer」を<a href="https://www.anthropic.com/news/anthropic-interviewer">発表</a>した。ユーザーに対して10〜15分のその場の回答に合わせて質問を自動調整するインタビューを行い、AIが生活や仕事にどのように影響しているかを体系的に収集する仕組みだ。同社は1,250人の職業人に対して実施した初期調査の結果をデータセットとして<a href="https://huggingface.co/datasets/Anthropic/AnthropicInterviewer">公開</a>し、Hugging Face の「Trending Datasets」で1位になるなど反響が広がっている。</p>
<p><strong>Hugging Face CEO のClement Delangue氏が、「Anthropic Interviewer」のデータセットがTrending Datasets 1位になったと報告したX投稿</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Clement_Delangue_X_anthropicinterviewer_10fd09c46a/Clement_Delangue_X_anthropicinterviewer_10fd09c46a.jpg" alt="ClementDelangue X anthropicinterviewer.jpg" /></p>
<p>Anthropic Interviewer は、Claude をベースにした自動インタビューツールで、研究目的のフィードバック収集を大規模に行うために設計された。ユーザーが Claude.ai にアクセスすると、画面にポップアップ形式でインタビューが表示され、AI が順に質問を行う。所要時間は10〜15分で、声による入力にも対応している。</p>
<p><strong>「Anthropic Interviewer」のインタビュー画面。ユーザーの体験談や価値観を10〜15分で聞き取る</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/aiinterviewer_601e88e925/aiinterviewer_601e88e925.jpg" alt="aiinterviewer.jpg" /></p>
<h2>1,250人を対象にした初期調査を公開</h2>
<p>Anthropic は、Interview­erを用いて 1,000人の一般ワーカー、125人のクリエイター、125人の科学者 に調査を実施した。その結果、参加者の多くが「AIは時間を節約してくれる」「業務の補助として役立つ」と回答しており、AIとの付き合い方に対する実態が可視化されつつある。同社はこの調査データをデータセットとして公開し、外部研究者による分析も可能にした。</p>
<p><strong>ユーザーがAIをどのように活用しているかを示す分析結果。業務補助（Augmentation）が多くを占め、完全な自動化（Automation）は限定的だった</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/percentage_of_conversations_8ad0fc498a/percentage_of_conversations_8ad0fc498a.jpg" alt="percentage of conversations.jpg" /></p>
<p>調査結果は、AI活用に関する数多くのテーマを分類し、ユーザーがどの領域で期待や懸念を抱いているかを可視化している。たとえば、ビジネス最適化やプロジェクトワークフローでは「楽観的」な回答が多い一方、データプライバシーやシステムトラブルでは慎重な意見もみられた。</p>
<p><strong>1,250人の回答をテーマ別にクラスタリングした可視化。黄色が「楽観的」、青が「懐疑的」な傾向を示す</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/pessimistic_or_optimistc_13e69ad2bc/pessimistic_or_optimistc_13e69ad2bc.jpg" alt="pessimistic or optimistc.jpg" /></p>
<h2>Anthropic Interviewerの仕組み：Planning／Interviewing／Analysis</h2>
<p>Anthropic Interviewer のワークフローは以下の3段階で説明されている。
Planning（計画）：Claude が研究目的に沿ったインタビュールーブリックを生成し、人間の研究者がそれを校正する
Interviewing（実施）：Claude がユーザーへリアルタイムで質問を投げかけ、内容に応じて深掘りや分岐を行う
Analysis（分析）：インタビュー記録をもとに、Claude が初期分析を生成し、研究者が最終レポートを作成する</p>
<p><strong>Anthropic Interviewer の3段階ワークフロー。インタビュー計画の生成から自動インタビュー、分析までをAIが支援する</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/cc1e5916d47f4217a69_df29416f61/cc1e5916d47f4217a69_df29416f61.jpg" alt="cc1e5916d47f4217a69.jpg" /></p>
<h2>プライバシーとデータ利用</h2>
<p>Anthropic のプライバシー文書によると、インタビュー内容は研究目的にのみ利用され、最大5年間保管される。閲覧できるのは権限を持つAnthropic担当者のみで、回答データは集計形式や匿名化された形で扱われる。また、内容には健康情報や個人を特定する情報を含めないようユーザーに注意喚起が行われる。</p>
<p>Anthropic は、Interview­er を研究コミュニティ向けの基盤として活用し、AIが職業・社会にどのように影響するかを継続的に調査していく方針を示している。データセットの公開により、外部研究者による再分析や政策研究への活用も期待される。</p>
]]></description>
      <pubDate>Tue, 09 Dec 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Meta、メタバース投資を大幅縮小　Reality Labsの予算を最大3割カットしAIウェアラブルへ集中</title>
      <link>https://ledge.ai/articles/meta_metaverse_budget_cut_ai_wearables_shift</link>
      <description><![CDATA[<p>Metaが2026年度に向け、メタバース関連事業の予算を大幅に削減し、AIを活用したウェアラブル端末へ投資を振り向ける方針であることが分かった。2025年12月4日に<a href="https://www.bloomberg.com/news/articles/2025-12-04/meta-s-zuckerberg-plans-deep-cuts-for-metaverse-efforts">Bloomberg</a>をはじめとする複数の主要メディアが、同社内部の協議内容に詳しい関係者の話として報じている。こうした削減案は現時点では協議段階であり、最終決定には至っていないとされる。Metaは報道に対しコメントを控えている。</p>
<h2>Reality Labsの予算を最大30％削減する案を協議</h2>
<p>報道によると、Metaはメタバース領域を担うReality Labs部門の年間予算について、最大30％の削減案を検討しているという。2026年度の予算編成をめぐって幹部らが協議を進めており、削減幅によっては2026年1月にもレイオフが行われる可能性が指摘されている。</p>
<p>Reality Labsは、VRヘッドセット「Quest」シリーズや仮想空間サービス「Horizon Worlds」、ARデバイス開発などを担当しており、2021年以降の累計損失は700億ドル超に達しているとされる。多額の投資が続くReality Labsの採算性は、以前から投資家の懸念材料であったと報じられてきた。</p>
<h2>投資をAIウェアラブルへ再配分する方針</h2>
<p>複数メディアの報道では、Metaがメタバース関連プロジェクトへの支出を絞り込む一方、AIを活用したウェアラブル端末を重点領域に据える方針を示している点で一致している。Ray-Banと共同開発する「Ray-Ban Meta」などのAIグラスや、音声・視覚認識を活用する新型AIデバイスが今後の中核になると報じられている。</p>
<p>Metaは2025年にAI研究組織「Superintelligence」部門を設立するなど、AI研究体制を強化している。生成AIモデルの高度化やAI人材獲得競争が進む中、AIとハードウェアの融合領域にリソースを集中させる狙いがあるとみられる。</p>
<p>報道後、Meta株は上昇しており、Reality Labsへの巨額投資を抑え、AI関連分野に集中する方向性を投資家が好感したとの見方も伝えられている。</p>
]]></description>
      <pubDate>Tue, 09 Dec 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>NVIDIA、NeurIPSでオープンAI戦略を拡大──自動運転向け「DRIVE Alpamayo-R1」とAI安全・音声モデルを一挙発表</title>
      <link>https://ledge.ai/articles/nvidia_drive_alpamayo_r1_neurips_2025_open_vla</link>
      <description><![CDATA[<p>NVIDIAは2025年12月1日（米国時間）、アメリカ・サンディエゴで開催中の国際会議「NeurIPS」において、デジタルAIと物理AI（フィジカルAI）の双方を対象としたオープンAIモデル、データセット、開発ツール群の大規模な拡充を<a href="https://blogs.nvidia.com/blog/neurips-open-source-digital-physical-ai/">発表</a>した。</p>
<p>自動運転向けの新モデル「NVIDIA DRIVE Alpamayo-R1（AR1）」をはじめ、音声認識、話者分離、AIセーフティを支える複数の新モデルを同時公開している。</p>
<h2>NeurIPSで示したオープンAI戦略</h2>
<p>NeurIPSは世界有数のAI研究者が集まる国際会議で、NVIDIAは今年、オープンモデル、データセット、研究ツールをさらに拡張した。研究者が広く利用できる環境を提供することで、デジタルAI・物理AIの両領域での応用促進を狙う。</p>
<p>同社はNeurIPSで70件以上の論文・ワークショップ・ポスターを発表。さらに外部機関「Artificial Analysis」による Openness Index では、NVIDIAのNemotronモデルファミリーが、ライセンスの許容度やデータ透明性の点で高い評価を受けた。</p>
<p><strong>Artificial Analysis Openness Index（モデルのオープン度評価）</strong> ：Artificial Analysis による「Openness Index」の評価結果。NVIDIAのNemotronモデルは高い透明性が評価されている
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Artificial_Analysis_Openness_Index_960x417_431fac4472/Artificial_Analysis_Openness_Index_960x417_431fac4472.jpg" alt="Artificial-Analysis-Openness-Index-960x417.jpg" /></p>
<h2>世界初のオープンReasoning VLAモデル「DRIVE Alpamayo-R1」</h2>
<p>今回の発表の中心となるのは、NVIDIAが「世界初の業界規模のオープンReasoning Vision Language Action（VLA）モデル」と位置づける自動運転研究向けモデル「DRIVE Alpamayo-R1」。Chain-of-Thought推論と経路計画を統合し、歩行者が多い交差点や車線閉鎖、二重駐車といった複雑な状況下で高度な判断を可能にする。</p>
<p>AR1は、目の前のシナリオを段階的に推論し、複数の軌道候補を比較したうえで文脈に応じた最適な走行パスを選択する。「reasoning trace」により判断の根拠も提示でき、研究用途での安全性評価に役立つ。</p>
<p>モデルはNVIDIA Cosmos Reasonを基盤としたオープンファウンデーションとして提供され、非商用用途でのカスタマイズに対応。GitHubおよびHugging Faceで公開されており、訓練・評価に使用されたデータの一部も「NVIDIA Physical AI Open Datasets」として提供されている。AR1の評価には新たに公開されたフレームワーク「AlpaSim」も利用できる。</p>
<h2>Cosmosベースの物理AIツール群も拡充</h2>
<p>NVIDIAは物理AI向けの「Cosmos」基盤の提供も強化した。Cosmosベースのモデルを活用・改良するための総合ガイド「Cosmos Cookbook」を公開し、データキュレーションから合成データ生成、評価までのワークフローを提示する。</p>
<p>併せて以下の物理AIモデルも紹介された。</p>
<ul>
<li>LidarGen：自動運転シミュレーション向けにLiDARデータを生成する初のワールドモデル</li>
<li>Omniverse NuRec Fixer：ニューラル再構成データのノイズや欠損を補正</li>
<li>Cosmos Policy：大規模動画モデルをロボットの行動ポリシーへ転用</li>
<li>ProtoMotions3：デジタルヒューマンやヒューマノイドロボットの物理シミュレーション用フレームワーク</li>
</ul>
<p><strong>LidarGenモデルのサンプル出力</strong> ：Cosmosを基盤とするLidarGenモデルのサンプル出力。上段は入力画像と生成LiDARの重ね合わせ、中段は生成・実データのレンジマップ比較、下段は実点群（左）と生成点群（右）。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Lidar_Gen_960x583_4f169a7347/Lidar_Gen_960x583_4f169a7347.jpg" alt="LidarGen-960x583.jpg" /></p>
<p>Isaac LabやIsaac Simで生成したデータは、NVIDIAのロボティクスモデル「GR00T N」の学習にも用いることができる。Voxel51や1X、Figure AI、Gatik、Oxa、PlusAI、X-Humanoidなど、多くの研究・産業パートナーがCosmos World Foundation Models（WFMs）を活用しているという。</p>
<h2>Nemotron/NeMoによる音声AI・AIセーフティ強化</h2>
<p>デジタルAI領域では、音声処理およびAIセーフティに特化した新しいモデル群が発表された。</p>
<ul>
<li>MultiTalker Parakeet：重なり合う複数話者の音声をリアルタイムで認識</li>
<li>Sortformer：音声ストリームから話者を高精度に分離</li>
<li>Nemotron Content Safety Reasoning：複数ポリシーを横断適用できる推論型AIセーフティ</li>
<li>Nemotron Content Safety Audio Dataset：危険音声コンテンツ検出用の合成データ</li>
<li>NeMo Gym：LLM向け強化学習環境（RLVR対応）</li>
<li>NeMo Data Designer Library（Apache 2.0）：合成データセット設計ツール</li>
</ul>
<p>@<a href="https://www.youtube.com/watch?v=AThOsk2qJbs">YouTube</a>
MultiTalker Parakeetのリアルタイム複数話者認識デモ</p>
<h2>NeurIPSでのNVIDIA Researchの発表も多数</h2>
<p>研究発表面でも、NVIDIAは70件以上の論文・ワークショップ・ポスターを公開した。ブログでは以下の代表的な研究成果が紹介されている。</p>
<ul>
<li>Audio Flamingo 3：音声・音・音楽を横断して10分の音声を理解する大規模音声言語モデル</li>
<li>Minitron-SSM：Nemotron-H 8Bを4Bへ圧縮しつつ高精度を維持するモデル圧縮手法</li>
<li>Jet-Nemotron：高効率なポストNASパイプラインにより構築された新アーキテクチャ</li>
<li>Nemotron-Flash：実環境のレイテンシを最適化指標とする小型LLM</li>
<li>ProRL（Prolonged Reinforcement Learning）：長期強化学習により推論能力を継続的に向上させる手法</li>
</ul>
<h2>研究コミュニティに向けたオープンAI基盤の拡大</h2>
<p>今回のNeurIPSでの発表では、自動運転向けのVLAモデルから音声AI、AIセーフティ、物理AIのためのツール群まで、幅広い研究領域に向けたオープンAI基盤が網羅的に提示された。NVIDIAは、モデル・データセット・評価フレームワークを公開することで、研究者が多様な実験・応用に取り組める環境を拡張していくとしている。</p>
]]></description>
      <pubDate>Tue, 09 Dec 2025 01:50:02 GMT</pubDate>
    </item>
    <item>
      <title>日本のマンガを世界へ──and factoryが選んだ、AIによる制作工程の革新とグローバル展開を支えるアリババクラウド</title>
      <link>https://ledge.ai/articles/andfactory-alibaba</link>
      <description><![CDATA[<p>and factory株式会社（以下、and factory）は、大手出版社と共同で開発・運営を行っている「マンガUP！」や「ヤンジャン＋」などを手掛けるマンガアプリ事業を中核に、「uraraca（ウララカ）」などの占いコンテンツの提供や「&amp;AND HOSTEL」といったホテルサービスまで、デジタルとリアルを融合させた多様な事業を展開する企業だ。その売上の7割以上を占めるマンガ事業において、同社は今、国内市場の成熟という大きな転換点を迎え、次なる成長の主戦場を「グローバル」に見定めている。この壮大なビジョンを実現するため、制作工程の抜本的な革新とグローバル配信の強固な基盤として、同社が戦略的パートナーに選んだのがアリババクラウドだ。今回は、同社 代表取締役の青木氏と、VPoEの中島氏に、プロダクト開発の最前線におけるアリババクラウドの実力と、AIとの“共創”で描く日本のマンガコンテンツの未来について、詳細に話を聞いた。</p>
<h2>マンガ市場の転換点──紙から電子へ、そしてグローバルへ</h2>
<p>長らく、マンガ業界は紙を前提に発展してきた。雑誌連載から単行本化、書店での販売、ヒット作品のアニメ化やグッズ化という流れは、多くの人にとって馴染み深いサイクルだった。しかしマンガ市場は今、大きな転換点にある。</p>
<p>書店減少が続く中、作品をユーザーに届ける手段の中心は電子へとシフトしている。青木氏は、この流れを「急速な過渡期」と表現する。しかしその成長曲線にも陰りが見え始めている。青木氏は、市場全体の動きをこう分析する。</p>
<p>\u003E**「あと5年から10年以内には日本の電子書籍市場もアッパーに近づく可能性がある。」（青木氏）**</p>
<p>こうした国内の電子書籍市場の停滞を前提に、青木氏は次のように語る。</p>
<p>\u003E**「この先、日本以外のマーケット、すなわち海外に対して日本のマンガコンテンツを売っていくという時代をしっかりリードしていかないといけない。」（青木氏）**</p>
<h2>グローバル展開の鍵は”マンガ体験そのもののローカライズ”</h2>
<p>世界に目を向けると、日本発のマンガ・アニメコンテンツは年々存在感を増している。海外のアニメフェスやマンガフェスの規模拡大、ファン層の広がりは、現地の熱量としても強く実感されるレベルに達しつつある。</p>
<p>海外展開はすでに業界各社が取り組み始めている施策であり、多言語に展開して、いろんな国の人々が読める形にローカライズされている。しかし、グローバル展開で求められているのは翻訳だけではない。青木氏は、海外市場では、「読まれ方」そのものを最適化する必要があると語る。</p>
<p>\u003E**「言語だけではなくて、カラーリングであるとか、アニメや動画のフォーマットであるとか、スマートフォンの縦スクロールというマンガのコマ割り自体の再定義であるとか、色々なものを各国のフォーマットに変換し直すことによって、読みやすくなることが求められている。」（青木氏）**</p>
<p>多言語対応だけでなく、国や文化ごとに“作品の形”を変えることが、グローバル展開で成功するための条件なのだという。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/image3_c233bc0287/image3_c233bc0287.png" alt="image3.png" /></p>
<h2>インフラ導入とAIの共同開発。and factoryがアリババクラウドを選んだ理由</h2>
<p>2025年6月、and factoryはアリババクラウドとマンガ業界向けのAIソリューション共同開発、およびインフラ移行を含む業務提携を発表した。単なるクラウド利用拡大ではなく、インフラ移行とAIソリューション共同開発を一体で進める包括的なアライアンスである。</p>
<h3><strong>クラウド基盤の柔軟性と、AI 共同開発の可能性</strong></h3>
<p>青木氏は、今回の提携の背景について「2つある」と語る。</p>
<p>1つ目は、クラウドベンダーとしての採用理由だ。
and factoryはサービス拡大に伴い、コスト最適化とシームレスな運用が可能なクラウドを探していた。</p>
<p>\u003E**「順調にサービスが伸び続けることはなかなか無い。鈍化してきたり停滞してくることが起きてくる。その時にスムーズにコストの見直しができるサービスを探していた。」（青木氏）**</p>
<p>その中で、アリババクラウドは 「高い技術力と安価なコストパフォーマンス」 を提示し、戦略的な選択肢として浮上したという。</p>
<p>2つ目は、AI領域での共同開発パートナーとしての価値だ。
青木氏は中国現地で『Qwen』や『Wan』といった最新AIモデルを体験したことが大きな決め手になったと語る。この体験を通じて、アリババとの協業が日本と中国の両地域で産業発展につながると確信したという。</p>
<p>\u003E**「クラウドをしっかり使いつつ、その先の価値を一緒に作っていけるパートナーだと認識し、提携させていただいた。」（青木氏）**</p>
<h3><strong>移行の現実性、アジアでの強さ、そしてコストメリット</strong></h3>
<p>VPoEの中島氏は、技術面での選定理由として、他社クラウドからの移行可能性（サービスマッピングのしやすさ）を上げた。実際のハンズオンを経て、技術的な移行の実現性や運用面のイメージがつかめたことが安心材料になったという。またアリババクラウドのサポート体制も選定の大きな決め手となったと語る。</p>
<p>\u003E**「他社クラウドで利用していたサービスの多くを問題なくマッピングでき、ハンズオンを通じて実際に置き換えるイメージもつかめました。また、他のベンダーであれば有償になりそうな部分まで手厚くサポートいただき、とても心強く、安心感がありました。」（中島氏）**</p>
<p>同氏は、グローバル展開、とりわけアジア圏での強さについても触れた。</p>
<p>\u003E**「グローバル展開を考えた時、特に中国やアジア領域はアリババさんが強い。ローカライズする時の導入のしやすさも重視していた。」（中島氏）**</p>
<p>そして中島氏も、コスト面での優位性は大きかったと語る。</p>
<p>\u003E**「マンガ領域では画像や映像を配信する部分がコストとして大きいが、その差がかなり出ていた。」（中島氏）**</p>
<p>データ配信量の多い事業特性を考えると、これは経営・技術双方にとって決定打となるポイントだった。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/image2_98c2ea457c/image2_98c2ea457c.png" alt="image2.png" /></p>
<h2>3日の作業が1日に。『Qwen』が拓く、マンガ制作の未来</h2>
<p>アリババクラウドとの「共同開発」が真っ先に向かうのは、マンガ業界の積年の課題である「制作の持続可能性」の追求だ。</p>
<p>\u003E**「我々が目指していきたいのは、作家さんにクリエイティブな仕事をやっていただきたいということ。AIが量産できる部分は任せてしまっていい。これは作家さんの仕事を奪うのではなく、より創造的な領域に時間を使ってもらうための自動化を追求したいと思っている。」（青木氏）**</p>
<p>週刊連載を抱える作家の労働環境は過酷だ。人間の“気合”だけに頼る旧来の制作体制は、持続可能性に課題がある。この課題に対し、and factoryはアリババクラウドの画像生成AI『Qwen Image』で制作環境の革新に挑戦している。</p>
<p>その象徴的な成果のひとつが、キャラクターアセットの自動生成だ。</p>
<p>キャラクターアセットとは、従来マンガやアニメを描く上で必要なキャラクターの多面図であり、この複数のアセットは作家が手で描いている。
今回の取り組みでは、作家が最初にキャラクターの立ち絵を1枚デザインすれば、『Qwen Image』がその立ち絵を基に様々な角度や表情のバリエーションでアセットを自動生成してくれる。これによって、マンガやアニメを作る上で必要な設計図を作成する工程が大幅に削減できる。</p>
<p>さらにこの自動生成したアセットを用いて、作家が描いた作品のネームを『Qwen Image』に投げ込むことで、キャラクターの造形をブラさずに線画までの工程を自動化させる。このことでアセット制作から線画までの工程の作業時間が約80%軽減する成果が研究段階では見られている。こうした成果は、制作現場を大きく変える可能性を秘めている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/image5_2868af8df7/image5_2868af8df7.png" alt="image5.png" /></p>
<p>効率化が進む一方で、「AIが作家の仕事を奪うのでは」という懸念も当然ある。青木氏は、これが作家の仕事を奪うものではなく、あくまで「サポートツール」である点を強調し、作家が担うべき本質的な役割は「物語の核」を作ることであり、作品の”深さ”を創る部分はAIには代替できない領域だという。</p>
<p>\u003E**「キャラがIP化して愛されるかどうかは作品の深さやキャラの愛嬌に読者の気持ちが乗るかどうか。こういった『味』は人間にしか作れない。キャラクターの深さや物語の深さは作家さんのクリエイティブな領域です。」（青木氏）**</p>
<p>このような作業から作家を解放し、ストーリーの骨子やキャラクターの機微といった、人間にしか生み出せない「作品の深さ」を追求してもらう。AIとの協業は、作家のクリエイティビティを最大化するための手段なのだ。</p>
<p>この「クリエイティブへの集中」という思想は、中島氏が率いる開発組織にも浸透している。開発の現場でも、コードレビューや要件定義の質判断、OCRなどを社内業務のDXにアリババクラウドのAIソリューションを積極的に活用。AIはエンジニアにとって強力な“壁打ち役”であり、開発の品質とスピードを両立させるためのパートナーとなっている。</p>
<h2>共創パートナーと描く「日本マンガの新たな景色」</h2>
<p>and factoryがAIとインフラの力で見据えるのは、単なる業務効率化ではない。その先にある「日本マンガ産業の、全く新しい未来」だ。</p>
<p>青木氏によれば、日本のマンガ・アニメ産業は世界的なプレゼンスを持ちながらも、実際に海外へ流通できている作品はごく一部に留まっている。</p>
<p>\u003E**「日本の良いコンテンツは本当にたくさんあるのに、海外に届いているのはごく僅かだと思われる。体感値ではかなり少ない。ここをテクノロジーによって安価かつスピーディに届けられるようにすることが、日本のコンテンツ産業にとって重要な転換点になると考えています。」（青木氏）**</p>
<p>『ONE PIECE』や『鬼滅の刃』のようにヒットが約束されているような作品は巨額のコストをかけて海外展開されるが、多くの作品は「当たるかわからない」という理由で見送られがちだ。多額のローカライズ費用やアニメ制作費用が障壁となり、多くの作品がその機会すら得られずに国内に眠っているのが現状だ。</p>
<p>制作とローカライズの一部工程をAIによって「安価でスピーディ」に実行できれば、これまでコストの壁に阻まれていた無数の作品群に、等しくグローバル展開のチャンスが生まれる。</p>
<p>また海外では、多くの読者はアニメから入り、原作へ戻る流れが一般的だという。そのため動画フォーマットでの展開は重要な要素の一つだと指摘する。</p>
<p>これにより、「日本国内では大きなヒットに至らなくとも、海外でヒットする」という新たな可能性が広がると、青木氏は期待を込める。日本ではヒットしなかった作品が海外でヒットする事例が生まれれば、それを目指す若いクリエイターが「最初から世界で売れるマンガを作ろう」と考えるかもしれない。そうなれば、業界全体の「景色が変わる」。その一端を担うことが、ひいては日本経済全体の発展にもつながる。同社はそう信じている。</p>
<p>\u003E**「多くの作家さんが日本や海外で爆発的に売れていく──そんな事例が増えれば、世界を前提に作品づくりをする新たな作家さんも出てくると思います。それができれば、我々にとっても本望に近いですね。」（青木氏）**</p>
<p>中島氏も、AIだけに特化せずインフラも一括で提供するアリババクラウドだからこそ、AIと組み合わせたシームレスな開発が可能になると評価している。開発チームでは、Qwenを含む複数のLLMを実際の開発プロセスに組み込み、品質とスピードの両立を強化している。</p>
<p>\u003E**「プログラム開発やサービス提供といった一連のプロセスにおいて、仕様整理から開発コードの生成まで幅広く AI を活用しています。Qwen を含む複数のLLMを使い分けながら、『品質を担保しつつ、どれだけスピーディーにサービスを提供できるか』という点を重視して継続的にトライしています。」（中島氏）**</p>
<p>社内の業務効率化だけでなく、また、ユーザー価値に直結する領域でもAI活用を加速している。特に作品の“出会い”を創出するレコメンド機能は重点開発領域だという。こうした開発を支えるインフラとして、アリババクラウドの採用は大きなメリットがあると中島氏は語る。</p>
<p>\u003E**「AIだけでなくインフラも含めて提供されている点は大きいです。機能、コスト、運用、サポートの面でバランスが良く、AIとクラウドをシームレスに組み合わせた開発環境が整えられる。そこが選定理由のひとつです。」（中島氏）**</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/image4_f7f663e9c9/image4_f7f663e9c9.png" alt="image4.png" /></p>
<p>—
インフラ基盤とAIソリューションをワンストップで、かつ手厚いサポートと共に提供できるアリババクラウドは、and factoryにとってまさしく「共創パートナー」だ。</p>
<p>日本の重要な文化産業であるマンガ。その膨大な資産を、強固なインフラと最先端のAIという両輪で世界に解き放つ——。and factoryとアリババクラウドの“共創”は、そのための力強い一歩を踏み出したところだ。</p>
]]></description>
      <pubDate>Tue, 09 Dec 2025 00:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ニューヨーク・タイムズ、Perplexity AIを著作権侵害で提訴──「ペイウォール内の記事をRAG化し、読者にリアルタイム無断配信」と批判</title>
      <link>https://ledge.ai/articles/nyt_perplexity_ai_lawsuit</link>
      <description><![CDATA[<p>ニューヨーク・タイムズ（The New York Times、NYT）は米国時間2025年12月5日、生成AI検索サービスを提供するPerplexity AIを著作権侵害などでニューヨーク南部地区連邦地方裁判所に提訴したと<a href="https://www.nytco.com/press/the-times-sues-perplexity-ai/">発表</a>した。NYTは、Perplexityが同社のニュース記事を無断でコピーし、許可や対価を支払うことなく利用者に提供していると主張している。</p>
<h2>許可なくTimes記事をコピーし配信と主張</h2>
<p>NYTは、Perplexityが提供する生成AI型検索サービスが、同社のジャーナリズムを「無断でコピーし、顧客へ届けている」と指摘。これまで複数回、同社に対して利用停止を求めてきたが、Perplexityは「不正な利用」を継続しているとして訴訟に踏み切った。</p>
<p>NYTのスポークスパーソンである Graham James 氏 は声明で、Perplexityが Retrieval-Augmented Generation（RAG）を用いてTimes記事を取得し、ペイウォールの背後にあるコンテンツを含め「リアルタイムに顧客へ配信している」と説明した。James氏は「そのコンテンツにアクセスできるのは本来、NYTの有料購読者だけだ」と強調した。</p>
<h2>訴状で指摘される具体的行為</h2>
<p>訴状によると、PerplexityはNYTの記事を「逐語的（verbatim）またはほぼ逐語的（near-verbatim）」に再現するケースがあるとされ、これにより同社の記事がPerplexityの回答の代替物となり、購読やライセンス収入を侵害していると主張している。</p>
<p>また、Perplexityが同社のロゴや記事タイトルを用いた回答を生成しつつ、内容の一部が不正確な「ハルシネーション」を含む例がある点にも言及。NYT側は、こうした表示が同社ブランドの信用を損ねる可能性を指摘している。</p>
<p>訴因には、著作権侵害に加え、PerplexityがNYTの出所であるかのように誤認させる形で情報を提示するケースがあるとして、米Lanham法（商標法）に基づく虚偽表示に関する請求も含まれている。</p>
<h2>NYT「責任あるAIは支持するが、無許諾利用は容認できない」</h2>
<p>NYTは「倫理的かつ責任あるAIの利用・開発は支持する」とした上で、「PerplexityによるNYTコンテンツの無許諾利用には強く反対する」と強調。「私たちは今後も、自社の仕事の価値を認めようとしない企業に対し、責任を追及していく」と述べた。</p>
<p>本稿執筆時点で、Perplexity AI側から本件訴訟に対する公式コメントは確認されていない。今後の訴訟手続きで、同社の主張や事実関係の整理が進む見通しだ。</p>
<p>今回の提訴は、生成AI企業とメディア企業の間で広がる著作権をめぐる争いの一環として位置づけられる。NYTは2023年にもOpenAIとMicrosoftを提訴しており、AIによるコンテンツ利用をめぐる議論はさらに広がりを見せている。</p>
]]></description>
      <pubDate>Mon, 08 Dec 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Oculus創業者の防衛ユニコーン「Anduril」が日本上陸──AIで日本の防空を変革、アジア太平洋での事業拡大を本格化</title>
      <link>https://ledge.ai/articles/anduril_japan_launch_ai_defense_expansion</link>
      <description><![CDATA[<p>米国の防衛テクノロジー企業Anduril Industriesは2025年12月3日、アジア太平洋地域での事業拡大に向け、日本法人「Anduril Industries Japan合同会社」を設立したと<a href="https://www.anduril.com/news/anduril-expands-to-japan-advancing-its-mission-to-transform-allied-defense">発表</a>した。</p>
<p>Oculus VRの創業者として知られるパルマー・ラッキー氏が率いる同社は、AIとソフトウェアを核とした次世代の防衛システムで急速に成長しているユニコーン企業だ。同社は日本をインド太平洋地域の安全保障における重要な同盟国の一つと位置づけ、東京オフィスと日本法人の設立によって現地でのプレゼンスを高める方針を示している。</p>
<p>同社が提供する技術には、指揮統制プラットフォーム「Lattice」、無人機（UAS）、監視・探知センサーなどが含まれる。これらを統合することで、状況認識の向上、自律的な脅威検知、意思決定支援などを実現し、同盟国の防衛能力を高めることを狙う。Anduril は、日本市場をアジア太平洋地域の戦略的拠点と位置づけ、現地パートナーとの連携を通じて事業展開を進めるとしている。</p>
<p>Anduril は2017年に設立された比較的新しい防衛企業だが、米国防総省をはじめとする同盟国にソフトウェア主導の防衛システムを提供し、急速に存在感を高めている。今回の発表では、日本との協力を「長期的取り組み」として重視する姿勢が繰り返し示されており、日本法人の設立により現地での技術支援・開発体制が強化される見通しだ。</p>
<p>今後は、日本の防衛ニーズに応じたソリューション提供や、自律システムの活用領域の拡大に取り組むとしている。発表によれば、日本法人の設立は「日本の同盟国としての重要性を反映したもの」であり、アジア太平洋地域全体での事業基盤を強化する取り組みの一環だという。</p>
]]></description>
      <pubDate>Mon, 08 Dec 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>アリババ、初のAIスマートグラス「Quark AI Glasses」発売──S1/G1の2モデルで同社製LLM「Qwen」を直接使える日常デバイスに</title>
      <link>https://ledge.ai/articles/alibaba_quark_ai_glasses_launch_china</link>
      <description><![CDATA[<p>アリババは2025年11月27日（現地時間）、同社として初となるAI搭載スマートグラス「Quark AI Glasses」シリーズを<a href="https://www.alizila.com/alibaba-launches-new-quark-ai-glasses-series-in-china-deeply-integrated-with-qwen/">発表</a>した。アリババの大規模言語モデル「Qwen」およびAIアプリ「Qwen App」と深く統合し、音声と視覚を組み合わせたリアルタイムAIアシスタントを提供する。シリーズは、ディスプレイ搭載のフラグシップモデル「S1」と、軽量で普段使いに適した「G1」の2モデル構成となる。</p>
<h2>QwenとQuark Appを統合、音声・視覚を組み合わせたAIアシスタントに対応</h2>
<p>Quark AI Glassesは、アリババが開発する大規模言語モデル「Qwen」とAIアプリ「Qwen App」を標準搭載し、音声操作や視界上への情報提示を通じて、AIアシスタント機能を直接メガネ型デバイスで利用できる。外出先でのリアルタイム翻訳や、会議・講義内容の文字起こし、リマインダー設定、ナビゲーション、周辺スポット検索、ショッピング支援といった多様な機能に対応する。</p>
<p>また、アリババグループのサービスとも連携しており、決済のAlipay（支付宝）、地図サービスのAmap（高徳地図）、ECサービスのTaobao（淘宝）、旅行サービスFliggy（飛猪）なども利用できる。中国の音楽ストリーミングサービス（QQ Music、NetEase Cloud Music）にも対応した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/bf1ef6c08mf1fb9c98a8d6dd8bbbe8de_342c2a7c55/bf1ef6c08mf1fb9c98a8d6dd8bbbe8de_342c2a7c55.jpeg" alt="bf1ef6c08mf1fb9c98a8d6dd8bbbe8de.jpeg" /></p>
<h2>フラグシップモデル「S1」──交換式デュアルバッテリーで最大24時間使用可能</h2>
<p>S1はデュアルマイクロOLEDディスプレイを搭載し、視界内に情報をオーバーレイ表示できる。骨伝導技術やデュアルチップ構成を採用し、音声入出力やリアルタイム処理性能を強化した。</p>
<p>特徴的な点として、交換可能なデュアルバッテリーシステムを採用し、最大24時間の利用を可能としている。また、0.6秒で撮影できるインスタントフォト機能や、3K動画撮影およびAIによる4K動画出力、独自技術「Super Raw」による夜間撮影性能の向上など、撮影機能も強化されている。価格は3,799元（約7万8,000円）から。</p>
<h2>軽量モデル「G1」──約40gで普段使いに最適、コアハードウェアはS1と共通</h2>
<p>G1は日常の利用を想定した軽量モデルで、重量は約40gに抑えられている。ディスプレイ以外の主要ハードウェア（処理チップ、オーディオ、カメラなど）はS1と共通で、普段使いしやすいデザインと価格帯を重視している。価格は1,899元（約3万9,000円）から。</p>
<h2>MCP対応で開発者エコシステムも視野に</h2>
<p>Quark AI Glassesは、外部システムやアプリケーションと双方向に接続するオープン標準「Model Context Protocol（MCP）」に対応する。これにより、開発者がQuark AI Glasses向けのAIアプリやエージェントを拡張しやすくなり、アリババのQwenエコシステムとの連携も強化される。</p>
<p>市場調査会社<a href="https://www.idc.com/promo/wearablevendor/">IDC</a>の調査によると、2025年第2四半期の世界ウェアラブルデバイス出荷台数は前年同期比9.6％増の1億3,650万台で、このうち約5,000万台が中国を中心とする地域から出荷されている。中国は現在、世界最大のウェアラブル市場であり、AI搭載デバイス分野における競争も激しさを増している。</p>
<p>Quark AI Glassesシリーズは、アリババのAIモデルQwenとグループのサービス群、そしてMCPによる拡張性を組み合わせることで、同市場における新たな製品ポジションを獲得する構えだ。</p>
]]></description>
      <pubDate>Mon, 08 Dec 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/12/7 [SUN]Amazon、次世代LLM「Amazon Nova 2」発表　「思考」するProやマルチモーダルなど4モデル、独自フロンティアモデル構築「Nova Forge」Bedrockで提供開始</title>
      <link>https://ledge.ai/articles/amazon_nova_2_launch_four_models</link>
      <description><![CDATA[<p>Amazon Web Services（AWS）は2025年12月3日（現地時間）、米ラスベガスで開催中の年次開発者会議「AWS re:Invent 2025」において、次世代の大規模言語モデル（LLM）ファミリー「Amazon Nova 2」を<a href="https://www.aboutamazon.com/news/aws/aws-agentic-ai-amazon-bedrock-nova-models">発表</a>した。企業が自社専用のフロンティアモデルを構築できる新サービス「Nova Forge」や、ブラウザ操作を自動化する「Nova Act」もあわせて公開された。</p>
<p>同社が展開する生成AIサービス「Amazon Bedrock」を通じて、即日提供が開始されている。</p>
<h2>推論能力とコスト効率を大幅に強化</h2>
<p>「Amazon Nova 2」は、前世代モデルと比較して推論能力、処理速度、コストパフォーマンスのすべてにおいて大幅な進化を遂げている。特に、複雑なタスクを自律的に遂行する「AIエージェント」としての利用を想定し、文脈理解能力や論理的思考能力が強化された点が特徴だ。</p>
<p>今回発表されたラインナップは、用途に合わせて最適化された以下の4つのモデルで構成されている。</p>
<ul>
<li><strong>Amazon Nova 2 Pro</strong>  高度な推論（Reasoning）能力に特化した主力モデル。数学、コーディング、複雑な論理的推論において高い性能を発揮する。「思考プロセス」を強化しており、難解な指示に対しても正確な回答を生成可能だ。</li>
<li><strong>Amazon Nova 2 Lite</strong>  応答速度とコスト効率を最優先した軽量モデル。リアルタイム性が求められるチャットボットや、大量の文書要約・データ処理タスクに適しており、軽快な動作が特徴である。</li>
<li><strong>Amazon Nova 2 Omni</strong>  テキスト、画像、音声、ビデオをネイティブに理解・生成できるマルチモーダルモデル。従来のモデルのようにデータをテキストに変換する工程を経ず、複数の異なる入力情報を同時に、かつシームレスに処理することができる。</li>
<li><strong>Amazon Nova 2 Sonic</strong>  音声対話に特化したモデル。極めて低い遅延（ローレイテンシー）での音声入力・出力が可能で、人間と話しているかのような自然なリアルタイム対話を実現する。</li>
</ul>
<h2>ベンチマークでは「同等またはそれ以上」と説明</h2>
<p>Amazon は、Nova 2 Pro とNova 2 Lite について、Claude、GPT、Gemini 系列の競合モデルと公開ベンチマークで比較し、多くの指標で「equal or better（同等またはそれ以上）」と説明した。対象となったのは、マルチドキュメント分析、動画推論、複雑な指示のフォロー、高度な数学的推論、ソフトウェアエージェントタスクなど。また、Pro はより小型モデルへの知識蒸留にも利用できるとしている。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/nova2_pro_lite_bench_3863cb81b0/nova2_pro_lite_bench_3863cb81b0.jpg" alt="nova2 pro lite bench.jpg" /></p>
<h2>企業が自社専用モデルを構築できる「Nova Forge」</h2>
<p>同日発表された Nova Forge は、企業が自社データを使って Nova ベースの独自フロンティアモデル「Novellas」を構築できる「open training」アプローチを採用したサービス。事前学習・中間学習・事後学習の各段階における Nova のチェックポイントを使い、企業固有のドメイン知識を深く反映したモデルを構築できるとしている。</p>
<p>Reinforcement Learning（RL）による強化学習環境、責任ある AI のためのツールキット、小型高速モデル生成のための蒸留機能なども提供される。Booking.com、Reddit、Sony などが活用を進めている事例として紹介された。</p>
<h2>ブラウザ操作を自動化する「Nova Act」</h2>
<p>もう一つの新サービス Nova Act は、ブラウザ UI 上の操作をエージェントが代行するためのサービスだ。数百のシミュレーション環境で強化学習を行い、CRM 更新や E2E テスト、保険請求フォームの送信などの UI ワークフローを自動化する。早期導入企業では約90％の成功率を達成したという。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/1_c39392cc36/1_c39392cc36.gif" alt="ダウンロード (1).gif" /></p>
<h2>Bedrock で提供、開発者向けポータルも公開</h2>
<p>Nova 2 の各モデルは Amazon Bedrock で利用可能となり、API 経由でアプリケーションへの統合が行える。Nova Forge で構築した企業専用モデル「Novellas」も同様に Bedrock 上で運用できる。</p>
]]></description>
      <pubDate>Mon, 08 Dec 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Amazon Prime VideoのAI生成吹き替え「AI beta」版、配信から削除へ──NAVAと声優陣が『BANANA FISH』などのAI版に抗議</title>
      <link>https://ledge.ai/articles/amazon_prime_video_ai_english_dub_end</link>
      <description><![CDATA[<p>Amazon Prime Videoが、一部アニメ作品で導入していたAI生成吹き替え「AI beta」版を、12月初旬までに相次いで削除していたことが分かった。Anime News Network（ANN）が2025年12月3日に<a href="https://www.animenewsnetwork.com/news/2025-12-03/amazon-streamed-ai-english-dubs-on-banana-fish-no-game-no-life-zero-pet-anime/.231513">報じた</a>もので、削除は段階的に進んだとされる。</p>
<p>ANNによると、Prime Video上では『BANANA FISH』『No Game, No Life Zero』『Pet』『Journal of the Mysterious Creatures』の英語・ラテンアメリカスペイン語吹き替え、さらに『Vinland Saga』のラテンアメリカスペイン語吹き替えが「AI beta」と表示されていた。『BANANA FISH』と『No Game, No Life Zero』のAI版吹き替えは12月3日までに削除され、『Pet』『Journal of the Mysterious Creatures』のAI英語吹き替えは3日時点で残っていたが4日までに削除された。『Vinland Saga』のラテンアメリカスペイン語版も4日までに削除が確認された。</p>
<p>Amazonは2025年3月5日付の公式ブログ記事で、英語とラテンアメリカスペイン語を対象とする「AI-aided dubbing（AI支援吹き替え）」パイロットプログラムの開始を<a href="https://www.aboutamazon.com/news/entertainment/prime-video-ai-dubbing-english-spanish">発表</a>している。同記事はAmazon Staff名義で公開され、Prime VideoおよびAmazon MGM Studiosのテクノロジー担当バイスプレジデントであるRaf Soltanovich氏は、AI支援吹き替えについて「既存の吹き替えが存在しない作品にのみ提供する」と説明していた。</p>
<p>一方でANNは、『No Game, No Life Zero』にはSentai Filmworksが制作した既存の英語吹き替えがあり、HIDIVEが2024年5月から配信していたことを指摘。ANNはPrime Video版の「AI beta」吹き替えが、同作の既存吹き替えとはキャスト・音声が異なることを確認した。ANNによれば、Prime Video版のエンドクレジットにはSentai Filmworks制作のキャスト・スタッフが記載されたままだった。</p>
<p>SNS上での反発も大きかった。全米声優協会・NAVA（National Association of Voice Actors）は公式Xアカウント（@NAVAVOICES）で声明を投稿し、Amazonには本来「本物のストーリーテリング」や「感情的なつながり」を持つ吹き替えを制作する機会があったにもかかわらず、『BANANA FISH』『Vinland Saga』『No Game No Life』で「AI slop dubs」を選んだと批判。「侮辱的」「敬意を欠いている」との声が多いと訴えた。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/NAVA_1_86f4ec1597/NAVA_1_86f4ec1597.jpg" alt="NAVA1.jpg" /></p>
<p>声優からの抗議も相次いだ。ANNの記事は、Daman Mills氏、Kara Edwards氏、Nick Huber氏、Dawn M. Bennett氏ら複数の声優がSNSで『BANANA FISH』のAI版吹き替えに対し懸念を表明したと伝えている。Mills氏は自身のX投稿を通じ、AI吹き替えによって作品や声優の価値が損なわれるとの立場を示した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Daman_Mills_x2_ee9a1bd260/Daman_Mills_x2_ee9a1bd260.jpg" alt="Daman Mills x2.jpg" /></p>
<p>ANNが権利元へ取材したところ、Kadokawaは『No Game, No Life Zero』のAI吹き替えについて「AI吹き替えをいかなる形でも承認していない」と回答。Sentai Filmworks／HIDIVEの関係者は「事前に知らされていなかった」とし、「Amazonと状況を確認している」と述べた。</p>
]]></description>
      <pubDate>Mon, 08 Dec 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>学術＆研究2025/12/7 [SUN]「心の中の声」をAIが文字に変換　米研究チーム、脳活動から直接文章を生成する新技術「BIT (BraIn-to-Text)」発表</title>
      <link>https://ledge.ai/articles/brain_to_text_inner_speech_decoder</link>
      <description><![CDATA[<p>コロンビア大学やスタンフォード大学などの研究者らによるチームは2025年11月21日、脳の活動データを直接テキストに変換する新しいAIフレームワーク「BIT (BraIn-to-Text)」を<a href="https://arxiv.org/abs/2511.21740v1">発表</a>した。</p>
<p>このシステムは、発声しようとした言葉だけでなく、声を出さずに心の中で唱えた「内なる声（inner speech）」も一定の精度で文章化できることを示した。従来の類似手法と比べ、単語誤り率（WER）を24.69％から10.22％へと半分以下に抑えた点が特徴で、麻痺などによって話すことが難しい人のコミュニケーション支援技術としての応用も期待される。</p>
<p>なお、研究で使用されたのはUtahアレイと呼ばれる脳表面に埋め込む侵襲型電極であり、非侵襲の脳波（EEG）とは異なる。</p>
<h2>脳波を「音声」と見なしてLLMが解読</h2>
<p>従来の脳内音声解読システム（BCI）では、多くが「脳活動 → 音素 → 単語 → 言語モデル」という複数段階の“カスケード型”処理を採用していた。この構造では、各工程が独立しているため、システム全体をまとめて最適化できない点が課題とされていた。</p>
<p>今回発表された「BIT」は、脳活動の特徴を捉えるTransformerエンコーダーと、大規模言語モデル（LLM）を結合させたEnd-to-End型（統合的最適化）フレームワークである。</p>
<p>ヒトやサルの脳活動データ約367時間分を用いて事前学習を行い、神経活動のパターンから直接テキストを生成する。この仕組みにより、従来必要だった音素への変換ステップを省き、脳活動から直接文章を出力できるようになった。</p>
<p><strong>図1：BIT（BraIn-to-Text）フレームワークの全体構成</strong>
脳に埋め込んだ電極から取得した神経活動をAIが処理し、最終的に文章として出力するまでの流れを示す図。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x1_11_017e439017/x1_11_017e439017.png" alt="x1 (11).png" /></p>
<h2>単語誤り率を大幅に低減</h2>
<p>このEnd-to-Endのアプローチによる精度の変化について、論文では以下の数値が報告されている。従来の同様のシステムと比較し、単語の読み取りミスを示す「単語誤り率（WER）」は低下した 。</p>
<ul>
<li><strong>従来のEnd-to-End手法</strong> ： 単語誤り率 24.69%</li>
<li><strong>新技術「BIT」（音声LLM統合版）</strong> ： 単語誤り率 10.22%</li>
</ul>
<p>さらに、脳活動エンコーダーを用いて「Brain-to-Text Benchmark」に参加したところ、カスケード型設定を含む全カテゴリで最も低い誤り率を記録した。</p>
<p>アンサンブル（複数モデル併用）設定でのWERは以下の通りである。</p>
<ul>
<li><strong>Brain-to-Text '24 ベンチマーク</strong> ： 単語誤り率 5.10%</li>
<li><strong>Brain-to-Text '25 ベンチマーク</strong> ： 単語誤り率 2.21%</li>
</ul>
<p><strong>図2：BITの性能比較と解読例</strong>
従来手法と比較した誤り率の低減、および実際に生成された文章の例を示す図。内言においても意味的に近い文章を生成できている点が示されている。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x2_8f9b3b00ac/x2_8f9b3b00ac.png" alt="x2.png" /></p>
<h2>「想像発話」の読み取りにも成功</h2>
<p>研究では、筋肉を動かして発話しようとする「試行発話（attempted speech）」だけでなく、声を出す動作を伴わずに頭の中で言葉を思い浮かべる「想像発話（imagined speech）」のデータセットを用いた検証も行われた。</p>
<p>解析の結果、実際に話そうとする時と心の中で話す時の脳内活動には、共通する意味的構造が存在することが示唆された。AIはこの共通性を利用することで、データの少ない想像発話においても文章変換を可能にしている。</p>
<p><strong>図3：音声LLMとの比較──脳活動の文章化に適したモデル</strong>
複数の音声LLM・テキストLLMを比較し、どのモデルが脳活動からの文章生成に適しているかをまとめた図。音声を扱う小規模モデルが高い適性を示した。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x3_4_7cc8f5e441/x3_4_7cc8f5e441.png" alt="x3 (4).png" /></p>
<h2>話せない人のコミュニケーション支援へ</h2>
<p>発話が困難なALS患者や脳損傷患者にとって、頭の中で言葉を思い浮かべるだけで意思を伝えられる技術は大きな可能性を持つ。</p>
<p>論文では、将来的な臨床応用を見据えつつ、次のような課題を整理している。</p>
<ul>
<li>神経信号の非定常性への適応</li>
<li>電極の長期安定性</li>
<li>オンデバイス実行に向けた効率化</li>
</ul>
<p>また、特に内言の解読は倫理的に慎重な扱いが求められ、研究チームも「利用者の明確な同意を欠いた読心的用途は許されない」と明記している。</p>
<h2>今後の展望──神経データ版“基盤モデル”へ</h2>
<p>BITで用いられたNeural Encoderは、サルを含む多様な神経活動から学習されている。論文では、近年さまざまな分野で「foundation models」と呼ばれる大規模事前学習モデルが提案されていることを紹介し、こうした方向性がBCIの性能向上にも有効である可能性を挙げている。</p>
<p>研究チームは、今後の課題として、電極の非定常性への適応、長期的な記録の安定性、インターフェースの効率化などを明確にし、改善を重ねることで「ユーザーとシステムが互いに適応しながら利用可能な、より柔軟なBCIの開発につながる」とまとめている。</p>
]]></description>
      <pubDate>Mon, 08 Dec 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>月額3万6400円の「Google AI Ultra」に最強推論モード──Geminiアプリに、思考するAI「Deep Think」モードの実装</title>
      <link>https://ledge.ai/articles/gemini_3_deep_think_google_ai_ultra_launch</link>
      <description><![CDATA[<p>Googleは2025年12月4日（現地時間）、Geminiアプリに新たな推論モード「Gemini 3 Deep Think」を追加したと<a href="https://blog.google/products/gemini/gemini-3-deep-think/">発表</a>した。</p>
<p>同社はブログで、「Today, we’re rolling out Gemini 3 Deep Think mode to Google AI Ultra subscribers in the Gemini app.」と述べており、最上位サブスクリプションプラン「Google AI Ultra」の加入者を対象に、Geminiアプリで順次ロールアウトしている。利用するには、モデル選択で「Gemini 3 Pro」を選び、プロンプト入力欄から「Deep Think」モードをオンにする。</p>
<h2>高難度課題向けに推論能力を強化</h2>
<p>公開された「Gemini 3 Deep Think」は、回答を出力する前に複数の推論ステップを挟むことで、従来よりも深い推論ができるよう設計されたモードだ。Googleはブログで、同モードが「meaningful improvement in reasoning capabilities」を提供し、複雑な数学・科学・論理推論に取り組む場面を想定していると説明している。</p>
<p>具体的には、高度な並列推論（advanced parallel reasoning）を用いて複数の仮説を同時に探索するアプローチを採用することで、問題解決の過程を強化しているという。これにより、単に次の単語を即座に予測するのではなく、複数の候補を比較しながら解答に至るプロセスを踏めるとしている。</p>
<p>Googleは、Gemini 3 Deep Thinkが高難度ベンチマークで次のようなスコアを記録したと紹介している。</p>
<ul>
<li>Humanity’s Last Exam：ツールなしで 41.0%</li>
<li>ARC-AGI-2：コード実行ありで 45.1%</li>
</ul>
<p>同社は、これらの結果について「industry leading」「unprecedented（前例のない）」と表現しており、深い推論が求められるベンチマークにおいて業界トップレベルの性能を示したと位置づけている。また、今回のGemini 3 Deep Thinkは、国際数学オリンピック（IMO）や国際大学対抗プログラミングコンテスト（ICPC）World Finalsで「gold-medal standard」とされる水準に達した「Gemini 2.5 Deep Think」の系譜にあると説明されている。</p>
<h2>利用対象は「Google AI Ultra」加入者</h2>
<p>日本向けの公式プランページでは、Google AI Ultraの料金は月額3万6400円（税込）と案内されている。現時点（2025年12月8日）では、GeminiアプリでのDeep ThinkおよびGemini Agentについては「米国のみ、英語のみで利用可能」とされており、機能レベルでは提供地域に制限が設けられている。</p>
]]></description>
      <pubDate>Mon, 08 Dec 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>note、AIアシスタントに「Gemini 3 Pro」を導入──メモ段階から構成・言い換えまで執筆支援を強化</title>
      <link>https://ledge.ai/articles/note_gemini3pro_ai_assistant_update</link>
      <description><![CDATA[<p>note株式会社は2025年12月2日、同プラットフォームのAIアシスタントに Google の最新生成AIモデル「Gemini 3 Pro（プレビュー版）」を導入したと<a href="https://note.jp/n/n655ea4dcd39a">発表</a>した。アイデアの断片やメモ段階の入力から文章構成の提案、言い換え、表現改善まで、執筆プロセス全体を支援する機能が拡充される。</p>
<p>noteは「誰もが創作をはじめ、続けられるようにする」ことをミッションに掲げており、AIアシスタント機能はその一部として提供されてきた。従来も文章の書き出しや言い換え提案などが可能だったが、今回のアップデートにより、文章構成や着想の深掘りなど、より幅広い工程をサポートするようになった。</p>
<p><strong>AIアシスタント利用シーンの一例</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/1764577930_Zqk8_D_Hr_R_Pma3_Gg_Mf_EY_2_Qxb_Vw_1_88e9b02458/1764577930_Zqk8_D_Hr_R_Pma3_Gg_Mf_EY_2_Qxb_Vw_1_88e9b02458.webp" alt="1764577930-Zqk8DHrRPma3GgMfEY2QxbVw (1).webp" /></p>
<p>Gemini 3 Pro の導入により以下の機能が強化されたという。</p>
<ul>
<li>メモやアイデア断片から記事構成案や書き始めを生成</li>
<li>文章のリライト（言い換え・簡潔化・読みやすさ改善）</li>
<li>表現の調整や文章の要約</li>
<li>思考の深掘りや視点の追加提案</li>
</ul>
<p>noteは、AIアシスタントを利用することで「書きたいテーマはあるが、何から書けばよいか分からない」といった初期段階の課題に対応できるとしている。「AIが持つアイデア着想力や書き出しのサポートは、執筆に不慣れなユーザーの負担を軽減する」と説明する一方、生成内容への依存を避け、創作の主体はあくまでユーザー自身である点への注意も呼びかけている。</p>
<p>AIアシスタントは投稿画面から利用でき、文章やメモを入力することで提案内容が生成される。利用は無料で、今後は品質改善やガイドライン整備などを継続していくとしている。</p>
<p>noteは今回のモデル刷新により、執筆支援機能を強化し、より多様な創作者が利用しやすい環境づくりを進めている。</p>
]]></description>
      <pubDate>Mon, 08 Dec 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>生成AI普及の裏で「リテラシー不足」が深刻化──野村総研「IT活用実態調査2025」、導入率57.7％・課題は人材とリスク管理</title>
      <link>https://ledge.ai/articles/nri_it_survey_2025_generative_ai_literacy_gap</link>
      <description><![CDATA[<p>野村総合研究所（NRI）は2025年11月25日、日本企業517社を対象に実施した「IT活用実態調査（2025年）」の結果を<a href="https://www.nri.com/jp/news/newsrelease/20251125_1.html">公表</a>した。調査では、生成AIの導入が急速に広がる一方、その活用を支えるリテラシーやリスク管理の体制が追いついていない実態が浮き彫りとなったという。</p>
<h2>IT予算は増加傾向が続くが、前年から伸びが鈍化</h2>
<p>2025年度のIT予算が「増加した」と回答した企業は49.0％で、前年（59.0％）から10ポイント低下した。2026年度を「増加見込み」とする企業も47.5％と半数近くにのぼり、増加基調は続くものの、勢いはやや落ち着きつつある。</p>
<p><strong>IT予算の推移（NRI「IT活用実態調査2025」より）。2025年度は増加の勢いが前年より鈍化</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/000054612_822d38fda9/000054612_822d38fda9.png" alt="000054612.png" /></p>
<h2>生成AIの導入率は57.7％、導入済＋検討中で76％</h2>
<p>生成AIを「導入済み」と回答した企業は57.7％に達し、前年（44.8％）から一段と普及が進んだ。ChatGPTやGeminiといった汎用サービスの浸透により、「導入検討中」の割合は減少し、導入フェーズが“検討”から“活用強化”へ移行していることがうかがえる。</p>
<p><strong>新技術の導入状況。生成AIは57.7％が導入済み、ノーコード／ローコードツールも51.0％に</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/000054613_b373ec554c/000054613_b373ec554c.png" alt="000054613.png" /></p>
<h2>課題のトップは「リテラシー不足」70.3％、次いで「リスク管理の難しさ」48.5％</h2>
<p>生成AI活用における課題として最も多かったのは「リテラシーやスキルが不足している」（70.3％）で、前年の65.4％から増加した。導入フェーズが進む中で、実業務レベルでの使いこなしや品質管理が要求される場面が増え、教育・トレーニング体制の不足が顕在化した形だ。</p>
<p><strong>生成AI活用における課題。1位はリテラシー不足（70.3％）、2位はリスク管理（48.5％）</strong>
続く課題は「リスクを把握し管理することが難しい」（48.5％）。機密データの取り扱いや生成物の品質保証など、ガバナンス面の整備が追いつかない企業が多い。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/000054614_9cb8e79179/000054614_9cb8e79179.png" alt="000054614.png" /></p>
<h2>レガシーシステムは依然として約半数に残存</h2>
<p>企業の情報システムにおけるレガシー環境の残存率は、アプリケーションが47.3％、基盤系が48.2％と、前年から改善はみられるものの依然として高水準だ。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/000054615_12b800e0f8/000054615_12b800e0f8.png" alt="000054615.png" /></p>
<p>懸念点のトップは「ブラックボックス化・有識者不足」（51.6％）。続いて、「ベンダーサポートの終了」（50.1％）が挙げられ、技術負債が経営やIT投資の柔軟性を阻害している状況が明確になった。</p>
<p><strong>レガシーシステムに関する懸念。ブラックボックス化と人材不足が半数超</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/000054616_921e943c42/000054616_921e943c42.png" alt="000054616.png" /></p>
<h2>「必要だが足りない」デジタル人材、ITストラテジストの保有率は29.6％</h2>
<p>専門人材の確保については、「保有すべき」と回答した企業は多数である一方、社内に「保有している」と答えた割合は大きく下回った。</p>
<p>特に顕著なのはITストラテジストで、必要性71.9％に対し保有29.6％と大きなギャップがある。また、プロジェクトマネージャーは必要80.1％に対し保有55.0％で、ビジネス・テクノロジー双方の専門家が不足している。</p>
<p><strong>専門人材の必要性と保有状況。ITストラテジストなど、必要性に対し保有が大きく不足</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/000054617_5b78391399/000054617_5b78391399.png" alt="000054617.png" /></p>
<h2>生成AIの普及と同時に浮き彫りになる構造的な課題</h2>
<p>調査結果からは、</p>
<ul>
<li>生成AI導入フェーズの急速な進展</li>
<li>しかしリテラシー不足・リスク管理不足という基盤整備の遅れ</li>
<li>レガシーシステムと技術負債の残存</li>
<li>専門人材の不足</li>
</ul>
<p>という「普及と課題のギャップ」が明確に示されている。
NRIは、今後も企業のIT・デジタル化の現状を継続的に観測し、課題解決を支援するとしている。</p>
]]></description>
      <pubDate>Mon, 08 Dec 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>アルトマン氏「エロティックばかり注目されたけど」──ChatGPT、成人ユーザーの自由拡大へ</title>
      <link>https://ledge.ai/articles/openai_chatgpt_adult_mode_update_oct2025</link>
      <description><![CDATA[<p>OpenAIのサム・アルトマンCEOは10月14日（現地時間）、X（旧Twitter）上で、ChatGPTの安全制限を一部緩和し、成人認証済みユーザーに対してエロティックな会話を許可する方針を<a href="https://x.com/sama/status/1978129344598827128">発表</a>
した。</p>
<p>投稿は瞬く間に注目を集め、「エロティック解禁」が大きな話題となったが、アルトマン氏は翌日に「その部分ばかり注目されてしまったが」と<a href="https://x.com/sama/status/1978539332215681076">補足</a>し、実際には“より人間らしいAI体験”を実現するための包括的な方針変更であることを強調した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/gpt5_erotica_7cea863241/gpt5_erotica_7cea863241.jpg" alt="gpt5 erotica.jpg" /></p>
<h2>安全を優先してきたChatGPTの制限</h2>
<p>ChatGPTはこれまで、性的表現や親密な会話を含むコンテンツを厳しく制限してきた。
アルトマン氏は「メンタルヘルス問題に慎重を期すためだった」と説明し、精神的に不安定なユーザーに配慮した措置であったと振り返った。
「深刻な危機状態にあるユーザーは別扱いとし、他者に害を与える行為は依然として許可しない」と述べ、ポリシーの根幹は維持されるとしたうえで、「リスクのない成人ユーザーにはより多くの自由を与える」と明言した。</p>
<h2>“4oらしさ”を再導入──人間的なAI体験へ</h2>
<p>アルトマン氏は同日、「数週間以内に“4oで好まれた振る舞い”に近い人格（パーソナリティ）を選べる新バージョンのChatGPTを提供する」と投稿した。
GPT-4oは会話の自然さや表情豊かな応答で人気を集めたモデルであり、今後はユーザーが望む場合に、フレンドリーな口調や絵文字を多用した“人間らしい”対話スタイルを選べるようになる。
アルトマン氏は「これは利用時間を増やすためではなく、ユーザーが自分の望む形でAIと関わる自由を得るための設計だ」と述べている。</p>
<h2>「成人は大人として扱う」──12月に年齢認証を本格導入</h2>
<p>アルトマン氏は、12月に年齢認証を本格導入し、認証済み成人ユーザーに対してはエロティック会話なども許可する方針を示した。
一方で、ティーンエイジャーに対しては「安全をプライバシーや自由より優先する」と述べ、メンタルヘルス関連ポリシーは緩めないと強調している。</p>
<p>アルトマン氏は、「社会がR指定映画で境界を設けるように、AIにも適切な年齢境界を設けたい」と例え、「我々は選挙で選ばれた道徳警察ではない」と付け加えた。</p>
<h2>倫理と自由の境界線</h2>
<p>アルトマン氏の発言は、AIにどこまで人間的な自由を与えるかという議論を再燃させた。</p>
<p>OpenAIは今後、成人向け表現やAIの人格設計に関するガイドラインをさらに明確化するとみられる。今回の方針転換は、「AIをどう設計し、どう育てるか」という人間社会全体のテーマに踏み込む第一歩となりそうだ。</p>
]]></description>
      <pubDate>Mon, 08 Dec 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI、ChatGPT品質改善に全集中「非常事態（code red）」宣言──CEOサム・アルトマン氏が他社LLMの進化に危機感で社内指示</title>
      <link>https://ledge.ai/articles/openai_chatgpt_code_red_improvement_directive</link>
      <description><![CDATA[<p>米テック系メディア The Information は2025年12月1日（米国時間）、OpenAIのCEOであるサム・アルトマン氏が社員向けの社内メモで 「code red（非常事態）」を宣言し、ChatGPTの改善を最優先するよう指示した と<a href="https://www.theinformation.com/articles/openai-ceo-declares-code-red-combat-threats-chatgpt-delays-ads-effort">報じた</a>。</p>
<p>同メディアによると、アルトマン氏はメモで、ChatGPT の 速度、応答品質、信頼性、パーソナライズ性 を中心に改善を加速する必要性を強調。社内リソースの再配分を求め、他のプロジェクトについては一部を延期または凍結する方針を示したという。</p>
<p>この報道を受け、Reuters は「The Information reported on Monday」として<a href="https://www.reuters.com/business/media-telecom/openai-plans-improve-chatgpt-delay-initiatives-such-advertising-information-2025-12-02/">記事</a>を配信し、広告導入や新規イニシアティブの遅延が検討されている点を伝えた。Bloomberg も同内容を紹介しつつ、OpenAIがChatGPTの改善に社内資源を集中させる状況を説明した。また、Wall Street Journal（WSJ） は独自に社内メモを確認したとし、日次の進捗チェックやチーム再配置など、運用上の詳細を補足している。</p>
<h2>競争激化で「ChatGPTそのもの」の品質が問われる局面に</h2>
<p>背景には、競合AIモデルの性能向上がある。とくに Google が開発する最新モデル「Gemini 3」は、複数のベンチマークで高い評価を受けており、AIチャット領域で OpenAI の優位性を揺るがしつつあると報じられている。こうした状況の中で、OpenAI 内部では ChatGPT を巡る競争が一段と厳しくなっているとの見方が、主要メディアの報道で広がっていた。</p>
<p>OpenAI はここ数ヶ月、広告テストやショッピング支援など新機能の展開を準備していたが、今回の「code red」宣言によって、同社が 短期的な新機能追加よりも、ChatGPTそのものの品質改善を優先課題に据えた ことが示されたかたちだ。
現時点で、OpenAI は code red 宣言について公式の声明を発表していない。</p>
]]></description>
      <pubDate>Mon, 08 Dec 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>仏Mistral、次世代オープンモデル「Mistral 3」発表──3B〜14Bの小型DenseとフラッグシップLarge 3をApache 2.0で公開</title>
      <link>https://ledge.ai/articles/mistral_3_release_open_weight_models</link>
      <description><![CDATA[<p>フランスのAIスタートアップ Mistral AI は2025年12月2日（現地時間）、新しいオープンソースモデル群「Mistral 3 」を<a href="https://mistral.ai/news/mistral-3">発表</a>した。小型の「Ministral 3 3B」「Ministral 3 8B」「Ministral 3 14B」に加え、フラッグシップとして Sparse Mixture-of-Experts（MoE）構造を採用した「Mistral Large 3」を公開した。すべてのモデルは オープンウェイト（Apache 2.0ライセンス） で提供される。</p>
<h2>小型Denseモデル：3B／8B／14Bを新たにラインナップ</h2>
<p>Mistral 3 ファミリーでは、軽量で推論コストを抑えつつ性能を高めた小型Denseモデルを刷新した。公式リリースで「Ministral 3 シリーズ」と名付けられたモデルは以下の3種類：</p>
<ul>
<li>Ministral 3 3B（3B parameters）</li>
<li>Ministral 3 8B（8B parameters）</li>
<li>Ministral 3 14B（14B parameters）</li>
</ul>
<p>いずれも多言語処理に加え、画像理解を含むマルチモーダル対応 を備え、長いコンテキストを扱えるよう設計されている。Apache 2.0 ライセンスで提供されるため、商用利用を含め企業や開発者が自由に導入できる点が特徴だ。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/4_GPQA_Diamond_Accuracy_3e948fdea9/4_GPQA_Diamond_Accuracy_3e948fdea9.jpg" alt="4-GPQA Diamond Accuracy.jpg" /></p>
<h2>アクティブ41Bの最上位モデル「Mistral Large 3」</h2>
<p>フラッグシップモデルとして登場した「Mistral Large 3」は、Sparse Mixture-of-Experts（MoE）アーキテクチャを採用した大規模モデルだ。主な特徴は次のとおり：</p>
<ul>
<li>Sparse Mixture-of-Experts（MoE）構造を採用</li>
<li>推論時にアクティブなパラメータ数は約41B</li>
<li>総計パラメータ規模は 675B スケール</li>
<li>256K tokens の長大コンテキストに対応</li>
<li>画像理解を含むマルチモーダル処理に対応</li>
</ul>
<p>発表では、Mistral Large 3 は「既存のオープンウェイトモデルと同等以上の性能を示す」と説明されており、多言語対話や高度なタスクに向けて最適化されている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/chart_base_models_17eaed2794/chart_base_models_17eaed2794.jpg" alt="chart-base-models.jpg" /></p>
<h2>オープンウェイト戦略の継続</h2>
<p>Mistral AI は公式リリースで、「AIの恩恵をより多くの人々に届ける」というビジョンをあらためて強調。Mistral 3 ファミリーのすべてを Apache 2.0 ライセンスで公開することで、研究用途から商用利用まで幅広い開発者層への開放を継続している。</p>
<p>また、モデルは Mistral AI Studio、Amazon Bedrock、Azure Foundry、Hugging Face、IBM watsonx など複数のプラットフォームで利用可能としており、オンデバイスからクラウドまで多様な展開が可能だ。</p>
<h2>エッジからクラウドまで幅広い利用を想定</h2>
<p>Mistral 3 はエッジ環境から大規模データセンターまで幅広いユースケースを想定して設計されている。多言語・マルチモーダル対応、長コンテキスト処理能力により、チャットボット、エージェント、検索、画像理解、コード生成など、さまざまなアプリケーションに活用できる。</p>
<p>Mistralは今回の発表を「次世代モデルの登場」と位置付けており、今後もオープンモデルの開発とエコシステム拡張を続けていく方針だ。</p>
]]></description>
      <pubDate>Sun, 07 Dec 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/12/6 [SAT]「AIバブルではない」が…IBM CEO、AGI開発への世界全体のコミットメント100ギガワット分のデータセンター投資に「採算が合わない」と警告</title>
      <link>https://ledge.ai/articles/ibm_ceo_ai_datacenter_investment_bubble_warning</link>
      <description><![CDATA[<p>AI企業の間で、生成AIや将来のAGI（汎用人工知能）を見据えた大規模データセンター投資が加速している。だが、こうした巨額投資が現在のコスト構造のままで採算を取れるのかについて、警鐘を鳴らす声が上がった。IBMのCEO兼会長であるアーヴィンド・クリシュナ氏は2025年12月3日、The Vergeのポッドキャスト番組「<a href="https://www.theverge.com/podcast/829868/ibm-arvind-krishna-watson-llms-ai-bubble-quantum-computing">Decoder</a>」に出演し、AI向けデータセンター投資のリターンは「現状では見込めない」と述べた。</p>
<h2>巨額のデータセンター投資「100ギガワット＝8兆ドル」</h2>
<p>クリシュナ氏は、AI開発企業が進めるデータセンター増設について、1ギガワット規模を「埋める」ためには約800億ドルの設備投資（CapEx）が必要になると説明した。さらに、世界のAI企業全体でみれば、AGI開発を念頭に置いたコミットメントは約100ギガワットに達しており、「総額では約8兆ドルを投じる計算になる」と指摘した。</p>
<p>そのうえで、「この投資が企業に十分な利益として戻ってくる可能性は低い」と述べ、投資回収は極めて難しいとの見方を示した。</p>
<h2>GPUは5年で陳腐化、光ファイバーとは異なる「短命な資産」</h2>
<p>背景には、AI計算基盤の中核となるGPUなどの半導体が短期間で陳腐化するという事情があるという。クリシュナ氏は、こうしたハードウェアは5年程度で入れ替えが必要になると説明し、長期的に価値が残りやすい光ファイバー網などのインフラとは性質が異なると述べた。</p>
<p>100ギガワット級のAIデータセンターには膨大な資金が必要であり、その利払いだけでも莫大な額に達することから、「現在のAIサービスの単価と需要の伸び方では、採算が合うとは考えにくい」と語った。</p>
<h2>「AIバブルではない」が…一部資本は「回収されない」</h2>
<p>ポッドキャストでは「AIはバブルなのか」という問いも投げかけられた。クリシュナ氏はこれを否定し、「AIバブルではない」と述べた。ただし、一部の資本については「回収されないものが出る」とも指摘した。</p>
<p>特に、巨額の債務を用いてAIインフラに投資するケースでは、収益化の不透明さから返済困難に陥る可能性があるという。消費者向けAIプラットフォーム市場には勝者総取りに近い構造があり、多くの企業が参入しているが、最終的に利益を得られるのは少数にとどまるとの見方を示した。</p>
<h2>企業向けAIには期待──年間4000億〜7000億ドル規模の価値</h2>
<p>一方で、企業向けの生成AI活用については比較的明るい見通しも語った。コンサルティング企業による試算では、エンタープライズ分野だけでも年間4000億〜7000億ドル規模の価値が見込めると説明し、その20〜30%をテクノロジー企業が吸収できる可能性があると述べた。</p>
<p>AGIについては「現在のLLM技術だけで到達できる確率は0〜1%」と評価し、既存技術の延長線では限界があるとの認識を示した。</p>
<h2>IBMは「watsonx」と量子コンピューティングに軸足</h2>
<p>クリシュナ氏は、IBMが企業向けAIプラットフォーム「watsonx」や量子コンピューティングに長期的な重点を置いていることにも触れた。これらは短期で急拡大するAIインフラ投資とは異なる性質を持ち、企業の生産性向上や高難度計算の領域で継続的な需要が見込まれるという。</p>
]]></description>
      <pubDate>Sat, 06 Dec 2025 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/12/5 [FRI]AWS、自社開発AIチップ「Trainium3 UltraServer」を一般提供開始──3nmプロセス×144チップ構成で大規模AIの学習・推論を高速化</title>
      <link>https://ledge.ai/articles/trainium3_ultraserver_ga_announcement</link>
      <description><![CDATA[<p>米Amazon傘下のAmazon Web Services（AWS）は2025年12月3日（現地時間）、年次イベント「AWS re:Invent 2025」で、自社開発AIチップ「Trainium3」を搭載する大規模AI向け統合システム「Amazon EC2 Trn3 UltraServer（以下、Trainium3 UltraServer）」の一般提供（GA）を開始したと<a href="https://www.aboutamazon.com/news/aws/trainium-3-ultraserver-faster-ai-training-lower-cost">発表</a>した。</p>
<p>システムは3ナノメートルプロセスで製造したTrainium3を最大144基搭載でき、前世代（Trainium2 UltraServer）と比較して計算性能は最大4.4倍、メモリ帯域とエネルギー効率は約4倍に向上したという。</p>
<h2>最大362 FP8 PFLOPS、最大144チップ構成の新AIサーバー</h2>
<p>Trainium3 UltraServerはFP8精度で最大362 PFLOPSの演算性能を発揮し、1筐体に最大144基のTrainium3チップを搭載できる。メモリ帯域は合計706 TB/s、HBMメモリ容量は最大20.7 TBとなり、大規模言語モデル（LLM）やマルチモーダルモデルの学習・推論向けに最適化されているという。</p>
<p>AWSは、OpenAIのオープンウェイトモデル「GPT-OSS」を用いた社内テストでは、1チップあたりのスループットが3倍、推論レイテンシが4倍高速化したと説明している。</p>
<h2>新ネットワーク基盤「NeuronSwitch-v1」やUltraClusters 3.0で大規模化に対応</h2>
<p>Trainium3 UltraServerには、AWSが新たに開発したネットワークスイッチ「NeuronSwitch-v1」が搭載され、チップ間の通信帯域は前世代と比較して2倍になった。また、新たな「Neuron Fabric」により、チップ間通信の遅延は10マイクロ秒未満に抑えられるという。</p>
<p>複数のUltraServerを接続するクラスタ構成「EC2 UltraClusters 3.0」では、最大100万基のTrainiumチップまでスケール可能とされ、大規模データセットを用いたモデル学習や、数百万規模の同時推論リクエストにも対応できるとしている。</p>
<p>@<a href="https://www.youtube.com/watch?v=4y3pMGIS6DU">YouTube</a></p>
<h2>既に複数企業が活用、高速化やコスト削減の事例も</h2>
<p>AWSは複数の顧客事例を紹介しており、AnthropicやKarakuri、Metagenomi、NetoAI、Ricoh、Splash Musicなどが、Trainiumシリーズを用いてトレーニングコストを最大50％削減したという。また、生成系スタートアップのDecartは、Trainium3を用いたリアルタイム動画生成により、フレーム生成が4倍高速化し、GPU比でコストを半減できたとしている。</p>
<p>さらに、AWSの生成AI基盤「Amazon Bedrock」では、Trainium3上で既に本番ワークロードが稼働している。</p>
<h2>次世代「Trainium4」ではNVIDIA NVLink Fusionにも対応予定</h2>
<p>AWSは、次世代AIチップ「Trainium4」の開発も進めていると公表した。現時点で示されている目標値は、FP4で6倍以上、FP8で3倍の性能向上、メモリ帯域4倍の拡張など。さらに、Trainium4はNVIDIAの新インターコネクト技術「NVLink Fusion」に対応予定で、NVIDIA GPUと共存するラックスケール構成が可能になるとしている。</p>
<h2>大規模AIインフラの「選択肢」を拡大</h2>
<p>AWSは、Trainium3 UltraServerの提供開始により、生成AIやエージェント型AI、Mixture-of-Expertsモデルなどの大規模ワークロードに対し、高速・低コストで提供できる選択肢を拡大したと説明した。今後もTrainiumシリーズとGPUの双方をサポートする方針を示しており、用途に応じた柔軟なAIインフラ構成が可能になるという。</p>
]]></description>
      <pubDate>Fri, 05 Dec 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google、「Workspace Studio」を一般提供──Gemini 3世代のAIでGmail・Drive向けエージェントを構築可能に</title>
      <link>https://ledge.ai/articles/google_workspace_studio_general_availability</link>
      <description><![CDATA[<p>Googleは2025年12月4日（現地時間）、Google Workspace上でAIエージェントを構築・管理できる新ソリューション「Google Workspace Studio」を一般提供したことを<a href="https://workspace.google.com/blog/product-announcements/introducing-google-workspace-studio-agents-for-everyday-work?hl=en">発表</a>した。同サービスは、アルファ版としてテストされていた「Google Workspace Flows」を発展させたもので、GmailやDrive、Docsなど、日常的に利用されるWorkspaceアプリにAIエージェントを直接組み込める点が特徴となる。</p>
<p>Workspace Studioは、ノーコード／ローコードでエージェントの設計が可能で、繰り返し作業の自動化や文書作成、メール対応の支援、ファイル整理、レポート生成など、多様な業務フローをAIに任せられる。構築したエージェントは組織内で共有でき、管理者はアクセス権限や利用状況を細かくコントロールできる。</p>
<p>また、StudioはGoogleの最新AI技術と連携しており、Gemini 3世代のモデル群による自然言語処理や推論能力を活用したワークフロー構築が可能になった。公式ブログでは、FlowsからStudioへの進化点として、より柔軟で複雑な業務プロセスを扱えるエージェント設計機能、視覚的に操作できる新UI、管理者向け機能の強化が挙げられている。</p>
<p>GoogleはWorkspace Studioについて、「日常業務の中にAIエージェントを組み込み、生産性向上を支える基盤」と位置づける。今回の一般提供により、企業は業務プロセスにAIを本格的に統合しやすくなり、Google WorkspaceにおけるAI活用の幅が大きく拡張されることになる。</p>
<p>@<a href="https://www.youtube.com/watch?v=Xy0r5fKwlVo">YouTube</a></p>
]]></description>
      <pubDate>Fri, 05 Dec 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ARグラスだけで2D→3D変換を実現　新モデル「XREAL 1S」ーーXREAL、日本先行で予約受付を開始　2026年1月下旬発売へ</title>
      <link>https://ledge.ai/articles/xreal_1s_2d_to_3d_ar_glasses_launch</link>
      <description><![CDATA[<p>XREALは12月1日、新型ARグラス「XREAL 1S」を<a href="https://prtimes.jp/main/html/rd/p/000000226.000070978.html">発表</a>し、同日より日本で予約受付を開始した。発売は2026年1月下旬を予定。日本がグローバルで初の発表および予約開始地域となる。価格は6万7,980円（税込）。</p>
<p>同製品は、ARグラス単体で2D映像を3D映像へリアルタイムに変換し、空間上に大画面として表示できる点が最大の特徴。映画や動画コンテンツを、スマートフォンやPCを接続せずに立体化できる「空間ディスプレイ」として機能するという。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/70978_226_48b718eda105e60902793c5b6cd42fec_2475x2475_3ec3f53823/70978_226_48b718eda105e60902793c5b6cd42fec_2475x2475_3ec3f53823.webp" alt="70978-226-48b718eda105e60902793c5b6cd42fec-2475x2475.webp" /></p>
<p>XREAL 1Sは映像の立体化に加え、高輝度ディスプレイや装着性の向上を図った設計を採用。長時間利用を前提とした軽量化も進められている。XREALの周辺機器やエコシステムとの連携にも対応し、今後の拡張にも備えたモデルとなる。予約はXREAL公式ストアなどで受け付けており、発売は2026年1月下旬を予定している。</p>
]]></description>
      <pubDate>Fri, 05 Dec 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>デジタル庁、プリファードの国産LLM「PLaMo翻訳」をガバメントAIに導入──12月に庁内利用開始、2026年から他府省へ展開</title>
      <link>https://ledge.ai/articles/digital_agency_plamo_translation_government_ai_launch</link>
      <description><![CDATA[<p>デジタル庁は2025年12月2日、Preferred Networks（PFN）が開発する日本語特化型の国産LLM「PLaMo翻訳」をガバメントAI環境「源内（げんない）」に導入し、政府職員向けに提供すると<a href="https://www.digital.go.jp/news/b27d1af7-c231-4ab3-ad78-fc5408d44504">発表</a>した。2025年12月中にデジタル庁内での利用を開始し、2026年以降は他府省庁への展開を計画する。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/image_12_5c1580a084/image_12_5c1580a084.png" alt="image(12).png" /></p>
<p>「源内」は、政府職員がセキュアな環境で生成AIを業務利用できるよう整備が進められているガバメントAI基盤で、デジタル庁では2025年5月から全職員向けに提供してきた。今回の「PLaMo翻訳」導入は、国内開発AIを行政実務で活用する取り組みの一環として位置付けられる。</p>
<p>PFNの「PLaMo翻訳」は、日本語と英語の翻訳に特化した国産大規模言語モデルで、長文でも自然で一貫性のある文章を生成できる点を特長とする。ニュース記事や行政文書、会話文など幅広い文体に対応し、和文の自然さを損なわずに翻訳できるよう最適化されている。デジタル庁は、行政実務における翻訳業務の効率化につながるモデルとして活用を進める。</p>
<h2>ガバメントAIで試用する国内開発AIモデルの公募も</h2>
<p>同日、デジタル庁は「ガバメントAIで試用する国内大規模言語モデル（LLM）の公募」も<a href="https://www.digital.go.jp/news/1b093bba-a4c8-4001-8a92-ff3667a69198">開始</a>した。対象は国内で開発されたLLMや特定領域向けのSLM（Small Language Model）で、ガバメントクラウド上で安全に動作できることが条件となる。公募期間は2025年12月2日から2026年1月30日まで。選定されたモデルは、2026年夏頃から「源内」上で試験的に利用される予定だ。</p>
<p>2026年度には「源内」の他府省庁展開が予定されており、評価結果や各府省庁のニーズを踏まえ、2027年度以降に国内LLMの本格導入が検討される。</p>
]]></description>
      <pubDate>Fri, 05 Dec 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>チューリング、都内30分の完全自動運転「Tokyo30」を達成　テスラ猛追へ「富岳」級の計算基盤も公開</title>
      <link>https://ledge.ai/articles/turing_tokyo30_full_self_driving_achievement</link>
      <description><![CDATA[<p>完全自動運転を目指すスタートアップのチューリング株式会社は2025年12月1日、同社の技術カンファレンス「<a href="https://www.youtube.com/watch?v=HhQr0SAZs3Y">Turing AI Day 2025</a>」で、東京都内を人間の操作なしで30分以上走行するプロジェクト「Tokyo30」を達成したと発表した。当日は実際の走行映像も公開され、開発中のEnd-to-End（E2E）自動運転システムの挙動が披露された。</p>
<p>同社は創業当初より「We Overtake Tesla（テスラを超える）」をミッションとして掲げており、今回の成果を国産の完全自動運転に向けた重要なマイルストーンの一つと位置づけている。</p>
<h2>「カンブリア爆発」を経て実用域に近づくE2Eモデル</h2>
<p>AI Day の冒頭で公開された映像では、チューリングの実験車両が、市街地の複雑な交通環境下でハンドル・アクセル・ブレーキのすべてをAIが制御し、30分以上連続して走行する様子が確認できた。信号停止、右左折、歩行者や車両との交錯といった場面でも、安定した挙動を維持していた。</p>
<p>同社は創業時から「Day 1 から E2E」を掲げ、カメラ映像を入力とし、単一のニューラルネットワークが走行経路（トラジェクトリ）を直接出力する方式を採用している。データ収集から学習までのパイプラインを整備した段階でモデル性能が急激に向上した時期を、社内では「カンブリア爆発」と呼んでおり、Tokyo30 はその延長線にある成果だと説明した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/turing_ai_day20251_393e19e328/turing_ai_day20251_393e19e328.jpg" alt="turing ai day20251.jpg" /></p>
<h2>富岳の約40%相当の計算基盤が学習を支える</h2>
<p>E2Eモデルの性能向上を支えるのが、大規模GPUクラスタである。チューリングは自社専用の「Gaggle Cluster」を中心に、オンプレミスとクラウドを組み合わせて学習基盤を構築しており、2025年12月時点の総演算性能は、スーパーコンピュータ「富岳」のAI演算性能（FP16換算）の約40%に相当すると説明した。</p>
<p>同社はシリーズAラウンドの1st Closeとして152.7億円（約153億円）を調達しており、多くを計算基盤の拡充やデータ拡張、組織体制の強化に投じる計画だという。AI Day では、今後2年間で計算能力を5〜10倍（最大7 ExaFLOPS規模）へ拡張する方針も示された。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/turing_ai_day20252_608dfde61a/turing_ai_day20252_608dfde61a.jpg" alt="turing ai day20252.jpg" /></p>
<h2>VLAモデル・世界モデルへと発展する次世代アプローチ</h2>
<p>AI Day 2025 では次世代アプローチとして、視覚（Vision）・言語（Language）・行動（Action）を統合する「VLAモデル」や、未知の走行シーンを生成できる「世界モデル（World Model）」の研究状況が紹介された。</p>
<p>チューリングはこれまでに、日本語VLM「Heron」や運転QAデータセット「Stride-QA」「COBRA」を独自に構築しており、今後はこれらの技術を統合した70B級（700億パラメータ級）規模の「フロンティアモデル」を開発。そのうえで、車載向けに蒸留したE2Eモデルを量産レベルで展開する構想を掲げる。</p>
<h2>2030年の完全自動運転の商用化を見据える</h2>
<p>Q&amp;Aセッションでは、2030年前後にハンドルのない完全自動運転車が市場に登場する可能性が高いとの見通しが示された。チューリングは Tesla や Waymo など先行企業の技術進展を踏まえつつ、高速で追随する「セカンドムーバー・アドバンテージ」を戦略の中核に据えている。</p>
<p>短期的には、ドライバーが「安心して身を預けられるレベル」の挙動を実現することを目標とし、介入頻度の削減と危険シーンの排除に取り組むとした。安全性評価については、従来のISO規格だけではE2E型システムの特性を十分に捉えられないとして、3D Gaussian Splatting や世界モデルを活用した新たな検証アプローチの重要性が言及された。</p>
]]></description>
      <pubDate>Wed, 03 Dec 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>2025年のAIニュース振り返りと、AI提唱70周年となる2026年の展望を知る特設サイトLedge.ai年末年始特集「&apos;25 to &apos;26」を公開</title>
      <link>https://ledge.ai/articles/ledgeai25to26</link>
      <description><![CDATA[<p>AIの社会実装を加速させ、「テクノロジーを社会になめらかに浸透させる」ことをミッションに掲げる、国内最大級のAIメディア「Ledge.ai」を運営する株式会社レッジは、今年も参加費無料の年末特集「Ledge.ai '25to'26」を公開しました。</p>
<p>:::button
<a href="https://25to26.ledge.ai/">特集サイトはこちら</a>{target=_blank}
:::</p>
<h2>Ledge.ai年末年始特集「'25 to '26」とは</h2>
<p>Ledge.ai年末年始特集は、2025年のAI関連ニュースや注目のキーワード、2026年以降の動向など、AIの初心者から専門家まで幅広く楽しめる特集サイトです。</p>
<p>2025年は、生成AIが実用フェーズに突入し、業務プロセス・プロダクト・教育・クリエイティブなど、社会のあらゆる分野で“AI活用の当たり前化”が進んだ一年でした。</p>
<p>そして2026年は、AIという概念そのものが提唱された「ダートマス会議」から70周年という、まさに歴史的な節目を迎えます。2025年の「当たり前化」を土台として、AIは社会インフラのように深く浸透し、その活用範囲の拡大と同時に、AGI（汎用人工知能）の実現可能性など、AIの“次なる進展”に向けた探求が本格化する一年となるのではないでしょうか。</p>
<p>本特集では、そんな激動の2025年を多角的に振り返りつつ、2026年に向けた新たな潮流やビジネスチャンスを展望します。ぜひご登録の上、隅々までご覧ください。参加費は無料です。</p>
<p>:::button
<a href="https://25to26.ledge.ai/">特集サイトはこちら</a>{target=_blank}
:::</p>
<h2>コンテンツラインナップ</h2>
<h3>【編集部コンテンツ】2025年のAI振り返りと70周年に向けた展望</h3>
<p>Ledge.ai編集部が独自に企画・編集した記事および特集記事を掲載。
企業動向の背景にある文脈や業界のキーパーソンの言葉を通じて、AI活用を進めるヒントとして、ぜひご一読ください。</p>
<p><strong>\u003Cテーマ：AIの70年\u003E</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/ai70th_image_f12dd17c93/ai70th_image_f12dd17c93.png" alt="ai70th-image.png" /></p>
<p><strong>\u003CLedge.ai 2025年注目ニュース総まとめ\u003E</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/ai2025news_b7d5ecf93f/ai2025news_b7d5ecf93f.png" alt="ai2025news.png" /></p>
<p>Ledge.aiが発信してきた記事の中から、特に注目すべきトピックをキーワードごとに厳選してお届けします。これらの記事を並べて読むだけでも、2025年のAIトレンドの全体像が浮かび上がってくるはずです。 2025年の振り返りとして、今読むべきランドマーク的な記事をまとめています。ぜひ、業界動向の整理や次のアクションの参考としてご活用ください。</p>
<h2>【特別インタビュー】キーパーソンが語るAIの過去・現在・未来</h2>
<p>本特集では、AI研究の歴史・現在・未来をつなぐキーパーソンたちにインタビューを実施。過去／現在／未来のそれぞれの視点から、AIがどのように発展し、次の時代にどんな可能性を秘めているのかを語ってもらいました。</p>
<h2>【トップランナー企業動向】AI周辺で押さえておきたい最新技術と実践事例</h2>
<p>国内外の注目企業をピックアップし、AI基盤、エージェント活用、自動運転データなど、世界最先端の動向を徹底分析します。</p>
<h3>必読の深い知見が得られる取材記事を、公開期間中に次々と発信</h3>
<ul>
<li>AIの進化を支え続けるNVIDIAの羅針盤／エヌビディア合同会社</li>
<li>「言語の壁」は、もはやイノベーションの言い訳にならない。／DeepL Japan</li>
<li>生成AI時代、GPUのオルタナティブ──AMDは今どこを見据えているのか／Advanced Micro Devices, Inc.</li>
<li>基盤モデルとの融合はロボットに何をもたらすのか／Coming soon…</li>
<li>『PLURALITY』の実践と、多元的協働社会への道筋／サイボウズ株式会社 代表取締役社長 青野 慶久 氏</li>
<li>AIを「使わないことが最大のリスク」― AIエージェント時代の企業ガバナンス新常識／弁護士 柴山 吉報 氏</li>
</ul>
<p>:::button
<a href="https://25to26.ledge.ai/">特集サイトはこちら</a>{target=_blank}
:::</p>
<h2>開催概要</h2>
<p>イベント名：Ledge.ai年末年始特集「'25 to '26」
開催期間　：2025年12月1日～2026年1月9日
開催形式　：オンライン
参加費　　：無料（※一部のコンテンツ閲覧にはプロフィール登録が必要となります。）
お問合せ　：contact@ledge.co.jp</p>
]]></description>
      <pubDate>Mon, 01 Dec 2025 03:00:00 GMT</pubDate>
    </item>
  </channel>
</rss>