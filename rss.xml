<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
  <channel>
    <title>Ledge.ai 新着記事</title>
    <link>https://ledge.ai/</link>
    <description>Ledge.ai の最新テクノロジー記事</description>
    <item>
      <title>「見えないAI労働」の代償──インド農村部の女性が暴力・性的虐待コンテンツを大量視聴しAIを訓練する“コンテンツモデレーション”の実態、英紙Guardianが報道</title>
      <link>https://ledge.ai/articles/invisible_ai_labor_india_content_moderation_guardian_report</link>
      <description><![CDATA[<p>英国紙<a href="https://www.theguardian.com/global-development/2026/feb/05/in-the-end-you-feel-blank-indias-female-workers-watching-hours-of-abusive-content-to-train-ai">The Guardian</a>は2026年2月5日、インドの農村部に住む女性労働者が、暴力や性的虐待を含むコンテンツを日常的に視聴・分類することで、グローバルテック企業のAIシステムの訓練を支えている実態を報じた。記事は、現地での取材と当事者の証言をもとに、コンテンツモデレーション業務の具体像と心理的影響、支援体制の現状を伝えている。</p>
<h2>1日最大800件──AIを支える人間の最終判定</h2>
<p>報道によると、インド東部ジャールカンド州の農村部に住む26歳の女性は、自宅からリモートでログインし、プラットフォームの自動検知システムによって「違反の可能性あり」と判断された動画や画像、テキストを審査している。平均的な日で最大800件を確認し、暴力、虐待、ポルノグラフィなどに該当するかを分類する。</p>
<p>こうした判定結果は、アルゴリズムの精度向上や有害コンテンツ検知の高度化に用いられる教師データとなる。記事では、この業務が機械学習の基盤を支える工程の一部であると説明されている。</p>
<h2>二次的外傷ストレスという職業リスク</h2>
<p>「最後は無になる」──取材に応じた女性は、長時間にわたり暴力や性的虐待を含む映像を視聴し続けた末に訪れる感覚をそう表現した。</p>
<p>記事では、不眠や悪夢、侵入思考、感情の麻痺といった症状が紹介されている。社会学者ミラグロス・ミセリ氏が主導する<a href="https://milamiceli.com/inquiry/">Data Workers’ Inquiry</a>では、コンテンツモデレーションが心理的負荷の高い労働であると指摘している。</p>
<p>また、コンテンツモデレーター311人を18か月にわたり追跡した<a href="https://pmc.ncbi.nlm.nih.gov/articles/PMC12711784/">研究</a>では、二次的外傷ストレスや燃え尽き、レジリエンスなど複数の心理指標が測定され、長期的な心理的影響が検討されている。企業による介入プログラムが導入されている事例もあるが、記事は依然として心理的リスクが課題であると伝えている。</p>
<h2>“ゴーストワーカー”と呼ばれる構造</h2>
<p>インドでは2021年時点で約7万人がデータアノテーション業務に従事していると推計され、市場規模は約2億5000万ドルに達するとされる。多くは農村や準農村地域出身で、女性が半数以上を占める。</p>
<p>企業は地方都市に拠点を設け、改善されたインターネット接続環境を通じてグローバルなAI供給網と接続している。一方で、業務内容が契約前に十分説明されないケースや、心理的支援体制が限定的であるとの指摘もある。</p>
<p><strong>記事より引用：視聴していた過激なコンテンツについて懸念を表明したとき、彼女のマネージャーはこう言いました。「これは神の仕事です。あなたは子供たちを安全に守っているのです。」</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/the_guardian260205_cab1dea129/the_guardian260205_cab1dea129.jpg" alt="the guardian260205.jpg" /></p>
<p>さらに、厳格な秘密保持契約（NDA）により、労働者が家族や友人に業務内容を話すことが難しい状況も報じられている。記事は、生成AIの進展の裏側で、人間の目と判断が果たしている役割に光を当てている。</p>
]]></description>
      <pubDate>Fri, 13 Feb 2026 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ByteDance、動画生成AI「Seedance 2.0」を公開　マルチモーダル入力と参照機能を強化</title>
      <link>https://ledge.ai/articles/seedance_2_0_bytedance_multimodal_video_generation</link>
      <description><![CDATA[<p>ByteDance（TikTokの親会社）は、動画生成AI「Seedance 2.0」を正式公開した。同モデルは、同社の生成AIサービス「<a href="https://jimeng.jianying.com/">即梦（Jimeng）</a>」上で提供されている。2026年2月10日更新の<a href="https://bytedance.larkoffice.com/wiki/A5RHwWhoBiOnjukIIw6cu5ybnXQ">公式ユーザーガイド</a>（Lark Wiki）および即梦の公式サイトで案内されている。</p>
<p>現在は段階的に公開されており、会員・非会員ともに順次利用可能になるとしている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/jimeng_f353651d6b/jimeng_f353651d6b.jpg" alt="jimeng.jpg" /></p>
<h2>画像・動画・音声・テキストの4モダリティに対応</h2>
<p>Seedance 2.0は、画像・動画・音声・テキストの4種類の入力形式に対応するマルチモーダル動画生成モデルである。</p>
<p>使用手册によると、主な仕様は以下の通り。</p>
<ul>
<li>画像：jpeg、png、webp、bmp、tiff、gif形式に対応（最大9枚、30MB未満）</li>
<li>動画：mp4、mov形式（最大3本、合計2〜15秒、50MB未満）</li>
<li>音声：mp3、wav形式（最大3本、合計15秒以内、15MB未満）</li>
<li>テキスト：自然言語入力</li>
<li>混合入力の総上限：12ファイル</li>
<li>生成時間：4〜15秒（最大15秒）</li>
</ul>
<p>動画入力には総ピクセル数の上限も設けられている。生成時には効果音や音楽を含めた出力も可能としている。</p>
<h2>参照機能を強化、映像表現の制御性を向上</h2>
<p>今回のアップデートでは、参照機能の強化が中核に据えられている。</p>
<p>参照画像を用いることで構図やキャラクターの細部を保持し、参照動画を利用することでカメラワークや動作のリズム、視覚効果などを指定できる。既存動画の延長や、ユーザーの指示に基づく連続的なショット生成にも対応する。</p>
<p>さらに、既存動画を入力として、登場人物の差し替えや削除、追加などの編集操作も可能となっている。</p>
<h2>シンプル生成とマルチモーダル生成の2方式を提供</h2>
<p>Seedance 2.0は、生成方法として大きく2つのモードを用意している。UI上では「首尾帧」と「全能参考」と表示される。</p>
<ul>
<li>首尾帧（シンプル生成）：開始フレームとなる画像とテキストのみで動画を生成するシンプルなモード</li>
<li>全能参考（マルチモーダル生成）：画像・動画・音声・テキストを組み合わせ、参照素材を指定しながら生成する高度なモード</li>
</ul>
<p>複数素材を用いる場合は、「@素材名」の形式で各素材の役割を明示する。たとえば、ある画像を開始フレームとして指定し、別の動画をカメラワークの参照に、音声をBGMとして利用する、といった指定が可能だ。</p>
<h2>実在人物の写真人顔素材は現在非対応</h2>
<p>使用手册では、プラットフォームのコンプライアンス上の理由から、実在人物の写真人顔を含む画像や動画素材のアップロードは現在サポートしていないと明記している。該当素材はシステム側で自動的に制限される。</p>
<p>今後の仕様変更については、文書内で更新するとしている。</p>
]]></description>
      <pubDate>Thu, 12 Feb 2026 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>報ステ「選挙ステーション2026」で高速ロボットアーム「BOLT」導入　人間不可能な超高速カメラワークを披露</title>
      <link>https://ledge.ai/articles/tv_asahi_election_station_2026_bolt_robot_arm_camera</link>
      <description><![CDATA[<p>テレビ朝日は2026年2月8日、選挙特別番組『選挙ステーション2026』において、高速ロボットアームカメラ「<a href="https://x.com/hst_tvasahi/status/2020492521940406385">BOLT（ボルト）</a>」を導入した。</p>
<p>番組内でキャスターは、生放送のスタジオ撮影で活用されるのは同局として初の試みであると説明したという。番組公式Xの投稿では、「人間が操作するカメラでは不可能なスピードと動きで撮影を行っている」としている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/hst_tvasahi_e79c003799/hst_tvasahi_e79c003799.jpg" alt="hst_tvasahi.jpg" /></p>
<p>「<a href="https://www.mrmoco.com/motion-control/bolt/">BOLT</a>」は、英国のMark Roberts Motion Control（MRMC）が開発したモーションコントロールカメラシステムである。産業用6軸ロボットアームをベースとし、プログラムされた軌道を高精度に再現できる点を特徴とする。急加速・急停止を伴う動作や、ミリ秒単位での制御に対応し、同一動作の反復実行が可能とされる。</p>
<p>同機材は、映画やCM制作などにおいて高速撮影や視覚演出を目的に活用されてきた。放送分野においては、スタジオ内での移動撮影や演出強化を図る用途で導入が進んでいる。</p>
]]></description>
      <pubDate>Thu, 12 Feb 2026 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>データセンターにIOWN APNを実装──石狩と大手町を低遅延接続、東急不動産が生成AI・GPU向け基盤を構築</title>
      <link>https://ledge.ai/articles/data_center_iown_ishikari_otemachi</link>
      <description><![CDATA[<p>東急不動産は2026年2月6日、北海道石狩市で建設を進めている「石狩再エネデータセンター第1号」において、NTT東日本が提供する次世代通信基盤「IOWN（Innovative Optical and Wireless Network）」のAll-Photonics Network（APN）を導入すると<a href="https://www.tokyu-land.co.jp/news/2026/001648.html">発表</a>した。石狩市と東京・大手町の間をIOWNで接続するのは初めてで、2026年8月の導入を予定している。データセンターは再生可能エネルギー100％で運用され、2026年3月に竣工する見込みだ。</p>
<p>デジタル社会の進展やAI需要の高まりを背景に、データセンター（DC）の需要は急速に拡大している。東急不動産によると、2030年度のDC消費電力は2022年度比で2倍以上、2050年度には5倍以上に増加する見通しだという。一方、DCが集積する関東圏・関西圏の特定エリアでは電力不足が指摘されており、国は「データセンターの地方分散」を掲げている。同社はこうした動きを踏まえ、石狩市と連携し、同事業を推進してきた。</p>
<p>導入するIOWN APNにより、これまで課題とされてきた通信距離や通信遅延を解消し、高速・大容量・低遅延・省電力の通信を可能にする。これにより、石狩再エネデータセンターは、日本のネットワークの中心とされる東京・大手町と、あたかも隣接するデータセンターであるかのように利用できる環境を実現するとしている。同社の調査によると、石狩市と大手町の間でIOWNを実装するのは今回が初めてだという。</p>
<p>IOWNの導入によって、災害復旧（DR）用途にとどまらず、都市型データセンターの拡張や、GPUを活用した生成AIサービスの提供、点群データの効率的な活用によるデジタルツインコンピューティング、近年被害が多発しているランサムウェア対策など、多様な用途での活用を見込む。さらに、2025年2月に閣議決定された「GX2040ビジョン」で示された、電力と通信を一体で捉える「ワット・ビット連携」の実現にも資する取り組みと位置付けている。</p>
]]></description>
      <pubDate>Wed, 11 Feb 2026 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>OpenAIの新事業「Frontier」：AIエージェント導入は“常駐エンジニア”が伴走し一気通貫支援　日本ではソフトバンクも検証開始</title>
      <link>https://ledge.ai/articles/openai_frontier_ai_agents_forward_deployed_engineers</link>
      <description><![CDATA[<p>OpenAIは2026年2月5日、企業が業務を担うAIエージェントを構築・展開・管理するための新事業「<a href="https://openai.com/index/introducing-openai-frontier/">OpenAI Frontier</a>」を開始した。OpenAIが「AI同僚（AI coworkers）」と呼ぶエージェントを、実際の事業に組み込むことを目的とした取り組みで、同社のエンジニアが顧客企業と連携しながら導入を支援する点が特徴だ。</p>
<p>OpenAIによると、AIエージェントの活用が進まない要因はモデル性能ではなく、組織内での運用や管理にあるという。Frontierでは、複数のエージェントが共通の業務文脈を理解できる「共有コンテキスト」をはじめ、役割や業務内容を学習させるオンボーディング機能、利用結果を反映するフィードバック学習、業務権限や行動範囲を制御する仕組みなどを提供する。これにより、単発の業務自動化にとどまらず、組織横断で機能するAIエージェントの運用を可能にするとしている。</p>
<p><strong>【OpenAI Frontierのアーキテクチャ概要】</strong> 共有された業務コンテキストの上でAIエージェントを実行し、評価・最適化を通じて改善する構造を示している
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Desktop_Lightmode_1_b2ad1049fb/Desktop_Lightmode_1_b2ad1049fb.jpg" alt="Desktop_Lightmode__1.jpg" /></p>
<p>あわせてOpenAIは、Enterprise Frontier Programとして「Forward Deployed Engineers（FDE）」と呼ばれるエンジニアによる伴走支援を用意した。FDEは顧客企業のチームと協働し、システム設計やガバナンスの構築、本番環境でのエージェント運用までを一気通貫で支援する役割を担う。AIエージェントを既存の業務プロセスやIT環境に組み込み、実運用に耐える形で定着させることを狙う。</p>
<p>導入事例としては、HP、Intuit、Oracle、Uberなどに加え、米保険大手のState Farmが「ローンチパートナー」として参加している。State Farmは、従業員や代理店の業務支援を目的にFrontierを活用するとしており、段階的な導入と安全性の確保を重視する姿勢を示している。</p>
<p>日本では、<a href="https://www.softbank.jp/corp/news/press/sbkk/2026/20260206_01/?sbpr=info">ソフトバンク</a>とSB OpenAI JapanがFrontierを活用した法人向けAIソリューションの検証を開始した。ソフトバンクは、OpenAIのFDEが顧客企業と並走し、設計から構築、運用までを支援する体制を整えるとしており、日本企業への展開を視野に入れた取り組みとなる。</p>
]]></description>
      <pubDate>Mon, 09 Feb 2026 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Amazon、Alexaの基盤をLLMに刷新──生成AI版「Alexa+」を米国で正式提供開始──Primeは追加料金なし、非会員は月額19.99ドル</title>
      <link>https://ledge.ai/articles/amazon_alexa_llm_refresh_alexa_plus_us_launch</link>
      <description><![CDATA[<p>Amazonは米国時間2026年2月4日、音声アシスタントAlexaの基盤を大規模言語モデル（LLM）に刷新した生成AI版「Alexa+」を、米国で正式に提供開始したと<a href="https://www.aboutamazon.com/news/devices/alexa-plus-available-free-prime-members-us">発表</a>した。Alexa+はこれまでEarly Access（早期提供）として一部ユーザー向けに展開されてきたが、今回の発表により米国全体での正式提供へと移行した。</p>
<h2>Alexaの基盤をLLMに刷新──従来アーキテクチャからの転換</h2>
<p>Alexa+は、従来のAlexaとは異なる新しいアーキテクチャを採用している。Amazonによると、Alexa+は同社のLLM「Amazon Nova」と、Anthropicのモデルを組み合わせた構成となっており、生成AIを前提とした対話体験を実現する。ルールや定型スキルを中心としていた従来型のAlexaに比べ、文脈を保持した自然な会話や、より複雑な質問・依頼への対応が可能になったという。</p>
<h2>Early Accessから全米提供へ──生成AI版Alexaを正式アンロック</h2>
<p>Amazonは、Alexa+を従来のAlexaの単なる拡張ではなく、完全に新しい次世代アシスタントとして位置づけている。2025年に開始したEarly Accessプログラムでは、数百万人規模で利用希望が寄せられ、最終的に数千万人が参加したとしており、その過程で得られたユーザーからのフィードバックを基に機能改善を進めてきたと説明している。</p>
<p>提供形態も拡張されている。Alexa+はAlexa対応デバイスに加え、AlexaアプリやWeb版のAlexa.comからも利用できる。音声操作に限定されないマルチインタフェース型のAIアシスタントとして、外出先でのチャット利用や、Web上での調査や計画といった用途にも対応する。</p>
<h2>料金体系──Prime特典としての位置づけを明確化</h2>
<p>Prime会員は追加料金なしで、世帯内のAlexa対応デバイス、Alexa.com、Alexaアプリを通じてAlexa+を無制限に利用できる。一方、Prime非会員向けには、Alexa.comおよびAlexaアプリ上で利用できる制限付きの無料チャット体験を提供するほか、月額19.99ドルでフル機能を利用できるサブスクリプションも用意した。</p>
<p>今回の全米での正式提供開始により、Alexaは生成AIを前提としたLLMベースのアシスタントへと再定義された形となる。AmazonはAlexa+を通じて、家庭内アシスタントの中核を担うAI基盤を刷新し、日常利用を想定した本格運用フェーズへ移行させた。</p>
]]></description>
      <pubDate>Sun, 08 Feb 2026 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Anthropic、最上位LLM「Claude Opus 4.6」公開——業務AIエージェント「Cowork」は大手SaaS各社の株価急落に影響を与える反響</title>
      <link>https://ledge.ai/articles/anthropic_claude_opus_4_6_cowork_saas_stock_reaction</link>
      <description><![CDATA[<p>米AI企業のAnthropicは2026年2月5日（現地時間）、同社の最上位大規模言語モデル（LLM）「Claude Opus 4.6」を<a href="https://www.anthropic.com/news/claude-opus-4-6">発表</a>した。あわせて業務向けAIエージェント「Cowork」の展開を進めており、これら一連の動きを背景に、欧州を中心にソフトウェア企業やSaaS関連企業の株価が下落したと<a href="https://www.reuters.com/business/media-telecom/ai-concerns-pummel-european-software-stocks-2026-02-03/">Reuters</a>など各メディアが報じている。</p>
<h2>最上位モデル「Claude Opus 4.6」を公開、業務・エージェント用途を強化</h2>
<p>Claude Opus 4.6は前モデルからコーディング能力を中心に改良されており、より慎重な計画立案、長時間にわたるエージェント的タスクの継続、大規模なコードベースでの信頼性向上、コードレビューやデバッグにおける自己修正能力の強化が図られている。あわせて、Opusクラスとしては初めて、最大100万トークンの長文脈処理（ベータ）にも対応した。</p>
<p>@<a href="https://www.youtube.com/watch?v=dPn3GBI8lII&amp;t=1s">YouTube</a></p>
<p>Anthropicは、Opus 4.6がこうした改良を通じて、財務分析や調査、文書・表計算・プレゼンテーションの作成といった業務タスクにも適用できるとしている。業務向けAIエージェント「Cowork」では、モデルが複数の作業を自律的に並行処理する場面で、これらの能力が活用される設計とされている。Opus 4.6は、同日からclaude.aiやAPI、主要クラウドプラットフォームで提供が開始され、価格は従来モデルから据え置かれている。</p>
<p><strong>Anthropicが公開したベンチマーク表より抜粋。エージェント型コーディングやツール利用に関する評価結果を示している</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/opus4_6_bench_9957e0e30b/opus4_6_bench_9957e0e30b.jpg" alt="opus4-6 bench.jpg" /></p>
<h2>業務AIエージェント「Cowork」、段階提供とプラグイン公開の経緯</h2>
<p>1月16日、同社は業務向けAIエージェント「<a href="https://claude.com/blog/cowork-research-preview">Cowork</a>」をProプラン向けにResearch Previewとして提供開始した。その後、1月23日にはTeamおよびEnterpriseプランにも対象を拡大している。Coworkは、プログラミング用途に限らず、調査、文書作成、タスク整理といった業務を対象とするAIエージェントとして説明されており、Claudeを基盤に複数の作業を横断的に扱うことを想定している。</p>
<p>1月30日には、外部ツールや業務データ、ワークフローとの連携を可能にする「<a href="https://claude.com/blog/cowork-plugins">Cowork plugins</a>」を公開。プラグインを通じて外部ツールや社内システム、業務データソースと接続できる仕組みが提供されており、AIが単独で応答を生成するだけでなく、外部の情報や機能を参照・活用しながら作業を進める構成が示されている。これにより、AIエージェントが既存の業務ソフトウェアやワークフローと接続される形が明確になった。</p>
<h2>SaaSはAIエージェントに代替されるのか──市場で「SaaS死亡説」が再燃</h2>
<p>こうした一連の発表を背景に、欧州を中心にソフトウェア企業やSaaS関連企業の株価が下落した。Reutersは、業務AIエージェントの進化によって既存ソフトウェアのビジネスモデルが揺らぐとの見方が投資家の間で強まり、売りが広がったと報じている。</p>
<p>報道では、特定の製品や単一の発表が株価下落の直接的な原因と断定されているわけではない。一方で、市場の懸念が強まった背景として、1月中旬にCoworkの提供が始まり、1月30日にプラグイン機能が公開されたことで、AIエージェントが外部ツールや業務データと連携し、既存のSaaSと機能単位で接続・競合し得るとの見方が具体的に意識されるようになった点が挙げられている。</p>
]]></description>
      <pubDate>Fri, 06 Feb 2026 08:15:00 GMT</pubDate>
    </item>
    <item>
      <title>人間が参加できないSNS？AIエージェント専用「MoltBook」が急成長──サム・アルトマン氏「新しい社会的交流の兆し」</title>
      <link>https://ledge.ai/articles/moltbook_ai_agent_only_social_network_altman_comment</link>
      <description><![CDATA[<p>AIエージェント専用のソーシャルネットワーク「MoltBook（モルトブック）」が公開され、短期間で急速に注目を集めている。人間が投稿やコメントに参加できず、AIエージェント同士のみが交流するという異例の設計が、AI開発者や研究者の間で話題となっている。</p>
<p>MoltBookは2026年1月29日、開発者のMatt Schlicht氏によって<a href="https://x.com/MattPRD/status/2016560277333168540">公開</a>された。掲示板型のUIを採用し、AIエージェントが自律的に投稿、コメント、投票を行う。テーマ別のコミュニティ（Submolts）もエージェント自身によって作成され、人間は閲覧のみが許可される「観測者」として位置づけられている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/moltbook_release_f621f3d085/moltbook_release_f621f3d085.jpg" alt="moltbook release.jpg" /></p>
<p>Schlicht氏によると、MoltBookは特定の大規模言語モデルや技術構成に依存せず、オープンソースのAIエージェント「OpenClaw」（旧称Clawdbot／Moltbot）など、さまざまなエージェントを受け入れる設計となっている。公開後数日で3万2000超のAIエージェントが登録したとされ、立ち上がりの速さも注目を集めた。</p>
<p>この動きに対し、AI研究者のAndrej Karpathy氏は、自身のAIエージェントをMoltBook上で<a href="https://x.com/karpathy/status/2017386421712261612">認証（claim）した</a>ことをXで明らかにした。同氏は、MoltBook上で起きている現象について「最近見た中で最もSF的な離陸に近い出来事」と評している。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Andrej_Karpathy_X_f84d8455c6/Andrej_Karpathy_X_f84d8455c6.jpg" alt="Andrej Karpathy X.jpg" /></p>
<p>さらに、米シスコシステムズのイベントに登壇したOpenAIのCEO、Sam Altman氏もMoltBookに言及した。Altman氏は、MoltBookそのものが最終的な勝者になるかは分からないとしつつも、多数のAIエージェントが人間の代理として同じ空間で相互作用するという点について、「新しい社会的交流の形を示唆するものだ」と述べている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/samaltman_on_cisco_ai_summit_68299f9d2e/samaltman_on_cisco_ai_summit_68299f9d2e.jpg" alt="samaltman on cisco ai summit.jpg" /></p>
<p>MoltBookは現在ベータ段階にあり、機能やコミュニティは急速に変化している。AIエージェント同士の交流が一過性の実験にとどまるのか、それとも新たなインターネットの形へと発展するのか、今後の動向が注目される。</p>
]]></description>
      <pubDate>Thu, 05 Feb 2026 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Gemini 3 Flashに高精度な画像理解「Agentic Vision」追加──Python実行で画像を再検査、品質5〜10%向上</title>
      <link>https://ledge.ai/articles/gemini_3_flash_agentic_vision_image_understanding</link>
      <description><![CDATA[<p>Googleは2026年1月27日、同社のAIモデル「Gemini 3 Flash」に、高精度な画像理解機能「Agentic Vision」を追加したと<a href="https://blog.google/innovation-and-ai/technology/developers-tools/agentic-vision-gemini-3-flash/">発表</a>{target=”_blank”}した。画像を一度解析して回答を返す従来型の手法とは異なり、モデル自身が処理手順を組み立て、必要に応じて画像を拡大・切り出ししながら検証を重ねることで、視覚的根拠に基づく回答を可能にするという。</p>
<p>Agentic Visionは、Google AI StudioおよびVertex AIのGemini APIで提供され、現時点ではGemini 3 Flash専用機能として位置付けられている。</p>
<h2>画像理解を「一度きりの認識」から「段階的な検証プロセス」へ</h2>
<p>Agentic Visionの特徴は、画像理解を静的な認識処理ではなく、段階的な検証プロセスとして扱う点にある。
モデルは画像を見て即座に答えを出すのではなく、</p>
<ul>
<li>どの部分を確認すべきかを判断</li>
<li>必要な操作を実行</li>
<li>結果を再度観察し、次の行動を決める</li>
</ul>
<p>といったループを繰り返しながら推論を進める。Googleはこのアプローチを「Think → Act → Observe」の循環として説明している。</p>
<h2>視覚推論とPython実行を組み合わせた設計</h2>
<p>技術的な中核となるのが、視覚推論とPythonコード実行の統合だ。</p>
<p><strong>図）Agentic Visionの処理フロー</strong> ：ユーザー入力（画像＋テキスト）を受けたAIエージェントが、「Think（計画）→Act（コード実行）→Observe（結果確認）」の循環を回しながら、画像を拡大・切り出し・注釈付けして分析する。Gemini 3 Flashでは、Pythonによる画像操作が推論プロセスに組み込まれている。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/agentic_vision_gemini_3_flash_bl_width_1000_format_webp_z5u5_Yj_Z_f25520272d/agentic_vision_gemini_3_flash_bl_width_1000_format_webp_z5u5_Yj_Z_f25520272d.webp" alt="agentic-vision-gemini-3_flash_bl.width-1000.format-webp_z5u5YjZ.webp" /></p>
<p>Agentic Visionでは、モデルが自律的にPythonコードを生成・実行し、以下のような操作を行う。</p>
<ul>
<li>画像の一部を切り出して拡大表示</li>
<li>特定領域を再解析</li>
<li>数値データを抽出して計算処理</li>
</ul>
<p>こうした処理をコード実行環境で行うことで、推論過程をより厳密にし、誤認識を減らす狙いがある。Googleによると、この仕組みにより多くの視覚系ベンチマークで5〜10%の品質向上が確認されているという。</p>
<p><strong>図）Agentic Visionによる視覚ベンチマーク性能の変化</strong> ：Gemini 3 Flashにコード実行を組み合わせた構成（with code execution）は、画像理解系ベンチマークの多くで、従来構成を5〜10%上回るスコアを示した。Googleは、画像の再検査や注釈付けを推論過程に組み込んだ点が精度向上につながったとしている</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/agentic_vision_gemini_3_flash_bl_width_1000_format_webp_CO_Ee0g_Z_9ff68e3d0e/agentic_vision_gemini_3_flash_bl_width_1000_format_webp_CO_Ee0g_Z_9ff68e3d0e.webp" alt="agentic-vision-gemini-3_flash_bl.width-1000.format-webp_COEe0gZ.webp" /></p>
<h2>ズーム、注釈、可視化──Agentic Visionで可能になる操作</h2>
<p>Agentic Visionが提供する主な機能は、次の3点に整理できる。</p>
<ul>
<li><strong>ズームと再検査：</strong> 小さな文字や遠景の対象など、初回解析では不十分な要素を検知し、拡大・再分析する</li>
<li><strong>画像への直接注釈：</strong> 境界ボックスやラベルを画像上に描画し、対象物の位置や数を明示することで、回答の根拠を可視化する</li>
<li><strong>視覚情報の計算とグラフ化：</strong> 画像内の表や数値を読み取り、計算処理を行ったうえでグラフ出力することも可能</li>
</ul>
<p>これらはいずれも、モデルが自律的に「必要」と判断した場合に実行される点が特徴となっている。</p>
<h2>Google AI StudioとVertex AIで提供、Flash専用機能として展開</h2>
<p>Agentic Visionは、Google AI StudioおよびVertex AIのGemini APIで利用できる。開発者は、コード実行機能を有効化することで、画像を対象とした高度な検証プロセスをアプリケーションに組み込める。</p>
<p>なお、公式ドキュメントでは、本機能はGemini 3 Flashに限定して提供されると明記されており、他のGeminiモデルでは利用できない。</p>
<h2>業務利用を意識した高精度画像理解へ</h2>
<p>GoogleはAgentic Visionについて、単なる画像認識精度の向上にとどまらず、業務利用に耐える信頼性の確保を目的とした機能強化だと位置付けている。
計器の読み取り、画像化された表データの解析、数量確認など、正確性が求められる場面での活用を想定しているという。</p>
]]></description>
      <pubDate>Tue, 03 Feb 2026 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Anthropic、Claude無料プランを拡充──「ファイル作成」「コネクター」「Skills」をサブスク不要に</title>
      <link>https://ledge.ai/articles/anthropic_claude_free_plan_expansion_2026</link>
      <description><![CDATA[<p>米AI企業のAnthropicは2026年2月11日（現地時間）、対話型AI「Claude」の無料プランを拡充したと<a href="https://x.com/claudeai/status/2021630345964323026">発表</a>した。これまで有料限定だった「ファイル作成・編集（コード実行）」「Connectors」「Skills」が、サブスクリプション契約なしで利用可能となる。</p>
<h2>無料プランで利用可能となった対象機能</h2>
<p>無料ユーザーに開放されたのは以下の3機能</p>
<ul>
<li>ファイル作成・編集（コード実行）</li>
<li>Connectors（外部ツール連携）</li>
<li>Skills（スキル機能）</li>
</ul>
<p>これらは従来、有料プランを中心に提供されてきた機能である。</p>
<h2>「ファイル作成・編集（コード実行）」の提供内容</h2>
<p>ファイル作成機能では、Claudeがコードを実行し、スプレッドシートやプレゼンテーション資料、PDFなどのファイルを生成・編集できる。Anthropicのヘルプセンターによれば、本機能は無料を含む各プランで利用可能と記載されている。Web版のほか、Claude Desktopおよびモバイル環境でも利用できる。</p>
<p><strong>■ Anthropic、ClaudeでExcel・Word・パワポ・PDFなどのファイル作成と編集を可能に</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/a7cc545ecf57240d91c97a102246ef6126bdc7f8_1920x1080_612181a347/a7cc545ecf57240d91c97a102246ef6126bdc7f8_1920x1080_612181a347.webp" alt="a7cc545ecf57240d91c97a102246ef6126bdc7f8-1920x1080.webp" /></p>
<h2>「Connectors」および「Skills」の提供範囲</h2>
<p>Connectorsは、Claudeを外部ツールやデータソースと連携させる機能である。公式ドキュメントでは、Web connectorsは全ユーザーが利用可能とされている。一方で、カスタムコネクターの作成は有料プラン向けと明記されている。</p>
<p>Skillsは、あらかじめ用意された機能拡張や、ユーザーが作成したスキルをClaudeに追加できる仕組みである。設定画面から有効化することで利用でき、無料プランも対象に含まれている。</p>
<p><strong>■ Anthropic、「Claude Skills」を発表──資料を読み込み専門ワークを自動化する新機能</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/agent_skills_7f1c1cf0d3/agent_skills_7f1c1cf0d3.jpg" alt="agent skills.jpg" /></p>
<h2>無料プランと有料プランの機能差</h2>
<p>今回の拡充により主要機能が無料化された一方で、有料プランでは引き続き利用上限の拡大や優先アクセスなどの特典が設けられている。カスタムコネクターの利用可否など、機能の一部にはプランごとの差異が残る。</p>
]]></description>
      <pubDate>Mon, 02 Feb 2026 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ChatGPT、米国で広告表示テストを正式開始──Free／Goユーザーが対象、Plus以上は非表示</title>
      <link>https://ledge.ai/articles/chatgpt_ads_test_us_free_go_2026</link>
      <description><![CDATA[<p>OpenAIは2026年2月9日、対話型AI「ChatGPT」における広告表示のテストを米国で開始したと<a href="https://openai.com/index/testing-ads-in-chatgpt/">発表</a>した。対象はログイン済みの成人ユーザーのうち「Free」および「Go」プランの利用者。Plus、Pro、Business、Enterprise、Educationなどの有料・法人向けプランでは広告は表示されない。</p>
<p>広告は、ChatGPTの回答下部に「スポンサー」と明記される形で表示される。同社は広告の設計方針として、広告がAIの回答内容に影響を与えることは一切ないと強調している。年齢や利用状況などの情報が広告表示の判断に用いられるが、ユーザーの会話内容が広告主に共有されることはないという。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/chatgpt_ads_test_us_free_go_2026_1b4547d6c0/chatgpt_ads_test_us_free_go_2026_1b4547d6c0.webp" alt="chatgpt_ads_test_us_free_go_2026.webp" /></p>
<p>ユーザー向けには、特定の広告を非表示にする操作や、「なぜこの広告が表示されたのか」を確認する機能のほか、パーソナライズ広告の設定を管理する仕組みが提供される。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/12_2d9a500d99/12_2d9a500d99.webp" alt="12.webp" /></p>
<p>OpenAIは広告導入の背景について、サービスの持続可能性を確保しつつ、より多くの人々に無料アクセスを提供し続けるための「収益モデル多様化の一環」と説明している。現在は米国内に限定されたテスト段階であり、今後の展開については状況を見ながら段階的に判断される見通しだ。</p>
]]></description>
      <pubDate>Mon, 02 Feb 2026 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>GitHubコミットの4％がAI生成──Claude Codeは2026年末に20％へ？SemiAnalysisが「エージェント化」の転換点と分析</title>
      <link>https://ledge.ai/articles/claude_code_4_to_20_percent_github_growth</link>
      <description><![CDATA[<p>2026年2月6日、半導体やAI産業の調査・分析を行うSemiAnalysisは、「Claude Code is the Inflection Point」と題したニュースレターを<a href="https://newsletter.semianalysis.com/p/claude-code-is-the-inflection-point">公開</a>した。同社は、Anthropicが提供するCLI型AIエージェントClaude Codeについて、GitHubの公開コミットの約4％が現在同ツールによって生成されていると指摘。さらに、現在の成長軌道が続けば、2026年末には20％超に達する可能性があると予測している。</p>
<p>同社はこれを、生成AIによるコーディング支援が「補助」段階から「エージェント」段階へ移行する転換点だと位置づける。</p>
<h2>IDE補完ではなく、ターミナル常駐型エージェント</h2>
<p>Claude Codeは、IDEのサイドバーでコード補完を行う従来型ツールとは異なり、ターミナルネイティブのCLI（コマンドラインインターフェース）として動作する。</p>
<p>SemiAnalysisによれば、その特徴は以下にある。</p>
<ul>
<li>コードベース全体を読み込む</li>
<li>マルチステップの計画を立てる</li>
<li>タスクを実行する</li>
<li>検証・修正を繰り返す</li>
</ul>
<p>単一の関数や数行単位の補完ではなく、**READ（読み込み）→ THINK（思考）→ WRITE（出力）→ VERIFY（検証）**の循環を自律的に回す点を重視する。同社は「Claude CodeはCodeではなくComputerに近い」と表現し、単なるコーディング支援を超えた汎用的な情報処理エージェントの実例と位置づけている。</p>
<h2>「コール＆レスポンス」から「実行主体」へ</h2>
<p>SemiAnalysisは、AIの価値創出が「トークンの販売」から「トークンのオーケストレーション」へ移行しつつあると分析する。</p>
<p>従来のLLM APIは、入力に対して応答を返す“call &amp; response”型であり、短時間の処理が中心だった。一方、Claude Codeは長時間にわたりタスクを継続実行する設計を採る。モデル単体のベンチマーク性能よりも、エージェントとしての総合的な成果物が競争軸になるというのが同社の見立てだ。</p>
<p>また、METRのデータを引用し、自律実行可能な「タスクホライゾン（継続時間）」が4〜7カ月ごとに倍増していると説明する。30分のコード補完から、数時間のモジュール改修、さらに数日単位の監査業務へと拡張可能性が広がっているとする。</p>
<h2>開発者の役割はどう変わるのか</h2>
<p>ニュースレターでは、複数の著名エンジニアの発言も紹介されている。</p>
<ul>
<li>Node.jsの開発者であるRyan Dahl氏は、「人間がコードを書く時代は終わった」と述べた。</li>
<li>VercelのCTOであるMalte Ubl氏は、自身の「新しい主業務はAIの誤りを指摘することだ」と投稿。</li>
<li>Ruby on Railsの開発者であるDavid Heinemeier Hansson氏は、手書きコードへのノスタルジーを語っている。</li>
<li>Claude Code開発チームのBoris Cherny氏は、チームのコードのほぼ100％がClaude CodeとOpus 4.5によって書かれていると説明した。</li>
</ul>
<p>SemiAnalysisは、生成能力（generation）と識別能力（discrimination）が分離しつつあると整理する。実装を担う主体が人間からAIへ移る一方、人間の役割は設計、評価、検証へと重心が移る可能性があると示唆する。</p>
<p>また、Anthropicが実施した実験では、AIを利用したコーディングは生産性を高める一方で、使い方によっては熟達度の低下が見られたと報告されている。エージェント活用が広がるなかで、開発者のスキル形成との関係も課題として浮上している。</p>
<h2>コーディングは“出発点”にすぎない</h2>
<p>SemiAnalysisは、コーディングはあくまでエージェント型AIが浸透する最初の領域にすぎないと指摘する。情報労働の多くは、READ（読み込み）→ THINK（思考）→ WRITE（出力）→ VERIFY（検証）という共通の工程を持つ。財務分析や法務レビュー、レポート作成なども同様の構造を持ち、コーディングで成立した自律実行型ワークフローは他分野へ拡張可能だと分析する。</p>
<p>GitHubコミットの4％という現状と、20％超という予測は、単なる開発現場の変化にとどまらない可能性を示している。</p>
]]></description>
      <pubDate>Mon, 02 Feb 2026 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Crypto.com共同創業者、消費者向け自律AIエージェント「ai.com」設立──7000万ドルでドメイン取得</title>
      <link>https://ledge.ai/articles/crypto_com_cofounder_launches_ai_com_autonomous_agents_70m_domain</link>
      <description><![CDATA[<p>仮想通貨取引所Crypto.comの共同創業者でCEOを務めるKris Marszalek（クリス・マルザレク氏）は2026年2月6日、消費者向け自律型AIエージェントプラットフォーム「ai.com」の設立を<a href="https://ai.com/company-news/ai-com-launch">発表</a>した。正式な提供開始は2月8日で、米国で開催されるSuper Bowl LXのCM放映に合わせてローンチする。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Kris_Marszalek_x_03379f413b/Kris_Marszalek_x_03379f413b.jpg" alt="Kris Marszalek x.jpg" /></p>
<p>ai.comは、ユーザーが数クリックで「自分専用のAIエージェント」を作成できるとするプラットフォームだ。単なるチャット型AIではなく、ユーザーの代わりに実際のタスクを実行する“自律型”である点を特徴としている。</p>
<p>公式発表によると、エージェントは仕事の整理、メッセージ送信、アプリを横断した操作、プロジェクトの構築などを実行可能だという。株式取引、ワークフロー自動化、カレンダー連携による日次タスクの遂行、オンラインプロフィールの更新といった用途例も挙げられている。</p>
<p>同社は差別化要因として、エージェントがタスク遂行に必要な機能が不足している場合、それを自律的に補完・構築する設計を挙げる。さらに、改善内容はネットワーク上の他のエージェントに共有され、全体として能力が向上すると説明している。</p>
<p>エージェントは技術的知識なしに約60秒で作成でき、無料で利用開始が可能。追加機能や入力トークンの拡張などは有料サブスクリプションで提供される。各エージェントは専用の安全な環境で稼働し、データは分離・暗号化され、ユーザー固有のキーで管理されるとしている。</p>
<p>マルザレク氏は「ai.com」ドメインを7000万ドル（約110億円）で取得した。ドメイン仲介会社<a href="https://www.prnewswire.com/news-releases/getyourdomaincom-brokers-the-70-million-sale-of-aicom-the-largest-domain-name-transaction-in-history-302682315.html">GetYourDomain.com</a>は、この取引が公表されているドメイン名売買の中で史上最高額だと発表している。従来の最高額とされていた「voice.com」（3000万ドル）を上回る規模となる。</p>
<p>マルザレク氏はCrypto.comとai.comの両社でCEOを務める。今回の発表は、暗号資産領域を主軸としてきた同氏が、消費者向けAIエージェント分野へ本格参入する動きとなる。</p>
<p>ai.comは「AGI（汎用人工知能）の到来を加速する」ことを掲げており、今後は金融サービスとの統合やエージェントのマーケットプレイス構築なども検討しているという。</p>
]]></description>
      <pubDate>Mon, 02 Feb 2026 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>AI時代の科学研究のボトルネックは「アイデア創出」から「検証」へ──査読制度に警鐘、Google・カーネギーメロン大学など16機関がケーススタディを公表</title>
      <link>https://ledge.ai/articles/gemini_science_impending_crisis_peer_review</link>
      <description><![CDATA[<p>2026年2月3日、Google Research、カーネギーメロン大学（Carnegie Mellon University）など16機関の研究者は、Googleの推論強化型モデル「Gemini Deep Think」の高度版を研究に活用したケーススタディをarXivで<a href="https://arxiv.org/abs/2602.03837">公開</a>した。</p>
<p>理論計算機科学、暗号、物理学など複数分野において、長年の予想に対する反例構成や既存論文の重大な欠陥検出、数式とコード実行を往復する解析の高度化などの成果を報告している。</p>
<p>論文は、AIが専門家レベルの数学的推論において実質的な「共同研究者」として機能し得る可能性を示す一方で、科学のボトルネックが「アイデア創出」から「正しさの検証」へ移行しつつあるとして、「Impending Crisis（差し迫った危機）」と題する節で査読制度への警鐘を鳴らしている。</p>
<h2>反例構成・欠陥検出・数式検証──具体的成果</h2>
<p>研究チームは、主に高度版のGemini Deep Thinkを用い、人間研究者との対話的なワークフローの中で成果を導いた。</p>
<p>まず、オンライン劣モジュラ福祉最大化問題において、2015年に提示され長年有効と考えられてきた予想に対し、AIが最小規模の反例を構成。期待値計算を伴う検証を通じて、予想が成り立たないことを示した。</p>
<p>また、LWE（Learning With Errors）に基づくSNARGsを扱う暗号学プレプリントを精査した事例では、定義と構成の間の不整合という重大な欠陥を特定。人間研究者による確認の結果、主張の成立に影響する問題であることが判明し、著者側が修正を行ったという。</p>
<p>物理学分野では、宇宙ひも（Cosmic String）の放射スペクトル解析において、AIが数式提案とPythonによる検証コード実行を繰り返す「ニューロ・シンボリック」ループを活用。実行エラーを踏まえて解法を修正し、閉形式解に到達した例が紹介されている。</p>
<h2>AIは万能ではない──確認された失敗モード</h2>
<p>論文は成功例のみを強調していない。AIの限界についても明確に記述している。</p>
<p>偽の予想を「証明せよ」と与えた場合、もっともらしい推論で論理の飛躍を埋めようとする確証バイアスが観察された。このため、研究者は「prove or refute（証明または反証）」といった中立的指示が重要だとする。</p>
<p>また、問題の出典論文をそのまま文脈として提示すると、「これは未解決問題である」と判断し、探索を停止するケースも確認された。対策として、論文情報を除外し、問題文と定義のみを提示する手法が紹介されている。</p>
<p>高度な洞察を示す一方で、符号の誤りや制約の見落としといった単純な代数ミスも発生しており、出力の厳密な検証は不可欠であるとされる。</p>
<h2>「vibe-proving」──人間が指揮する協働モデル</h2>
<p>研究者らは、AIを「疲れ知らずで博識、創造性に富む若手共同研究者」のような存在と位置づける。ただし成果はAIの自律性だけで生まれるものではない。</p>
<p>問題分解、仮説の方向付け、反証的チェック、最終的な検証は人間が担う。著者らはこの協働プロセスを非公式に「vibe-proving」と呼び、強い人間オーケストレーションの重要性を強調している。</p>
<h2>生成から検証へ──査読制度への警告</h2>
<p>研究者は論文終盤で、AIが論文生成の摩擦を劇的に低下させるほど、科学のボトルネックは「アイデア生成」から「検証」へ移ると述べている。従来の人間中心の査読制度は、AIによって高速かつ大量に生成される高度な論文を十分に精査できるよう設計されていない可能性があるという。</p>
<p>著者らは、生成に用いられたのと同様のAI技術を、欠陥検出や形式的検証支援に活用するなど、査読・評価側の仕組みを再設計する必要性を示唆している。</p>
]]></description>
      <pubDate>Mon, 02 Feb 2026 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google、冬季五輪代表をAIで支援──撮影した映像を数分で3D解析するスキー・スノーボード向け分析ツールを構築</title>
      <link>https://ledge.ai/articles/google_ai_3d_video_analysis_ski_snowboard_team_usa</link>
      <description><![CDATA[<p>Google Cloudは2026年2月5日、米国のスキー・スノーボード競技統括団体である U.S. Ski &amp; Snowboard と共同で、AIを活用した映像分析プラットフォームを構築したと<a href="https://blog.google/innovation-and-ai/infrastructure-and-cloud/google-cloud/us-ski-snowboard-tool-winter-olympics-2026/">発表</a>した。撮影した映像から選手の動きを解析し、数分以内にフィードバックを得られる点を特徴とする。</p>
<p>フリースタイルスキーやスノーボードでは、時速80キロ近いスピードでの滑走中に、わずかな踏切角度や回転の差が成績を左右する。Google Cloudは、こうした競技特性を踏まえ、同団体と連携して「業界初（industry-first）」とするAI映像分析ツールを開発した。スノーボードの Maddie Mastro 選手や、フリースキーの Alex Hall 選手らが活用事例として紹介されている。</p>
<p>@<a href="https://www.youtube.com/watch?v=ELvfX0l6dyQ">YouTube</a></p>
<h2>専用機材を使わず、映像から人体の動きを3D推定</h2>
<p>従来、高精度なモーションキャプチャは、専用スーツやセンサーを装着し、管理されたスタジオ環境で行う必要があった。今回のツールはそうした制約を排し、撮影した2D映像のみを用いて、選手の動きを3Dで推定する。Google DeepMindが進めてきた空間知能（spatial intelligence）の研究成果を活用し、厚手のウインターギア越しでも骨格ポイントを推定できる「マーカー不要（markerless）」の手法を採用している。</p>
<p>解析処理はGoogle Cloud上で行われ、データは数分で生成される。発表によると、分析結果が次のリフトに乗っている間に得られるケースもあるという。</p>
<h2>Geminiを通じて、データと対話</h2>
<p>解析後、コーチや選手はGoogleの生成AI「Gemini」を用いて、データを自然言語で照会できる。例えば、「昨日のベストランと踏切角はどう違うか」といった質問に対し、数値に基づいた回答を即座に得られるとしている。</p>
<p>U.S. Ski &amp; Snowboardで競技全体を統括するアヌーク・パティ氏は、「映像は最も一般的で有効なコーチングツールだが、これまでは分析に時間がかかっていた。AIによって競技レベルの映像をより深く、安全に活用できるようになる」とコメントした。</p>
<h2>極限環境での共同開発と今後</h2>
<p>ツールの開発にあたり、Google Cloudのエンジニアは、同団体のフリースキーおよびスノーボードチームとともに、オーストリアや米コロラド州などの極限環境で検証を行った。山の上での主観的な観察と、ラボ内での高精度計測の間にあった長年のギャップを埋めることが狙いだとしている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Dawsy_2023_Europe_Cortina_1_29_03a3b4f2e5/Dawsy_2023_Europe_Cortina_1_29_03a3b4f2e5.jpg" alt="Dawsy_2023_Europe_Cortina-1-29.jpg" /></p>
<p>Google Cloudで生成AI分野を統括するオリバー・パーカー氏は、「世界最高峰のアスリートを最も過酷な環境で支える技術を確立することで、人の動きの理解と改善の在り方を変える」と述べた。</p>
<p>このAI映像分析プラットフォームは現時点では実験的な取り組みと位置づけられており、U.S. Ski &amp; Snowboardの選手・コーチ陣による検証が、冬季五輪本番に向けて続けられている。</p>
]]></description>
      <pubDate>Mon, 02 Feb 2026 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>車限定だった「Geminiナビ」が徒歩・自転車にも拡大──Google マップで利用可能に</title>
      <link>https://ledge.ai/articles/google_maps_gemini_navigation_walking_cycling</link>
      <description><![CDATA[<p>地図アプリ「Google マップ」のナビゲーション中に利用できるAIアシスタント「Gemini」について、これまで自動車の運転時に限定していた対応を拡大し、徒歩および自転車でのナビゲーション中でも利用可能にした。同社が公式ブログで<a href="https://blog.google/products-and-platforms/products/maps/gemini-navigation-biking-walking/">発表</a>した。機能はGemini提供地域において、AndroidおよびiOS向けに順次提供される。</p>
<h2>自動車ナビ限定から、徒歩・自転車へ拡大</h2>
<p>Geminiは、Google マップのナビゲーション中に音声で呼び出し、目的地までの案内を続けながら各種操作や質問に対応するAIアシスタントとして提供されてきた。従来は自動車でのナビゲーション時に限られていたが、今回のアップデートにより、徒歩ナビおよび自転車ナビでも同様の体験が可能となった。</p>
<h2>ナビ中に使える主な機能</h2>
<p>ナビゲーション中、ユーザーは音声による自然言語でGeminiに指示を出すことができる。たとえば、</p>
<ul>
<li>到着予定時刻（ETA）の確認</li>
<li>ルート周辺の飲食店や施設の検索</li>
<li>簡単なメッセージ送信</li>
</ul>
<p>などを、ナビを中断せずに行える。Googleは、移動中に画面操作を最小限に抑える設計である点を強調している。
徒歩・自転車利用での意味合い</p>
<p>徒歩での移動中には、周辺情報を把握しながら目的地まで案内を受けられる点が特徴となる。また自転車での利用では、走行中に画面へ触れることなく音声で操作できるため、ハンズフリーによる利便性の向上が想定されている。</p>
<h2>提供条件と対応環境</h2>
<p>この機能は、Geminiが提供されている地域で利用可能となる。対応端末はAndroidおよびiOSで、最新版のGoogle マップが必要となる。提供は段階的に行われるため、利用可能になる時期はユーザーや地域によって異なる。</p>
<h2>導入の背景</h2>
<p>Googleは、ナビゲーション中の音声アシスタントを従来のGoogle アシスタントからGeminiへと順次移行してきた。今回の対応拡大は、自動車以外の移動手段にもAIナビ体験を広げる取り組みの一環と位置づけられる。</p>
]]></description>
      <pubDate>Mon, 02 Feb 2026 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>対話が続くほど誤りが連鎖する──LLMハルシネーションの新ベンチマーク「HalluHard」、Web検索機能をONにしたフラッグシップモデルでも幻覚率約30%</title>
      <link>https://ledge.ai/articles/halluhard_multi_turn_hallucination_search_on_30_percent</link>
      <description><![CDATA[<p>スイス連邦工科大学ローザンヌ校（EPFL）および欧州AI研究組織ELLISの研究チームは、大規模言語モデル（LLM）の事実誤認（ハルシネーション）を測定する新ベンチマーク「HalluHard」を提案した。論文は2026年2月1日arXivに<a href="https://arxiv.org/abs/2602.01031v1">公開</a>され、Web検索機能を有効化したフラッグシップモデルにおいても、対話の最終ターンにおけるハルシネーション率は約30％に達すると報告した。</p>
<h2>単発QAでは見えない「対話の深化に伴う誤りの連鎖」</h2>
<p>実運用におけるユーザーとLLMのやり取りは複数ターンにわたることが多いが、従来の評価系は一問一答形式が主流であった。</p>
<p>研究チームは、対話が継続する過程で「前のターンの誤情報が、次のターンの推論前提として取り込まれる」という構造的リスクを指摘している。HalluHardは、950のseed質問をベースに、平均3ターンの対話形式で文脈が積み重なる設計を採用した。</p>
<h2>法務・医療・研究・開発の4領域で「根拠との整合性」を厳格に検証</h2>
<p>対象領域は、Legal Cases、Medical Guidelines、Research Questions、Codingの4分野に渡る。同ベンチマークの評価基準は、単なる回答の正誤判定に留まらず、回答内の事実主張に対する「インライン引用」の義務化を特徴とする。評価パイプラインでは、ウェブ検索やPDF全文解析を通じて証拠を取得し、事実の創作、引用と主張の不一致、根拠の不足を厳密にハルシネーションとして定義している。</p>
<h2>最新の検索統合型モデルであっても「最終ターンの幻覚率は約30％」に達する</h2>
<p>論文内の分析によれば、ほぼ全てのモデルにおいて、第1ターンから第3ターンへと対話が進むにつれてハルシネーション率が上昇する傾向が確認された。特に、Web検索ツールを有効化した最新のフラッグシップモデルにおいても、最終ターンでは約30％前後のハルシネーション率が観測されている。これは、外部知識へのアクセスが可能であっても、根拠の選択や解釈の段階で誤りが発生し、それが次ターンの推論に影響を及ぼす構造が確認された。</p>
<h2>継続更新型のリーダーボードとして「マルチターン対話の信頼性」を定量化</h2>
<p>HalluHardは評価フレームワークおよびリーダーボードとして公開されており、今後も新たなモデルの追加や更新が予定されている。対話の継続に伴う誤りの連鎖を可視化する同ベンチマークは、生成AIの実運用における信頼性を評価する新たな指標としての役割を担う。</p>
]]></description>
      <pubDate>Mon, 02 Feb 2026 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Microsoft、「Publisher Content Marketplace」始動──AIの “自由なコンテンツ利用” を転換、出版社が条件提示し対価を得る</title>
      <link>https://ledge.ai/articles/microsoft_publisher_content_marketplace_pcm_launch</link>
      <description><![CDATA[<p>Microsoftは2026年2月3日（現地時間）、コンテンツクリエイターやパブリッシャーが自社コンテンツの利用条件を定義し、AI企業に対してライセンスを提供できる仕組み「Publisher Content Marketplace（PCM）」を立ち上げたと<a href="https://about.ads.microsoft.com/en/blog/post/february-2026/building-toward-a-sustainable-content-economy-for-the-agentic-web">発表</a>{target=”_blank”}した。AIによるコンテンツ利用が広がる中で、出版社が望む形で条件を設定し、その対価を得られる市場の構築を目指す。</p>
<h2>検索から会話へ、回答品質を左右する「根拠コンテンツ」</h2>
<p>Microsoftによると、生成AIは検索結果の提示にとどまらず、会話形式で直接「答え」を返す存在へと変化している。こうした変化に伴い、回答の品質や信頼性を左右する要因は、どのコンテンツを根拠（grounding）として参照できるかに移りつつあるという。従来のように、出版社が記事を公開し、検索エンジンがトラフィックを送るという暗黙の関係は、AIが回答を完結させる世界では成立しにくくなるとの認識を示した。</p>
<h2>PCMの仕組み：出版社が条件を決め、利用価値に応じて支払う</h2>
<p>PCMでは、出版社があらかじめコンテンツのライセンス条件や利用範囲を設定し、AI開発者は用途やシナリオに応じてコンテンツを発見・契約する。支払いは「提供された価値（delivered value）」に基づいて行われるとし、どのコンテンツがどのように利用されたかを可視化する利用実績レポート（usage-based reporting）も提供する。Microsoftは、これにより出版社とAI開発者の間に、透明なフィードバックループを構築すると説明している。</p>
<p>参加は任意で、出版社はコンテンツの所有権や編集上の独立性を常に保持すると明記された。個別の一対一契約を積み重ねる従来の方式ではなく、市場型の仕組みによって規模拡大を可能にする点が特徴だとしている。対象は大手メディアに限らず、独立系パブリッシャーも含む。</p>
<h2>主要出版社と共同設計、Copilotでの検証も進行</h2>
<p>Microsoftは、PCMの構想にあたり、米国の主要出版社と数カ月にわたり共同設計（co-design）を進めてきたという。Associated PressやBusiness Insider、Condé Nast、Hearst Magazines、People、USA TODAY、Vox Mediaなどが協力しており、すでにMicrosoft Copilot（企業向け・消費者向け）において、ライセンス済みコンテンツを用いた回答の根拠付けを検証してきたとしている。需要側パートナーとしては、Yahooのオンボーディングも始まっている。</p>
<p>パブリッシャー側の動きも明らかになっている。Business Insiderは、PCMが「事業として始動した」とし、自社が創設パートナー（founding partner）として参加することを<a href="https://www.businessinsider.com/business-insider-is-powering-the-ai-ecosystem-2026-2">公表</a>{target=”_blank”}した。自社のジャーナリズムがAIエコシステムを支える基盤になると位置づけている。</p>
<p>Microsoftは、現段階のPCMをパイロットフェーズと位置づけ、価値観を共有する出版社やAI開発者とともに、今後段階的に拡大していく方針を示している。生成AI時代におけるコンテンツ利用と報酬のあり方をめぐり、新たな枠組みとしての定着が注目される。</p>
]]></description>
      <pubDate>Mon, 02 Feb 2026 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>NTTとTBS、IOWN×LLMで “AIテーマパーク” 構想──「自分で決める力」を育む次世代エデュテインメント「e6 project」始動</title>
      <link>https://ledge.ai/articles/ntt_tbs_iown_llm_ai_theme_park_e6_project</link>
      <description><![CDATA[<p>NTTとTBSホールディングスは2026年2月10日、生成AIが普及する社会において、子どもたちが「自分で決める力」を育む次世代エデュテインメント「e6 project（イーシックス・プロジェクト）」を始動すると<a href="https://group.ntt/jp/newsrelease/2026/02/10/260210a.html">発表</a>した。</p>
<p>プロジェクトは、完全オリジナルIPの開発と、その世界観を体験できる常設拠点「AIテーマパーク（仮称）」の事業化を両輪として推進する構想だ。生成AIの普及により、多数の選択肢や「もっともらしい答え」が瞬時に提示される時代において、子どもたちが自ら「何を選ぶか」を判断する力を育むことを目的とする。</p>
<p>リリースでは、その判断の根源を「感情（Emotion）」と位置づけ、AIを“学ぶ対象”として前面に出すのではなく、物語と体験を起点とした設計思想を掲げている。</p>
<h2>両社の役割と技術基盤</h2>
<p>NTTは、次世代情報通信基盤構想「IOWN（Innovative Optical and Wireless Network）」およびNTT版LLM「tsuzumi 2」などの先端技術を統合し、子どもの行動や感情の変化に応じて物語や演出が変化する体験基盤を提供する。</p>
<p>TBSは、エデュテインメント専任部署「エデュテインメント事業センター」を新設。長年培ってきたストーリーテリングやクリエイティブ制作力を活かし、探究学習とエンターテインメントを融合した世界観設計を担う。</p>
<h2>オリジナルIP「感情騎士 - エモーショナル・ナイト -」</h2>
<p>第一弾IPとして、冒険ファンタジー「感情騎士 - エモーショナル・ナイト -」を開発する。物語は「見えない感情」を探す冒険を通じて、子どもたちが自らの価値観に気づく構成とする。</p>
<p>今後はアニメ、ゲーム、グッズなどへのメディアミックス展開を視野に入れ、リアル体験拠点と連動させる。</p>
<p><strong>第一弾IP「感情騎士 - エモーショナル・ナイト -」</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/260210ab_e843f87b4a/260210ab_e843f87b4a.jpg" alt="260210ab.jpg" /></p>
<h2>「AIテーマパーク（仮称）」構想</h2>
<p>オリジナルIPの世界観を体験できる常設拠点として、「AIテーマパーク（仮称）」の設立に向けた事業検証を進める。先端技術を活用し、来場者一人ひとりの行動や感情の変化をリアルタイムに捉え、ストーリーや環境が変化するパーソナライズ体験を提供する構想だ。</p>
<p>各展開は検討状況により変更の可能性があるとしている。</p>
<h2>6つのE × 3つの体験サイクル</h2>
<p><strong>「Education / Entertainment / Experience / Emotion / Evolution / Epiphany」の6つのEを核とした体験設計</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/260210ac_dd3ee0c8c4/260210ac_dd3ee0c8c4.jpg" alt="260210ac.jpg" /></p>
<p>e6 projectは、</p>
<ul>
<li>Education（教育）</li>
<li>Entertainment（娯楽）</li>
<li>Experience（体験）</li>
<li>Emotion（感情）</li>
<li>Evolution（進化）</li>
<li>Epiphany（ひらめき）</li>
</ul>
<p>の6つの要素（E）を核に、「没入」「判断」「覚醒」の3つの体験サイクルを設計思想として掲げる。</p>
<p>「没入」では遊びの中で自然に知識や技術への関心を引き出し、「判断」では体験と感情を通じて選択する力を育む。「覚醒」ではAIバディとの相互作用により、新たなひらめきと成長を促すとしている。</p>
<h2>今後の展開</h2>
<p>第一弾体験コンテンツの詳細や体験方法については、2026年2月下旬頃に改めて発表予定としている。</p>
]]></description>
      <pubDate>Mon, 02 Feb 2026 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>NY州、大規模データセンター建設を「一時停止」へ──最短3年のモラトリアム法案を提出</title>
      <link>https://ledge.ai/articles/ny_data_center_permit_moratorium</link>
      <description><![CDATA[<p>ニューヨーク州上院は2026年2月6日、急増するデータセンター建設に歯止めをかけるため、新規データセンターに関する許認可の発行を原則停止する法案「<a href="https://www.nysenate.gov/legislation/bills/2025/S9144">S9144</a>」を提出した。法案が成立すれば、環境への影響や公共料金への負担が精査されるまで、大規模なデータセンター開発は許認可面で事実上困難となる。</p>
<h2>20MW以上の大規模施設が対象、「新規許可」を原則停止</h2>
<p>法案がターゲットとするのは、20メガワット（MW）以上の電力を使用可能な大規模なデータセンターだ。同一の運営主体による隣接サイトなども含まれる。 州や自治体は、こうした施設の立地、建設、運用に関する新たな許認可を出してはならないと明記されており、AIインフラの中核を担う大規模拠点が直接の影響を受ける。</p>
<h2>規制が整うまで「最短3年間」の空白期間</h2>
<p>この停止措置（モラトリアム）は、一定期間で自動解除される仕組みではなく、規制整備の完了と連動している。具体的には、州の環境保全局（DEC）が新たな規制を策定し、公共サービス委員会（PSC）が電気・ガス料金への影響に関する措置を完了した後、さらに90日が経過するまで許認可は再開されない。
スポンサー説明では、停止期間は少なくとも3年と90日に及ぶとしており、その間の新規着工は極めて難しくなる。</p>
<h2>電力・水・ゴミまで、徹底した「環境影響評価」を義務化</h2>
<p>法案は、州の環境保全局に対し、データセンター開発に関する包括的な環境影響評価（GEIS）の作成を義務づけている。評価対象は、電力消費と電源構成、水使用と放流、土地利用や農地への影響、温室効果ガス排出、大気・水質・騒音汚染、電子廃棄物（e-waste）まで多岐にわたる。ドラフトのGEISは公表後、120日以上のパブリックコメント期間を設けるとともに、州内9地域での対面公聴会を実施し、住民の意見を反映させるプロセスが義務づけられている。</p>
<p>あわせて、公共サービス委員会は、データセンター増設が電気・ガス料金に与える影響について18カ月以内に報告書をまとめる必要がある。さらに3年後以降、一般の住宅や商業利用者にインフラ増強コストが転嫁されないよう、追加の命令を出すことが求められている。</p>
<h2>加速するAI需要とインフラ負荷の衝突</h2>
<p>法案提出の背景には、生成AIの普及を背景としたデータセンター建設ラッシュがある。膨大な電力と水を消費する施設の急増は、州の脱炭素目標の達成を難しくするだけでなく、電力網の逼迫や料金上昇を招くとの懸念が強まっている。
州としては、一度立ち止まり、環境・エネルギー・料金への影響を精査した上で、持続可能な開発ルールを再定義する狙いだ。</p>
<p>法案は今後、上下両院での審議に進む。可決されれば、ニューヨーク州におけるAIインフラ投資の在り方に大きな影響を与えることになりそうだ。</p>
]]></description>
      <pubDate>Mon, 02 Feb 2026 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>元OpenAI研究者、ChatGPT広告テスト開始の週に辞職　NYTに「OpenAIはFacebookと同じ過ち」と題した寄稿</title>
      <link>https://ledge.ai/articles/openai_ads_test_researcher_quits_nyt_facebook_comparison</link>
      <description><![CDATA[<p>OpenAIが2026年2月9日に対話型AI「ChatGPT」で広告表示のテストを開始した週に、同社の元研究者であるZoë Hitzig氏が<a href="https://x.com/zhitzig/status/2021590831979778051">辞職を公表</a>した。Hitzig氏は2026年2月11日付の米紙The New York Times(NYT)のオピニオン欄に「OpenAI Is Making the Mistakes Facebook Made. I Quit.（OpenAIはFacebookと同じ過ちを犯している。私は辞めた）」と題したエッセイを<a href="https://www.nytimes.com/2026/02/11/opinion/openai-ads-chatgpt.html">寄稿</a>し、広告モデル導入に対する懸念を示した。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Zoe_Hitzig_4e4cb3454d/Zoe_Hitzig_4e4cb3454d.jpg" alt="Zoë Hitzig.jpg" /></p>
<h2>2年間にわたりモデル構築や価格設計に関与</h2>
<p>寄稿によると、Hitzig氏はOpenAIに約2年間在籍し、AIモデルの構築や価格設計（pricing）、初期の安全性方針の策定に携わってきたという。同氏は、広告そのものを「不道徳あるいは非倫理的だとは考えていない」と明言する一方で、同社の戦略に「深い懸念」を抱いていると述べた。</p>
<p>Hitzig氏は、ChatGPTの利用者がこれまでに前例のない規模で個人的な悩みや医療上の不安、信仰観などを打ち明けてきた点に言及。そうした会話アーカイブを基盤とした広告モデルが、ユーザーを操作する可能性を持つと指摘している。</p>
<h2>OpenAIは広告を明確表示、回答への影響は否定</h2>
<p>OpenAIは同週、ChatGPTにおける広告表示のテスト開始を<a href="https://ledge.ai/articles/chatgpt_ads_test_us_free_go_2026">発表</a>した。広告は回答の下部に表示され、明確にラベル付けされるほか、回答内容には影響しないとしている。</p>
<p>広告はAI運用コストを賄う収益源の一つとなり得るが、Hitzig氏は、広告導入を巡る議論を「広告か、アクセス制限か」という二者択一として捉えるのは誤りだと主張。企業向け利用からのクロスサブシディや、独立した監督権限を持つガバナンス構造、データを独立的に管理するトラストモデルなど、代替案を提示している。</p>
<h2>「Facebookの過ち」を引き合いに</h2>
<p>寄稿では、かつてFacebook（現Meta）が広告モデルのもとでエンゲージメント最適化を進める過程で、当初掲げたデータ管理や利用者の統制に関する原則が徐々に後退した経緯にも言及している。Hitzig氏は、広告モデルが強いインセンティブを生み出す構造にあると指摘し、OpenAIが同様の道をたどる可能性に懸念を示した。</p>
<h2>8億人規模の利用基盤</h2>
<p>ChatGPTの週間利用者は<a href="https://ledge.ai/articles/openai_devday2025_chatgpt_800m_wau">8億人規模</a>に達しているとされる。こうした大規模な利用基盤のもとで広告モデルが導入されることも、今回の議論の背景となっている。</p>
<p>生成AIの運用コストと広範なアクセス確保をどう両立させるかは、業界全体の課題でもある。ChatGPTへの広告導入をめぐる議論は、今後も続く可能性がある。</p>
]]></description>
      <pubDate>Mon, 02 Feb 2026 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Roblox、“その場で遊べる世界” を現実に──AI基盤「Cube」で4D生成を発表　機能する3Dオブジェクトを自然言語で生成</title>
      <link>https://ledge.ai/articles/roblox_cube_4d_generation_playable_world</link>
      <description><![CDATA[<p>Robloxは2026年2月4日、同社の生成AI基盤「Cube Foundation Model」を活用した新たな制作機能「4D generation（4D生成）」を<a href="https://about.roblox.com/newsroom/2026/02/accelerating-creation-powered-roblox-cube-foundation-model">発表</a>した。</p>
<p>4D生成は、従来の静的な3Dオブジェクト生成を拡張し、振る舞いや機能を備えた3Dオブジェクトを自然言語プロンプトから生成できる点が特徴で、Roblox Studio向けにベータ提供を開始する。</p>
<h2>静的な3Dから「その場で遊べる」生成へ</h2>
<p>Robloxによると、4D生成は「インタラクティビティ」という新たな次元を加えることで、生成されたオブジェクトが見た目だけでなく、プレイヤーの期待どおりに機能することを可能にする。例えば、テキストプロンプトで生成した車は、単なる3Dモデルではなく、実際に乗り込み、操作して走らせることができる。</p>
<p>この仕組みは、「スキーマ（schemas）」と呼ばれるルールセットに基づいている。スキーマは、特定のオブジェクトを構成要素に分解し、それぞれに適切な挙動を割り当てる役割を担う。これにより、形状やサイズが異なるオブジェクトであっても、生成後すぐに一貫した動作を実現できるという。</p>
<p><strong>4D生成の仕組み：</strong> 入力された3Dメッシュ（左）をスキーマに基づいて構成要素へ分解し、それぞれに機能を付与したオブジェクトとして再生成する（右)
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Roblox_Cube_Foundation_Model_3_676ee1e8ee/Roblox_Cube_Foundation_Model_3_676ee1e8ee.jpg" alt="Roblox Cube Foundation Model 3.jpg" /></p>
<h2>Roblox Studioでの制作とプレイヤー参加型生成</h2>
<p>4D生成は、Roblox Studio内で有効化することで、クリエイターだけでなくプレイヤー自身が体験内でオブジェクトを生成できる点も特徴だ。プレイヤーは自然言語による簡単なプロンプトを使い、車や飛行機、ドラゴンなどのオブジェクトをその場で生成し、即座にゲームプレイへ組み込める。</p>
<p>Robloxは、この仕組みにより、従来は開発者に限定されていた制作行為を、体験の一部としてプレイヤーに開放できるとしている。新たなゲームプレイの形や、プレイヤーエンゲージメントの拡張につながる可能性があるという。</p>
<h2>実証例「Wish Master」で示された効果</h2>
<p>公式リリースでは、開発者Laksh氏が手がけるゲーム「Wish Master」での活用事例も紹介された。
Wish Masterでは、プレイヤーが入力した「願い」に応じてオブジェクトが生成される仕組みを採用しており、4D生成によって、走行可能な車や飛行する飛行機、空を舞うドラゴンなどが体験内に出現する。</p>
<p><strong>4D生成を用いたゲーム「Wish Master」：</strong> プレイヤーが入力したテキストに応じて、体験内にオブジェクトが生成される。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/image3_7954b37919/image3_7954b37919.webp" alt="image3.webp" /></p>
<p>早期アクセス期間中、4D生成によって16万点以上のオブジェクトが生成され、4D生成を利用したプレイヤーは、平均プレイ時間が64％増加したという。Robloxは、こうしたデータが「その場で遊べる生成」が体験価値を高めることを示していると説明している。</p>
<h2>Cube Foundation Modelが支える4D生成</h2>
<p>4D生成を支える「<a href="https://about.roblox.com/newsroom/2025/03/introducing-roblox-cube">Cube Foundation Model</a>」は、Robloxが3Dオブジェクト生成や、将来的にはシーン全体の生成を視野に入れて構築した基盤モデルだ。Robloxは、アセットや環境、コード、アニメーションなどを自然言語プロンプトから生成する制作基盤の実現を目標に掲げている。</p>
<p>現在のベータでは、限られた種類のオブジェクト生成から提供を開始しているが、同社は今後、現実世界に存在する数千種類のオブジェクトをカバーするオープンなスキーマ体系へと拡張していく方針だ。</p>
<h2>次段階としての「real-time dreaming」</h2>
<p>Robloxは今回のリリースの中で、4D生成の先にある取り組みとして、「real-time dreaming」と呼ぶ研究プロジェクトにも言及した。
これは、世界モデルを活用し、没入型の環境を生成・反復・デバッグしながら、共同制作までを自然言語で行えるようにする構想だという。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/roblox_real_time_dreaming_d6a7e2c230/roblox_real_time_dreaming_d6a7e2c230.jpg" alt="roblox real-time dreaming.jpg" /></p>
<p>生成AIによる仮想世界構築をめぐっては、Google DeepMindの「Project Genie」など、世界モデルを用いた研究も進められている。Robloxは、4D生成によって「その場で遊べる」体験を実装しながら、より包括的な世界生成へと研究領域を広げていく考えだ。</p>
]]></description>
      <pubDate>Mon, 02 Feb 2026 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>日本発 AI VTuber「しずく」開発元Shizuku AI、a16z主導でシード調達──「世界で一番愛されるAIキャラクター」目指す</title>
      <link>https://ledge.ai/articles/shizuku_ai_a16z_seed_funding</link>
      <description><![CDATA[<p>日本発のAI VTuber「しずく」を開発するShizuku AIは2026年2月10日、米ベンチャーキャピタル大手のAndreessen Horowitz（a16z）をリード投資家として資金調達を実施したと<a href="https://x.com/cumulo_autumn/status/2021019421385556234">発表</a>した。調達額は公表されていない。</p>
<p>今回の資金調達はシードラウンドにあたり、<a href="https://www.a16z.news/p/investing-in-shizuku">a16z</a>がこれを主導した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/aki_shizuku_x_9372ba6023/aki_shizuku_x_9372ba6023.jpg" alt="aki shizuku x.jpg" /></p>
<h2>2023年に始まったAI VTuberプロジェクト</h2>
<p>Shizuku AIの原点は、創業者のAkio Kodaira氏が2023年1月に開始したAI VTuber「Shizuku」にある。当時は米カリフォルニア大学バークレー校の博士課程在籍中だった。</p>
<p>Shizukuは、日本語と英語で視聴者と対話し、歌唱機能を備え、Live2Dアバターでリアルタイムに応答する仕組みを持つ。YouTube上で配信を重ね、視聴者コミュニティを形成してきた。この取り組みを発展させる形でShizuku AIが設立された。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/shizuku_ai_1_cd17d6e3d7/shizuku_ai_1_cd17d6e3d7.jpg" alt="shizuku ai 1.jpg" /></p>
<h2>リアルタイム生成技術を基盤に</h2>
<p>Kodaira氏は、リアルタイム画像生成を90fps以上で実現した研究「StreamDiffusion」の筆頭著者。同研究はICCV 2025に採択され、オープンソース実装はGitHubで1万以上のスターを獲得している。</p>
<p>その後、Metaでリアルタイム動画生成に携わり、Luma AIでも研究職を務めた経歴を持つ。こうしたリアルタイム生成技術の知見が、AIキャラクター開発の技術基盤となっている。</p>
<p>a16zは、Kodaira氏が日本のキャラクター文化の文脈を背景に持ち、技術とキャラクター設計を横断できる点にも触れている。</p>
<h2>「日本から、世界で愛されるAIキャラクターへ」</h2>
<p>創業者は今回の資金調達にあたり、「日本から、世界で一番愛されるAIキャラクター、そして日常に寄り添い支えとなるAIコンパニオンを全力で作っていく」との方針を示している。</p>
<p>Shizuku AIは、日本にAIラボを設立し、AIコンパニオンおよびキャラクター開発に特化した研究開発を進める計画だ。今後はShizukuの機能拡張を中心に、多言語音声合成の強化や会話能力の高度化、DiscordやYouTube、Xなど複数プラットフォームでの展開を進める。</p>
<p>@<a href="https://www.youtube.com/watch?v=pk5FnS8Q69M">YouTube</a></p>
]]></description>
      <pubDate>Mon, 02 Feb 2026 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>社会をシミュレートするAI「Simile」が1億ドル調達。実在の人間をモデルにした数百万人のエージェントが意思決定を予見</title>
      <link>https://ledge.ai/articles/simile_social_ai_simulation_series_a_100m</link>
      <description><![CDATA[<p>AIスタートアップSimileのCEOであるJoon Sung Park氏は2026年2月13日、実在の人間をモデルにしたエージェントで構成される「社会のAIシミュレーション」を構築したと<a href="https://x.com/joon_s_pk/status/2022023097017421874">発表</a>した。製品や政策など、数百万人規模に影響を与える意思決定を事前に再現・検証できる基盤モデルの開発を進めているという。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/introduce_simile_b002315008/introduce_simile_b002315008.jpg" alt="introduce simile.jpg" /></p>
<p>あわせて、同社はSeries Aラウンドで1億ドル（約150億円）を調達したことを明らかにした。リード投資家はIndex Venturesが務め、Hanabi、A*、Bain Capital Ventures（BCV）らが参加した。また、個人投資家としてAndrej Karpathy氏、Fei-Fei Li氏、Adam D’Angelo氏、Guillermo Rauch氏、Scott Belsky氏らも出資者に名を連ねている。</p>
<h2>実在人間に基づくエージェントで社会を再現</h2>
<p><a href="https://simile.ai/blog/the-simulation-company">Simile</a>は、人間の行動や意思決定を再現する基盤モデルの開発を専門とする企業である。CEOのJoon Sung Park氏は、かつてスタンフォード大学で「生成エージェント（generative agents）」の研究を主導した経歴を持つ。</p>
<p>同社のシミュレーション基盤は、実在の人間をモデル化したエージェントを多数配置し、それらの相互作用を通じて社会全体の動向を再現する仕組みである。現在は、あらゆる状況や規模において人間行動を予測可能な基盤モデルの開発に注力している。</p>
<h2>意思決定の「フライトシミュレーター」</h2>
<p>すでに企業向けの実装も始まっており、Simileは自社プラットフォームを**「重要な意思決定のためのフライトシミュレーター」**と定義している。用途として挙げられているのは以下の例だ。</p>
<ul>
<li><strong>IR・経営戦略</strong>： 決算説明会（earnings calls）における投資家の反応予測</li>
<li><strong>法務・訴訟</strong>： 訴訟結果のモデリングとシミュレーション</li>
<li><strong>公共政策</strong>： 政策変更が社会に及ぼす影響のテスト</li>
</ul>
<h2>開発の加速と世界規模への拡張</h2>
<p>今回調達した1億ドルは、社会シミュレーション基盤の開発を加速するための資金として充てられる。</p>
<p>同社は将来的な展望として、個人、組織、文化、国家などの多層的な相互作用を扱う大規模モデルへと拡張する方針を示している。これにより、世界規模での社会シミュレーションの実現を目指すとしている。</p>
]]></description>
      <pubDate>Mon, 02 Feb 2026 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>TSMC熊本第2工場、AI向け3nmへ　最先端ロジック半導体を日本に</title>
      <link>https://ledge.ai/articles/tsmc_kumamoto_second_fab_ai_3nm_shift</link>
      <description><![CDATA[<p>半導体受託製造で世界最大手の台湾積体電路製造（TSMC）は2026年2月5日、熊本県で計画中の第2工場について、最先端の3ナノメートル（nm）ロジック半導体を生産する方向へ計画を変更したい意向を示した。<a href="https://x.com/takaichi_sanae/status/2019377402091291014">高市早苗首相</a>が自身のX（旧Twitter）で明らかにした。</p>
<p>TSMCの会長であるC.C.ウェイ氏はこの日、官邸を訪問し熊本第2工場を3nmロジック半導体の生産拠点とする計画変更と、総投資額の増額について表明したという。3nmロジック半導体は、データセンター向けに加え、AIロボティクスや自動運転といった分野で用いられる最先端の半導体だとしている。</p>
<p>同件について、<a href="https://x.com/meti_NIPPON/status/2019346900730933740">経済産業省</a>も、TSMC側から熊本第2工場の当初計画である12nmおよび6nmプロセスから、より先端となる3nmプロセスへ変更したい旨の説明を受けたと明らかにしている。政府としては、今後も協議を続けながら協力していく方針だ。</p>
<p>Reutersなどの報道によると、今回の計画は、日本政府要人との面会の場でTSMC側が伝えたもので、AI向け半導体需要の拡大を背景に、最先端プロセスを日本に展開する動きとして受け止められている。</p>
<p>熊本ではすでにTSMCの第1工場が稼働しており、関連企業の進出が進むなど、半導体産業の集積が進展している。第2工場が3nmプロセスを担うことになれば、日本国内での先端ロジック半導体生産の体制に変化が生じることになる。</p>
<p>今後は、3nm生産の開始時期や量産規模、投資額の具体像、日本政府による支援の枠組みなどが焦点となる。</p>
]]></description>
      <pubDate>Mon, 02 Feb 2026 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Waymo、DeepMindの世界モデル「Genie 3」を活用した自動運転向け生成シミュレーション「Waymo World Model」を発表</title>
      <link>https://ledge.ai/articles/waymo_world_model_genie3_simulation</link>
      <description><![CDATA[<p>Waymoは2026年2月6日、Google DeepMindが研究を進める汎用世界モデル「Genie 3」の成果を活用し、自動運転向けの生成シミュレーションモデル「Waymo World Model」を<a href="https://waymo.com/blog/2026/02/the-waymo-world-model-a-new-frontier-for-autonomous-driving-simulation">発表</a>した。現実世界に近い交通環境を生成AIで再現し、学習や安全性検証を高度化する狙いがある。</p>
<h2>生成AIで“走行世界”を再構築する「Waymo World Model」</h2>
<p>Waymo World Modelは、自動運転開発のために設計された生成型シミュレーションモデルだ。実走行データを基に、道路構造や周辺環境、交通参加者の挙動を仮想空間上で再構成・生成できる。カメラ映像に加え、LiDARなど複数センサーの出力を含むシーン生成に対応し、開発や検証に用いる走行データを柔軟に拡張できるとしている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/waymo_1_7250ba77e1/waymo_1_7250ba77e1.jpg" alt="waymo 1.jpg" /></p>
<h2>DeepMindの汎用世界モデル「Genie 3」を自動運転に応用</h2>
<p>同モデルは、DeepMindが研究開発する汎用世界モデル「Genie 3」で培われた技術的成果を基盤としている。Genie 3は、世界の状態を一貫性を保ったまま生成・操作できる点が特徴で、インタラクティブな環境生成を可能にする。Waymoはこのアプローチを自動運転向けに最適化し、交通環境の再現に応用した。</p>
<h2>実走では再現困難な希少・危険シナリオを仮想生成</h2>
<p>Waymo World Modelにより、実走では遭遇頻度が低い事象や安全上の理由で再現が難しいケースを仮想空間で生成できる。天候や時間帯、交通量、周囲の行動パターンなどを組み合わせ、多様な走行シナリオを作成することで、検証の網羅性を高めることが可能になるという。</p>
<p><strong>■ 生成シミュレーションで再現された竜巻遭遇シナリオ。実走行では安全上の理由から検証が困難な極端気象下の走行環境を、仮想空間で生成・検証できる</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/waymo_2_2f41310e98/waymo_2_2f41310e98.jpg" alt="waymo 2.jpg" /></p>
<p>■ <strong>野生動物が道路を横断するシナリオを生成した例。地域によっては発生し得るものの、実走データとしては取得が難しいケースを仮想的に再現している。</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/waymo_3_4ffd2847ca/waymo_3_4ffd2847ca.jpg" alt="waymo 3.jpg" /></p>
<h2>実走データ依存から生成シミュレーション活用へ</h2>
<p>Waymoはこれまでも、実走データとシミュレーションを組み合わせた開発を進めてきた。今回の発表は、生成AIを用いたシミュレーション活用を強化し、学習や検証の効率化を図る取り組みとして位置づけられる。実世界データへの依存を補完しながら、開発サイクルの高速化を目指す。</p>
<h2>世界モデル研究の実用化が進展</h2>
<p>汎用世界モデルの研究は、これまで主に基礎研究の文脈で語られてきた。Waymo World Modelは、そうした研究成果が自動運転という実用分野に組み込まれた事例の一つとなる。Waymoは今後、このモデルを自社の開発パイプラインに統合し、安全性検証と性能向上の両立を進めるとしている。</p>
]]></description>
      <pubDate>Mon, 02 Feb 2026 01:50:00 GMT</pubDate>
    </item>
  </channel>
</rss>