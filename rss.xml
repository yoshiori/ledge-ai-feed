<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
  <channel>
    <title>Ledge.ai 新着記事</title>
    <link>https://ledge.ai/</link>
    <description>Ledge.ai の最新テクノロジー記事</description>
    <item>
      <title>日本プロ野球選手会、SNS上の誹謗中傷を検出するAIを導入──クライマックスシリーズと日本シリーズで運用開始</title>
      <link>https://ledge.ai/articles/npb_ai_antislander_2025</link>
      <description><![CDATA[<p>日本プロ野球選手会は2025年10月10日、クライマックスシリーズおよび日本シリーズに出場する選手を対象に、SNS上での誹謗中傷を自動検出するAIシステムを導入すると<a href="https://jpbpa.net/2025/10/10/12723/">発表</a>した。AIがSNS上の投稿を常時モニタリングし、不適切な内容を検出・通報する仕組みを構築する。選手会は「選手が安心して競技に集中できる環境をつくる」ことを目的としている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/JPBPA_Press1_eeab9f12c0/JPBPA_Press1_eeab9f12c0.jpg" alt="JPBPA_Press1.jpg" /></p>
<h2>クライマックスシリーズと日本シリーズで試験導入</h2>
<p>対象となるのは、「2025 JERA クライマックスシリーズ セ」「2025 パーソル クライマックスシリーズ パ」および「SMBC日本シリーズ2025」に出場登録された全選手。</p>
<p>AIがSNS上の投稿を監視し、誹謗中傷に該当する事案を自動的に検出。検出後は選手ごとにリスト化し、SNS運営元への通報や削除要請、NPB球団との情報共有、ダイレクトメッセージ（DM）対応、発信者情報開示請求や証拠保全などを通じて迅速な対応を行う。</p>
<h2>英Signify GroupのAIを活用</h2>
<p>導入されるのは、英国に本社を置くSignify Group社の誹謗中傷検出・通報支援サービス「Threat Matrix」。日本語を含む42言語と絵文字に対応し、主要SNS上の投稿をAIが自動的に分析して不適切な内容を検出する。
同システムは「FIFAワールドカップカタール2022」や「ラグビーワールドカップフランス2023」、女子テニス協会（WTA）および国際テニス連盟（ITF）などでも採用されており、英プレミアリーグ・アーセナルFCでは導入後、誹謗中傷の検出件数が<a href="https://www.arsenal.com/news/24-supporters-banned-abusive-behaviour">約90％減少</a>したという。</p>
<h2>選手と家族を守る取り組み</h2>
<p>選手会は「誹謗中傷が選手の家族に及んだ場合には家族へのサポートも行う」としており、対象を家族にも拡大。
SNS上の誹謗中傷が社会問題化する中、選手とその家族の精神的安全を守るための包括的な取り組みとして位置づけている。
同会は今後も、日本プロフェッショナル野球組織（NPB）および12球団と連携しながら、選手が安心して競技に集中できる環境づくりを進めていく。</p>
]]></description>
      <pubDate>Fri, 17 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>NTTとNTTドコモビジネス、自動運転レベル4対応の「通信安定化ソリューション」を提供開始──IOWN技術で遠隔監視の通信を高信頼化</title>
      <link>https://ledge.ai/articles/ntt_docomo_business_autonomous_driving_stable_communication_solution</link>
      <description><![CDATA[<p>NTTドコモビジネス株式会社とNTT株式会社は2025年10月8日、自動運転車両など移動体向けに通信の安定性を高める「通信安定化ソリューション」の提供開始を<a href="https://www.ntt.com/about-us/press-releases/news/article/2025/1008.html">発表</a>した。</p>
<p>両社が開発したIOWN（Innovative Optical and Wireless Network）技術を活用し、無線品質の予測に基づく複数回線のマルチパス通信制御と、データ連携システムを組み合わせることで、自動運転レベル4の遠隔監視を支える高信頼・低遅延な通信を実現する。</p>
<h2>自動運転レベル4の社会実装を支える通信基盤</h2>
<p>全国で進む自動運転レベル4の社会実装では、基地局の切り替えや通信干渉による一時的な映像途切れが課題となっている。NTTドコモビジネスとNTTは、これまで各地の自動運転実証実験で得た知見をもとに、通信の安定化に必要な複数技術をパッケージ化。導入までのリードタイムを短縮し、自治体や企業が容易に利用できる形で提供する。</p>
<h2>3つの技術を統合したパッケージ構成</h2>
<p>ソリューションは、次の3つの技術で構成される。</p>
<ol>
<li><strong>無線品質予測</strong> ：公衆ネットワークやローカル5G、Wi-Fiなどの無線品質を機械学習により予測（IOWN技術「Cradio」を活用）</li>
<li><strong>マルチパス通信制御</strong> ：通信状況に応じて複数回線を制御し、高い接続性を実現（IOWN技術「協調型インフラ基盤」を活用）</li>
<li><strong>リアルタイムデータ伝送</strong> ：車載カメラ映像やセンサーデータなどをリアルタイムに遠隔監視システムへ伝送（アプトポッド社「intdash」を活用）</li>
</ol>
<p>これにより、車両と遠隔監視システム間の通信を複数経路で同時接続し、無線品質の変化を先読みして制御することで、映像の途切れを抑えた安定した通信環境を構築できるという。</p>
<p><strong>通信安定化ソリューションの構成イメージ</strong>：車載クライアント、クラウドサーバー（ドコモMEC）、監視センタを連携し、IOWN技術「Cradio」による無線品質予測とマルチパス通信制御、intdashによるデータ伝送を統合している。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/1008b_423a2f1ca4/1008b_423a2f1ca4.jpg" alt="1008b.jpg" /></p>
<h2>実証で通信遅延を大幅に改善</h2>
<p>両社は公道で自動運転車両を用いた実証実験を実施。クライアントとクラウド間の通信遅延400ミリ秒以下を維持できた割合は、非適用時の92％（1回線）および53％（2回線）に対し、本ソリューション適用時は99％に達したとしている。</p>
<h2>各社の役割と今後の展開</h2>
<p>このソリューションはNTTドコモビジネスが提供を担当し、intdashを用いた環境構築や活用提案を行う。NTTは「Cradio」および「協調型インフラ基盤」に関する研究開発を担う。</p>
<p>今後は、自動運転における遠隔監視の安定化だけでなく、建設現場や工場、倉庫などでの遠隔操作・自動化分野への展開も見据える。両社は、通信安定化技術を通じて人手不足や安全確保などの社会課題の解決に貢献するとしている。</p>
]]></description>
      <pubDate>Fri, 17 Oct 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>清華大学、AIエージェントが仮想都市で経済活動を行う「SimCity」開発──フィリップス曲線やエンゲルの法則を自律再現</title>
      <link>https://ledge.ai/articles/tsinghua_simcity_ai_urban_economy</link>
      <description><![CDATA[<p>清華大学の研究チームは、複数のAIエージェントが仮想都市内で経済活動を行うシミュレーションシステム「SimCity」を開発した。研究成果は2025年10月1日付で論文「SimCity: Multi-Agent Urban Development Simulation with Rich Interactions」（arXiv:2510.01297）として<a href="https://arxiv.org/abs/2510.01297">公開</a>した。</p>
<p>SimCityは、家庭・企業・政府・中央銀行という4種類のエージェントを大規模言語モデル（LLM）で駆動し、都市の発展やマクロ経済の動きを自律的に再現する。エージェントは自然言語による推論を行い、労働市場・財市場・金融市場で相互に作用。人間のように意思決定を行いながら、都市経済の変動を模倣することができるという。
なお、論文中にはその記載はないが、同名の商用ゲームシリーズとは無関係と見られる。</p>
<h2>4種類のエージェントがつくる「AI経済」</h2>
<p>SimCityには、以下の4つの主要な役割を担うエージェントが登場する。</p>
<ul>
<li><strong>家計（Households）</strong> ：消費、就業、住宅選択、貯蓄・投資を決定する。</li>
<li><strong>企業（Firms）</strong> ：生産や雇用、価格設定、設備投資を行う。</li>
<li><strong>政府（Government）</strong> ：所得税や付加価値税（VAT）の徴収、公共支出や福祉政策を担当する。</li>
<li><strong>中央銀行（Central Bank）</strong> ：インフレ率やGDPの動向に応じて、修正版テイラー・ルールに基づき政策金利を設定する。</li>
</ul>
<p>また、視覚・言語モデル（VLM）が企業の立地や都市構造を決定。住宅地や工業地帯が自然に分かれるなど、都市の空間構成も自律的に形成される。</p>
<h2>経済法則を自然に再現</h2>
<p>研究チームは、44種類の財と最大200世帯から成る仮想経済を構築し、180ステップ（約15年）にわたるシミュレーションを実施した。前半36カ月を「移住フェーズ」、後半144カ月を「発展フェーズ」として進行した。</p>
<p>その結果、SimCityは以下のような実経済の特徴（stylized facts）を再現できたという。</p>
<ul>
<li><strong>フィリップス曲線</strong> ：インフレ率と失業率の逆相関</li>
<li><strong>オークンの法則</strong> ：失業率の変化とGDP成長率の負の関係</li>
<li><strong>ベバリッジ曲線</strong> ：求人率と失業率の逆相関</li>
<li><strong>エンゲルの法則</strong> ：所得の上昇に伴い食費の割合が低下</li>
<li><strong>需要の価格弾力性</strong> ：必需品は非弾力的、贅沢品は弾力的</li>
<li><strong>投資の高ボラティリティ</strong> ：消費よりも投資が景気に敏感</li>
</ul>
<p>これらは実際の経済データ（FRED, 1970Q1–）との比較でも一致傾向を示し、従来のエージェント・ベース・モデル（ABM）では再現が難しかった現象を多く含むと報告されている。</p>
<h2>都市発展と価格ショックへの応答</h2>
<p>SimCityは「移住フェーズ（36カ月）」と「発展フェーズ（144カ月）」の2段階で進行する。
初期段階では移住者の流入によりGDPが上昇し、住宅と生産拠点の配置が形成される。VLMの判断により、住宅は都市中心部に、工場は周辺部に集中するなど、現実的なゾーニング構造が自律的に現れた。</p>
<p>さらに研究チームは、一部の財の価格を50％変動させる「価格インパルス実験」を実施。価格は短期的に大きく変動したが、数年のうちに均衡へ回帰。実際の経済で観察される「価格の粘着性（menu cost）」と同様の挙動が確認された。</p>
<h2>「AIがつくる社会をAIで観察」</h2>
<p>研究では、OpenAIの「GPT-4o-mini」を中心に、Azure OpenAI APIを通じてLLMを運用。各エージェントは独立して推論し、シミュレーション1ステップあたり約0.25ドルのコストで実行された。
総トークン数は約80万で、全体の実行コストは約180ドル。</p>
<p>研究チームは、SimCityを「AIがつくる社会をAIで観察するための基盤」と位置づけており、都市計画や経済政策の設計、AI社会の倫理実験などへの応用を見込んでいる。今後は、金融市場や株式取引などの要素を導入し、より現実的な経済表現へ拡張する予定だ。</p>
]]></description>
      <pubDate>Thu, 16 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Boston Dynamics、トヨタ・リサーチ・インスティテュートと共同でヒューマノイドロボット「Atlas」の新型グリッパーを公開──“器用さ”を極める7DoFハンド開発の舞台裏</title>
      <link>https://ledge.ai/articles/bostondynamics_atlas_gripper_tri_202510</link>
      <description><![CDATA[<p>ボストン・ダイナミクス（Boston Dynamics）は2025年10月8日、トヨタ・リサーチ・インスティテュート（TRI）と共同研究を進めるヒューマノイドロボット「Atlas」の最新映像を<a href="https://www.youtube.com/watch?v=gS4rOqNDTBk">公開</a>した。</p>
<p>動画は同社公式シリーズ「Inside the Lab」の新作で、「Perception and Adaptability（知覚と適応）」をテーマに、ヒューマノイドの“手”にあたるグリッパー（ハンド）の進化を紹介している。</p>
<h2>油圧式から電動式へ──操作能力に焦点</h2>
<p>Atlasはこれまで、油圧駆動によるジャンプや宙返りなどのモビリティ性能で知られてきた。
今回の研究では、「移動」から「操作」へと重心を移し、より高い器用さ（dexterity）を持つ電動式グリッパーの開発に注力している。
Boston Dynamicsは「より人間に近い作業能力を備えたヒューマノイド」を目指し、把持（grasping）と認識（perception）の融合を進めているという。</p>
<h2>7DoF＋触覚＋掌カメラ──“感じてつかむ手”</h2>
<p><a href="https://bostondynamics.com/blog/ask-a-roboticist-meet-karl/">公式ブログ</a>によれば、現行のグリッパは7つの自由度（DoF）を備え、指先の触覚センサーと掌部カメラを搭載。滑りや力加減を検知して把持を調整でき、3本指＋対向親指（opposable thumb）の構成により、大型・重量物から繊細な小物まで幅広く扱える。</p>
<h2>左右ミラーハンドと指の可動</h2>
<p>グリッパーには左右のミラーバージョンがあり、指は内側に90°／後方に90°曲げられる設計。Atlasは人のような“利き手”を持たず、立ち位置や対象物に応じて最適な手を選んで作業する。</p>
<p>@<a href="https://www.youtube.com/watch?v=gS4rOqNDTBk">YouTube</a></p>
<h2>製造現場の実用タスクを見据える</h2>
<p>目標は自動車製造ラインなどで役立つ“実用タスク”の達成だ。外装部品のような重い部材を傷つけずに扱う作業と、ネジや工具の扱いといった繊細な作業の両立が求められる——今回のグリッパー設計は、まさにその要件に合わせて磨かれている。</p>
]]></description>
      <pubDate>Thu, 16 Oct 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ライオン、研究開発データで追加学習した独自LLM「LION LLM」開発に着手──AWS協力のもと、ものづくりDXを加速</title>
      <link>https://ledge.ai/articles/lion_llm_development_aws_dx_2025</link>
      <description><![CDATA[<p>ライオン株式会社は2025年10月8日、自社の長年にわたる研究開発データを用いて追加学習を行った独自の大規模言語モデル（LLM）「LION LLM」の開発に着手したと<a href="https://prtimes.jp/main/html/rd/p/000000218.000039983.html">発表</a>した。アマゾン ウェブ サービス ジャパン合同会社（AWSジャパン）の生成AI実用化推進プログラムを通じた協力を受け、ものづくり分野のデジタルトランスフォーメーション（DX）を加速する狙いだ。</p>
<h2>暗黙知の継承と「ものづくりDX」の推進</h2>
<p>ライオンは、「次世代ヘルスケアのリーディングカンパニーへ」を掲げる中期経営計画「Vision 2030 2nd STAGE」を2025年から始動。デジタル分野では「ものづくりDX」を重点テーマに位置づけ、収益力とクリエイティビティを両立させた競争力のある製品・サービスを迅速に市場へ投入できる体制を目指している。</p>
<p>同社では、製造業において重要な「暗黙知」の継承が課題となっていた。熟練技術者の退職により貴重なノウハウが失われつつあることを背景に、2023年12月には生成AIと検索システムを組み合わせた研究ナレッジ検索ツールを導入。情報検索時間を従来の5分の1以下に短縮するなどの成果を上げたが、専門知識を要する複雑な質問には対応が難しい状況が続いていた。</p>
<p>この課題を解決するため、ライオンはAWSジャパンの協力のもと、自社の知見を生かした独自LLMの内製開発を進めることを決定した。</p>
<h2>AWS支援のもとで社内に分散学習基盤を構築</h2>
<p>ライオンは2025年4月から、AWSジャパンが提供する「生成AI実用化推進プログラム」に参加。クレジット付与によるコスト支援や科学的助言などの技術協力を受け、内製開発体制を整備した。</p>
<p>社内には分散学習環境を構築。AWS ParallelClusterとNVIDIAのMegatron-LMを組み合わせ、複数GPUサーバーを効率的に連携させる仕組みを採用した。ベースモデルには「Qwen 2.5-7B」を使用し、研究報告書、製品組成情報、品質評価データなど、数十年にわたる社内知見を学習データとして投入している。</p>
<p>初期フェーズの評価では、過去の知見に基づく具体的なアドバイスや、複数の事例を統合した回答が可能であることを確認。従来ツールと比較して、回答の網羅性が大幅に向上したと社内で報告されている。</p>
<h2>今後の展開</h2>
<p>今後は、学習データの拡充と品質向上を目的に、プレゼン資料など構造化が難しいデータのクリーニングを進める。さらに、経済産業省およびNEDOが主導する「Generative AI Accelerator Challenge（GENIAC）」で開発された国産モデルの活用など、多角的なアプローチで精度向上を図る計画だ。</p>
<p>これらを既存のナレッジ検索ツールと統合し、高度な質問やタスクにも対応できるシステムを目指す。同社は、知識資産の最大活用を通じて「ものづくりDX」を加速し、競争優位性の強化につなげるとしている。</p>
]]></description>
      <pubDate>Wed, 15 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>東京大学・ソフトバンク・LINEヤフー、「Beyond AI技術研究組合」を設立──AI研究成果の事業化を加速</title>
      <link>https://ledge.ai/articles/beyond_ai_technology_consortium_tokyo_univ_softbank_line_yahoo</link>
      <description><![CDATA[<p>東京大学とソフトバンク、LINEヤフーは2025年10月10日、AI（人工知能）研究の成果を迅速に社会実装・事業化するための新たな産学連携組織「Beyond AI技術研究組合」を9月19日に設立したと<a href="https://beyondai.jp/contents/2025/10/10/cip3/">発表</a>した。</p>
<p>同組合は、経済産業省の「CIP（コーポレート・イノベーション・プラットフォーム）」制度を活用し、AI研究から事業化までを一貫して推進する体制を構築するという。</p>
<h2>研究と事業化の橋渡しを強化</h2>
<p>東京大学とソフトバンク、LINEヤフーの3者は、2020年設立の「Beyond AI研究推進機構」を通じてAI分野の基礎研究を進めてきた。今回の「Beyond AI技術研究組合」は、その研究成果を社会や産業へ迅速に還元するための組織で、知的財産や人材育成、資金調達などの面から事業化を支援する「プラットフォーム型CIP」として位置づけられる。</p>
<p>また、2024年6月に経済産業省がCIP設立・運営ガイドラインを改正し、1つのCIPから複数の事業会社を設立できる枠組みを導入したことを受け、同組合もその新制度を活用して設立された。</p>
<h2>今後の展開</h2>
<p>同組合は、パーソナルAIエージェント時代を見据えたAI技術の高度化や基盤技術開発を進めるとともに、Beyond AI連携事業で取り組んできた医療ヘルスケア領域などへのAI応用研究を推進する。
また、ソフトバンクグループと連携し、産業領域横断のデータ活用や社会実装に向けた実証実験（PoC）を進めていく。</p>
]]></description>
      <pubDate>Wed, 15 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Microsoft、自社設計の画像生成モデル「MAI-Image-1」を発表──フォトリアルな表現と高速生成を両立</title>
      <link>https://ledge.ai/articles/microsoft_mai_image_1_announcement</link>
      <description><![CDATA[<p>MicrosoftのAI部門であるMicrosoft AIは2025年10月14日（米国時間）、同社が初めて自社で設計・開発した画像生成モデル「MAI-Image-1」を<a href="https://microsoft.ai/news/introducing-mai-image-1-debuting-in-the-top-10-on-lmarena/">発表</a>した。</p>
<p>フォトリアルな質感や構図の整った出力に加え、既存モデルと比べて生成速度を大幅に向上させた点が特徴で、今後はCopilotやBing Image CreatorなどMicrosoft製AIサービスへの順次統合を予定している。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/sand_1_07e63f2f03/sand_1_07e63f2f03.jpg" alt="sand-1.jpg" /></p>
<h2>Microsoft初の完全社内開発モデル</h2>
<p>MAI-Image-1は、Microsoft AIが完全に社内で設計・訓練した初の画像生成モデルであり、既存の外部モデルを基盤とせず独自開発された。研究チームは、効率的な学習パイプラインとデータ選別技術を構築し、独自の高品質データセットを用いて訓練を行ったという。</p>
<p>モデルの評価は、画像生成AIの国際ベンチマーク「<a href="https://lmarena.ai/leaderboard/text-to-image">LMArena</a>」で実施され、初登場にしてトップ10入りを果たした。</p>
<h2>フォトリアルな描写と生成速度の両立</h2>
<p>公式ブログによると、MAI-Image-1は「現実に近い光表現」「安定した構図」「被写体の一貫性」に優れており、特に人間や風景のレンダリング品質が高いとされる。また、内部最適化により生成処理を高速化し、既存の大規模モデルよりも短時間で高解像度の画像を生成可能にした。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/crosswalk_7e79e8b8d5/crosswalk_7e79e8b8d5.jpg" alt="crosswalk.jpg" /></p>
<p>Microsoft AIは、テキストから画像を生成するプロンプト理解力の向上にも重点を置き、指示の意図や文脈をより正確に反映するアルゴリズムを採用したという。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/roadrunner_HQ_f5373a8b23/roadrunner_HQ_f5373a8b23.jpg" alt="roadrunner-HQ.jpg" /></p>
<h2>今後の展開：CopilotやDesignerに統合へ</h2>
<p>MAI-Image-1は今後、Copilot、Designer、Bing Image Creatorなど、同社の生成AIプロダクト群に順次統合される予定。Microsoftはこれにより、「安全で信頼性の高い画像生成を誰もが利用できる環境を提供する」としている。</p>
<p>発表ではまた、MAI-Image-1を同社の新しい社内モデル群「MAI-」シリーズの第1弾と位置づけており、今後は動画生成や3D生成分野への展開も視野に入れている。</p>
]]></description>
      <pubDate>Wed, 15 Oct 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI、Broadcomと戦略的提携──自社設計AIアクセラレーターを10GW規模で展開へ</title>
      <link>https://ledge.ai/articles/openai_broadcom_ai_accelerator_10gw_partnership</link>
      <description><![CDATA[<p>OpenAIは2025年10月13日（米国時間）、米半導体大手Broadcomとの戦略的提携を<a href="https://openai.com/index/openai-and-broadcom-announce-strategic-collaboration/">発表</a>した。両社は、OpenAIが設計したAIアクセラレーターとBroadcomのネットワークソリューションを組み合わせ、総計10ギガワット（GW）規模のAIクラスターを共同で構築する。展開は2026年後半に開始し、2029年末までの完了を予定している。（<a href="https://jp.broadcom.com/company/news/product-releases/63631">Broadcomのリリース</a>）</p>
<h2>10GW規模のAIアクセラレーターを共同開発・展開</h2>
<p>発表によるとOpenAIは自社でAIアクセラレーターとシステムを設計し、Broadcomが開発・展開を担う。ラックにはBroadcomのEthernet、PCIe、光接続などのソリューションを採用し、完全にEthernetベースで構築される。これにより、スケールアップ（拡張）とスケールアウト（分散拡張）の両面で柔軟なAIインフラを実現するという。</p>
<p>両社は、AIアクセラレーターおよびBroadcomのネットワーク技術を組み込んだラックを展開するための契約書（term sheet）を締結済み。すでに長期にわたる共同開発・供給の枠組みを構築しており、今後はOpenAIの自社施設およびパートナーデータセンターを中心に順次導入を進める。</p>
<h2>アルトマン氏「AIの可能性を引き出すための重要な一歩」</h2>
<p>OpenAIの共同創業者兼CEOであるサム・アルトマン氏は次のように述べた。
「Broadcomとの提携は、AIの潜在能力を解き放ち、人々や企業に実際の恩恵をもたらすための重要なステップです。自社アクセラレーターの開発は、AIの最前線を押し広げ、人類全体に利益を届けるための取り組みでもあります。」</p>
<p>また、共同創業者兼プレジデントのグレッグ・ブロックマン氏も、「自社設計チップによって、これまでのモデル開発で得た知見をハードウェアに直接組み込み、新たなレベルの知性と能力を引き出す」とコメントしている。</p>
<h2>Broadcom「AGIへの道を拓く協業」と強調</h2>
<p>Broadcomの社長兼CEOホック・タン氏は、「OpenAIとの協業は汎用人工知能（AGI）実現に向けた転換点だ」と述べ、10GW規模の次世代アクセラレーターとネットワークシステムの共同開発が「AIの未来への道を切り開く」と強調した。</p>
<p>同社の半導体ソリューション事業プレジデントであるチャーリー・カワス氏（Ph.D.）も、「オープンでスケーラブル、かつ電力効率の高いAIクラスター設計において新たな業界基準を打ち立てる」と述べ、Ethernetベースのアプローチがコストと性能の両立を可能にすると説明した。</p>
<h2>OpenAI、週8億人の利用基盤を背景に次世代インフラを整備</h2>
<p>OpenAIは現在、ChatGPTをはじめとする製品群を通じて、週あたり8億人を超えるアクティブユーザーを抱える。今回のBroadcomとの連携により、AIインフラの自社開発と最適化を進め、同社が掲げる「AGIの恩恵をすべての人に届ける」という使命の実現を加速させる狙いだ。</p>
]]></description>
      <pubDate>Tue, 14 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>AIがソフトウェアの脆弱性を自動修正──Google DeepMind、新エージェント「CodeMender」を発表</title>
      <link>https://ledge.ai/articles/deepmind_codemender_ai_vulnerability_fix</link>
      <description><![CDATA[<p>Google DeepMindは2025年10月10日、ソフトウェアの脆弱性を自動的に検出・修正するAIエージェント「CodeMender」を<a href="https://deepmind.google/discover/blog/introducing-codemender-an-ai-agent-for-code-security/">発表</a>した。</p>
<p>同エージェントは大規模言語モデル「Gemini Deep Think」を基盤とし、深層推論を活用して安全で安定したコード改変を行う。DeepMindは「AIが自ら脆弱性を理解し、修正案を提案・検証する」という新しいセキュリティサイクルを構築したと説明している。</p>
<h2>72件の脆弱性を自動修正</h2>
<p>CodeMenderはすでにオープンソースソフトウェアの実環境でテストされており、これまでに72件のセキュリティ修正を開発元のリポジトリへ統合した。対象コードには約450万行規模の大規模プロジェクトも含まれる。
DeepMindによれば、修正対象の一例として画像処理ライブラリ「libwebp」でのメモリ管理エラーがあり、AIによる自動改修によってバッファオーバーフロー耐性が向上したという。</p>
<h2>「反応型」と「先制型」AIによる修正</h2>
<p>CodeMenderは、既知の脆弱性に即応する反応型（reactive）と、将来的に問題を起こす可能性のある構造を事前に書き換える先制型（proactive）の2モードを備える。</p>
<p>内部では、静的解析・動的解析・ファジングなどを組み合わせた検査手法を使用し、修正案の妥当性をGemini Deep Thinkが多段階で検証。AIが生成したパッチは必ず人間エンジニアのレビューを経る仕組みで、誤修正を防ぐ安全策も講じられている。</p>
<p><strong>図：CodeMenderの自動修正フロー。LLMエージェントが脆弱性を検出し、Validatorが修正案を検証した後、人間によるパッチレビューを経てコードリポジトリへ統合する。</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/fixing_vulnerabilities_4975259e2f/fixing_vulnerabilities_4975259e2f.jpg" alt="fixing vulnerabilities.jpg" /></p>
<h2>セキュリティ運用の自動化へ</h2>
<p>DeepMindは、「CodeMenderはAIによるソフトウェアセキュリティの自動化を推進する重要な一歩」と位置づけている。今後はGoogleのSecure AI Framework（SAIF）やAI Vulnerability Reward Program（AI VRP）など、既存のセキュリティ施策と連携して展開を進める予定だ。
また、オープンソース開発者コミュニティとの協力を通じて、より多くのプロジェクトでのAI自動修正を実用化していくとしている。</p>
]]></description>
      <pubDate>Tue, 14 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>リコー、推論性能を強化した日本語LLMを発表──GPT-5相当の性能で金融業務に特化</title>
      <link>https://ledge.ai/articles/ricoh_reasoning_llm_gpt5_equivalent_20251010</link>
      <description><![CDATA[<p>リコーは2025年10月10日、推論（）性能を追加搭載した日本語大規模言語モデル（LLM）を<a href="https://jp.ricoh.com/release/2025/1010_1">発表</a>した。</p>
<p>このモデルは700億パラメータ規模のオンプレミス対応型LLMで、金融業務に特化して開発されたもの。同社によれば、多段推論能力（Chain-of-Thoughts: CoT）の導入により、融資稟議など専門的な業務遂行力を強化し、代表的な日本語ベンチマークでOpenAIの「GPT-5」と同等レベルの性能を確認したという。</p>
<h2>金融分野に特化した推論能力</h2>
<p>リコーは、有価証券報告書などの公開データを活用し、金融分野に特有の専門知識を追加学習したうえで、多段推論能力（Chain-of-Thoughts: CoT）を導入。複数の情報を論理的ステップに分解し、融資稟議業務などの複雑な判断を支援する能力を強化した。</p>
<p>このモデルはオンプレミス環境で運用できる「プライベートLLM」として提供され、企業が自社データを安全に学習させることが可能だとしている。</p>
<h2>ベンチマーク評価でGPT-5と同等スコア</h2>
<p>発表資料によると、代表的な日本語ベンチマークで次の結果を得た。
評価には「ELYZA-tasks-100」「Japanese MT-Bench」「japanese-lm-fin-harness」およびリコー独自の融資稟議ベンチマークが用いられた。</p>
<ul>
<li><strong>ELYZA-tasks-100</strong> ：複雑な指示タスク（要約、対話、意図の理解など）</li>
<li><strong>Japanese MT-Bench</strong> ：マルチターン対話能力</li>
<li><strong>japanese-lm-fin-harness</strong> ：金融分野における感情分析・証券知識・試験問題など</li>
<li><strong>融資稟議ベンチマーク</strong> ：企業・財務・信用の総合評価</li>
</ul>
<p>評価の結果、リコーの「Llama-3.3-Ricoh-70B-20251001」は日本語タスク全般でGPT-5に匹敵するスコアを示し、金融ベンチマークでは同規模以上のオープンソースモデルを上回ったとされる。</p>
<p><strong>ベンチマークツールにおける他モデルとの比較結果</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/_d289815140/_d289815140.jpg" alt="ベンチマークツールにおける他モデルとの比較結果.jpg" /></p>
<h2>高精度化を支える独自技術</h2>
<p>リコーのモデルは、Meta社の「Llama-3.3-70B-Instruct」を基に、日本語性能を強化した「Llama-3.3-Swallow-70B-v0.4」をベースモデルとして構築。独自のインストラクション・チューニングや、Chat Vectorを用いたモデルマージ技術、独自カリキュラムを組み合わせることで高精度化を実現した。大規模ながら省コストで動作し、GPUリソースの少ない環境でも運用できる点が特徴とされる。</p>
<h2>今後の展開</h2>
<p>リコーは今後、金融以外にも製造業・医療などの業種特化型モデルを順次開発するとしている。
同社は「使える・使いこなせるAI」の提供を掲げ、企業のデジタルトランスフォーメーション（DX）を支援していく方針だ。</p>
]]></description>
      <pubDate>Tue, 14 Oct 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google、企業向けAIの“入口”「Gemini Enterprise」発表──従業員全員にGoogle AIを届ける統合プラットフォーム</title>
      <link>https://ledge.ai/articles/google_gemini_enterprise_ai_platform_launch</link>
      <description><![CDATA[<p>Googleは米国時間2025年10月9日、企業向けの新しいAIプラットフォーム「Gemini Enterprise（ジェミニ・エンタープライズ）」を<a href="https://blog.google/products/google-cloud/gemini-enterprise-sundar-pichai/">発表</a>した。
職場におけるAI活用の“入口（front door）”として位置づけられ、従業員全員が直感的なチャットインターフェースを通じて、GoogleのAI機能を安全かつ統合的に利用できる環境を提供する。</p>
<h2>部門単位のAIから、全社横断のAIへ</h2>
<p>Googleは発表の冒頭で、「AIは働き方、ビジネス運営、顧客体験のすべてを変革する歴史的機会をもたらす」としながらも、従来のAI活用が部門ごとに孤立していた課題を指摘した。Gemini Enterpriseは、この分断を解消し、ワークフローと従業員をつなぐ包括的なAIプラットフォームとして開発された。</p>
<p>Google Cloudによると、同社の強みは「信頼性の高いAIインフラ」「Google DeepMindによる先駆的研究」「多用途なGeminiモデルファミリー」の3層構造にある。このフルスタックAI戦略が、企業におけるAI変革を支える中核基盤になるという。</p>
<h2>6つの中核コンポーネントを統合</h2>
<p>Gemini Enterpriseは、以下の6つの要素を単一のインターフェースで統合する：</p>
<ul>
<li><strong>最新のGeminiモデル</strong> ：あらゆる業務タスクの“頭脳”として、高度な推論と生成を実現。</li>
<li><strong>ノーコード・ワークベンチ</strong> ：非エンジニアでもデータ分析やエージェント連携が可能。</li>
<li><strong>事前構築エージェント</strong> ：リサーチやデータインサイトなどの専門業務に即対応。</li>
<li><strong>企業データとの安全な接続</strong> ：Google WorkspaceやMicrosoft 365、Salesforce、SAPなどにシームレス接続。</li>
<li><strong>統合ガバナンス</strong> ：すべてのエージェントを一元的に可視化・保護・監査。</li>
<li><strong>オープンなエコシステム</strong> ：10万社を超えるパートナーによる拡張性を確保。</li>
</ul>
<p>Googleはこれにより、「単一タスクの効率化を超え、ワークフロー全体を自動化する」としている。</p>
<h2>業務アプリとの統合と新機能</h2>
<p>Gemini EnterpriseはGoogle Workspaceとも密接に連携する。
テキスト・画像・動画・音声を理解・生成できる初のマルチモーダルエージェントが導入され、文書作成や会議運営などの作業を支援。
「Google Vids」によるAI生成動画の作成機能や、「Google Meet」でのリアルタイム音声翻訳機能も提供される。
後者は発話者のトーンや表現を反映し、自然な多言語コミュニケーションを可能にするという。</p>
<p>さらに、「データサイエンスエージェント（プレビュー）」が発表された。
データ取り込みから探索、モデルトレーニングの自動化までを担い、VodafoneやWalmartなどの企業が既に活用している。</p>
<h2>導入事例の拡大と実用成果</h2>
<p>Googleは、Banco BV、Klarna、Mercedes-Benz、Swarovskiなどの導入事例を紹介した。
Banco BVでは、分析作業の自動化により、マネージャーが新規ビジネス開拓に注力できるようになった。Mercedes-BenzはGeminiを用いてドライバーと自然な会話ができる自動車内AIアシスタントを構築している。</p>
<p>@<a href="https://www.youtube.com/watch?v=ijqTReRzG8M&amp;t=26s">YouTube</a></p>
<p>日本企業では、メルカリがGoogle AIをコールセンターに導入。
AI主導のカスタマーサービス体験を実現し、業務量を20％削減、ROI（投資収益率）を500％向上させる見込みとしている。</p>
<h2>開発者×エージェント経済──A2AとAP2で拡張</h2>
<p>Gemini Enterpriseは企業だけでなく、開発者にも開かれたプラットフォームとして進化する。
すでに100万人以上が利用する「Gemini CLI」に加え、AIをコマンドラインから拡張できる「Gemini CLI Extensions」を導入。
さらに、開発者やISV（独立系ソフトウェアベンダー）がエージェントを構築・販売・収益化できる「エージェントエコノミー」の構想を発表した。</p>
<p>この仕組みを支えるのが、</p>
<ul>
<li><strong>Agent2Agent Protocol（A2A）</strong> ：エージェント間通信を標準化する新プロトコル</li>
<li><strong>Agent Payments Protocol（AP2）</strong> ：エージェントによる安全な金融取引を実現する決済標準
の2つである。AP2は、American Express、Mastercard、PayPalなど100社超のパートナーと共同で策定された。</li>
</ul>
<h2>学習・導入支援プログラム</h2>
<p>Googleは、AI人材育成と現場導入支援の両面から変革を後押しする。
新たに全従業員が無料でAIスキルを学べる「Google Skills」を開設し、開発者向けの教育プログラム「Gemini Enterprise Agent Ready（GEAR）」を開始した。
さらに、顧客企業に伴走して導入支援を行うAIエンジニアチーム「Team Delta」も新設している。</p>
<p>@<a href="https://www.youtube.com/watch?v=Qbix0BOPcgE">YouTube</a></p>
<h2>AIの“入口”としての意義</h2>
<p>Googleは、AIの未来を「オープンで協力的なエコシステム」にあるとし、Box、Workday、ServiceNowなど主要企業との連携を拡大している。クロスプラットフォームでのワークフロー連携や導入支援、検証済みエージェントの発見、収益化の仕組みなど、10万社超のパートナー基盤を活用し、AI導入を全層で支援する方針だ。</p>
<p>\u003E「Gemini Enterpriseは、Google AIの最高の機能をすべての従業員とワークフローに届ける“職場AIの新たな入口”です」（Google Cloud公式ブログより）</p>
<p>Googleは、企業にとってのAI導入を「一部の実験」から「全員の変革」へと引き上げる転換点として、この新プラットフォームを位置づけている。</p>
]]></description>
      <pubDate>Mon, 13 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>公共2025/10/13 [MON]AIとの出会い、早すぎる？──Pew調査で明らかになった子どものチャットボット利用の低年齢化</title>
      <link>https://ledge.ai/articles/pew_children_ai_chatbot_early_use</link>
      <description><![CDATA[<p>米国の調査機関Pew Research Centerは2025年10月8日、保護者3,251人を対象にした全国調査レポート「How parents manage screen time for kids」を<a href="https://www.pewresearch.org/internet/2025/10/08/how-parents-manage-screen-time-for-kids/">発表</a>した。報告によると、5〜7歳の子どもの3％、8〜10歳の7％、11〜12歳の15％が、ChatGPTやGeminiなどのAIチャットボットを利用した経験があると回答。生成AIとの接触が、幼児期を含む低年齢層にまで広がりつつあることが明らかになった。</p>
<h2>スクリーンタイム調査で浮かび上がる「AIの早期接触」</h2>
<p>この調査は、子どものスクリーンタイム管理やデジタル機器利用をテーマとするもので、ゲームやSNSと並び、AIチャットボットの利用状況も尋ねている。</p>
<p>Pew Researchは「幼児では少数だが、年齢が上がるにつれてAI利用が明確に増加している」と指摘。AIが“スクリーン利用”の一形態として、家庭生活の中に入り込み始めている様子が浮かび上がった。</p>
<p><strong>12歳以下の子どものデバイス利用（12歳以下、米国）</strong>：TV 90％、タブレット 68％、スマートフォン 61％など。AIチャットボットは全体で8％。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/PI_2025_10_08_parents_kids_screens_0_01_540d7def3a/PI_2025_10_08_parents_kids_screens_0_01_540d7def3a.webp" alt="PI_2025.10.08_parents-kids-screens_0-01.webp" /></p>
<p>Pew Researchは「幼児では少数だが、年齢が上がるにつれてAI利用が明確に増加している」と指摘。AIが“スクリーン利用”の新たな形態として、家庭生活の中に入り込み始めている様子が浮かび上がった。</p>
<h2>ChatGPTを“友達”と語る子どもも──親世代の戸惑い</h2>
<p>レポートには、10歳の子どもを持つ親のコメントとして、
\u003E「My child talks about ChatGPT like it’s a friend. I’m not sure how much I should worry about that.」
（うちの子はChatGPTのことを友達のように話す。どの程度心配すべきかわからない）</p>
<p>という声が掲載されている。AIを単なるツールではなく、対話相手として受け入れる子どもが現れ始めており、親世代との認識差も表れている。</p>
<h2>理解が追いつかない保護者、監督と教育のはざまで</h2>
<p>Pew Researchは、AIツールをめぐって「多くの親がまだ理解を深めている段階にある」と報告。一部の保護者は、子どものAI利用実態の把握に課題を感じているとの記述もある。</p>
<p>調査では、AIチャットボットを「学習支援」や「創作のきっかけ」として肯定的に見る一方で、内容の信頼性や年齢に応じた利用ルールの整備を求める声も寄せられた。</p>
<p><strong>年齢別にみたデバイス利用状況（米国）</strong>：5–7歳 3％／8–10歳 7％／11–12歳 15％。年齢上昇とともに利用が増える傾向。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/PI_2025_10_08_parents_kids_screens_0_02_1058158cf7/PI_2025_10_08_parents_kids_screens_0_02_1058158cf7.webp" alt="PI_2025.10.08_parents-kids-screens_0-02.webp" /></p>
<h2>家庭に入り込むAI、問われる「年齢相応の付き合い方」</h2>
<p>報告書では、AIチャットボットを
\u003E“conversational tools such as ChatGPT, Gemini, or similar products that allow users to generate text responses through typed prompts（ChatGPT、Gemini、またはユーザーが入力されたプロンプトを通してテキスト応答を生成できる類似製品のような対話ツール）”</p>
<p>と定義し、会話型の生成AI全般を対象にしている。スクリーンタイムという枠組みの中で、AIチャットボットが独立の利用カテゴリーとして扱われている。Pew Researchは、今後の課題として「家庭でのAI利用に関する年齢相応のガイドラインづくり」の必要性を指摘した。</p>
<h2>調査概要</h2>
<ul>
<li>発表日：2025年10月8日</li>
<li>調査名：How parents manage screen time for kids</li>
<li>実施機関：Pew Research Center（米国ワシントンD.C.）</li>
<li>対象：米国の保護者 3,251人（オンライン調査）</li>
<li>実施期間：2025年5月〜6月</li>
<li>調査対象年齢：5〜12歳の子ども</li>
<li>調査目的：家庭におけるスクリーンタイム管理とテクノロジー利用の実態把握</li>
</ul>
]]></description>
      <pubDate>Mon, 13 Oct 2025 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>LLMが「心の中でイメージ」を描く？──人間の想像課題を超える精度で解答、GPT-5が人間平均を12％上回る</title>
      <link>https://ledge.ai/articles/artificial_phantasia_llm_visual_reasoning</link>
      <description><![CDATA[<p>米ノースイースタン大学の研究チームは2025年9月27日、言語モデル（LLM）が視覚情報なしに、頭の中でイメージを描くような課題を解けることを示した論文「Artificial Phantasia: Evidence for Propositional Reasoning-Based Mental Imagery in Large Language Models」を<a href="https://arxiv.org/abs/2509.23108">発表</a>した。</p>
<p>人間の「心的イメージ（mental imagery）」を模した課題を、テキスト入力だけで解答させた結果、OpenAIのGPT-5とo3モデル群が平均67%の正答率を示し、人間（54.7%）を上回ったという。</p>
<h2>言葉だけで「形」を思い描くタスク</h2>
<p>研究は、認知心理学で半世紀以上議論されてきた「心的イメージが言語的か、それとも視覚的か」という論争をAIで再検証したもの。
参加者には、頭の中で文字や図形を組み合わせて新しい形を作り、それが何に見えるか答える課題が与えられた。</p>
<p>たとえば――</p>
<p><strong>図：心的イメージ課題の一例</strong> ：「大文字のD」を左に90度回転し、「J」を下に組み合わせると傘の形になる。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x1_7_26c25bc70d/x1_7_26c25bc70d.png" alt="x1 (7).png" /></p>
<p>こうした「視覚イメージなしでは解けない」とされてきたタスクを、研究チームは60題（うち48題を新規作成）用意し、LLMに文章だけで解答させた。</p>
<h2>GPT-5とo3が人間を上回る精度</h2>
<p>結果、GPT-5は67.0%、OpenAI o3 Proは66.6%、標準o3は64.1% の正答率を記録。人間の平均54.7%を9〜12%上回り、統計的に有意な差が確認された（p \u003C .00001）。
一方、Claude Sonnet 4やGemini 2.5 Proは40〜46%と低迷し、画像生成を併用した場合はむしろ精度が下がった。研究者は「画像を使わせると推論が乱れ、言語ベースの方が安定する」と分析している。</p>
<h2>「見ずに考える」命題的推論</h2>
<p>論文では、LLMが絵を思い浮かべているのではなく、言語構造に基づいて空間関係を再構築する「命題的推論（propositional reasoning）」を行っていると結論づけている。この結果は、「心的イメージは視覚的でなければならない」とする通説を覆す可能性があり、人間の想像力に関する認知科学の議論にも新たな示唆を与える。</p>
<h2>人間とAIの“アファンタジア”の比較へ</h2>
<p>研究チームは、視覚イメージを持たない「アファンタジア（aphantasia）」の人々も同様の課題をこなせる点に着目。「視覚表象を持たずとも、言語的・構造的な推論でイメージ依存課題を解ける」ことを、AIと人間の両方で確認した形だ。
今後は、アファンタジアの被験者とLLMの思考過程を比較し、「人工的想像力（Artificial Imagination）」の本質を探るとしている。</p>
<p>研究チームはGitHubで実験コードとデータを公開し（subjectivitylab/artificial_phantasia）、今後はマルチモーダルAIや新しい推論ベンチマークへの応用を予定している。</p>
]]></description>
      <pubDate>Mon, 13 Oct 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>富士通とジーンクエスト、因果AIで遺伝子と生活習慣の関係を解析──「Fujitsu Kozuchi」活用で新たな知見</title>
      <link>https://ledge.ai/articles/fujitsu_genequest_causal_ai</link>
      <description><![CDATA[<p>富士通株式会社と株式会社ジーンクエストは2025年10月9日、富士通のAI技術群「Fujitsu Kozuchi（コヅチ）」の中核技術である因果AIを活用し、遺伝子データとライフスタイルデータの関係性を解析した結果、新たな知見を得たと<a href="https://global.fujitsu/ja-jp/pr/news/2025/10/09-01">発表</a>した。</p>
<p>実証では、同意が得られた約4,000名分の遺伝子情報と生活習慣アンケートデータを対象に、因果AIが両者の間に潜む因果構造を探索。特定の遺伝子多型が睡眠時間や食習慣、BMIなどの生活習慣に与える影響を可視化した。これにより、従来の統計的相関分析では把握しにくかった複雑な要因連鎖を明らかにしたとしている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/sub1_7b9b5b5a0a/sub1_7b9b5b5a0a.png" alt="sub1.png" /></p>
<h2>因果AIで「相関」から「原因と結果」へ</h2>
<p>富士通の因果AIは、膨大な多変量データの中から要因と結果の方向性を自動的に推定する技術。同社が開発した高速因果構造探索アルゴリズムにより、従来の分析手法よりも大規模・高精度な因果モデルを生成できる。富士通はこれを、産業・医療・科学分野を横断するAI基盤「Fujitsu Kozuchi」に搭載し、研究開発支援に活用している。</p>
<h2>生活習慣改善や疾病予防へ応用</h2>
<p>両社は、今回得られた知見を今後のヘルスケアサービスに応用する方針を示している。ジーンクエストは個人向け遺伝子解析サービスの高度化を進め、生活習慣病予防やパーソナライズド医療への展開を目指す。富士通は医療機関や製薬企業との連携を通じ、因果AIを「科学的エビデンス創出を支援する技術」として普及させる考えだ。</p>
<p>富士通は今回の取り組みを「AIによって科学的知見を創出する“サイエンスDX”の推進」と位置づけており、生命科学領域での新たなAI応用事例として注目される。</p>
]]></description>
      <pubDate>Sun, 12 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/10/11 [SAT]ソフトバンクとオラクル、AI活用のためのソブリンクラウドを共同構築──国内運用基盤「Cloud PF Type A」でデータ主権を確保</title>
      <link>https://ledge.ai/articles/softbank_oracle_sovereign_cloud_pf_type_a</link>
      <description><![CDATA[<p>ソフトバンク株式会社と米オラクルは2025年10月8日、データ主権（ソブリン性）を確保しつつ、AI活用を前提としたクラウド基盤「Cloud PF Type A」を日本国内で運用するための協業を開始したと<a href="https://www.softbank.jp/corp/news/press/sbkk/2025/20251008_01/">発表</a>した。オラクルのクラウド技術「Oracle Alloy」を採用し、ソフトバンクが国内データセンターで独自運用を担う。政府機関や企業が自国の管理下で安全にAIやクラウドを活用できるソブリンクラウドの実現を目指す。</p>
<h2>データ主権を守る「ソブリンクラウド」</h2>
<p>両社が開発する「Cloud PF Type A」は、すべてのデータ処理・保存・管理を日本国内で完結させる構成をとる。国外へのデータ移転を伴わずにクラウドやAIを活用できるようにすることで、経済安全保障やガバナンス上のリスクを最小化する狙いがある。</p>
<p>ソフトバンクは通信事業で培った国内ネットワークと運用ノウハウを活用し、オラクルは独自のクラウド基盤「Oracle Alloy」を通じて技術支援を提供。両社は「日本の社会インフラの一部として信頼されるクラウドを構築する」としている。</p>
<h2>「Cloud PF Type A」を2026年に提供開始</h2>
<p>新たに構築される「Cloud PF Type A」は、2026年4月に東日本拠点、同年10月に西日本拠点での提供を予定している。
この基盤では、Oracle Cloud Infrastructure（OCI）の約200種類のクラウドおよびAIサービスを国内運用で利用可能になる。</p>
<p>主な特徴は次の通り。</p>
<ul>
<li><strong>鍵管理（KMS）</strong> ：Oracle Vaultとソフトバンク独自システムを併用し、暗号鍵を完全に国内で管理。</li>
<li><strong>通信構成</strong> ：「OnePort」や「SmartVPN」などの閉域網接続に対応し、安全なデータ通信を実現。</li>
<li><strong>災害対策</strong> ：東西データセンターの冗長構成を採用し、事業継続（BCP）に対応。</li>
<li><strong>運用支援</strong> ：ソフトバンクがMSP（運用管理代行）を提供し、導入から保守まで一括サポート。</li>
</ul>
<p>この構成により、政府・自治体・企業などがAIモデルの学習や推論を行っても、データが国外へ移動することはない。</p>
<h2>両社のコメント</h2>
<p>ソフトバンクは、自社DCの高いセキュリティ水準に適合したクラウドを提供し、生成AIやGPUを統合して多様な顧客ニーズに応える方針と述べた。オラクルは、Oracle Alloyにより日本国内のデータ主権要件に対応し、OCIの幅広いAI/クラウドを国内DCで利用可能にする取り組みだと説明した。</p>
<h2>国内で広がる“主権クラウド”構想</h2>
<p>欧州を中心に広がる「ソブリンクラウド（主権クラウド）」の潮流は、日本国内でも加速している。
NTT、富士通、日立などが相次いで国産クラウド基盤の整備を進めており、今回のソフトバンクとオラクルの取り組みもその一環と位置づけられる。</p>
<p>両社は今後、生成AIや自然言語処理、画像解析などの高負荷AIワークロードにも対応する予定で、AI時代のデータ主権を支える中核的なインフラを目指す。</p>
]]></description>
      <pubDate>Sat, 11 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/10/10 [FRI]イーロン・マスク率いるxAI、動画生成AI「Imagine v0.9」を公開──静止画に声と動きを与える“ネイティブ映像生成”モデル</title>
      <link>https://ledge.ai/articles/xai_imagine_v09_release</link>
      <description><![CDATA[<p>イーロン・マスク氏が率いるAI開発企業xAIは2025年10月8日（現地時間）、新たな動画生成モデル「Imagine v0.9」を<a href="https://x.com/xai/status/1975607901571199086">発表</a>した。視覚品質、動き、音声生成などを全面的に改良し、すべてのxAI製品で無料利用が可能となっている。</p>
<p>xAIは公式X（旧Twitter）で「Imagine v0.9は、映像品質・モーション・音声生成などにおいてv0.1から大幅に進化した」と投稿。<a href="https://grok.com/imagine">grok.com/imagine</a> では、Grokプラットフォーム上で同モデルを試せるようになっている。</p>
<h2>音声と映像を同時生成──“編集不要”の体験を掲げる</h2>
<p>xAIは今回の発表で、「Imagine v0.9は音声と映像を同時に生成する“ネイティブ・オーディオ＋ビデオ生成”を実現した」と説明。</p>
<p>\u003E“Imagine v0.9 pushes the boundaries of native audio + video generation, creating cinematic experiences straight out of the box—no editing required.”</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x_AI_Imagine_v0_9_1_dragon_6a3a28422f/x_AI_Imagine_v0_9_1_dragon_6a3a28422f.jpg" alt="xAI Imagine v0-9-1 dragon.jpg" /></p>
<p>投稿では、音声と映像が同期したドラゴンのデモ映像を公開。ユーザーは、生成後の編集作業なしで“完成された動画”を得られるとしている。</p>
<h2>モーション精度とカメラ効果を強化</h2>
<p>Imagine v0.9では、被写体の滑らかな動きとリアリズムを高精度で再現するモーション制御を導入した。
さらに「インテリジェント・フォーカスシフト」と呼ばれる自動焦点移動など、ストーリーテリングに適したダイナミックなカメラ効果も追加された。</p>
<p>\u003E“And lets you add dynamic camera effects like intelligent focus shifts for better storytelling.”</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x_AI_Imagine_v0_9_1_camera_effects_5febfab05e/x_AI_Imagine_v0_9_1_camera_effects_5febfab05e.jpg" alt="xAI Imagine v0-9-1 camera effects.jpg" /></p>
<h2>自然な対話・歌唱・リズムまで再現</h2>
<p>Imagine v0.9では、音声表現の幅も拡大。自然な対話や歌唱を含む“声の演出”が可能になった。</p>
<p>\u003E“v0.9 also brings videos to life with natural dialogue and strong audio-visual harmony.”</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x_AI_Imagine_v0_9_1_natural_dialogue_f30bf3d090/x_AI_Imagine_v0_9_1_natural_dialogue_f30bf3d090.jpg" alt="xAI Imagine v0-9-1 natural dialogue.jpg" /></p>
<p>\u003E“It also brings expressive singing to life with clear vocals and synchronized emotion.”</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x_AI_Imagine_v0_9_1_singing_05b4553dab/x_AI_Imagine_v0_9_1_singing_05b4553dab.jpg" alt="xAI Imagine v0-9-1 singing.jpg" /></p>
<p>xAIはさらに、音と動きを合わせたダンス生成の例として、キャラクター「Ani」の動画を紹介している。</p>
<p>\u003E“Plus good rhythm: here's Ani with smooth dance moves.”</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x_AI_Imagine_v0_9_1_ani_dancing_c19660cb25/x_AI_Imagine_v0_9_1_ani_dancing_c19660cb25.jpg" alt="xAI Imagine v0-9-1 ani dancing.jpg" /></p>
<p>xAIはユーザーからのフィードバックを積極的に求め、モデル改良に反映させる方針を示した。</p>
]]></description>
      <pubDate>Fri, 10 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>オープンLLMの日本語性能でトップ──FLUX、「Flux Japanese LLM」公開　独自手法でQwen2.5を進化</title>
      <link>https://ledge.ai/articles/flux_japanese_llm_release</link>
      <description><![CDATA[<p>国内スタートアップのFLUX株式会社は2025年9月29日、日本語特化の大規模言語モデル「Flux Japanese LLM」を<a href="https://flux.jp/news/1093/">発表</a>した。</p>
<p>同モデルはAlibaba Cloudの大規模言語モデル「Qwen2.5-32B」を基盤に、日本語理解・生成性能を独自の新手法で強化したもので、Open Japanese LLM Leaderboard（通称：LLM勉強会ランキング）で総合スコア第1位（0.7417）を記録したという。</p>
<h2>日本語能力を高める新手法「Precise-tuning」とは</h2>
<p>FLUXは今回のモデル開発にあたり、従来のファインチューニングとは異なる「Precise-tuning（プリサイズチューニング）」手法を導入した。日本語データセット全体でパラメーターを再学習するのではなく、日本語能力強化に必要なネットワーク回路のみを特定して再調整することで、効率的かつ精度の高い言語理解を実現したとしている。</p>
<p><strong>FLUXが開発した「Precise-tuning」手法の概念図</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/1440_810_4_6b464affdc/1440_810_4_6b464affdc.jpg" alt="1440-810-4.jpg" /></p>
<h2>ベンチマークで国内首位に</h2>
<p>同モデルは、LLM勉強会ランキング<a href="https://ledge.ai/articles/open_japanese_llm_leaderboard">オープン日本語LLMリーダーボード</a>において総合スコア0.7417を記録し、他の日本語モデルを上回る評価を得たという。</p>
<p>このランキングは、日本の有志研究者・エンジニアによるコミュニティ「<a href="https://llm-jp.nii.ac.jp/">LLM-jp</a>（通称：LLM勉強会）」が運営しており、複数の日本語LLMを自然言語推論・要約・コード生成などのタスクで比較評価するオープンベンチマークとして知られる。</p>
<p>LLM-jpは、国立情報学研究所（NII）を事務局とする共同研究プロジェクトで、2024年4月にNII内に設立された大規模言語モデル研究開発センター（LLMC）と連携して、「日本語に強いオープンな大規模言語モデル」を開発・評価する活動を進めている。そのため、このランキングは国内の学術・産業両分野で日本語LLMの性能を客観的に測る基準として広く参照されている。</p>
<p><strong>Open Japanese LLM Leaderboardでの評価結果。Flux Japanese LLMが第1位を記録</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/News_main_240718_3_e93e607d1f/News_main_240718_3_e93e607d1f.jpg" alt="News_main_240718-3.jpg" /></p>
<p>モデルはHugging Face上で公開されており、<a href="https://huggingface.co/flux-inc/Flux-Japanese-Qwen2.5-32B-Instruct-V1.0">モデルカード</a>には、自然言語処理・要約・コード生成タスクでの性能指標や学習設計の概要が掲載されている。</p>
<h2>企業・業界別モデル展開へ</h2>
<p>FLUXは、「Flux Japanese LLM」を自社のノーコードAIプラットフォーム群と連携させる計画を進めており、金融業界向けの特化モデル開発も行っている。同社は「AIをすべての人の手に」をミッションに掲げ、企業・研究機関・行政などが安全にLLMを活用できる基盤づくりを目指している。</p>
]]></description>
      <pubDate>Fri, 10 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google、AIがPCを操作する「Gemini 2.5 Computer Use model」を開発者向けに公開──ClaudeやOpenAIモデルを上回る性能を実証</title>
      <link>https://ledge.ai/articles/google_gemini_2_5_computer_use_release</link>
      <description><![CDATA[<p>Google DeepMind は2025年10月7日（米国時間）、AI が実際のコンピューター画面を理解し、クリックや入力などの操作を実行できる新モデル「Gemini 2.5 Computer Use model」を開発者向けにプレビュー提供したと<a href="https://blog.google/technology/google-deepmind/gemini-computer-use-model/">発表</a>した。</p>
<p>Gemini API を通じて利用でき、AI が人間と同様にブラウザやアプリのUI（ユーザーインターフェース）を操作することを可能にする。</p>
<h2>Gemini API に“computer_use”ツールを追加</h2>
<p>今回発表された新モデルは、Gemini 2.5 の機能拡張として API に追加された「computer_use」ツールを用いて動作する。</p>
<p>AI はユーザーからの指示に加え、スクリーンショットと直近の操作履歴を入力として受け取り、次に取るべきアクション（クリック・入力・スクロールなど）を出力。実行結果を再び画面キャプチャとして取得し、目標達成までループ処理を行う。これにより、設定変更やフォーム入力、情報検索など、複数ステップを自律的に完了できる。</p>
<p>Google は公式ブログで、「このモデルはユーザー許可を前提に、安全性と透明性を重視して設計されている」と強調している。</p>
<p><strong>Computer Use model の処理ループ。AI がスクリーンショットと操作履歴をもとに次の行動を生成し、クライアント環境で実行 → 状況を再取得して次の判断へとつなげる</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/CTU_Diagram_RD_4_V01_width_1000_format_webp_b3415f41ee/CTU_Diagram_RD_4_V01_width_1000_format_webp_b3415f41ee.webp" alt="CTU-Diagram-RD4-V01.width-1000.format-webp.webp" /></p>
<h2>プレビュー提供と利用方法</h2>
<p>開発者は Google AI Studio および Vertex AI を通じて Computer Use model にアクセスできる。プレビュー版の段階では主にブラウザ操作に最適化されており、今後はより広範なアプリやデスクトップ環境への対応も検討されているという。</p>
<p>Google は、操作範囲やデータアクセスを制御する仕組みを組み込み、「責任ある自動化（Responsible Automation）」の実現を掲げている。</p>
<h2>ベンチマーク性能：Claude Sonnet 4.5 を上回る</h2>
<p>Google DeepMind は、Gemini 2.5 Computer Use model の性能を複数の標準ベンチマークで検証した。
Browserbase による Online-Mind2Web テストでは 65.7 % の精度を記録し、Claude Sonnet 4.5 や OpenAI Computer-Using Model を上回った。
さらに WebVoyager や AndroidWorld でも高スコアを達成し、実行速度（レイテンシ）でも優位性を示している。</p>
<p><strong>Gemini 2.5 Computer Use model は、Claude Sonnet 4.5 や OpenAI Computer-Using Model に比べ、低レイテンシかつ高精度を示した。</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/CTU_Scatterplot_RD_7_width_1000_format_webp_7e4e545c1c/CTU_Scatterplot_RD_7_width_1000_format_webp_7e4e545c1c.webp" alt="CTU-Scatterplot-RD7.width-1000.format-webp.webp" /></p>
<p><strong>複数ベンチマークで高い精度を記録。特に WebVoyager と AndroidWorld で際立ったスコアを達成した。</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/CTU_Benchmark_Chart_RD_5_V01_width_1000_format_webp_e0982edd33/CTU_Benchmark_Chart_RD_5_V01_width_1000_format_webp_e0982edd33.webp" alt="CTU-Benchmark_Chart-RD5_V01.width-1000.format-webp.webp" /></p>
<h2>動作デモ：AI がブラウザを自律操作</h2>
<p>公式ブログでは、実際の操作デモ動画も公開されている。
動画では AI が画面を認識し、ブラウザ上でリンクをクリックしたり、テキストを入力してタスクを完了する様子が確認できる。</p>
<p>@<a href="https://www.youtube.com/watch?v=_lu-FcPUIfM">YouTube</a></p>
<h2>AI による“手の届く自動化”へ</h2>
<p>今回の発表は、AI が人間の指示をもとに実際のUI を操作できる「エージェント時代」の幕開けを示す。
Google は Computer Use を “次世代の AI アシスタント” 開発の基盤と位置づけており、将来的には業務支援やウェブ操作、アプリ間連携など、より幅広い自動化領域への展開が期待される。</p>
]]></description>
      <pubDate>Fri, 10 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>GoogleのノーコードAIミニアプリ「Opal」日本を含む15カ国に拡大──ワークフロー可視化と高速化を提供</title>
      <link>https://ledge.ai/articles/google_opal_global_expansion</link>
      <description><![CDATA[<p>Googleは2025年10月7日（現地時間）、自然言語からウェブアプリを作成できるノーコードAIツール「Opal」の提供地域を、日本を含む15カ国へ拡大したと<a href="https://blog.google/technology/google-labs/opal-expansion/">発表</a>した。OpalはGoogle Labsの実験プロジェクトで、コードを書くことなくAIミニアプリを構築できるのが特徴だ。</p>
<h2>米国先行公開からグローバル展開へ</h2>
<p>Opalは7月24日（現地時間）に米国で<a href="https://ledge.ai/articles/google_opal_no_code_ai_tool">初公開</a>。ユーザーが「タスク管理アプリを作成」「画像から色を抽出」といった指示を与えると、AIがHTML/CSS/JavaScriptで構成されたアプリを自動生成する。初期ユーザーの利用が想定以上に高度化したことを受け、今回、日本、韓国、インド、カナダ、ブラジル、アルゼンチン、ベトナム、インドネシア、シンガポール、コロンビア、エルサルバドル、コスタリカ、パナマ、ホンジュラス、パキスタンの15カ国での提供を開始した。</p>
<h2>ワークフローの可視化と高速化</h2>
<p>同時に、Opalの実用性を高める改良が行われた。</p>
<ul>
<li><strong>高度デバッグ</strong> ：ノーコードのまま、ビジュアルエディタ上でワークフローをステップごとに実行・検証。エラーは発生ステップにリアルタイム表示され、原因特定を容易にする。</li>
<li><strong>パフォーマンス改善</strong> ：新規Opal作成の起動時間を短縮。**並列実行（parallel runs）**により複数ステップを同時に処理でき、待機時間を抑える。</li>
</ul>
<h2>クリエイティブから業務効率化まで</h2>
<p>Opalは、個人の創作活動やマーケティング支援、業務プロセスの自動化など、多様な用途に対応する。Googleは「ユーザーが複雑なプロセスを自動化したり、アイデアを素早く形にしたりできるよう支援する」としており、開発者コミュニティはDiscord上でも展開されている。</p>
<p>@<a href="https://youtu.be/g9RBGnz-vqk">YouTube</a></p>
]]></description>
      <pubDate>Fri, 10 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>学術＆研究2025/10/11 [SAT]Huawei、LLMの精度を保持したまま最大70%メモリ削減できる新手法を発表──コンシューマーGPUでの高精度生成AI実行も視野に</title>
      <link>https://ledge.ai/articles/huawei_sinq_quantization_llm</link>
      <description><![CDATA[<p>中国の大手テクノロジー企業Huawei（華為技術）は2025年9月26日、大規模言語モデル（LLM）を一般的なGPU環境でも高品質に動作させるための新しい量子化技術「Sinkhorn-Normalized Quantization（SINQ）」を<a href="https://www.arxiv.org/abs/2509.22944">発表</a>した。</p>
<h2>Sinkhorn正規化で“再調整なし”を実現</h2>
<p>従来のLLM量子化では、精度を維持するために一部データを用いて再調整（キャリブレーション）を行う必要があった。SINQはその工程を省略し、「再調整なし」で精度を保つ新しい方式だ。</p>
<p>仕組みの中核となるのが、「Sinkhorn-Knoppアルゴリズム」を応用した正規化手法である。モデルの重み行列に対して、行方向と列方向の2つのスケーリングベクトルを設定（dual-scaling）し、両軸の分散を均一化することで、外れ値（outlier）が特定の行や列に偏る問題を防ぐ。この工程により、量子化後の誤差を最小限に抑えられるという。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x1_818acd143f/x1_818acd143f.png" alt="x1.png" /></p>
<h2>精度を保ちながら70％のメモリ削減</h2>
<p>Huaweiの研究チームは、同社のQwen3モデル（1.7B〜235B）やDeepSeek-V2.5（236B）などで実験を行い、SINQが既存手法（RTN、HQQ、Hadamard変換など）を上回る精度を示したと報告している。</p>
<p>具体的には、4bit量子化時にパープレキシティ（文章予測精度）を最大40％改善し、メモリ使用量を最大70％削減。さらに、8GB程度の一般的なGPU上でQwen3-7Bモデルを実行できたとしている。処理時間も高速で、量子化プロセスは従来のRTN法に比べてわずか1.1倍。再調整を伴う手法（AWQやGPTQなど）よりも最大30倍速いという。</p>
<h2>幅広いモデルで動作、非一様量子化とも互換</h2>
<p>SINQは、Qwenシリーズだけでなく、Llama 2・Llama 3・DeepSeek-V3などの異なるモデルにも適用可能。
また、非一様量子化フォーマット（NF4）との併用でも精度を維持しており、調整を行うAWQと組み合わせた「A-SINQ」ではさらに高い性能を達成した。論文では、Mixture-of-Experts（MoE）構造の大型モデルでも安定して動作することが示されている。</p>
<h2>コンシューマーGPUでのLLM実行を視野に</h2>
<p>Huaweiは、SINQを「キャリブレーション不要の汎用量子化手法」と位置づけており、高性能GPUに依存しないLLM運用を可能にする技術として注目されている。論文著者らは、SINQの目的を「メモリ効率と速度を両立し、エッジデバイスでも高品質な生成AIを実行可能にすること」と説明している。</p>
<p>コードはGitHub上で<a href="https://github.com/huawei-csl/SINQ">公開</a>されており、研究者や開発者が自由に評価・応用できる環境が整っている。</p>
]]></description>
      <pubDate>Fri, 10 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ジョニー・アイブとサム・アルトマンが語る「AIと人間の新しい関係」──次世代デバイス構想をDevDay 2025で明かす</title>
      <link>https://ledge.ai/articles/openai_jony_ive_ai_device_philosophy</link>
      <description><![CDATA[<p>OpenAIのCEOであるサム・アルトマン氏と、Apple製品のデザインを手がけたジョニー・アイブ氏（LoveFrom代表）は、2025年10月に米サンフランシスコで開催された開発者会議「DevDay 2025」で<a href="https://www.youtube.com/watch?v=7cKbPLzNYws">対談</a>を行い、人間中心のAIデバイス構想を明らかにした。両氏は、テクノロジーがもたらす「過剰な情報と不安」を乗り越え、AIを通じて「穏やかで幸福な体験をもたらす新しいデバイス」を目指すと語った。</p>
<h2>パートナーシップの起点はChatGPT</h2>
<p>アイブ氏は、約30年間在籍したAppleを退社後、デザイナーや建築家などの専門家で構成するチーム「LoveFrom」を設立した。当初は明確な目標を持たずに活動していたが、ChatGPTの登場が転機になったという。
「ChatGPTを見たとき、私たちの目的が結晶化した」とアイブ氏は語る。この出会いが、サム・アルトマン氏との協業を決定づけた。両者は約3年前から新しいAIデバイスの構想を練り始めたという。</p>
<h2>「クラフト」と「ケア」に支えられた創造哲学</h2>
<p>アイブ氏は創造の出発点を「人類への愛」と表現し、ものづくりの根幹に「クラフト（職人技）」と「ケア（思いやり）」を置く。
彼は「人に見えない部分へのこだわりこそが、作り手の誠実さを示す」と述べ、細部まで丁寧に仕上げる姿勢を強調した。さらに「人々は、ケアが込められた製品を直感的に感じ取る。逆に、無頓着さ（ケアレスネス）は容易に見抜かれる」とも語った。</p>
<p>この“ケアの精神”をAIデバイスにも反映させ、「技術のための技術」ではなく、「人の幸福のためのテクノロジー」を設計することを目指すという。</p>
<p>@<a href="https://www.youtube.com/watch?v=7cKbPLzNYws">YouTube</a></p>
<h2>AIがもたらす「人間らしい関係」</h2>
<p>両氏が共通して強調したのは、AIを用いて人とテクノロジーの関係を再構築するというビジョンだ。
アイブ氏は、スマートフォンが2007年に登場して以降、人々がテクノロジーとの関係で「圧倒的な情報量と絶望感」に直面していると指摘。「AIはこの問題を悪化させるものではなく、正面から向き合うチャンスだ」と述べた。</p>
<p>理想とする体験として、彼は「必然性と自明性」「ユーモアと喜び」「穏やかさと幸福感」を挙げる。アルトマン氏も「AIは、人間とテクノロジーの関係を再設計する力を持つ」と応じ、「このプロジェクトは、人がテクノロジーを“感じる”新しい方法を探る実験だ」と語った。</p>
<h2>「誰もが初心者」──AI時代の創造者の課題</h2>
<p>アイブ氏は、急速に進化するAI分野では「誰もが初心者」であり、過去の経験が時に足枷になることもあると述べた。
「AIの勢いがあまりに速く、焦点をどこに定めるかが難しい。しかし、動機が“人類への愛”である限り、進むべき方向は見失わない」と語った。</p>
<h2>理念と現実、その間にある課題</h2>
<p>両氏が掲げた人間中心のビジョンは、AI時代のデバイス設計に新たな指針を与えるものだ。
一方で、<a href="https://www.ft.com/content/58b078be-e0ab-492f-9dbf-c2fe67298dd3">Financial Times</a>などによると、同プロジェクトはハードウェア面の設計やプライバシー制御などで難航しており、思想と実装のギャップにも注目が集まっている。
理念と現実の交差点に立つこの挑戦が、どのような形で結実するのかが問われている。</p>
<h2>「今を変えるチャンスがある」</h2>
<p>両氏は、AIデバイス開発の目的を「人を幸せで穏やかにし、不安を和らげること」に置いている。
アイブ氏はセッションの最後にこう締めくくった。</p>
<p>同氏はAIの可能性を「現状の延長ではなく、根本的な変化をもたらすもの」として位置づけた。</p>
]]></description>
      <pubDate>Fri, 10 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>政府、OpenAIに著作権侵害防止を要請──「Sora 2」問題で平デジタル相は“オプトイン方式”を提言</title>
      <link>https://ledge.ai/articles/openai_sora2_government_copyright_request_oct2025</link>
      <description><![CDATA[<p>OpenAIの動画生成AI「Sora 2」による日本のアニメ作品に酷似した映像がSNS上で拡散している問題を受け、政府が対応に乗り出した。城内実内閣府特命担当大臣（知的財産戦略・クールジャパン戦略担当）は10月10日の<a href="https://www.gov-online.go.jp/press_conferences/minister_of_state/202510/video-303104.html">記者会見</a>で、OpenAIに対し著作権侵害となる行為を行わないよう要請したと明らかにした。</p>
<p>城内大臣は「アニメや漫画は世界の人々を魅了し続ける、我が国が世界に誇る宝」と述べ、知的財産権の保護を重視する姿勢を強調。要請は内閣府の知的財産戦略推進事務局からオンラインで直接行われたという。記者質問の内容から、実施時期は10月上旬で、Sora 2による“酷似動画”が相次いだ直後とみられる。</p>
<p>一方、平将明デジタル大臣は10月12日、TBS番組でAIの学習段階における権利処理の在り方について言及した。「OpenAIには、きちんと権利処理をしていただくようお願いしている」と述べたうえで、「AIの学習データについても、事前の同意を得るオプトイン方式が望ましい」と発言。AI事業者に対し、無断利用ではなく同意制に基づくデータ利用の仕組みを導入するよう求めた。</p>
<p>これに先立つ10月3日、OpenAIのCEOであるサム・アルトマン氏は、動画生成AI「Sora 2」に関連する著作権保護と収益分配制度に関する方針をブログで明らかにしていた。同氏は「試行錯誤を重ねながら早期に開始する」と述べ、経済的な利益と新しい関係構築の双方を実現したい考えを示している。</p>
<p>政府は今後もAI事業者に対し、著作権および文化的資産の保護を重視した対応を求める方針を示している。AIによる創作支援が拡大するなかで、学習データの扱いと権利保護の両立が国際的な課題となりつつある。</p>
]]></description>
      <pubDate>Fri, 10 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>iRobotの共同創業者 ロドニー・ブルックス氏、「人型ロボットの器用さはまだ数十年先」──触覚なきAIを批判、“車輪付きポスト人型”の未来を予見</title>
      <link>https://ledge.ai/articles/rodney_brooks_humanoid_dexterity_post_humanoid_future</link>
      <description><![CDATA[<p>ロボット工学の第一人者であり、ルンバを開発したiRobotの共同創業者として知られるロドニー・ブルックス氏が、2025年9月26日付の<a href="https://rodneybrooks.com/why-todays-humanoids-wont-learn-dexterity/">公式ブログ</a>{target=\</p>
]]></description>
      <pubDate>Fri, 10 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>世界初のAI女優「ティリー・ノーウッド」にハリウッドが揺れる──SAG-AFTRAや著名俳優が「創造性の危機」と警告</title>
      <link>https://ledge.ai/articles/tilly_norwood_ai_actress_controversy_sagaftra</link>
      <description><![CDATA[<p>世界初の「AI女優」と称される Tilly Norwood（ティリー・ノーウッド）が、ハリウッドの俳優や映画俳優組合SAG-AFTRA（全米映画俳優組合・テレビ・ラジオ芸術家連盟）から強い批判を受けている。</p>
<p>SAG-AFTRAは2025年9月30日に「創造性は人間中心であるべき」と声明を<a href="https://www.sagaftra.org/sag-aftra-statement-synthetic-performer">発表</a>した。女優のエミリー・ブラント氏やウーピー・ゴールドバーグ氏も相次いで懸念を表明し、映画・テレビ業界全体に議論が広がっている。</p>
<h2>「AI Commissioner」──ティリー誕生の舞台</h2>
<p>Tilly Norwoodは、ロンドン拠点の制作会社Particle6が2025年9月にYouTube上で公開したコメディスケッチ『AI Commissioner | Comedy Sketch | Particle6』で初登場した。</p>
<p>この作品は、テレビ業界におけるAIの急速な普及を風刺的に描いたもので、AIが脚本作成からキャスティング、予算編成まですべてを自動化する世界を描く。劇中では、AIが生み出したインタラクティブスリラー『I Know What You Streamed Last Summer』に登場する100％AI生成の俳優としてTillyが紹介される。</p>
<p>@<a href="https://www.youtube.com/watch?v=3sVO_j4czYs">YouTube</a></p>
<p>登場人物の1人はTillyについて「僕の言うことを何でも聞いてくれる。恋をしてしまったかもしれない（She’ll do anything I say. I think I’m in love）」と評し、AIによる従順で“最適化された”俳優像を象徴的に表現。
さらに、「“She’ll cry on Graham Norton and be monetized on TikTok by lunchtime.”（彼女はグレアム・ノートンの番組で泣いて、その日の昼までにTikTokで収益化されるだろう）」という台詞が、人間の感情までもがAIによって即座に商業化される未来への皮肉として話題を呼んだ。</p>
<h2>SAG-AFTRAが声明「Tillyは俳優ではない」</h2>
<p>SAG-AFTRAは9月30日に「Statement on Synthetic Performer（合成パフォーマーに関する声明）」を<a href="https://www.sagaftra.org/sag-aftra-statement-synthetic-performer">発表</a>し、次のように明言した。</p>
<p>\u003E“Tilly Norwood is not an actor. Creativity must remain human-centered.”
（ティリー・ノーウッドは俳優ではない。創造性は人間中心であるべきだ。）</p>
<p>声明では、AIによって作られた “合成俳優” が芸術表現を侵食する可能性を指摘し、「経験や感情を持たない存在を“俳優”と呼ぶことは、芸術の根幹を損なう」と警鐘を鳴らした。SAG-AFTRA会長のショーン・アスティン氏もVarietyの取材に対し、「AI倫理と補償問題を正式な交渉テーマとして扱う」と述べている。</p>
<h2>著名俳優からの反発</h2>
<p>女優のエミリー・ブラント氏は、Varietyのポッドキャスト番組でTillyの画像を見せられ、驚きを隠さずこう語った。</p>
<p>\u003E“Good Lord, we’re screwed. That is really, really scary. Come on, agencies, don’t do that. Please stop taking away our human connection.”
「なんてこと、私たちは終わりね。本当に恐ろしいわ。お願い、エージェントはそんなことをやめて。人間のつながりを奪わないで。」</p>
<p>また、取材でTillyを「次のスカーレット・ヨハンソンに」と問われると、「“But we have Scarlett Johansson.”（でも私たちにはスカーレット・ヨハンソンがいる」 と返し、人間俳優の価値を強調した。</p>
<p>俳優・司会者のウーピー・ゴールドバーグ氏も、ABCのトーク番組『The View』（9月30日放送）で次のように発言した。</p>
<p>\u003E“You’re looking at 5,000 actors rolled into one synthetic person. That’s not fair. We all move differently.”
「5,000人の俳優の特性をひとつの合成存在にまとめるなんてフェアじゃない。私たちは皆、動きも表情も違う。」</p>
<h2>開発側の見解：「人間の代替ではなく、芸術表現」</h2>
<p>Tillyを制作したエライン・ファン・デル・フェルデン氏（Eline Van der Velden）は9月28日、ティリーのInstagram（<a href="https://www.instagram.com/tillynorwood/">@tillynorwood</a>）に投稿し、AI俳優の創作意図について次のように述べている。</p>
<p>\u003E “She is not a replacement for a human being, but a creative work — a piece of art.”
「ティリーは人間の代替ではなく、創造的な作品＝ひとつのアートです。」</p>
<p>同氏は、AIを「人間の代わり」ではなく「新しい絵筆のようなツール」と位置づけ、「アニメーションや人形劇、CGIがライブ演技を奪うことなく新しい可能性を開いたように、AIも物語を構築する新たな手段を提供する」と説明した。</p>
<p>\u003E “I’m an actor myself, and nothing — certainly not an AI character — can take away the craft or joy of human performance.”
「私は俳優でもあり、AIキャラクターであっても、人間の演技の技や喜びを奪うことはできません。」</p>
<p>投稿ではさらに、AIを“人間と競わせる存在”ではなく“芸術の新しいジャンルの一部”として評価すべきだと訴えている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/instagram_eline_at_tillynorwood_b80dee2451/instagram_eline_at_tillynorwood_b80dee2451.jpg" alt="instagram eline at tillynorwood.jpg" /></p>
<p>同氏が率いるParticle6は「AI俳優を活用すれば制作コストを最大90％削減できる」と説明し、複数のAIタレントを育てる「デジタル・タレント・ユニバース」構想を掲げている。</p>
<h2>今後の焦点</h2>
<p>各報道や関係者の声明からは、主に次の3点が論点として浮上している。</p>
<ul>
<li>著作権と肖像権：AI俳優の訓練データやモデル構築に使用された素材の扱い</li>
<li>契約・補償問題：AI使用を前提とした俳優契約の新たな枠組み</li>
<li>文化的受容：AIが「演技」を行うことを人々がどう受け入れるか</li>
</ul>
<p>SAG-AFTRAをはじめとする業界団体や俳優たちは、AIが創作活動や雇用に及ぼす影響を注視しており、今後は法制度や契約の整備を含めた議論が進む見通しだ。</p>
]]></description>
      <pubDate>Fri, 10 Oct 2025 05:50:00 GMT</pubDate>
    </item>
  </channel>
</rss>