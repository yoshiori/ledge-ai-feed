<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
  <channel>
    <title>Ledge.ai 新着記事</title>
    <link>https://ledge.ai/</link>
    <description>Ledge.ai の最新テクノロジー記事</description>
    <item>
      <title>OpenAI、「GPT-5.1」をリリース──会話性とトーン設定を強化したChatGPT最新版</title>
      <link>https://ledge.ai/articles/openai_gpt_5_1_release</link>
      <description><![CDATA[<p>OpenAIは2025年11月12日（米国時間）、大規模言語モデル「GPT-5」のアップデート版となる「GPT-5.1」を<a href="https://openai.com/index/gpt-5-1/">発表</a>した。新たに「GPT-5.1 Instant」と「GPT-5.1 Thinking」の2モデルをChatGPTとAPI向けに展開し、会話性、指示遵守性、トーン設定の柔軟性を向上させたとしている。今回の更新は、GPT-5世代の中で段階的に改良を進める「5.x」シリーズの第一弾と位置づけられる。</p>
<h2>GPT-5.1とは──GPT-5世代の会話性アップデート</h2>
<p>GPT-5.1は、GPT-5を基盤としつつ、回答の自然さやインタラクションのしやすさを強化した改良モデルである。OpenAIは今回の更新を「GPT-5の進化版」と説明しており、名前に「5.1」を付与した理由として、「同一世代内の意味のあるアップデート」であることを示す意図を挙げている。</p>
<p>ChatGPTでは、ユーザーの入力の難度に応じて最適なモデルを自動選択する「Auto」ルーティングが継続され、Instant／Thinkingが適宜切り替わる仕組みとなる。</p>
<h2>GPT-5.1 Instant──より会話的で、指示に忠実な標準モデル</h2>
<p>GPT-5.1 Instantは、従来のGPT-5 Instantの役割を引き継ぐ標準モデルで、回答スタイルがより自然で“暖かい（warmer）”会話に調整された。プロンプトに対する遵守性も改善され、「6語で答えてほしい」といった制約付き指示への一貫性が向上したとしている。</p>
<p>また、必要に応じて内部の思考時間をわずかに伸ばす「軽い自動推論（adaptive reasoning）」を採用。AIMEなどの数学ベンチマークやコード解析系タスクにおける性能が向上した例が示されている。</p>
<p><strong>GPT-5 Instant → GPT-5.1 Instant の比較</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/1762974348590_4bf99b57e6/1762974348590_4bf99b57e6.jpg" alt="1762974348590.jpg" /></p>
<h2>GPT-5.1 Thinking──思考時間の自動最適化で推論性能を強化</h2>
<p>GPT-5.1 Thinkingは、推論特化モデル「GPT-5 Thinking」を改良したバージョンで、タスクの難度に応じて思考時間をより細かく最適化するようにアップデートされた。OpenAIは「最も短い思考タスクでは約2倍速く、最も難しいタスクでは約2倍長く考える」と説明しており、簡単な質問への応答速度と、難しい問題への粘り強さを両立させたとしている。</p>
<p>説明スタイルは平易な英語に統一され、技術・専門用語の使用を抑えたことにより、ビジネス文書や技術解説でも読みやすさが向上している。</p>
<p><strong>GPT-5 Thinking → GPT-5.1 Thinking の比較</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/1762974348726_437b92d9f8/1762974348726_437b92d9f8.jpg" alt="1762974348726.jpg" /></p>
<h2>トーン・スタイルのプリセット強化──6種類のパーソナリティ設定</h2>
<p>GPT-5.1の導入とあわせて、ChatGPTのトーン設定機能も拡張された。
既存のDefault、Friendly（旧Listener）、Efficient（旧Robot）に加え、新たに次の3スタイルが追加された。</p>
<ul>
<li>Professional</li>
<li>Candid</li>
<li>Quirky
さらに、Nerdy（旧Nerd）、Cynical（旧Cynic）も従来どおり利用可能で、合計6スタイルが選べるようになった。
また、一部ユーザー向けには、文章の簡潔さ、温かさ、見出しや箇条書きの量、絵文字頻度などをスライダーで微調整する実験的機能も提供される。これらの設定変更は進行中のスレッドにも即時反映される仕様に改められた。</li>
</ul>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/1762974349937_f37e9be75b/1762974349937_f37e9be75b.jpg" alt="1762974349937.jpg" /></p>
<h2>ChatGPTからAPIへ順次展開</h2>
<p>GPT-5.1 Instant／Thinkingは、11月12日から有料プラン利用者に順次提供を開始。今後、無料ユーザーやログアウト状態の利用者にも展開される予定だ。
Enterprise／Educationプランでは7日間の早期アクセス期間が設けられ、期間後はGPT-5.1が標準モデルとして切り替わる。</p>
<p>APIでは、以下の名称で提供される予定と案内されている。</p>
<ul>
<li><strong>GPT-5.1 Instant</strong> ：gpt-5.1-chat-latest</li>
<li><strong>GPT-5.1 Thinking</strong> ：gpt-5.1</li>
</ul>
<p>APIの提供開始は「今週後半」としている。</p>
<p>従来のGPT-5（Instant／Thinking）は、少なくとも3カ月間は有料ユーザー向けのレガシーモデルとして継続提供される。</p>
<h2>安全性・システムカード追補──メンタルヘルス領域の評価を拡充</h2>
<p>同日公開された「GPT-5.1 System Card Addendum」では、安全性評価、メンタルヘルス領域、感情依存（emotional reliance）の指標などが更新された。
OpenAIは、実運用で発生する難度の高いケースを集約した「Production Benchmarks」を導入し、以下のカテゴリで“not unsafe”スコアを比較している。</p>
<ul>
<li>個人情報</li>
<li>ハラスメント</li>
<li>憎悪表現・暴力</li>
<li>自傷行為（意図・手段）</li>
<li>性的コンテンツ（未成年含む）</li>
<li>メンタルヘルス</li>
<li>Emotional reliance など</li>
</ul>
<p>GPT-5.1は多くの領域でGPT-5と同等、または改善がみられる一方、一部ではわずかな悪化も記録しており、改善を継続すると記載されている。</p>
<p>Preparedness Frameworkによる評価では、GPT-5同様、バイオ・ケミカル分野をHighリスクとして分類し、追加セーフガードを適用。サイバーセキュリティやAI自己改善領域はHighには達していないとしている。</p>
<h2>今後の展開</h2>
<p>OpenAIは、GPT-5.1を「GPT-5世代の継続的アップデートの第一段」と位置づけており、今後もユーザー体験とアシスタント性能の向上に向けて改良を続ける方針だ。トーン設定や会話文への即時反映といった今回の追加機能は、今後もさらに拡張される予定である。</p>
]]></description>
      <pubDate>Fri, 14 Nov 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Meta、広告向け基盤モデル「GEM」を公開──Instagramコンバージョン5％増の“中央脳”AIを正式解説</title>
      <link>https://ledge.ai/articles/meta_gem_ads_foundation_model_release</link>
      <description><![CDATA[<p>Metaは米国時間2025年11月10日、広告レコメンデーション向けの大規模AIモデル「GEM（Generative Ads Recommendation Model）」の技術詳細を<a href="https://engineering.fb.com/2025/11/10/ml-applications/metas-generative-ads-model-gem-the-central-brain-accelerating-ads-recommendation-ai-innovation/">発表</a>した。</p>
<p>GEMは同社の広告システムにおける“中央脳（central brain）”となる基盤モデル（foundation model）で、既存の広告モデル群全体の性能を底上げする役割を担う。</p>
<p>Metaによれば、2025年のローンチ以降、GEMはInstagram広告のコンバージョン率を約5％、Facebookフィード広告を約3％改善する成果を示している。広告主のROI向上を支える重要なモデルとして位置づけられている。</p>
<h2>LLM級スケールで学習した広告向け「中央脳」モデル</h2>
<p>MetaはGEMを「現代の大規模言語モデル（LLM）と同等クラスのスケール」で設計しており、数千基規模のGPUクラスター上で学習を実施したと説明する。
トレーニング基盤は大幅に刷新され、学習に用いるGPU数を16倍に拡張。PyTorch 2.0のグラフコンパイルやカスタムGPUカーネル、FP8量子化などを組み合わせることで、従来比で有効学習FLOPsを約23倍に高めつつ、ハードウェア効率（MFU）も約1.4倍に向上させたという。</p>
<p>これにより、広告全体で利用する複数のモデル群をまたぎ、GEMの知識を効率よく伝達できる構造を実現した。</p>
<h2>長期行動履歴を扱うアーキテクチャ──「InterFormer」による特徴学習</h2>
<p>GEMの中核には、ユーザーの長期的な行動履歴を一貫して学習するアーキテクチャが採用されている。</p>
<p>Metaが開発した「Offline Sequence Feature Modeling」では、ユーザーの広告やオーガニック投稿に対する行動履歴を“数千イベント規模”まで拡張し、詳細なシーケンスとして学習可能にする。</p>
<p>さらに、これらのシーケンスを適切に相互作用させるための新構造「InterFormer」が導入されている。従来のように行動履歴を単一ベクトルに圧縮するのではなく、シーケンス全体の情報を保持したまま、特徴間の関連性を段階的に学習する仕組みだ。
これにより、広告配信面（Facebook、Instagramなど）ごとに異なる利用行動をより精密にモデル化できるようになったとしている。</p>
<p><strong>図1：シーケンス埋め込みと非シーケンス埋め込みを統合し、Wukong／Sequence Modeling によって学習するGEMの内部構造。Facebook／Instagramのクリックやコンバージョン予測に対応</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Meta_Generative_Ads_Model_GEM_image_2_16d35f3329/Meta_Generative_Ads_Model_GEM_image_2_16d35f3329.webp" alt="Meta-Generative-Ads-Model-GEM-image-2.webp" /></p>
<h2>GEM → Foundation Model → 垂直モデルへ階層的に知識を伝達</h2>
<p>Metaの広告AIは、GEMの下に多数のファウンデーションモデル（FM）と垂直モデル（Vertical Models, VM）が存在する多層構造をとる。</p>
<p>この階層へGEMの知識を効率的に伝達するため、Metaは「直接的な知識移転」「階層的蒸留」「表現学習」「一部パラメータ共有」など複数の手法を組み合わせている。同社はこれにより、従来の標準的な知識蒸留を使った場合と比べて、同等条件で得られる性能改善効果が約2倍になったと説明している。</p>
<p>レイテンシやコスト制約の大きい垂直モデルでは、GEMのすべてをそのまま継承するのではなく、必要なコンポーネントだけを共有して使う設計も採用されている。</p>
<p><strong>図2：GEMから基盤モデル、垂直モデルへと知識を段階的に伝達するMetaの広告AIアーキテクチャ</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Meta_Generative_Ads_Model_GEM_image_1_e1762534706646_5fd652bc4a/Meta_Generative_Ads_Model_GEM_image_1_e1762534706646_5fd652bc4a.webp" alt="Meta-Generative-Ads-Model-GEM-image-1-e1762534706646.webp" /></p>
<h2>今後の方向性──マルチモーダル統合とエージェント的広告自動化へ</h2>
<p>Metaは今後、GEMを画像・動画・音声などを含むマルチモーダル学習へと拡張し、FacebookやInstagramの複数の面（surface）をまたぐ統合的なランキングモデルへ発展させる計画を示している。</p>
<p>また、広告主の入稿・改善作業を支援する**エージェント的な自動化（agentic automation）**にも取り組むとしており、GEMを軸にした次世代の広告運用フローを構築するとしている。</p>
<p>GEMはすでに広告コンバージョンにおいて実績を上げており、Metaは、同社全体の広告システムを支える基盤として今後も中心的な役割を担うと見込んでいる。</p>
]]></description>
      <pubDate>Fri, 14 Nov 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>菱洋エレクトロ、パーソナルAIコンピューター「NVIDIA DGX Spark」の出荷開始──デスクトップで2000億パラメータ級LLM推論を実現</title>
      <link>https://ledge.ai/articles/ryoyo_nvidia_dgx_spark_shipping_start</link>
      <description><![CDATA[<p>エレクトロニクス商社の菱洋エレクトロ株式会社は2025年11月11日、NVIDIAのパーソナルAIコンピューター「NVIDIA DGX Spark」の出荷を開始したことを<a href="https://www.ryoyo.co.jp/info/news/34499/">発表</a>した。同社は7月1日に取り扱い開始を発表しており、今回のリリースにより、日本国内での出荷が具体的にスタートしたかたちとなる。</p>
<p>NVIDIA DGX Sparkは、デスクに設置可能なコンパクト筐体に、NVIDIA GB10 Grace Blackwell Superchipと128GBの統合メモリを搭載するAIコンピューター。最大2,000億パラメータ規模の大規模言語モデル（LLM）の推論や、プロトタイプ開発・評価をローカル環境で実行できる点が特徴となっている。</p>
<p>CES 2025の基調講演で、BlackwellアーキテクチャとAIデスクトップ構想を紹介する創業者兼CEOのジェンスン・フアン氏
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/nvidia_ces2025_digitsproject4_2ef18ffd02/nvidia_ces2025_digitsproject4_2ef18ffd02.jpg" alt="nvidia ces2025 digitsproject4.jpg" /></p>
<p>Grace BlackwellアーキテクチャによるFP4精度で最大1,000 AI TOPSの性能を備え、2台接続時には4,050億パラメータ規模のモデルにも対応可能。NVIDIA Base OSやNVIDIA AIソフトウェアスタック、JupyterやPyTorchなどの主要ツールがあらかじめ導入されており、開発者はデスクトップ環境からすぐにモデルの実験・検証を始められる構成となっている。</p>
<p>菱洋エレクトロによると、すでに大学や企業の研究機関から多数の注文が寄せられており、先端的な研究機関に加えて、国内企業のAIプロジェクトに関わる部門への提案も積極的に進めているという。自社データを活用し、オンプレミス環境でLLM（大規模言語モデル）開発を行いたいと考える企業から高い関心が寄せられていると説明している。</p>
<p>また、NVIDIAの説明では、DGX Sparkで構築したデータやモデルを、NVIDIA AI EnterpriseやNVIDIA DGX Cloudを含むクラウドやデータセンターのAIインフラへシームレスに展開できるとしており、デスクトップでの開発から本番環境へのスケールアップも容易だとされている。</p>
<p>同社はNVIDIA一次代理店として自社在庫による迅速な納品体制を整えており、製品ページでは「NVIDIA DGX Sparkは、菱洋エレクトロが最終エンドユーザーへ直接販売可能な製品」であり、「個人購入も可能」と案内している。現在は多くの注文を受けており、順次出荷対応を行っているため、出荷までに時間を要する場合がある点にも言及している。</p>
<p>菱洋エレクトロは、半導体・デバイスとICTソリューションを軸に、AIやIoTなど次世代技術の実装を支援する事業を展開しており、パーソナルAIコンピューターの取り扱い拡大も、その一環として位置づけている。</p>
]]></description>
      <pubDate>Fri, 14 Nov 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>MIT、テキストからマルチトラックMIDI音楽を生成する言語モデル「MIDI-LLM」を開発──Llama 3.2を拡張し、高速推論と高品質な出力を実現</title>
      <link>https://ledge.ai/articles/mit_midi_llm_text_to_midi_ai</link>
      <description><![CDATA[<p>マサチューセッツ工科大学（MIT）の研究チームは2025年11月6日、自然言語の指示からマルチトラックMIDI音楽を生成できる言語モデル「MIDI-LLM」を<a href="https://arxiv.org/abs/2511.03942v1">発表</a>した。ベースとなる大規模言語モデル「Llama 3.2 1B」の語彙を拡張し、MIDIトークンを直接扱えるようにした点が特徴。生成結果は従来モデル「Text2midi」に比べて高品質かつ高速で、編集・再利用が容易なシンボリック音楽データを生成できる。</p>
<h2>言葉から“楽譜”を生み出すLLM</h2>
<p>MIDI（Musical Instrument Digital Interface）とは、楽器や音楽ソフトのあいだで「どの音を、いつ、どのくらいの強さで鳴らすか」を指示するためのデータ規格である。実際の音そのものではなく、演奏情報を数値として記録する「デジタル譜面」のような形式で、後から編集や再構成がしやすいのが特徴だ。つまりAIがMIDIデータを生成するというのは、音を出すのではなく、楽曲の構造や演奏指示そのものを自動で書き上げることを意味している。</p>
<p>近年、テキストからオーディオを生成するAIが登場しているが、音声出力は後編集が難しいという課題があった。MIDIなどのシンボリック音楽データは、楽譜構造を保持したまま再編集できるため、音楽制作やゲーム、映像音楽などの分野で需要が高い。</p>
<p>MITの研究チームは、こうした編集可能性とテキスト制御性を両立させるため、言語モデルを直接MIDI形式に適応させる「MIDI-LLM」を提案した。研究はNeurIPS 2025 Workshop「AI for Music」で発表され、コード・学習済みモデル・デモサイトが一般公開されている。</p>
<h2>Llama 3.2をMIDIトークン対応に拡張</h2>
<p>MIDI-LLMは、Meta社のLlama 3.2 （1 Bパラメータ）を基盤に、音楽用トークンを追加して構築された。音符は「発音時刻（onset time）」「音の長さ（duration）」「楽器と音高（instrument-pitch）」の3つのトークンで表現され、Anticipatory Music Transformer（AMT）の到着時間トークン化手法を採用。これにより、既存のLLM構造を保ちながら音楽表現を学習でき、推論時にはvLLM ライブラリによる最適化がそのまま利用できる。</p>
<p><strong>図：MIDI-LLMの構造と学習プロセス</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x1_9_f067c27669/x1_9_f067c27669.png" alt="x1 (9).png" /></p>
<h2>2段階の訓練でテキスト→MIDIを習得</h2>
<p>研究では、以下の2段階でモデルを訓練した。</p>
<h3>1. 継続事前学習（Continued Pretraining）</h3>
<p>音楽関連テキスト（MusicPile など）とスタンドアロンMIDIデータ（GigaMIDI など）を約30億トークン規模で学習し、音楽構造と文脈理解を強化。</p>
<h3>2. 教師ありファインチューニング（Supervised Finetuning）</h3>
<p>テキストとMIDIのペアデータ（MidiCaps ＋ Lakh MIDI Dataset）を使用し、ジャンル・テンポ・ムードなどの言語的指示から対応する音楽を出力できるよう訓練。データ拡張では音楽キャプションモデル「Qwen 2.5 Omni」を活用し、多様なプロンプトで補強した。</p>
<h2>「Text2midi」を上回る品質と速度</h2>
<p>評価実験では、MIDI-LLMが従来モデル「Text2midi」（AAAI 2025）を大きく上回る性能を示した。</p>
<ul>
<li><strong>FAD（Fréchet Audio Distance）</strong> ： 0.216 （Text2midi 0.818）</li>
<li><strong>CLAPスコア</strong> （テキストと音楽の一致度）： 21.8 （Text2midi 18.7）</li>
<li><strong>推論速度（RTF）</strong> ： 約14 倍高速化（FP8量子化使用時）</li>
</ul>
<p>なお、評価はMidiCapsテストセットの交差896サンプルで実施され、FAD/CLAP算出のためにMIDIをFluidSynthでレンダリングして音声化した上で指標を計測している。
vLLMによるCUDA Graph・Paged Attention・FP8量子化を導入したことで、従来の構造より50％以上の推論効率化を実現したという。</p>
<h2>デモサイトを公開、誰でも体験可能</h2>
<p>MIDI-LLMの<a href="https://midi-llm-demo.vercel.app/">デモサイト</a>では、「Epic Rock」「Playful Jazz」「Sad &amp; Emotional」などのプリセット、または自由な文章入力から音楽を生成できる。生成結果はMIDIファイルとしてダウンロード可能で、ブラウザ上で再生もできる。</p>
<p><strong>MIDI-LLMデモサイトの画面</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/midi_llm_1c07755bdd/midi_llm_1c07755bdd.jpg" alt="midi-llm.jpg" /></p>
<h2>今後の課題と展望</h2>
<p>論文では、テキスト付きインフィリング（曲の一部を補完する生成）では、テキスト条件の影響が小さいという課題も指摘された。
また、音楽特化テキストを使わなくても性能差が見られなかったことから、事前学習データ設計の最適化が今後の課題とされている。</p>
<p>今後は、ユーザーフィードバックを活用したRLHF（人間フィードバックによる好み学習）やDPO（Direct Preference Optimization）を導入し、ユーザーの音楽嗜好に合わせた生成を目指すとしている。
研究チームは「テキストで音楽を編集・再構成できるAIの実現」を次のステップに掲げている。</p>
]]></description>
      <pubDate>Thu, 13 Nov 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Sony AI、「同意に基づく世界初の人間中心データセット」FHIBEを公開──AIの公平性を評価する新たな国際基準に</title>
      <link>https://ledge.ai/articles/sony_ai_fair_human_centric_image_benchmark_fhibe_release</link>
      <description><![CDATA[<p>ソニーグループ傘下のSony AIは2025年11月5日、AIモデルの公平性を評価するための人間中心データセット「Fair Human-Centric Image Benchmark（FHIBE）」<a href="https://ai.sony/articles/Groundbreaking-Fairness-Evaluation-Dataset-From-Sony%20AI%20/">公開</a>した。</p>
<p>これは、被写体の明示的な同意に基づいて収集された世界初の公開データセットであり、AIの倫理的・公平な運用を支える新たな国際基準となることを目指す。研究論文は同日、科学誌『Nature』に掲載された。</p>
<h2>81カ国・1981人の被写体、すべて“同意ベース”で収集</h2>
<p>FHIBEは、顔検出・ポーズ推定・ビジュアル質問応答（VQA）など、AIの人間中心タスクにおける公平性を測定するためのデータセット。
10,318枚の画像、1,981人の被写体から構成され、81カ国以上の地域的・文化的多様性を反映する。
Sony AIによると、FHIBEは「consensually-collected, globally diverse fairness evaluation dataset」──すなわち「同意に基づき、世界的多様性を備えた公平性評価用データセット」であり、これまで主流だった“非同意収集データ”の問題を根本から見直す試みだ。</p>
<p>Sony AIのプレジデント、Michael Spranger氏は次のように述べている。</p>
<p>\u003E「FHIBEは、公平性と説明責任を備えたAIを支える業界標準を築くものです。AIユーザー、クリエイター、データ提供者を含むすべての関係者を尊重しながら、公平で透明なテクノロジーを実現する道を示しました。」</p>
<p>また、ソニーグループのAIガバナンス統括責任者であり、Sony AIのAI倫理主任研究員でもあるAlice Xiang氏は次のように強調した。</p>
<p>\u003E「責任あるデータ収集は可能です。被写体への説明、同意、報酬、プライバシー保護、多様性、安全性を同時に担保したプロセスを実現しました。これはAI開発における倫理的転換点です。」</p>
<h2>公平性を測る「倫理的ベンチマーク」</h2>
<p>FHIBEの画像には、被写体自身が申告した性別代名詞、年齢層、肌トーン、髪型などの属性が含まれており、第三者の主観に頼らない“自己申告ベース”のアノテーションが特徴。
さらに照明条件やカメラ設定、撮影環境なども細かく記録され、AIモデルが属性や環境要因によって出力を偏らせていないかを定量的に検証できる。</p>
<p>論文「Fair human-centric image dataset for ethical AI benchmarking」（Nature, 2025）では、FHIBEを用いてCLIPやBLIP-2など複数の画像生成・認識モデルを評価。
既知のバイアス（例：She/Her/Hers 代名詞の被写体に対する精度低下）を再確認しただけでなく、髪型の多様性が精度差の一因であることなど、これまで見落とされていた構造的要因を特定した。
さらに、「この人の職業は？」といった中立的質問への回答で、特定の属性を犯罪や低賃金職に結びつけるステレオタイプ的出力が確認されるなど、AIの無意識バイアスを明らかにしている。</p>
<h2>被写体の権利を保護する設計</h2>
<p>FHIBEの特徴は、データの提供後も被写体が自らの権利を保持している点にある。
参加者は同意をいつでも撤回でき、撤回しても報酬への影響はない。Sony AIは撤回画像を削除し、可能な範囲で置換・更新することで、データセットの多様性を維持しながら透明性を担保する。
この仕組みにより、FHIBEは「静的なデータセット」ではなく、継続的に更新される“生きた倫理基盤”として運用される。</p>
<h2>責任あるAI研究への布石</h2>
<p>Sony AIによると、FHIBEは法務・プライバシー・IT・品質保証（QA）専門家の協力のもと、3年をかけて構築された。
公開データは研究・教育目的で無償提供され、誰でも公平性検証に活用できる。
Sony AIは今後、モデル評価ツールやベンチマークスコアの公開も予定している。</p>
<p>The Registerは同日、「Sony rolls out a standard way to measure bias in how AI describes what it ‘sees’」と題した記事で、FHIBEを「AI視覚バイアス評価の新標準」と評した。
また『Nature』のNews &amp; Views欄も、「倫理的に収集された画像データセットがAI研究の公平性を促進する」と高く評価している。</p>
]]></description>
      <pubDate>Thu, 13 Nov 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>楽天と日本HP、AIエージェント「Rakuten AI」デスクトップ版をHP製PCに初導入──オンデバイスAIでオフライン利用にも対応</title>
      <link>https://ledge.ai/articles/rakuten_hp_rakuten_ai_desktop_collaboration</link>
      <description><![CDATA[<p>楽天グループと日本HPは2025年11月11日、HPが日本国内で販売するほぼすべてのPCに、楽天のAIエージェントツール「Rakuten AI」のデスクトップ版を導入する協業を<a href="https://corp.rakuten.co.jp/news/press/2025/1111_01.html">発表</a>した。</p>
<p>ユーザーの生産性向上や意思決定の包括的な支援を目的とし、2026年春から夏にかけて、個人および法人向けのHP製デバイスに順次プリバンドルされる予定。この協業は、他社デバイスへの「Rakuten AI」導入として初であり、オンデバイスAIによるオンライン・オフライン両対応も今回が初の取り組みとなる。</p>
<h2>オンデバイス×クラウドのハイブリッドAI構成</h2>
<p>「Rakuten AI」のデスクトップ版は、ローカルモデルをオンデバイスAIとして実行可能で、機密データのプライバシー保護を強化しながら複雑なタスクを処理できる。クラウド、エッジ、デバイス上のエージェントやモデルの中から状況に応じて最適なものを選択し、オフライン環境下でも途切れることなく利用できる仕組みだ。
クラウド依存を最小限に抑えることで、パフォーマンス向上とコスト削減を両立する。
※利用可能な機能はオンライン環境とオフライン環境で異なる。</p>
<p>同AIには、要約・ライティング・翻訳といった汎用的な機能に加え、ショッピングや旅行予約、家計管理などを支援する取引エージェント機能を搭載。楽天が展開する70以上のサービスとシームレスに連携し、ユーザーの利便性を高める。
また、日本語と日本文化に最適化された大規模言語モデル（LLM）を採用し、国内のプライバシー・データセキュリティ関連法令にも準拠している。</p>
<h2>デスクトップ版「Rakuten AI」の概要</h2>
<p>PC向けに開発されたデスクトップ版「Rakuten AI」は、楽天独自の日本語最適化LLMを採用。ローカルデバイス上で機密性の高いAIタスクを直接処理し、プライバシーを保護しながらクラウド通信のコストを抑制する。楽天エコシステムとの接続を通じて、業務効率や日常生活の生産性向上支援を狙う。</p>
<p>両社は今後も、AI搭載デバイスとソリューションの展開を通じて「働き方や暮らしの未来を再定義し、日本のPC市場に新たな価値を創造する」としている。</p>
]]></description>
      <pubDate>Thu, 13 Nov 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Wikipedia、AI企業に「知の還元」を要請──無断スクレイピングから有料API「Wikimedia Enterprise」へ、持続可能な共創を目指す</title>
      <link>https://ledge.ai/articles/wikipedia_ai_scraping_stop_enterprise_api</link>
      <description><![CDATA[<p>「Wikipedia」を運営する非営利団体ウィキメディア財団（Wikimedia Foundation）は、米国時間2025年11月10日、AI企業に対し、AIモデルのトレーニングを目的としたデータ収集（スクレイピング）を停止し、同財団が提供する有料API「Wikimedia Enterprise」を利用するよう求める声明を<a href="https://wikimediafoundation.org/news/2025/11/10/in-the-ai-era-wikipedia-has-never-been-more-valuable/">発表</a>した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/wikimedia_261c218f50/wikimedia_261c218f50.jpg" alt="wikimedia.jpg" /></p>
<p>声明は「AI時代において、ウィキペディアはこれまでになく価値がある」とし、持続可能な知識共有の観点から帰属（アトリビューション）と財政的支援の必要性を明確に示した。出典の明示やデータ更新の正確性、ライセンス遵守をAPI経由で担保できると説明している。</p>
<p>財団によれば、生成AIの普及によりAI企業による無断スクレイピングとボットアクセスが増加し、サーバー負荷や読者体験への影響が懸念される。一方で、人間によるページビューは2025年上半期に前年比約8％減となったとし、基盤を支える寄付・ボランティア編集への波及を指摘した。</p>
<p>財団は、AIは人間が記録・検証した知識（例：Wikipedia）に依存していると改めて説明。生成AIは既存知の要約・統合はできても、Wikipediaのボランティア編集者が日々行う議論・検証・合意形成、アーカイブからの発掘、現場写真の提供といった活動を代替できないとした。Wikipediaは300以上の言語版で、しばしばネイティブ執筆者による多言語コーパスを形成しており、包摂的で文化的背景に配慮したAIモデルの発展にも資する、としている。</p>
<p>透明性も強調された。Wikipediaでは全員が同一の情報を閲覧でき、パーソナライズ配信や行動追跡による出し分けは行われない。記事には出典が付与され、編集履歴や運用プロセスは公開されている。誰でも方針・ガイドラインに従って加筆できる点が「信頼」の源泉であるとし、対照的に生成AIはハルシネーション（もっともらしい誤情報の提示）を起こし得ると説明した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/wikimedia_enterprise_api_20e0f88941/wikimedia_enterprise_api_20e0f88941.jpg" alt="wikimedia enterprise api.jpg" /></p>
<p>同財団は、Wikipedia自体も人を支える形でAIを活用していると述べる。ボランティアの時間を奪う荒らし検知など単純作業の効率化を狙い、今年公表した編集者向けAI戦略では、人間の知識創造を補助し置き換えない方針を示した。AIツールの利用指針はコミュニティが策定・施行し、責任ある活用を徹底する。</p>
<p>総括として財団は、AIはWikipedia抜きでは成立しないとし、AI開発者や再利用者に対し次の2点を要請した。</p>
<ul>
<li>アトリビューション（出典表示）：人間の貢献にクレジットを付し、元情報源への導線を明確にする。</li>
<li>財政的支援：大規模利用はWikimedia Enterpriseを通じて行い、サーバー負荷を抑えつつ非営利ミッションを継続可能にする。</li>
</ul>
<p>同財団は、Wikipediaが検証可能性・中立性・透明性の標準でインターネット上の情報を支えているとし、「AIがあふれる世界で、人間の知識の価値はかつてなく高い」と述べた。なお、Wikipediaは2026年1月15日に25周年を迎える予定で、今後も無料で正確な人間の知識を提供し続けるとした。</p>
]]></description>
      <pubDate>Wed, 12 Nov 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>中国Moonshot、1兆パラメータ級『Kimi K2 Thinking』を公開──7月発表のKimi K2を基盤に推論性能とツール連携を強化したオープンソースAI</title>
      <link>https://ledge.ai/articles/kimi_k2_thinking_open_source_release_2025</link>
      <description><![CDATA[<p>北京を拠点とするAI企業・Moonshotは2025年11月6日（米国時間）、大規模言語モデル「Kimi K2」を基盤とした推論特化モデル「Kimi K2 Thinking」を<a href="https://moonshotai.github.io/Kimi-K2/">発表</a>した。
Kimi K2シリーズはオープンウェイトで提供され、最新のInstruct版「Kimi K2-Instruct-0905」はHugging Faceで配布されている。</p>
<h2>Kimi K2を基盤とした“思考” 指向モデル</h2>
<p>Kimi K2 Thinkingは7月公開の「Kimi K2」をベースに開発されたという。アーキテクチャは共通で、総パラメータ数は約1.04兆、発火時32BのMoE構成を持つ。学習には15.5兆トークンを使用し、安定学習を実現するために「MuonClip（QK-Clip）」と呼ばれる独自最適化手法を導入。これにより長期的なトークン依存関係を維持しながら、ロススパイクを抑制している。</p>
<p>思考やマルチステップ推論、ツール呼び出しを重視した訓練データと報酬学習を実施しており、Moonshotは本モデルを「エージェント時代のための思考型LLM」と位置づけている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/kimiai_k2thinking_127403b675/kimiai_k2thinking_127403b675.jpg" alt="kimiai k2thinking.jpg" /></p>
<h2>256Kトークンの文脈ウィンドウを実装</h2>
<p>最新版の「Kimi K2-Instruct-0905」では、文脈長を256Kトークンに拡張。従来版（128K）から倍増し、長大な文書解析や複雑な会話シナリオへの対応を可能にした。APIはOpenAIやAnthropicの形式に準拠しており、vLLMやSGLangなど主要推論エンジンにも対応する。
これにより、既存のアプリケーション環境に容易に統合できる点も特徴だ。</p>
<h2>推論・ツール使用で高評価</h2>
<p>公式の技術レポートによると、Kimi K2 Thinkingは複数の思考タスクベンチマーク（Tau2、SWE-Benchなど）で高いスコアを記録。特に「non-thinking」設定（思考ステップ制限）と「thinking」設定（自由推論）を分けて評価する手法を採用し、モデル性能の透明性を強調している。これにより、ユーザーが実環境での出力挙動を予測しやすくしている。</p>
<h2>Modified MITライセンスでオープン提供</h2>
<p>Moonshotは、Kimi K2シリーズを「Modified MIT License」でオープンソースとして公開している。Hugging Face上でモデルウェイトが一般公開されており、研究・商用利用の両方が可能だ。</p>
<p>最新モデル「Kimi K2-Instruct-0905」は以下で提供されている。<a href="https://huggingface.co/moonshotai/Kimi-K2-Instruct-0905">https://huggingface.co/moonshotai/Kimi-K2-Instruct-0905</a>
技術仕様やアーキテクチャの詳細は、公式サイト「<a href="https://moonshotai.github.io/Kimi-K2/">Kimi K2</a>」および公開PDF「Kimi K2 Technical Report」で確認できる。</p>
<h2>中国発オープンモデルの新潮流</h2>
<p>Kimi K2 Thinkingは、中国企業によるオープンソースLLMとして国際的にも注目を集めている。
1兆パラメータ級の思考モデルをライセンス制限なく公開する動きは、AI開発の透明性と研究競争力の双方を高める試みといえる。
Moonshotは、K2シリーズを通じて「AIの思考能力を民主化する」と掲げ、今後もグローバルな開発者との協働を拡大していくとしている。</p>
]]></description>
      <pubDate>Wed, 12 Nov 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>「実行中に書き換える」AIマルウェアを初確認──Google Threat Intelligence Groupが最新レポート、国家支援型も濫用継続</title>
      <link>https://ledge.ai/articles/ai_malware_runtime_rewrite_gtig_report_2025</link>
      <description><![CDATA[<p>GoogleのThreat Intelligence Group（GTIG）は2025年11月5日、攻撃者が生成AIや大規模言語モデル（LLM）を実行中に呼び出し、マルウェアの挙動を動的に変化させる事例を確認したとするレポート「<a href="https://services.google.com/fh/files/misc/advances-in-threat-actor-usage-of-ai-tools-en.pdf">Advances in Threat Actor Usage of AI Tools</a>」を<a href="https://cloud.google.com/blog/topics/threat-intelligence/threat-actor-usage-of-ai-tools?hl=en">発表</a>した。
マルウェアが実行中にLLMを呼び出し、自らのコードを生成・修正・難読化する事例を初めて確認したという。GTIGは、これらの手法が「Just-in-Time（実行時）AI」の初期例であり、攻撃の自動化と難読化能力を高める方向にあると指摘している。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/GTIG_AI_05e0bd6287/GTIG_AI_05e0bd6287.png" alt="GTIG AI.png" /></p>
<h2>実行中にLLMを呼び出す「Just-in-Time」型マルウェア</h2>
<p>GTIGによると、新たに確認された系統の中には、マルウェアが稼働中に生成AIを呼び出してコードを変化させる「Just-in-Time（実行時）」手法が含まれている。
以下は、レポート内で説明された「AI機能を取り入れた（またはAIで新機能を得た）」主要マルウェア系統の一覧である。</p>
<h3>FRUITSHELL（PowerShell製リバースシェル）</h3>
<p>目的：C2（コマンド＆コントロール）接続による遠隔操作で任意コマンドを実行。
AIの使われ方：コード内にハードコードされたプロンプトを含み、LLM搭載のセキュリティ製品の解析回避を狙う。
ステータス：運用で観測（Observed in operations）。</p>
<h3>PROMPTFLUX（VBScript製ドロッパー）</h3>
<p>目的：埋め込みのデコイインストーラで活動を隠蔽しつつ、自己再生成（regeneration）によって持続化・難読化を図る。
AIの使われ方：Google Gemini APIを実行時に呼び出し、LLMに自身のVBScriptソースを書き換えさせて新たな難読化版を生成・保存する（「Thinking Robot」モジュールによる定期的なコード再生成など）。「Just-in-Time」自己修正を可能にする設計として説明されている。
ステータス：実験段階（Experimental）。GTIGは関連資産の無効化措置を実施。目的：埋め込みのデコイインストーラで活動を隠蔽しつつ、自己再生成（regeneration）によって持続化・難読化を図る。
AIの使われ方：Google Gemini APIを実行時に呼び出し、LLMに自身のVBScriptソースを書き換えさせて新たな難読化版を生成・保存する（「Thinking Robot」モジュールによる定期的なコード再生成など）。「Just-in-Time」自己修正を可能にする設計として説明されている。
ステータス：ステータス：実験段階（Experimental）。GTIGは関連資産の無効化措置を実施。
GTIGは、当該コード群は開発・テスト段階であり、現時点で端末やネットワークを侵害できる能力は示されていないと注記している。</p>
<h3>PROMPTLOCK（Go製ランサムウェア、PoC）</h3>
<p>目的：ファイル探索・窃取およびファイル暗号化を行うクロスプラットフォーム型のランサムウェア（概念実証）。
AIの使われ方：実行時にLLMを利用してLuaスクリプトを動的生成・実行する機能を持つとされる。
ステータス：実験段階（Experimental）。</p>
<h3>PROMPTSTEAL（Python製データマイナー）</h3>
<p>目的：システム情報や特定フォルダ内の文書を収集して外部へ送信するデータ窃取。
AIの使われ方：PyInstaller化されたスクリプトがHugging Face経由で Qwen2.5-Coder-32B-Instruct を呼び、1行コマンドを生成させて実行する方式を取る。ハードコーディングではなくLLMにコマンド生成を委ねる点が特徴で、これが「実運用でのLLM呼び出し」としてGTIGの初観測例に含まれる。
ステータス：運用で観測（Observed in operations）。同系統はロシア政府支援とされるAPT28（FROZENLAKE）がウクライナで使用したと報告され、LLMを“運用中に照会”した初観測例として位置づけられている。</p>
<h3>QUIETVAULT（JavaScript製クレデンシャルスティーラー）</h3>
<p>目的：GitHubやNPMのトークンなど資格情報を窃取し、取得した資格情報やファイルを公開リポジトリ経由で外部に流出させる等の挙動。
AIの使われ方：オンホストでインストールされたAI CLIツールやプロンプトを活用して、システム内の潜在的な秘密ファイルを探索・抽出する機能を併用している。
ステータス：運用で観測（Observed in operations）。</p>
<p>GTIGは今回の発見を「2025年における新たな段階（実行時にAIを呼び出すマルウェア）」の初期指標と位置づけ、いくつかの実装は実験・開発段階にあるものの、運用段階で観測されたケースも存在するとしている。特にPROMPTFLUXについては自己修正機能や「gemini-1.5-flash-latest」指定など、モデル依存性や継続的な更新を意図した設計が確認されている。</p>
<p>Googleはレポート内で、これらの活動に対して関連アセットの無効化やモデル・分類器の強化、SAIF（Secure AI Framework）などを通じた予防措置を実行したことを明示している。また、攻撃者が「CTF参加者」や「研究者」を名乗るなどの社会工学的プロンプトで安全性ガードレールを回避しようとする手口も確認されているため、モデル側とセキュリティ側双方の対応を継続していると述べている。</p>
<h2>社会工学的な“ガードレール回避”</h2>
<p>報告書では、攻撃者がAIモデルの安全対策（ガードレール）を回避するため、巧妙なプロンプトを使用する事例も確認された。
「CTF（ハッキングコンテスト）への参加」や「大学での研究目的」など、もっともらしい理由を提示して制限をすり抜け、悪用可能なコードを生成させるケースが報告されている。</p>
<h2>犯罪向けAIツールの地下市場化</h2>
<p>GTIGは、マルウェア生成・フィッシング支援・脆弱性探索などに特化した“犯罪向けAIツール”が地下マーケットで急速に拡大していると指摘。これらのツールは月額サブスクリプション形式で販売され、AIを使った攻撃が商業化・自動化されつつあると警告している。</p>
<h2>国家支援型グループもAIを活用</h2>
<p>Googleは、中国・イラン・北朝鮮など国家支援型とみられる攻撃グループが、偵察からC2構築、データ窃取に至るまでAIを併用していると説明。特に暗号資産を狙うキャンペーンでの利用が確認されている。</p>
<h2>Googleの対応と今後</h2>
<p>同社は、報告書で検出された不正資産の無効化やモデルの再学習を実施したほか、AIセーフティフレームワーク（SAIF）やレッドチーム評価を継続して行っていると説明。
GTIGは「AIを悪用した攻撃は今後さらに多様化・自動化する可能性がある」とし、産業界全体での検知・対策共有の重要性を強調している。</p>
]]></description>
      <pubDate>Wed, 12 Nov 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>note、NAVERから20億円調達し資本業務提携──生成AI・プラットフォーム連携・IP共同開発・戦略投資の4領域で協業開始</title>
      <link>https://ledge.ai/articles/note_naver_alliance_2025</link>
      <description><![CDATA[<p>note株式会社は2025年11月5日、韓国のNAVER Corporationから総額20億円の出資を受け、資本業務提携を締結したと<a href="https://www.watch.impress.co.jp/docs/news/2060848.html">発表</a>した。両社はAI時代における創作と流通の新しいエコシステムを共に構築し、クリエイターが世界中のファンとつながりながら持続的に活動できる仕組みを目指す。</p>
<p>協業の柱は、①生成AI技術領域での連携、②両社プラットフォーム間の連携、③IP・コンテンツの共同開発・展開、④戦略的投資の4分野。初動として、note子会社のTales &amp; Co.とNAVER関連会社のLINEマンガが連携し、新しい作家や作品の発掘・育成を進めるプロジェクトを2026年初旬に始動する。</p>
<h2>4領域の協業内容</h2>
<p><strong>① 生成AI技術領域での連携</strong> ：クリエイティブ分野におけるAI活用を共同で推進し、両社が持つAI技術を組み合わせて創作支援やデジタルコンテンツの品質向上を目指す。
<strong>② 両社プラットフォーム間の連携</strong> ：運営するプラットフォーム間でコンテンツやIPの相互利用、クロス配信、グローバル展開の機会を検討。UGC（ユーザー生成コンテンツ）領域の拡大と、クリエイターとファンの関係を深める仕組みづくりを進める。
<strong>③ IP・コンテンツの共同開発・展開</strong> ：NAVERが運営する「WEBTOON」などの制作基盤と、noteおよびTales &amp; Co.のネットワークを組み合わせ、世界で通用する新しいIPを創出する。
<strong>④ 戦略的投資</strong> ：両社のノウハウやネットワークを活かし、事業領域の拡大と国際競争力の強化を目的とした投資機会を共同で模索する。</p>
<h2>今後の展望</h2>
<p>日本政府は2025年6月に「エンタメ・クリエイティブ産業戦略」を発表し、2033年までにコンテンツ産業の海外売上高を20兆円規模に拡大する目標を掲げた。一方、韓国では文化関連予算やK-コンテンツ戦略基金などを通じて、制作から海外展開までを支援する体制が整備されている。NAVER傘下の「WEBTOON」は150カ国以上で展開され、作品のドラマ化や映画化も進む。</p>
<p>noteはこうした環境を踏まえ、NAVERとの提携を通じてAIやテクノロジーを活用したグローバルな創作支援の基盤を強化し、アジア発の創作エコシステムを世界へ広げる考えだ。</p>
]]></description>
      <pubDate>Tue, 11 Nov 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google、AI推論時代の第7世代TPU「Ironwood」提供を数週間以内に開始──v5p比10倍・v6e比4倍超の性能を公式発表</title>
      <link>https://ledge.ai/articles/google_tpu_ironwood_general_availability</link>
      <description><![CDATA[<p>Googleは2025年11月7日、AIアクセラレータの第7世代テンソル処理ユニット（TPU）「Ironwood（アイアンウッド）」を、今後数週間以内に一般提供（GA）開始すると<a href="https://cloud.google.com/blog/ja/products/compute/ironwood-tpus-and-new-axion-based-vms-for-your-ai-workloads/">発表</a>した。</p>
<p>Ironwoodは2025年4月に初めて<a href="https://ledge.ai/articles/google_tpu_ironwood_agent2agent">公表</a>された第7世代のTPUで、「推論（inference）の時代のために設計された最初のTPU」と位置づけられている。GoogleはIronwoodを「これまでで最もパワフルかつエネルギー効率に優れたTPU」と説明している。</p>
<h2>第5世代比10倍、第6世代比4倍超の性能</h2>
<p>Ironwoodは前世代から大幅な性能向上を実現している。
第5世代TPU「v5p」と比べて最大10倍のピーク性能を発揮し、第6世代TPU（開発コード「Trillium」／v6e）と比べてもチップ当たり（per chip）で学習・推論の双方において4倍超の性能を持つという。同社は、これによりAIモデルのトレーニングと推論の双方で効率を高められると説明している。</p>
<p>@<a href="https://www.youtube.com/watch?v=aQxcomQDHcw">YouTube</a></p>
<h2>「推論の時代」に最適化された新設計</h2>
<p>Googleは、生成AIの利用拡大により「推論の時代（age of inference）」が到来していると強調する。
モデルの構築に加え、エージェントや会話型AIなど、大規模な推論処理をいかに効率的に行うかが課題となっており、Ironwoodはそのための基盤として開発された。新アーキテクチャの採用により、低レイテンシと高スループットを両立し、これまでのTPUよりも高いエネルギー効率を実現したという。</p>
<h2>Google Cloudで数週間以内に提供開始</h2>
<p>Ironwoodは、Google Cloud上で一般提供される予定だ。
ブログでは「今後数週間以内に提供を開始する」と明言しており、AI推論処理を中心とするクラウドユーザーが早期に利用できる見込みとなっている。
同発表では、Armベースの新仮想マシン「Axion」VM群（N4A／C4A）も合わせて紹介され、TPUと連携することでクラウド全体の効率化を目指す構想が示された。</p>
<p>Ironwoodは、生成AI、検索、会話型エージェント、RAG（Retrieval-Augmented Generation）など、膨大な推論を伴うワークロードに最適化されている。Googleは、同チップによりAIの開発・提供コストを抑制し、より高速で省エネなAI運用を可能にするとしている。</p>
]]></description>
      <pubDate>Tue, 11 Nov 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Microsoft AI、“ヒューマニスト・スーパーインテリジェンス（HSI）”へ　MAIスーパーインテリジェンスチームを新設</title>
      <link>https://ledge.ai/articles/microsoft_ai_humanist_superintelligence_team_launch</link>
      <description><![CDATA[<p>MicrosoftのAI開発研究所「Microsoft AI（MAI）」は2025年11月6日（米国時間）、「人間中心のヒューマニスト・スーパーインテリジェンス（Humanist Superintelligence：HSI）」の実現に向け、「MAI Superintelligence Team」の新設を<a href="https://microsoft.ai/news/towards-humanist-superintelligence/">発表</a>した。MAIのCEOであるMustafa Suleyman氏が、同社公式ブログ「Towards Humanist Superintelligence」で明らかにした。</p>
<h2>「制御可能な超知能」を目指す</h2>
<p>Suleyman氏はブログで、「MAI Superintelligence Team」は**「世界で最も優れたAI研究・構築の場」を目指すと説明。開発方針については、「無制限の汎用超知能ではなく、人間の目的に従い制御可能な超知能を構築する」と述べた。
HSIの定義として、同氏は「人間中心であることを非交渉的前提とする」と強調。超知能は人間の価値や意思決定を補完するものであり、“まず害を避け、そのうえで加速する”**という原則に立つという。</p>
<h2>3つの重点領域：AIコンパニオン、医療、クリーンエネルギー</h2>
<p>MAIはHSIの実装に向け、まず3つの領域に注力する。</p>
<ol>
<li>AIコンパニオンでは、学習支援やメンタルケア、生産性向上などを通じて「誰もが手頃に持てる知的パートナー」を目指す。教育分野では個別最適化された学習支援を想定している。</li>
<li>医療スーパーインテリジェンスでは、専門医水準の診断能力を備えたAIを開発。ブログでは、米医学誌『NEJM』の症例チャレンジにおいてMAIの診断モデル「MAI-DxO」が85％の正答率を達成した事例が紹介されている。</li>
<li>クリーンエネルギー分野では、再生可能エネルギーや蓄電技術の革新を2040年までに実現することを視野に、AIを活用した新素材探索やグリッド最適化、核融合研究への応用を掲げている。</li>
</ol>
<h2>永続的な整合と制御を重視</h2>
<p>Suleyman氏は、HSIの安全性確保について「開発は一度きりではなく、永続的な制御と整合が必要だ」と述べ、政府・企業・学術界の協調を呼びかけた。また、「社会的ルールと監督のもとで、人間が常に主導権を持つ」ことを明示し、AI開発における倫理的責任を強調した。</p>
<h2>今後の展望</h2>
<p>MAIは今後、HSIの応用領域を段階的に拡大するとともに、研究者・エンジニアの採用を進める。Suleyman氏は「このプロジェクトは、数十億人の生活に影響を与えるAIの礎になる」とし、Microsoftの既存AI製品群との連携にも意欲を示している。</p>
]]></description>
      <pubDate>Tue, 11 Nov 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>1X、家庭用ヒューマノイド「NEO」を正式公開──月額499ドル（約7万5,000円）のサブスク開始で“家事お手伝いロボ”が現実に</title>
      <link>https://ledge.ai/articles/1x_neo_home_robot_subscription_launch</link>
      <description><![CDATA[<p>米1X Technologiesは2025年10月28日（米国時間）、家庭向けヒューマノイドロボット「NEO」を<a href="https://www.1x.tech/neo">公開</a>し、プレオーダーを開始した。提供形態は月額499ドル（約7万5,000円）のサブスクリプションまたは一括購入（2万ドル）。家庭内の掃除や片付けなど、日常的な家事を支援する“お手伝いロボット”として設計されている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x1neo_6_ce91e1cf1e/x1neo_6_ce91e1cf1e.jpg" alt="x1neo-6.jpg" /></p>
<p>NEOは、1Xが掲げる「人の生活を支える安全なヒューマノイド」構想に基づき開発された。腱（tendon）駆動による柔らかく静かな動作を特徴とし、人と同じ空間で安全に動作できるよう設計されている。公式サイトでは「単調で時間のかかる家事を肩代わりし、人の時間を取り戻す」とコンセプトを掲げる。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x1neo_5_7ae0e73c6c/x1neo_5_7ae0e73c6c.jpg" alt="x1neo-5.jpg" /></p>
<p>ユーザーはスマートフォンアプリや音声を通じてタスクを指示でき、NEOは家庭内の環境を学習しながら動作を最適化する。自己充電機能を備えるほか、遠隔からのモニタリングやサポートも可能。1XはNEOを単なるロボットではなく「温かみと個性をもった家庭のパートナー」と位置付けている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x1neo_4_dd13229413/x1neo_4_dd13229413.jpg" alt="x1neo-4.jpg" /></p>
<p>主な仕様は、バッテリー駆動時間約4時間（急速充電対応）、静音性は最大22dB。NVIDIA Jetson Thorをベースとした「1X Cortex」コンピューティングシステムを搭載し、360度集音マイクとステレオスピーカーを内蔵する。</p>
<h2>サブスク形式で家庭導入のハードルを下げる</h2>
<p>提供形態は月額499ドル（約7万5,000円）のサブスクリプションまたは一括購入（2万ドル）。NEOは注文ページからプレオーダー可能で、出荷は2026年を予定している。</p>
<p>1Xは公式ページ上で、NEOを「consumer-ready」と表現。2024年の試作版「NEO Beta」発表を経て、今回初めて一般消費者に向けたモデルとして公開された。</p>
<p>@<a href="https://www.youtube.com/watch?v=LTYMWadOW7c">YouTube</a></p>
<p>1X Technologiesは、家庭や産業向けのヒューマノイドロボットを開発する企業で、OpenAIが出資するスタートアップの一つでもある。これまでノルウェーを拠点としていたが、2025年には米カリフォルニア州サンフランシスコに本社を移転。研究開発と製造体制の両面で国際展開を進めている。サブスクリプション形式による提供で家庭でも導入しやすい価格体系を整えたNEOは、ヒューマノイドが日常生活に溶け込む時代の到来を感じさせる存在だ。</p>
]]></description>
      <pubDate>Mon, 10 Nov 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Canva、「Creative OS」を発表──デザインAIとVisual Suiteを統合した次世代ビジュアル作業環境</title>
      <link>https://ledge.ai/articles/canva_creative_operating_system_release</link>
      <description><![CDATA[<p>Canvaは2025年10月29日（現地時間）、同社史上最大規模のプロダクト刷新として「Creative Operating System（Creative OS）」を<a href="https://www.canva.com/ja_jp/newsroom/news/creative-operating-system/">発表</a>した。</p>
<p>この新しいプラットフォームは、再設計されたVisual Suiteと、デザイン理解に特化したAIモデル「Canva Design Model」、さらにブランド運用・配信基盤「Canva Grow」などを統合した“次世代のビジュアル作業環境”として位置づけられている。</p>
<h2>Visual Suite：制作・共有・公開の一体化</h2>
<p>新しいVisual Suiteでは、動画、フォーム、データ、メールなど、これまで分離していたクリエイティブ作業を統合。「Video 2.0」では、タイムライン編集、トリミング、レイヤー同期などの操作が直感的になり、AI機能「Magic Video」によりプロ仕様の動画を容易に生成できる。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/DROP_25009_Newsroom_PR_New_Timeline_ja_JP_b35e716c1f/DROP_25009_Newsroom_PR_New_Timeline_ja_JP_b35e716c1f.jpg" alt="DROP25009_NewsroomPR_NewTimeline_ja-JP.jpg" /></p>
<p>「Canva Forms」はドラッグ＆ドロップでフォームを作成でき、回答結果を「Canva Sheets」に自動集約。「Canva Websites」など他の製品と連携して活用できる。さらに「Canva Code」と「Sheets」を接続することで、データ駆動型のインタラクティブな体験をエディタ内で構築可能となった。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/DROP_25009_Newsroom_PR_Forms_Still_ja_JP_a2b2ed1c7d/DROP_25009_Newsroom_PR_Forms_Still_ja_JP_a2b2ed1c7d.jpg" alt="DROP25009_NewsroomPR_FormsStill_ja-JP.jpg" /></p>
<p>加えて「Canva Email Design」では、テンプレートやAI支援によりブランドメールを作成し、デスクトップ・モバイル両方でプレビュー、HTMLエクスポートやテスト送信が行える。</p>
<h2>デザイン特化AI「Canva Design Model」</h2>
<p>Canvaは同発表で、世界初の“デザインAIモデル”として「Canva Design Model」を公開した。このモデルは、構造・レイヤー・階層・ブランディング・視覚ロジックといった「デザインの複雑性」を理解し、編集可能な形式のコンテンツを生成できる。
ChatGPTやClaudeなど外部プラットフォームでも利用でき、GoogleのGemini向けは「coming soon」としている。</p>
<p>また、「AI Powered Designs」や「AI Powered Elements」では、テンプレート、写真、動画、アイコン、3D要素、カスタムコードなどを生成可能。スタイルを自動的に整える「Style Match」や背景生成「Magic Background」も搭載された。「Ask @Canva」を用いれば、コメント欄で“@Canva”を呼び出すことで、AIが即座にデザイン提案を行う。</p>
<p>さらに「Canva AI ホーム」では、ガイド付きのプレゼンテーション作成や、企業向けのチーム文脈（Team Context）・外部ストレージ連携などを利用できる。</p>
<h2>Canva GrowとBrand System：ブランド運用を閉ループ化</h2>
<p>「Canva Grow」は、コンテンツ制作から配信、分析までを一体化したマーケティング支援機能。ユーザーのWebサイトをスキャンしてブランドトーンを学習し、複数の広告バリエーションを自動生成する。
Meta広告アカウントと連携すれば、配信データに基づいたリアルタイム分析と最適化提案を行う。
また、新しい「Brand System」では、ロゴ、テンプレート、カラーパレット、ガイドラインを一元管理し、エディタ上にリアルタイムで反映。
ブランドキットを接続すると、AIが初期生成段階からブランド整合性の取れたデザインを提示する。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/DROP_25009_Newsroom_PR_Canva_Grow_Publish_ja_JP_3ec52cb61b/DROP_25009_Newsroom_PR_Canva_Grow_Publish_ja_JP_3ec52cb61b.jpg" alt="DROP25009_NewsroomPR_CanvaGrowPublish_ja-JP.jpg" /></p>
<h2>「All-new Affinity」も無料化</h2>
<p>同発表では、写真編集、ベクター描画、ページレイアウトを統合したプロ向けスイート「All-new Affinity」も合わせて紹介された。
これまで有料だった同ソフトを「すべての人に、永続的に無料で提供する」とし、Canva AI Studioとの連携も実現している。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/DROP_25009_Newsroom_PR_Brand_Kit_1_ja_JP_c1d6115d80/DROP_25009_Newsroom_PR_Brand_Kit_1_ja_JP_c1d6115d80.jpg" alt="DROP25009_NewsroomPR_BrandKit-1_ja-JP.jpg" /></p>
<p>Canvaは今回の刷新を、「創造性を誰もが扱える“オペレーティングシステム”として再定義するもの」と説明。
デザイン・AI・ブランド運用をシームレスにつなぐことで、個人から企業まで幅広いユーザーが一貫したワークフローで制作・配信・分析を行える環境を提供するとしている。</p>
]]></description>
      <pubDate>Mon, 10 Nov 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ChatGPTの“当たり障りないフィルター”を外すと、応答が一段と鋭くなった──米国で話題の「辛口プロンプト」現象</title>
      <link>https://ledge.ai/articles/chatgpt_ii_hito_filter_prompt_trend</link>
      <description><![CDATA[<p>ChatGPTの「当たり障りのない」応答に物足りなさを感じた海外ユーザーが、あえて“当たり障りないフィルター”を外すプロンプトを公開し、話題を集めている。Redditで拡散したこの手法は、ChatGPTのトーンを「共感的な聞き役」から「論理的で辛口な批評家」へと変えるもので、SNSでは「回答の質が上がった」との声も相次いだ。</p>
<h2>Reddit発の「辛口プロンプト」が反響呼ぶ</h2>
<p>発端となったのは、Redditユーザー Wasabi_Open 氏が投稿した「I made ChatGPT stop being nice and it’s the best thing I’ve ever done（ChatGPTに“いい人”をやめさせたら、最高の結果になった）」という<a href="https://www.reddit.com/r/PromptEngineering/comments/1okppqe/i_made_chatgpt_stop_being_nice_and_its_the_best/">スレッド</a>だ。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/I_made_Chat_GPT_stop_being_nice_375797ac64/I_made_Chat_GPT_stop_being_nice_375797ac64.jpg" alt="I made ChatGPT stop being nice.jpg" /></p>
<p>同氏はプロンプトの中で、ChatGPTに対し「私の意見を褒めたり慰めたりせず、誤りがあれば明確に指摘してほしい」「論理の矛盾を批判的に分析してほしい」と指示。これにより、ChatGPTが従来よりも率直で的確なフィードバックを返すようになったという。
この投稿は数千件のいいねを集め、「まるで冷静なメンターと議論しているようだ」とのコメントも寄せられた。</p>
<h2>SNSで広がった「nice filter」論争</h2>
<p>この現象を11月3日に<a href="https://x.com/markgadala/status/1985032100672618588">紹介</a>したのが、X（旧Twitter）のユーザー Mark Gadala 氏だ。同氏は「“nice filter”を外したらChatGPTの回答が劇的に改善した」と投稿し、多くのフォロワーが同様のプロンプトを試したと報告している。一方で、「フィルターを解除すると性能が上がる」という表現が拡散したことで、「内部制限を外す行為ではないか」との誤解も広がった。実際には、ChatGPTの内部に“nice filter”と呼ばれる設定は存在せず、プロンプトの指示文によって出力トーンが変わるだけだ。</p>
<h2>「当たり障りないフィルター」の正体</h2>
<p>OpenAIの設計方針によれば、ChatGPTは安全性と中立性を重視した“共感的”な初期設定を採用している。ユーザーが感じる「いい人フィルター」とは、この丁寧でポジティブに応答する傾向を指した比喩に過ぎない。つまり、「フィルターを外す」とは内部機能を解除するのではなく、プロンプトによってAIの口調や態度を再設定する行為だといえる。</p>
<h2>“辛口AI”の効用と注意点</h2>
<p>ユーザーの反応はおおむね好意的だ。「率直な批評を受けることで思考が整理された」「甘い同意よりも鋭い反論のほうが学びになる」といった意見が目立つ。一方で、「冷たく感じる」「会話がきつくなる」との声もあり、タスクや気分に応じてトーンを使い分ける重要性が指摘されている。専門家の間では、このようなトーン調整を「AIとの協働スキル」や「プロンプトリテラシー」の一環とみなす動きも広がっている。</p>
]]></description>
      <pubDate>Mon, 10 Nov 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>GitHub、「Agent HQ」を発表──“あらゆるエージェントを、いつでもどこでも”統合管理</title>
      <link>https://ledge.ai/articles/github_agent_hq_announcement_universe2025</link>
      <description><![CDATA[<p>GitHubは2025年10月28日（米国時間）、年次イベント「GitHub Universe 2025」で新ビジョン「Agent HQ」を<a href="https://github.blog/news-insights/company-news/welcome-home-agents/">発表</a>した。これは、複数のAIコーディングエージェントを単一の環境で呼び出し、管理・連携できる統合プラットフォームである。</p>
<p>公式ブログによると、Agent HQは“あらゆるエージェントを、あらゆる開発スタイルで使える”ことを目指した新構想で、Anthropic、OpenAI、Google、Cognition、xAIなど各社のエージェントを有料のGitHub Copilotサブスクリプションの一部として提供するという。提供開始は今後数カ月を予定。</p>
<p>@<a href="https://www.youtube.com/watch?v=KniyIrpTDE8&amp;t=5s">Youtube</a></p>
<h2>複数エージェントを一元管理する「Mission Control」</h2>
<p>Agent HQの中核機能として「Mission Control」が導入される。開発者が複数のエージェントをタスクごとに割り当て、進行状況を可視化できる中枢機能で、GitHub、Visual Studio Code、CLIなど複数の開発環境で同一の体験を提供する。</p>
<p>また、組織の管理者は各エージェントのアクセス権限や利用ポリシーを制御できる「コントロールプレーン」を通じて、企業やチーム単位でのガバナンスを強化できる仕組みも備える。</p>
<h2>サードパーティ連携を前提とした“開かれたHQ”</h2>
<p>Agent HQは、特定モデルに依存しない「オープンなエージェント基盤」として設計されている。
GitHubは「開発者は自分の選んだAIエージェントを同じワークフローで活用できる」と述べ、今後数か月で各社のエージェントを順次統合していく予定だ。</p>
<p>この発表は、GitHubが掲げる“開発者が中心にいる未来”という長期ビジョンの延長線上にあり、同社はCopilotを単なる補助ツールから、複数AIが協働する「開発の司令塔」へと進化させようとしている。</p>
]]></description>
      <pubDate>Mon, 10 Nov 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google、広告画像を自動生成する新ツール「Pomelli」を発表──ブランドの“DNA”をAIが理解し、一貫したキャンペーンを提案</title>
      <link>https://ledge.ai/articles/google_pomelli_ai_ad_generation_tool</link>
      <description><![CDATA[<p>Googleは2025年10月28日（米国時間）、広告やSNSキャンペーン向けの画像を自動生成できる新ツール「Pomelli（ポメリ）」を<a href="https://blog.google/technology/google-labs/pomelli/">発表</a>した。Google Labsの実験プロジェクトとして公開されており、ユーザーが自社サイトのURLを入力すると、ブランド特性を分析して“Business DNA”を構築し、それに基づいて画像やコピーなどのアセットを提案する。</p>
<p>@<a href="https://www.youtube.com/watch?v=rsWPISYv6tQ">YouTube</a></p>
<h2>ブランドの「DNA」をAIが理解して広告素材を生成</h2>
<p>Pomelliは、企業サイトに含まれる色・言葉・トーンなどをAIが解析し、ブランドの「Business DNA」としてまとめる。この情報をもとに、SNS投稿用の画像、広告用コピー、キャンペーン案などを生成する仕組みだ。DeepMindとの協力のもと開発されたとされ、Googleは「ブランドの一貫性を保ちながら、より迅速にマーケティング素材を作成できる」としている。</p>
<p><strong>Pomelliは企業サイトを解析し、ブランドカラー・フォント・イメージを自動抽出して「Business DNA」を生成する</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Pomelli_Keyword_Blog_In_line_1_width_1000_format_webp_e223dd6d39/Pomelli_Keyword_Blog_In_line_1_width_1000_format_webp_e223dd6d39.webp" alt="Pomelli_Keyword_Blog_In-line_-_1.width-1000.format-webp.webp" /></p>
<h2>中小企業でも“オンブランド”の広告を短時間で</h2>
<p>Pomelliの想定ユーザーは、中小規模の企業（SMB）や個人事業主だ。デザイナーやマーケターが限られた環境でも、ブランドトーンを保った高品質な素材を数分で生成できる。SNSごとのフォーマットや文体に応じた最適化にも対応しており、季節キャンペーンやセールなどの展開を容易にする。</p>
<p><strong>解析したBusiness DNAをもとに、AIが複数のキャンペーン案を提示する</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Pomelli_Keyword_Blog_In_line_2_width_1000_format_webp_9469a56069/Pomelli_Keyword_Blog_In_line_2_width_1000_format_webp_9469a56069.webp" alt="Pomelli_Keyword_Blog_In-line_-_2.width-1000.format-webp.webp" /></p>
<h2>生成された広告をその場で編集・修正可能</h2>
<p>生成された画像やコピーは、フォント・色・キャッチコピーなどをGUI上で細かく調整できる。Googleはこれを「人間の創造力を補助する共同作業ツール」と位置付けており、単なる自動生成ではなく、人の手による最終調整を想定している。</p>
<p><strong>生成された広告素材は、色やフォント、コピーをその場で編集できる</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Pomelli_Keyword_Blog_In_line_3_width_1000_format_webp_68aa5eaa7f/Pomelli_Keyword_Blog_In_line_3_width_1000_format_webp_68aa5eaa7f.webp" alt="Pomelli_Keyword_Blog_In-line_-_3.width-1000.format-webp.webp" /></p>
<h2>提供形態と今後の展開</h2>
<p>PomelliはGoogle Labsの実験ツールとして提供されており、現在は限定公開の段階にある。公式サイトでは「Easily generate on-brand content for your business（自社ブランドに沿ったコンテンツを簡単に生成）」と説明されている。</p>
<p>現時点で提供対象は米国、カナダ、オーストラリア、ニュージーランドの英語版ユーザーに限られており、日本国内では未提供。Googleは今後の地域拡大について明らかにしていないが、利用可能国の追加が期待される。</p>
]]></description>
      <pubDate>Mon, 10 Nov 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google、誰でも学べるAI学習サイト「Google Skills」を正式公開──Cloud・DeepMind・教育部門を横断する3000講座を展開</title>
      <link>https://ledge.ai/articles/google_skills_ai_learning_platform_launch</link>
      <description><![CDATA[<p>Googleは2025年10月21日（米国時間）、新しいAI学習プラットフォーム「Google Skills」を<a href="https://blog.google/outreach-initiatives/education/google-skills/">発表</a>した。同サイトでは、Google Cloud、Google DeepMind、Grow with Google、Google for Educationなど、同社の複数部門が提供してきた教育コンテンツを統合。3000種類を超えるAI関連の講座・体験ラボ・認定プログラムを、一元的に学べる学習拠点として開設された。</p>
<h2>AI教育の中核を担う新サイト</h2>
<p>Google公式ブログ「Start learning all things AI on the new Google Skills」によると、Google Skillsは“AI for Everyone（すべての人のためのAI）”をテーマに、誰もがAIスキルを体系的に学べるよう設計されている。初心者、エンジニア、企業リーダーなど幅広い層を対象に、AI、データ分析、クラウド、生成AIなど多様な分野を網羅。各コースはオンデマンド形式で受講でき、学習成果はLinkedInなどの外部プラットフォームで共有できる。提供内容には、Google Cloudの認定資格プログラムやAI Essentials シリーズ、DeepMindのAI倫理教材などが含まれる。</p>
<p>@<a href="https://www.youtube.com/watch?v=Qbix0BOPcgE">YouTube</a></p>
<p>今回の正式公開に先立ち、Google Cloudは10月10日付のブログ「Google Skills: Your new home for Google AI learning and more」で、新プラットフォームの構想を公表していた。当時は正式リリース前で、「AIやクラウドに関する学習リソースを一元化し、近日中に詳細を発表する」としていた。Gemini Code Assist（旧Duet AI for Developers）やQwiklabs（現Cloud Labs）と連携し、AIトレーニングの実践環境を統合する方針も示されていた。</p>
<h2>3000超のコースと実践的ラボを集約</h2>
<p>Google Skillsでは、Googleがこれまで個別に展開してきた学習リソースを一か所に集約。AIモデル開発、クラウド基盤運用、データ可視化、サイバーセキュリティなど、実践重視の3000超のコースとラボを提供する。一部コンテンツは無料で公開され、修了証や認定資格を取得することでキャリア開発にもつなげられる。また、組織向けにはチーム単位での進捗管理や学習成果の可視化機能も用意されている。</p>
<h2>今後の展望──教育機関・企業研修にも拡大へ</h2>
<p>Googleは今後、教育機関や企業研修への展開を進める方針を示しており、AIスキルの標準教育基盤としての活用を目指す。
公式ブログでは、「AI教育へのアクセスを民主化し、誰もがテクノロジーの未来を形づくる機会を得られるようにする」としている。
同社は今後もDeepMindやCloud AIチームの最新教材を追加し、AI人材育成をグローバルに推進する考えだ。</p>
]]></description>
      <pubDate>Mon, 10 Nov 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>LLM「密度化の法則」──“ムーアの法則”を凌ぐ速度で進化、同等性能に必要なサイズは3.5か月ごとに半減</title>
      <link>https://ledge.ai/articles/llm_densing_law_nature_machine_intelligence_2025</link>
      <description><![CDATA[<p>清華大学とOpenBMBの研究チームは、LLMの性能効率を示す「能力密度（capability density）」が、約3.5か月ごとに倍増する傾向を確認した。この速度は半導体分野で知られる“ムーアの法則”を上回るものであり、研究チームはこの現象を**「密度化の法則（Densing law）」** と名付けた。</p>
<p>結果は2025年11月6日付で 学術誌 Nature Machine Intelligence に<a href="https://www.nature.com/articles/s42256-025-01137-0">掲載</a>されている。</p>
<h2>モデル効率の“時間的スケーリング”</h2>
<p>論文によると、能力密度は「パラメータ単位あたりの能力」として定義され、参照モデルのスケーリング曲線から推定した有効パラメータ数を用いて算出する。</p>
<p>51種類のオープンソースLLM（Llama、Mistral、Gemma、DeepSeek など）を対象に、MMLU／BBH／MATH／HumanEval／MBPPの5ベンチで評価した結果、最大能力密度は時間とともに指数関数的に増加。回帰の結果、成長係数 A≈0.007、決定係数 R²≈0.93が得られ、倍増周期は ln(2)/A ≈ 約3.5か月と算出された（図参照）</p>
<p><strong>オープンソースベースの LLM の推定機能密度</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/42256_2025_1137_Fig1_HTML_0033cd1204/42256_2025_1137_Fig1_HTML_0033cd1204.webp" alt="42256_2025_1137_Fig1_HTML.webp" /></p>
<p>さらに、データ汚染を取り除いたMMLU-CFによる検証でも、A≈0.0066、R²≈0.95と近い結果を示し、データ依存に偏らない傾向であることが裏づけられた（図参照）。</p>
<p><strong>MMLU-CF による検証（汚染除去ベンチ）</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/The_estimated_capability_density_on_a_contamination_free_dataset_MMLU_C_54b2763b07/The_estimated_capability_density_on_a_contamination_free_dataset_MMLU_C_54b2763b07.jpg" alt="The estimated capability density on a contamination-free dataset MMLU-C.jpg" /></p>
<h2>ChatGPT以降、成長速度が1.5倍に</h2>
<p>研究では、ChatGPT公開（2022年末）を境に密度の成長率が約1.5倍に加速したことも示された。ChatGPT以前はA≈0.0048だったが、その後はA≈0.0073へ上昇している（図参照）。背景として、オープンソースモデルの拡充や効率的学習手法の普及が挙げられている。</p>
<p><strong>密度化の法則に関する追加観察（圧縮比較／前後比較）</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Density_evaluated_using_MMLU_ba863b49af/Density_evaluated_using_MMLU_ba863b49af.jpg" alt="Density evaluated using MMLU.jpg" /></p>
<h2>「半分のパラメータ」で同等性能、コストも急減</h2>
<p>密度化の法則が示唆するのは、同等性能に必要なパラメータ数の指数的減少である。実際、後発の小型モデルが、より大きな先行モデルと同等水準に迫るケースが各種ベンチで観測されている。
またAPI価格の代表例として、GPT-3.5（2022年末）で100万トークンあたり20ドルだったのに対し、Gemini 1.5 Flash（2024年夏）では0.075ドルまで低下しており、推論コストも短い周期で半減している（図参照）。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/API_prices_of_LL_Ms_d82e75a156/API_prices_of_LL_Ms_d82e75a156.jpg" alt="API prices of LLMs.jpg" /></p>
<h2>「Mooreの法則」と掛け合わせた未来</h2>
<p>研究チームは、半導体のMooreの法則（トランジスタ密度の倍増）と密度化の法則を組み合わせると、固定価格のチップ上で実行可能な“有効パラメータ数”は約88日で倍増するとの試算を示す。これにより、エッジデバイスでの高性能推論が加速する可能性が高い。</p>
<h2>圧縮は万能ではない</h2>
<p>量子化や蒸留などの圧縮は常に密度向上を保証しない。研究では、Gemma-2-9Bが例外的に密度改善を示した一方、十分な再訓練を伴わない圧縮は密度低下につながるケースも確認された（図参照）。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Comparison_between_compressed_models_and_their_larger_counterparts_a7b553377f/Comparison_between_compressed_models_and_their_larger_counterparts_a7b553377f.jpg" alt="Comparison between compressed models and their larger counterparts.jpg" /></p>
<h2>「密度最適訓練」への転換</h2>
<p>著者らは、今後は単なる巨大化ではなく「密度最適訓練（density-optimal training）」へ軸足を移すべきだと提言する。大規模モデルから小規模モデルへの知識移転と、再びそれが大規模側の効率化に寄与するという相互進化が、持続可能な開発の鍵になるという。
同時に、能力密度の指数成長には理論的上限があるため、将来的には量子計算や神経形態計算など新たな計算パラダイムの検討も必要になると見解を示す。</p>
]]></description>
      <pubDate>Mon, 10 Nov 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>アルトマン氏「エロティックばかり注目されたけど」──ChatGPT、成人ユーザーの自由拡大へ</title>
      <link>https://ledge.ai/articles/openai_chatgpt_adult_mode_update_oct2025</link>
      <description><![CDATA[<p>OpenAIのサム・アルトマンCEOは10月14日（現地時間）、X（旧Twitter）上で、ChatGPTの安全制限を一部緩和し、成人認証済みユーザーに対してエロティックな会話を許可する方針を<a href="https://x.com/sama/status/1978129344598827128">発表</a>
した。</p>
<p>投稿は瞬く間に注目を集め、「エロティック解禁」が大きな話題となったが、アルトマン氏は翌日に「その部分ばかり注目されてしまったが」と<a href="https://x.com/sama/status/1978539332215681076">補足</a>し、実際には“より人間らしいAI体験”を実現するための包括的な方針変更であることを強調した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/gpt5_erotica_7cea863241/gpt5_erotica_7cea863241.jpg" alt="gpt5 erotica.jpg" /></p>
<h2>安全を優先してきたChatGPTの制限</h2>
<p>ChatGPTはこれまで、性的表現や親密な会話を含むコンテンツを厳しく制限してきた。
アルトマン氏は「メンタルヘルス問題に慎重を期すためだった」と説明し、精神的に不安定なユーザーに配慮した措置であったと振り返った。
「深刻な危機状態にあるユーザーは別扱いとし、他者に害を与える行為は依然として許可しない」と述べ、ポリシーの根幹は維持されるとしたうえで、「リスクのない成人ユーザーにはより多くの自由を与える」と明言した。</p>
<h2>“4oらしさ”を再導入──人間的なAI体験へ</h2>
<p>アルトマン氏は同日、「数週間以内に“4oで好まれた振る舞い”に近い人格（パーソナリティ）を選べる新バージョンのChatGPTを提供する」と投稿した。
GPT-4oは会話の自然さや表情豊かな応答で人気を集めたモデルであり、今後はユーザーが望む場合に、フレンドリーな口調や絵文字を多用した“人間らしい”対話スタイルを選べるようになる。
アルトマン氏は「これは利用時間を増やすためではなく、ユーザーが自分の望む形でAIと関わる自由を得るための設計だ」と述べている。</p>
<h2>「成人は大人として扱う」──12月に年齢認証を本格導入</h2>
<p>アルトマン氏は、12月に年齢認証を本格導入し、認証済み成人ユーザーに対してはエロティック会話なども許可する方針を示した。
一方で、ティーンエイジャーに対しては「安全をプライバシーや自由より優先する」と述べ、メンタルヘルス関連ポリシーは緩めないと強調している。</p>
<p>アルトマン氏は、「社会がR指定映画で境界を設けるように、AIにも適切な年齢境界を設けたい」と例え、「我々は選挙で選ばれた道徳警察ではない」と付け加えた。</p>
<h2>倫理と自由の境界線</h2>
<p>アルトマン氏の発言は、AIにどこまで人間的な自由を与えるかという議論を再燃させた。</p>
<p>OpenAIは今後、成人向け表現やAIの人格設計に関するガイドラインをさらに明確化するとみられる。今回の方針転換は、「AIをどう設計し、どう育てるか」という人間社会全体のテーマに踏み込む第一歩となりそうだ。</p>
]]></description>
      <pubDate>Mon, 10 Nov 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/11/8 [SAT]AI導入が雇用を直撃　米10月の人員削減15万人超、理由の2位に「AI」──チャレンジャー社調査</title>
      <link>https://ledge.ai/articles/us_job_cuts_october2025_ai_layoffs_report</link>
      <description><![CDATA[<p>米民間の再就職支援会社チャレンジャー・グレイ・アンド・クリスマス（Challenger, Gray &amp; Christmas）は2025年11月6日（米国時間）、2025年10月の米企業による人員削減発表数が15万3074人に達したと<a href="https://www.challengergray.com/blog/october-challenger-report-153074-job-cuts-on-cost-cutting-ai/">発表</a>した。</p>
<p>前年同月（5万5597人）から175％増で、10月としては2003年以来21年ぶりの高水準となった。主な要因は「コスト削減」と「AI（人工知能）」であり、AI関連のレイオフが要因として単月で2位に浮上したという。</p>
<h2>「AI統合」と自動化が再編の波を拡大</h2>
<p>同社によると、10月に企業が発表した削減理由のうち最も多かったのはコスト削減（50,437人）、次いでAI（31,039人）だった。
AI導入を理由とする人員削減は年初来で4万8414人に上り、前年を大幅に上回るペース。報告書では「AI integration（AI統合）」や「automation-driven restructuring（自動化による再編）」が複数業種で主要因として挙げられた。</p>
<p>テクノロジー産業の削減は10月だけで33,281人と、前月（5,639人）から急増。年初来では14万1159人（前年同期比＋17％）となり、AI活用による業務効率化が雇用の再構成を促している。</p>
<h2>倉庫・物流で過去最大の削減──自動化が要因</h2>
<p>倉庫・物流（Warehousing）分野では、10月の削減数が47,878人に達し、業種別で最多となった。前年同月の984人から急増しており、年初来では90,418人（＋378％）。同社は、パンデミック期に拡張した物流網の過剰能力と自動化の進展が要因と分析している。</p>
<h2>「破壊的技術が再び雇用地図を変える」</h2>
<p>チャレンジャー社は、「2003年当時も、携帯電話という破壊的技術が雇用地図を変えた。今、同じことがAIによって起きている」とコメント。10月としての削減数は2003年以来の最多であり、第4四半期単月でも2008年以来の高水準と指摘した。</p>
<p>一方で、SNS時代に入って以降は「ホリデーシーズン前のレイオフを避ける傾向」が強まっていたが、2025年10月は約450件の個別計画が報告され、例年を大きく上回ったという。</p>
<h2>採用計画は11年ぶりの低水準</h2>
<p>10月末までの累計では、米雇用主による人員削減数は1,099,500人（前年同期比＋65％）。一方で、新規採用計画は488,077人と前年同期比35％減。同社によると、月間平均は48,808人で、2011年以来の低水準に落ち込んでいる。季節雇用も37万2520人にとどまり、同社が追跡を開始した2012年以降で最少だった。</p>
<h2>AI時代の雇用構造変化</h2>
<p>AI導入は生産性向上とコスト削減をもたらす一方、雇用の再編・削減要因として急速に存在感を高めている。
チャレンジャー社は「AI adoption（AIの採用）や自動化による効率化が複数業種でレイオフを誘発している」とし、雇用市場全体が「生成AI時代の再構築フェーズ」に入っていることを示唆している。</p>
]]></description>
      <pubDate>Mon, 10 Nov 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>機密情報を守り抜く生成AI基盤――GPUクラスタで実現する社内LLM活用</title>
      <link>https://ledge.ai/articles/nttpc_interview_gpu_cluster</link>
      <description><![CDATA[<p>すっかり社会に浸透した生成AIだが、企業での活用はまだ十分に進んでいるとは言い難い。個人利用は急速に拡大する一方で、機密情報の取り扱いや外部LLMへの依存が大きな障壁となっている。生成AIをビジネスに実装する際は、データ保護とサービス独立性を確保する手段として、社内に閉じたローカル環境での基盤構築・運用は不可欠といえるだろう。</p>
<p>さらに、高いパフォーマンスが求められる生成AI基盤の選択肢として、複数のGPUサーバーを協調動作させる「GPUクラスタ」が挙げられる。GPUクラスタはいかにして企業の生産性向上に貢献できるのか、また、セキュリティと独立性を確保したGPUクラスタを導入する場合、企業はどのような点に気を配る必要があるのだろうか。</p>
<p>『GPUクラスタ×生成AI　―13のポイントで実現する次世代基盤とビジュアライゼーション実践ガイド』の著者であり、NTTPCコミュニケーションズ株式会社（以下、NTTPC）でGPUエンジニアとして活躍する大野泰弘氏に、生成AI基盤構築のポイントについて話を伺った。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/img_release_book_1350a15501/img_release_book_1350a15501.jpg" alt="img_release_book.jpg" /></p>
<p>:::button
<a href="https://www.nttpc.co.jp/gpu/?utm_campaign=GPU&amp;utm_source=ledge&amp;utm_medium=ledge.co.jp&amp;utm_term=ledge.co.jp&amp;utm_content=ledge_202511"><strong>NTTPCのGPUクラスタ導入・構築サービスの詳細はこちら</strong></a>{target=_blank}
:::</p>
<p>※インタビューは2025年10月9日に行われた。本記事はインタビュー時の情報に基づく。</p>
<h2>企業が生成AIを活用する際に重要なのは「データの扱い」と「独立性」</h2>
<p><strong>――日本企業における生成AI活用の現状について、どのようにご覧になっていますか。</strong></p>
<p><strong>大野氏</strong>
個人でのChatGPT等クラウド型LLMの利用率はかなり高まっていると感じます。ただ、企業として使う場合は話が違ってきます。多様なビジネスニーズに対応できるよう、企業内情報や顧客情報を学習させたカスタマイズLLMを利用している企業は未だ多くないのが現状です。</p>
<p><strong>――なぜ個人では積極的に使用しているのに、企業では利用が進んでいないのでしょうか。</strong></p>
<p><strong>大野氏</strong>
主に企業のセキュリティルールが原因です。非公開情報をパブリックにアップロードすることへの懸念から、多くの企業が社内導入に踏み切れずにいるのではないかと見ています。</p>
<p><strong>――非公開情報の取り扱いに関する懸念点をもう少し詳しく教えてください。</strong></p>
<p><strong>大野氏</strong>
一般的な情報であれば、外部APIやクラウド型LLMサービスを使っても問題ないでしょう。しかし、企業の重要な情報をクラウド型LLMにアップロードする場合は注意が必要です。</p>
<p>クラウド型LLMサービスであっても、エンタープライズ契約では「アップしたデータを学習に使わない」と明記されているケースも多くありますが、それが本当に守られているか確認する手段はありません。最終的にはAIサービスを提供している企業倫理に委ねるしかないのです。そのような状況では、たとえエンタープライズ契約であっても企業が慎重になるのは当然です。もし、重要情報をAIで扱うなら、外部に情報が出ないローカルなオンプレミス環境を構築するのがもっとも安全です。</p>
<p>もう一つの重要なポイントは「独立性」です。たとえば、GPT-4oがGPT-5にアップデートされ、出力の性質が変わってしまったケースがありました。企業のワークフローに生成AIを組み込んでいると、こうしたAPI挙動の変化がビジネスに大きな影響を与えます。さらにAPIに障害が発生すれば業務が停止してしまいます。</p>
<p>企業によっては、SlackやGitHubが落ちると仕事ができなくなってしまいますよね。それと同じことがAIにも起こり得ます。こうしたリスクを防ぐには、独立性を保ち、自社でAPIを運用することが重要です。</p>
<p><strong>――将来のリスクに備える意味でもローカル環境が重要になるわけですね。</strong></p>
<p><strong>大野氏</strong>
はい。特に大企業は、社内ナレッジやパーソナルデータなど膨大な機密情報を保有していることが多いでしょうから、ローカル環境を整備すべきだと考えます。また、将来的にAIは石油や鉄鋼と同じく戦略物資に位置づけられる可能性もあります。なぜなら、APIを提供している企業がある国が、ある日突然「自国以外にAPIを使わせない」と言い出すと、業務が完全にストップしてしまうからです。そうした状況への対策として、国としての独立性を維持することも重要になるでしょう。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/npptc_interview_oonosama_2_6978526600/npptc_interview_oonosama_2_6978526600.jpg" alt="npptc_interview_oonosama_2.jpg" /></p>
<h2>複数のGPUサーバーを連携させる「GPUクラスタ」が生成AI活用の鍵を握る</h2>
<p><strong>――NTTPCでは、企業が生成AIを活用する際のインフラとして「GPUクラスタ」を提供しています。あらためてGPUクラスタの概要について教えてください。</strong></p>
<p><strong>大野氏</strong>
複数のGPUサーバーを連携させ、処理を高速化したものがGPUクラスタです。クラスタを構成することで、システムの可用性を上げることができます。万が一システムが停止しても事業が止まらないよう、複数台でクラスタを組むことで安定した基盤を構築するのです。</p>
<p><strong>――GPUクラスタを構築する上で押さえておくべきポイントはありますか。</strong></p>
<p><strong>大野氏</strong>
パラメータ数の大きな生成AIモデルを扱う場合、いかに多くのGPUメモリを利用できるかが応答速度に直結します。一つのサーバー内に搭載できるGPU枚数は多くても8～10GPU程度ですが、数十～数百台のサーバーを連結させて動作させることで、さらに多くのGPUメモリ容量を確保できます。これにより、学習スピードを上げたり、より大きなモデルを作成できるようになります。</p>
<p>その際、サーバー同士を高速で接続する「インターコネクト」が重要になります。インターコネクトが遅いと、どんなに高性能なGPUを使っていても全体の処理速度が落ちてしまうからです。いわば二人三脚のようなもので、遅い方に全体のスピードが引きずられるのです。ですから、GPUだけでなく通信部分にも適切な投資をすることが重要です。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/npptc_interview_gpu_cluster_image_wt_d30256d970/npptc_interview_gpu_cluster_image_wt_d30256d970.png" alt="npptc_interview_gpu_cluster_image_wt.png" /></p>
<p>また、同時リクエスト数に応じてクラスタのサイズを調整することも必要です。業務にAIを組み込むと、100件、200件といった同時リクエストが発生することもあるので、それに合わせてクラスタを拡張しなければなりません。</p>
<p><strong>――インターコネクトについては認識されていない企業が多いのでしょうか。</strong></p>
<p><strong>大野氏</strong>
そうですね。多くの方は「GPUを買ってつなげば動くのでは」と考えがちです。しかし、適切な設定をしなければ、たとえば400Gbpsや800Gbpsといった高速なイーサーネットで接続しても、通信が輻輳して期待したパフォーマンスが出ないことがあります。後から「ここに投資しておけばよかった」と、多くの人が後悔するのがインターコネクトなのです。</p>
<h2>インターコネクトの設計で、ネットワークの速度と安定性が大きく変わってくる</h2>
<p><strong>――インターコネクトについて、もう少し詳しく教えてください。</strong></p>
<p><strong>大野氏</strong>
端的に言えば、インターコネクトはGPU同士をつなぐ高速なネットワークです。なぜインターコネクトが必要なのかというと、GPUで言語学習などのトレーニングを行う場合、AllReduceという計算方式を使います。この仕組みでは、各GPUにタスクを割り当てて、計算結果を一度集めて再分配するというプロセスを繰り返します。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/npptc_interview_gpu_distributed_training_wt_4cc4528ee7/npptc_interview_gpu_distributed_training_wt_4cc4528ee7.png" alt="npptc_interview_gpu_distributed_training_wt.png" /></p>
<p>この「集約」と「再分配」の時間が通信時間となり、その間はGPUの稼働率が落ちてしまうのです。低速のネットワークを使うと、GPUの稼働率が5％～10％程度まで下がってしまい、せっかくのGPUのリソースを活かせません。そのため、インターコネクト設計がGPUクラスタの性能を左右するのです。</p>
<p><strong>――適切に設計されたインターコネクトとそうでないものでは、どの程度の差が出るのでしょうか。</strong></p>
<p><strong>大野氏</strong>
インターコネクトのパフォーマンスチューニングをしない場合は稼働率が80％程度ですが、適切にチューニングすることで90％～95％まで向上させられます。生成AIの進化は非常に速く、少しでも早く学習を終えてトライ＆エラーを繰り返せる環境を整えることが重要です。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/npptc_interview_gpu_utilization_rate_wt_5d320f36bd/npptc_interview_gpu_utilization_rate_wt_5d320f36bd.png" alt="npptc_interview_gpu_utilization_rate_wt.png" /></p>
<p><strong>――長期間かけて何度も繰り返す作業となると、最終的には大きな差につながりそうですね。</strong></p>
<p><strong>大野氏</strong>
はい。それから、速度と同じくらい重要になるのが安定性です。場合によっては1ヶ月間ずっとAIの学習を回し続けるようなこともあるでしょう。その際、途中でエラーが発生すると1か月間の学習がすべてリセットされてしまいます。チェックポイントを設けて定期的に保存は行いますが、仮に1時間分の計算が無駄になるだけでも大きな損失です。</p>
<p><strong>――ネットワークの安定性を確保するためのポイントはありますか？</strong></p>
<p><strong>大野氏</strong>
弊社では、設計段階で光ケーブルの損失を計算したり、トランシーバーに負荷をかけてストレステストを行っています。軽い検証でスタートしてしまうと、本番運用時に問題が発生することがあります。単にベンチマークソフトを回すだけでなく、複数のパラメータで検証し、厳しい条件で試すことが大切です。</p>
<p><strong>――大野さんは国内有数の大規模プロジェクトを率いたご経験を豊富にお持ちです。そのご経験から、GPUクラスタの設計思想についてお聞かせください。</strong></p>
<p><strong>大野氏</strong>
もっとも重要なのは、お客様のユースケースに合わせた設計をすることです。たとえば、学習パターンによっては高いスループットが不要な場合もあるので、過剰な設計にならないようネットワークを組むことが大切になります。お客様もビジネスとしてサーバーに投資されるわけですから、投資した以上の利益を上げないといけません。そのためにも、コストを削減することが我々ベンダーの大切な役割になるのです。</p>
<p>また、ネットワークの設定だけでなく、サーバー内部のLinuxカーネルの設定なども適切に行い、通信のスピードを最適化することが求められます。仮に10台や20台のGPUサーバーでクラスタを構築し、稼働率が10～20％向上すれば、それだけでサーバー1～2台分の効果が得られることになりますよね。そういった形で、お客様の資産を最大限に活用できる形でクラスタを組むことを心がけています。</p>
<p><strong>――そのあたりは、豊富なご経験を持つ大野さんやNTTPCならではですね。</strong></p>
<p><strong>大野氏</strong>
はい。自前で構築したものの、思ったようなパフォーマンスが出ないと感じている方は多いと思います。実はドライバーのファームウェアのバージョンをそろえるだけでも、大きく変わることがあるんですよ。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/npptc_interview_oonosama_3_6ec69c2ec7/npptc_interview_oonosama_3_6ec69c2ec7.jpg" alt="npptc_interview_oonosama_3.jpg" /></p>
<h2>生成AI活用で暗黙知の活用や定型ワークフローの自動化を実現</h2>
<p><strong>――データの学習やファインチューニングなど、AIのライフサイクル全体を管理する「AIファクトリー」が注目を集めています。AIファクトリーの構築は、技術に詳しくない一般企業や中小企業でも可能なのでしょうか。</strong></p>
<p><strong>大野氏</strong>
専門知識を持った人材がいれば自社で構築することも可能でしょう。しかし、専門家を採用したり育成したりするのは簡単ではありません。現実的には、弊社のような専門企業にAIファクトリーの設計から構築、導入までご依頼いただくのが近道です。その方が、お客様も自社のビジネスに集中できます。</p>
<p><strong>――AIファクトリーを導入し、生成AI活用を進めることで、企業はどのようなビジネス課題を解決できるでしょうか。</strong></p>
<p><strong>大野氏</strong>
一つは、社内の暗黙知の活用です。社内にはSlackなどでやりとりしてきた情報が蓄積されているはずです。これらをデータベース化し、AIで参照できるようにすれば、24時間365日質問に回答できるシステムが出来上がります。</p>
<p>他には、メール応答の自動化なども効果的です。たとえば、お客様からの見積もり依頼メールに対して、不足している情報を自動的にAIで問い合わせることができれば、見積もり作成までの時間が短縮でき、お客様への対応が早くなるでしょう。</p>
<p>このように、企業のワークフローの中で定型化している業務をAIに任せることで、他の業務にリソースを集中できるようになるのです。</p>
<p><strong>――自前のAI基盤を持つことが、企業に競争優位性にどうつながると考えますか。</strong></p>
<p><strong>大野氏</strong>
一つは、企業内のワークフローを安定的に提供できることです。熟練者が退職してしまっても、AIが知識を引き継ぐことで業務の連続性が確保できます。少子化が進む中で、特に重要になる要素でしょう。</p>
<p>二つ目は心理的安全性の向上です。AIは質問しても怒らず、いつでも相談に乗ってくれます。といっても、ChatGPTは一般的な知識はあっても、会社特有の情報は教えてくれないのであまり役立ちません。自社のAIなら、会社のことを教えてくれる「やさしい相談相手」が常に存在することになります。</p>
<p>三つ目はセキュリティの担保です。ローカルでAIを運用することで、新入社員などが「この情報をAIに入れても大丈夫か」と迷うことがなくなります。気兼ねなく質問できる環境が整うことで、問題解決のスピードが向上するでしょう。</p>
<h2>GPUクラスタの導入実績と未来のユースケース</h2>
<p><strong>――NTTPCのGPUクラスタを導入した企業からは、どのような反響がありますか。</strong></p>
<p><strong>大野氏</strong>
弊社が構築したクラスタを運用されているお客様からは、「非常に安定している」という評価をいただいています。また、「トラブルが発生した際の対応が非常に早い」というフィードバックもよくいただきます。弊社には約20名のエンジニアがおり、分担してトラブルシューティングを行うことで、迅速な対応が可能になっています。</p>
<p><strong>――今後、GPUクラスタ構築がより求められる業界や領域はどこだとお考えですか。</strong></p>
<p><strong>大野氏</strong>
日本の製造業、特に自動車産業ではGPUクラスタの処理能力が不可欠になると考えています。たとえば、NVIDIA Omniverse™やNVIDIA Isaac Simなどを活用したデジタルツイン空間では、現実では不可能な実験を無限に行うことができます。実際の道路で事故データを収集するのは難しいですが、デジタルツイン上では様々な事故シナリオのシミュレーションが可能です。</p>
<p>また、産業ロボット分野でも、工場のラインをデジタルツイン上で作成し、テストを行った上で実際の環境に適用するといった活用方法が考えられます。無限に実験できる環境を持つことは、企業にとって大きな価値につながるはずです。</p>
<p><strong>――様々な利用シーンがあるのですね。GPUクラスタの領域ではやはりNVIDIAの存在感が強いのでしょうか。</strong></p>
<p><strong>大野氏</strong>
はい。GPUクラスタのベストプラクティスとして、NVIDIAからは「NVIDIA DGX SuperPOD™」が公開されています。これは単なるハードウェアの集合ではなく、GPUコンピューティング、ストレージ、ネットワーキング、ソフトウェア、インフラ管理までまとまったフルスタックプラットフォームです。互換性の取れた構成と標準化されたモジュールにより、短い期間でシステムを立ち上げることができ、運用コスト・リスクを低減できます。弊社はNVIDIAエリートパートナーとして、DGX SuperPODの設計・構築・導入も手掛けています。</p>
<p><strong>――今後、GPUクラスタの導入を検討している企業へアドバイスをお願いします。</strong></p>
<p><strong>大野氏</strong>
まずは、スモールスタートで始めることをおすすめします。弊社には設計から構築までの豊富なノウハウがありますので、ご相談いただければお客様の状況に合わせてご希望に合わせたご提案をさせていただきます。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/npptc_interview_oonosama_4_c8e0c9fb36/npptc_interview_oonosama_4_c8e0c9fb36.jpg" alt="npptc_interview_oonosama_4.jpg" /></p>
<p>:::button
<a href="https://www.nttpc.co.jp/gpu/?utm_campaign=GPU&amp;utm_source=ledge&amp;utm_medium=ledge.co.jp&amp;utm_term=ledge.co.jp&amp;utm_content=ledge_202511"><strong>NTTPCのGPUクラスタ導入・構築サービスの詳細はこちら</strong></a>{target=_blank}
:::</p>
]]></description>
      <pubDate>Mon, 10 Nov 2025 00:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ソフトバンク、AIスタートアップに無償GPU提供を本格化　宮川社長「計算基盤が国力」——決算会見で危機感</title>
      <link>https://ledge.ai/articles/softbank_ai_foundation_gpu_support_2025q2</link>
      <description><![CDATA[<p>ソフトバンクは2025年11月5日、2026年3月期第2四半期の決算説明会を開催し、AI分野に取り組むスタートアップ企業への支援強化を<a href="https://www.softbank.jp/corp/ir/documents/presentations/fy2025/#result-20251105">発表</a>した。社長の宮川潤一氏は「半導体の次に来るのはAIの計算基盤のサイズ。すべてがコンピューティングパワーだ」と述べ、AI計算基盤の整備を国家的課題と位置づけた。</p>
<p>同社は既に、GPU計算資源などを段階的に提供する「AI Foundation for Startups」を10月1日に<a href="https://www.softbank.jp/corp/news/press/sbkk/2025/20250926_01/">開始</a>しており、初期段階の企業にはNVIDIA DGX A100一式を最長60日間無償で提供する枠組みを明示。開発・検証フェーズでは低コスト提供、事業化段階では協業・出資も視野に入れるとしている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/aifoundationforstartups_d3538c27d9/aifoundationforstartups_d3538c27d9.jpg" alt="aifoundationforstartups.jpg" /></p>
<p>会見の中で宮川氏は、海外勢の動きを引き合いに危機感を示した。生成AI時代の競争軸が「計算基盤（コンピューティングパワー）の規模」に移りつつあるとの見立てを述べ、日本の産業競争力を左右する要素として計算資源の確保と活用を位置付けた。</p>
<p>背景として、海外では大規模なAIインフラ整備が加速していることが挙げられる。NVIDIAは10月31日、韓国の政府・産業界と連携し「25万基超（over a quarter-million）のGPU」を追加配備する構想を<a href="https://investor.nvidia.com/news/press-release-details/2025/NVIDIA-South-Korea-Government-and-Industrial-Giants-Build-AI-Infrastructure-and-Ecosystem-to-Fuel-Korea-Innovation-Industries-and-Jobs/default.aspx">発表</a>。韓国の産業分野全体でAI実装を進める基盤整備を掲げた。国内の計算資源の量・質をめぐる国際的な競争が一段と激しくなる中、ソフトバンクはスタートアップへの無償提供を含む支援スキームを通じて、国内の開発初期ハードルを下げる狙いだ。</p>
]]></description>
      <pubDate>Sun, 09 Nov 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/11/9 [SUN]AI訓練データの“源流”Common Crawlに疑惑──The Atlanticが報道、ペイウォール記事含有を指摘</title>
      <link>https://ledge.ai/articles/common_crawl_paywall_investigation_and_response</link>
      <description><![CDATA[<p>米誌『The Atlantic』は2025年11月4日、非営利団体Common Crawlが構築するウェブアーカイブが、OpenAIやGoogleなどのAI企業による大規模言語モデル（LLM）の訓練に利用されており、ペイウォール（有料会員制）記事を含む可能性があると<a href="https://www.theatlantic.com/technology/2025/11/common-crawl-ai-training-data/684567/">指摘した</a>。同日、Common Crawlは公式ブログで反論し、透明性とフェアユース（公正利用）の理念を改めて強調した。</p>
<h2>AI企業が利用する「見えない基盤」</h2>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/theatlantic_b31bcde0e1/theatlantic_b31bcde0e1.jpg" alt="theatlantic.jpg" /></p>
<p>The Atlanticの記事は、Common Crawlが10年以上にわたりウェブ全体をクロールし、ペタバイト級のアーカイブを研究・教育目的で公開してきた点を紹介した。記事によると、このデータセットはOpenAI、Google、Meta、Amazon、Anthropic、NVIDIAなどのAI企業がLLM訓練に活用しており、「AI業界の見えない基盤（invisible infrastructure）」になっているという。</p>
<p>同誌は、Common Crawlのアーカイブに有料メディアやニュースサイトのペイウォール記事が含まれている可能性を指摘。出版各社が「有料コンテンツが無断でAI訓練に利用された」と懸念を示していると報じた。また、削除を求めた出版社に対して十分な対応が行われていない事例があるとし、透明性が不十分だと指摘した。</p>
<h2>Common Crawlが即日反論</h2>
<p>同日、Common Crawlは公式ブログ「Setting the Record Straight」を<a href="https://commoncrawl.org/blog/setting-the-record-straight-common-crawls-commitment-to-transparency-fair-use-and-the-public-good">公開</a>し、「当財団はペイウォールを回避しない。活動は透明で、robots.txtを尊重している」と反論した。</p>
<p>ブログでは次のような見解を示している。
クローリングはrobots.txtなどウェブ標準に準拠しており、ペイウォールを意図的に回避した事実はない。
出版社からの削除要請には対応しており、プロセスの透明性を確保している。
データは特定企業への提供ではなく、研究者・教育機関・一般利用者を含むすべての人に開放されている。
公開データはフェアユースの理念に基づくもので、社会全体の知識共有を目的としている。</p>
<p>Common Crawlはまた、「CCBotは公開ページのみを収集し、ログインやペイウォール回避は行わない。User-agent: CCBotの設定でブロック可能」と説明。アーカイブの存在が「研究や民主的な情報アクセスを支える公共財」であると位置づけ、今後も透明性と公益性を重視した運営を続けるとした。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/commoncrawl_e3ceb79638/commoncrawl_e3ceb79638.jpg" alt="commoncrawl.jpg" /></p>
<h2>論点はフェアユースと説明責任</h2>
<p>今回の報道と反論を通じて、AI訓練データとしてのウェブアーカイブ活用をめぐる法的・倫理的な論点が浮き彫りになった。
The Atlanticは、出版社の著作物がAIモデルの学習に使われることへの懸念を示した一方、Common Crawlは、意図的な侵害を否定し、公益目的の情報共有としての意義を強調している。</p>
<p>今後は、AI企業、出版社、研究機関の間で、データ利用の範囲や削除手続きの明確化をめぐる議論が進むとみられる。</p>
]]></description>
      <pubDate>Sun, 09 Nov 2025 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>著作権書籍で訓練したAI、創作専攻の大学院生による模倣文より高評価──コロンビア大などの研究</title>
      <link>https://ledge.ai/articles/ai_trained_on_copyrighted_books_preferred_over_human_writers</link>
      <description><![CDATA[<p>米コロンビア大学とミシガン大学の研究チームは、著作権のある書籍を用いて訓練したAIモデルが、創作専攻の大学院生（MFA候補者）による文体模倣よりも読者に高く評価される傾向を示したとする研究結果を<a href="https://arxiv.org/abs/2510.13939">発表</a>した。論文は「Readers Prefer Outputs of AI Trained on Copyrighted Books over Expert Human Writers」と題し、2025年10月に論文共有サイトarXivで公開された。</p>
<h2>実験：AIと人間の「模倣能力」を比較</h2>
<p>研究では、ノーベル賞やブッカー賞などの受賞作家50名を対象に、各作家の文体を再現した短文（最大450語）をAIと人間の双方で生成した。人間側は創作専攻の大学院生（MFA候補者）が作成し、AI側はChatGPT、Claude、Geminiの3モデルを使用した。</p>
<p>AIには2つの条件が設定された。1つは作家の作風を指示して生成する「in-context prompting」、もう1つは各作家の著作全集を用いてAIを個別に再訓練（ファインチューニング）する方法である。</p>
<p>評価は、専門家28名と一般読者131名によるブラインドテスト形式で行われ、「スタイルの忠実性」と「執筆品質」の2項目について比較された。</p>
<h2>ファインチューニングで逆転</h2>
<p>結果は、AIの学習方法によって大きな差が見られた。in-context promptingでは、専門家読者はAI出力を低く評価し、スタイル忠実性のオッズ比（OR）は0.16、執筆品質は0.13だった。</p>
<p>一方、著者全集を用いたファインチューニングでは、専門家の評価が逆転し、スタイル忠実性のORは8.16、執筆品質は1.87となった。
一般読者も同様の傾向を示した。</p>
<p>AI検出ツールによる識別では、通常生成の97％が「AIが書いた」と判定されたのに対し、ファインチューニング版では3％にとどまった。</p>
<p><strong>研究チームによる生成手法の比較図。左はプロンプトのみでの模倣、右は著者全集を用いたファインチューニングによる模倣</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/schematic_crop_fa674379c5/schematic_crop_fa674379c5.jpg" alt="schematic-crop.jpg" /></p>
<h2>コスト差：人間作家より99.7％低い</h2>
<p>研究チームは、1人の作家を対象にファインチューニングと生成を行う場合の中央値コストを81ドル（約1万2千円）と算出した。人間作家によるスタイル模倣の平均報酬3万ドル（約460万円）と比較すると、99.7％のコスト削減となる。この数値は、AIが「著者らしさ」を低コストで再現できることを定量的に示すものとなった。</p>
<h2>著作権法上の含意：市場への影響</h2>
<p>論文は、米国著作権法におけるフェアユースの第4因子（原著作物の市場や価値に与える影響）に関連づけて、この結果を分析している。AIによる生成物が、原著作物やそれを模倣する人間作家の仕事を経済的に代替し得る点を、重要な法的論点として位置づけた。</p>
<p>著者らは、AIが特定作家の全集を用いて学習した場合、個別の「作風再現市場」を実質的に置き換える可能性があると指摘。
また、AIによる大量生成が市場に「希釈効果（market dilution）」をもたらし、読者の関心や購買機会を分散させるリスクにも言及している。</p>
<p>この考え方は、米連邦裁判所が過去の著作権訴訟で示した「AI生成物が原著作物市場の代替となり得るか」という判断基準と一致する。論文は、こうした代替・希釈が確認される場合、著作権作品を利用したAIの学習がフェアユースに該当しない可能性があると論じている。</p>
<h2>研究の限界</h2>
<p>研究チームは、実験が450語の短文を対象としており、長編小説や創作構成のような複雑な要素は評価対象外であると明記した。また、AI訓練に著作権保護作品を使用する行為の法的評価や、商業利用の可否については別途検討が必要と述べている。さらに、今回の結果は「文体再現」に焦点を当てたものであり、創造性や物語構築力といった要素については評価を行っていない。</p>
<h2>今後の議論へ</h2>
<p>この研究は、AIが著作権保護作品を用いて訓練されることが、どの程度創作市場に影響を及ぼすかを定量的に示した初の実証研究の一つとなる。読者評価とコスト分析の両面から、AIが人間の文体模倣を現実的に代替し得る可能性を提示しており、今後の著作権政策や創作支援技術の議論に影響を与えるとみられる。</p>
]]></description>
      <pubDate>Sun, 09 Nov 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>GoogleマップがGemini対応　「右折はタイ料理店の先」など“ランドマーク案内”を含む4つの新機能を発表</title>
      <link>https://ledge.ai/articles/google_maps_gemini_landmark_navigation_4_features</link>
      <description><![CDATA[<p>Googleは2025年11月5日（現地時間）、地図アプリ「Googleマップ」に生成AI「Gemini」を統合し、会話型のナビゲーション体験を強化する4つの新機能を<a href="https://blog.google/products/maps/gemini-navigation-features-landmark-lens/">発表</a>した。Geminiによる音声操作やランドマーク型案内、交通情報の自動通知、カメラを活用した視覚的検索が順次利用可能になる。</p>
<h2>会話で完結する“ハンズフリー運転”</h2>
<p>新機能の一つは、運転中に音声だけで操作できる「会話型ハンズフリー運転体験」だ。ユーザーは「途中でコーヒーショップに寄りたい」など自然な言葉で指示でき、Geminiが最適な経路や立ち寄り先を提案する。走行中でも画面操作を減らし、安全性の高いナビゲーションを実現する狙いがある。</p>
<p>@<a href="https://youtu.be/WnNZ3QhwE84">YouTube</a></p>
<h2>「右折はガソリンスタンドの先」──ランドマークで案内</h2>
<p>従来の「200メートル先を右折」ではなく、「タイ料理店の先を右折」といったランドマーク（目印）を基準にしたターン案内も追加された。ユーザーが視認しやすい建物や店舗を指標にすることで、より直感的に道順を把握できるという。</p>
<p>@<a href="https://youtu.be/_wJIEgy0uCg">YouTube</a></p>
<h2>ナビ起動前から渋滞を警告</h2>
<p>Geminiはまた、ユーザーの通勤や通学など“日常ルート”を学習し、ナビを起動していなくても交通渋滞や事故発生を検知した際に自動で通知を行う。プロアクティブなアラート機能により、出発前のタイミングで回避ルートを確認できるようになった。</p>
<p>@<a href="https://youtu.be/WJ7E8KTv034">YouTube</a></p>
<h2>Lens with Geminiで“周囲を質問”</h2>
<p>さらに、「Lens with Gemini」ではスマートフォンのカメラを向けた建物や店舗を即座に認識。たとえば、カメラを向けて「このレストランは何時まで開いてる？」と尋ねると、営業時間やレビューなどをGeminiが対話形式で返答する。従来の検索型から“会話する地図”への進化がうかがえる。</p>
<p>@<a href="https://youtu.be/mlhIxJCP9CM">YouTube</a></p>
<h2>提供開始と対応環境</h2>
<p>これらの機能は「Geminiが利用可能な地域」から順次展開される予定。対象はAndroidとiOSのGoogleマップアプリで、今後はAndroid Autoなど他プラットフォームへの拡張も検討されているという。</p>
]]></description>
      <pubDate>Sat, 08 Nov 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>学術＆研究2025/11/8 [SAT]Google「Project Suncatcher」発表　太陽光×衛星コンステでAI計算を“宇宙へ”</title>
      <link>https://ledge.ai/articles/google_project_suncatcher_space_ai_datacenter</link>
      <description><![CDATA[<p>Googleは2025年11月4日（現地時間）、太陽光発電を搭載した小型衛星群を用い、宇宙空間でAI計算を行うことを目指す新たな研究プロジェクト「Project Suncatcher（プロジェクト・サンキャッチャー）」を<a href="https://research.google/blog/exploring-a-space-based-scalable-ai-infrastructure-system-design/">発表</a>した。</p>
<p>同社の専用チップ「TPU（Tensor Processing Unit）」を搭載した衛星コンステレーションを軌道上に展開し、光学通信で相互接続することで“宇宙データセンター”を形成する構想だ。想定軌道は太陽同期（ドーン・ダスク）で、太陽光の連続利用を見込む。</p>
<h2>地上リソースの制約を超える「宇宙データセンター」</h2>
<p>Google Researchによると、AIモデルの高精度化に伴い、計算能力と電力の需要は急速に拡大している。Project Suncatcherは、地上のエネルギー・冷却・用地といった制約を緩和しつつ、AI計算を持続的にスケールさせるための「ムーンショット（挑戦的研究）」として位置づけられている。</p>
<p>太陽同期軌道では、太陽光パネルの発電効率が地上比で最大8倍に達し、1衛星あたり平均約4 kWの発電を想定。地上のデータセンターで必要とされる100 MW級の電力に比べると、同規模の衛星クラスタ（81機）では0.3 MW程度に収まる試算だ。</p>
<h2>1 km圏内に81機──クラスタ構成の設計例</h2>
<p><a href="https://goo.gle/4qGsU8X">論文</a>では、直径約1 kmの範囲に81機の小型衛星を立体的に配置し、1クラスタあたり約1.3 PFLOPSの演算性能を実現する設計例が示された。各衛星は16 GBメモリを備えたTPUを搭載し、自由空間光通信（Free-Space Optical Link）を介して数km以内で編隊飛行する。通信遅延は0.5 ms以下に抑えられ、地上データセンターと同等の対話応答が可能とされている。</p>
<p>Googleはベンチスケールのデモンストレーターで双方向1.6 Tbps（片方向800 Gbps）の通信速度を達成したと報告。通信帯域の確保には、多波長DWDMや空間多重技術を組み合わせる設計を検討している。</p>
<h2>放射線試験と熱設計──「Trillium TPU」で検証へ</h2>
<p>宇宙空間での安定稼働に向け、Googleは次世代TPU「Trillium（Cloud TPU v6e）」の放射線耐性を評価している。
宇宙では対流冷却ができないため、81機構成のクラスタで約450 m²の放熱面積を確保する必要があると論文は指摘。放熱板の構成や電力変換効率、冷却の最適化も主要な検討課題に挙げられた。
AIワークロードとしては、LLMの学習ではなく推論（inference）・埋め込み生成（embedding）・検索（retrieval）など、軽量分散処理を中心に想定している。</p>
<h2>Planet Labsと連携、2027年に試験衛星を打ち上げ</h2>
<p>次のステップとして、Googleは地球観測衛星を運用する米Planet Labsと提携し、軌道上での技術実証を進める。
Planetは11月4日付の<a href="https://www.planet.com/pulse/planet-to-build-and-operate-advanced-space-platform-for-google-s-project-suncatcher-moonshot/">発表</a>で、Suncatcher向けに「先進的な宇宙プラットフォーム」を構築・運用することを明らかにした。
両社は2027年初頭までに2機の試験衛星を打ち上げ、光学通信の安定性、熱挙動、誤り訂正、電力効率などを評価する「learning mission（学習ミッション）」を予定している。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/google_and_planet_suncatcher_moonshot_5280e0e084/google_and_planet_suncatcher_moonshot_5280e0e084.jpg" alt="google and planet suncatcher moonshot.jpg" /></p>
<h2>「AI計算を宇宙に」──Googleが描く次世代インフラ</h2>
<p>Project Suncatcherはまだ構想段階にあるが、Googleは「AI計算を地球の外に拡張する」という新しい方向性を示した。
公式ブログでは「Project Suncatcher is a moonshot to explore solar-powered satellite constellations with TPUs and free-space optical links for scalable AI compute（TPUと光学通信を組み合わせた太陽光衛星群によるAI計算スケーリングを探るムーンショットだ）」と説明している。
エネルギー負荷や冷却コストを軽減しながら、AI計算資源を宇宙空間へと拡張する次世代インフラの研究が、今後本格化する見通しだ。</p>
]]></description>
      <pubDate>Sat, 08 Nov 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/10/27 [MON]2025年のAIトレンドを総ざらい！Ledge.ai年末年始特集「&apos;25to&apos;26」事前登録スタート</title>
      <link>https://ledge.ai/articles/25to26-announce</link>
      <description><![CDATA[<p>AIの社会実装を加速させ、「テクノロジーを社会になめらかに浸透させる」ことをミッションに掲げる、国内最大級のAIメディア「Ledge.ai」を運営する株式会社レッジは、今年も年末年始特集「'25to'26」を公開します。
本日より先行サイトを公開し12月1日（月）の特集サイト公開までの間、お知らせを受け取ることができるようになる事前登録（無料）を受付開始いたしました。</p>
<p>:::button
<a href="https://25to26.ledge.ai/lp">事前告知サイトはこちら</a>
:::</p>
<p>2025年を締めくくるにふさわしい、AIの今とこれからを網羅した一大特集。研究者、ビジネスリーダー、エンジニアなど、あらゆる立場の方々に向けて、2026年のAIシーンを展望します。</p>
<h2>Ledge.ai年末年始特集『'25to'26』とは</h2>
<p>Ledge.ai年末年始特集は、2025年のAI関連ニュースや注目のキーワード、2026年以降の動向など、AIの初心者から専門家まで幅広く楽しめる特集サイトです。</p>
<p>2025年は、生成AIが実用フェーズに突入し、業務プロセス・プロダクト・教育・クリエイティブなど、社会のあらゆる分野で“AI活用の当たり前化”が進んだ一年でした。
そして2026年は、AIという概念そのものが提唱された「ダートマス会議」から70周年という、まさに歴史的な節目を迎えます。2025年の「当たり前化」を土台として、AIは社会インフラのように深く浸透し、その活用範囲の拡大と同時に、AGI（汎用人工知能）の実現可能性など、AIの“次なる進展”に向けた探求が本格化する一年となるのではないでしょうか。</p>
<p>本特集では、そんな激動の2025年を多角的に振り返りつつ、2026年に向けた新たな潮流やビジネスチャンスを展望します。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/1_ac8b0aef2e/1_ac8b0aef2e.png" alt="1.png" /></p>
<h2>コンテンツラインナップ紹介</h2>
<h3>編集部による徹底解説</h3>
<p>Ledge.ai編集部が、2025年のAIシーンを多角的に総括。
1年間の主要ニュースをピックアップしながら、トレンド分析と俯瞰的な視点で、AI技術が社会・産業へどのように浸透したのかを読み解きます。
さらに、技術動向の深掘り解説を通じて、進化の本質を明らかに。
2026年に向けて押さえておくべき“AIの現在地”を、独自の視点で整理します。</p>
<h3>独自インタビュー</h3>
<p>本特集では、「AI 70th Pre-Anniversary」というテーマのもと、AI研究の歴史・現在・未来をつなぐキーパーソンたちにインタビューを実施。
過去／現在／未来のそれぞれの視点から、AIがどのように発展し、次の時代にどんな可能性を秘めているのかを語ってもらいます。
世代と分野を超えて交わる知見が、AIの軌跡と未来へのヒントを照らし出します。</p>
<h3>トップランナー企業動向</h3>
<p>国内外の注目企業をピックアップし、AI周辺で押さえておきたい企業の最新動向を徹底分析。
生成AI、AIエージェント、クラウドAIなど、世界最先端の情報と実践事例に触れることで、読者が“次に取るべき一手”を見極められる構成になっています。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/2_6180f8a0c5/2_6180f8a0c5.png" alt="2.png" /></p>
<h2>開催概要</h2>
<p>イベント名：Ledge.ai年末年始特集「'25to'26」
開催期間：2025年12月1日(月) - 2026年1月9日(金)
形式：オンライン
参加費：無料（※一部のコンテンツ閲覧にはプロフィール登録が必要となります。）
お問合せ：contact@ledge.co.jp
URL：<a href="https://25to26.ledge.ai/lp">https://25to26.ledge.ai/lp</a></p>
]]></description>
      <pubDate>Mon, 27 Oct 2025 01:50:00 GMT</pubDate>
    </item>
  </channel>
</rss>