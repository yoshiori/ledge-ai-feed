<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
  <channel>
    <title>Ledge.ai 新着記事</title>
    <link>https://ledge.ai/</link>
    <description>Ledge.ai の最新テクノロジー記事</description>
    <item>
      <title>学術＆研究2026/1/12 [MON]「AI 2027」予測を“後ろ倒し”──元OpenAI研究者ココタジロ氏ら、超人的AIの到来見通しを更新</title>
      <link>https://ledge.ai/articles/ai_2027_timeline_revision_kokotajlo</link>
      <description><![CDATA[<p>元OpenAIのAI研究者であるDaniel Kokotajlo氏らは2025年12月31日、AIの急速な進展とその潜在的リスクを描いた将来予測シナリオ「AI 2027」に関連する予測を見直し、AIが人間を明確に上回る能力を獲得する時期についての見通しを、従来より後ろ倒しにしたと<a href="https://blog.ai-futures.org/p/ai-futures-model-dec-2025-update">発表</a>した。</p>
<p>この見直しは、同氏らが運営するAI Futures Projectが2025年12月末に公開した最新の予測モデルによるものだ。公式の説明によると、新モデルでは、AIが人間並み、あるいはそれ以上の能力でコーディングや研究開発を自動化する段階に到達するまでのタイムラインを、従来の想定より数年単位で長く見積もっているという。</p>
<h2>予測シナリオ「AI 2027」が提示してきたもの</h2>
<p>「<a href="https://ai-2027.com/">AI 2027</a>」は2025年に公開されたシナリオで、AIの能力向上が加速した場合に、2020年代後半にも人間を超える汎用的な知能が出現する可能性と、それに伴う社会的・安全保障上のリスクを描いたものだ。物語形式のシナリオとともに、能力向上のマイルストーンやタイムライン予測を示し、AI安全性やガバナンスをめぐる議論の中で広く参照されてきた。</p>
<p>同シナリオは、特定の年を断定するものではなく、複数の前提条件に基づく予測モデルを用いて将来像を提示している点が特徴とされている。</p>
<h2>最新モデルで示されたタイムラインの修正</h2>
<p>今回公開された最新モデルでは、AIが「完全なコーディング自動化」に至るまでの期間について、旧モデル（AI 2027で用いられた予測）よりも慎重な見積もりが採用された。AI Futures Projectはその理由として、完全自動化に至る前段階でのAIによる研究開発（R&amp;D）加速の効果を、現実的な制約を踏まえて再評価した点を挙げている。</p>
<p><strong>AI Futures Projectが公開した最新の予測モデル画面。AI 2027で用いられた従来モデルを更新し、AI研究開発の自動化や計算資源の前提を再評価している</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/f2bf7c4b_33b1_4411_acb4_ffe5c2178fa0_1600x906_7bde2dd933/f2bf7c4b_33b1_4411_acb4_ffe5c2178fa0_1600x906_7bde2dd933.png" alt="f2bf7c4b-33b1-4411-acb4-ffe5c2178fa0_1600x906.png" /></p>
<p>研究開発の自動化が進めばAIの進化は複利的に加速する可能性がある一方、その速度や実用化の範囲については不確実性が大きいとし、前回の想定を修正した形だ。</p>
<h2>当事者による位置づけと説明</h2>
<p>ココタジロ氏は<a href="https://x.com/DKokotajlo/status/2006257807721652588">自身のX投稿</a>でも、「AI 2027のシナリオよりも進展はやや遅い可能性が高い」と言及している。あわせて、当初から予測には幅があり、今回の更新は新たなデータや前提条件を反映した結果だと説明している。</p>
<p><strong>AI Futures Modelに基づく1つの想定トラジェクトリー。完全なコーディング自動化（AC）、超人的AI研究者（SAR）、人工超知能（ASI）に至る時期を示している</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/G9dm_El_Oa_YA_Ettaj_2dbcf7c8fd/G9dm_El_Oa_YA_Ettaj_2dbcf7c8fd.jpg" alt="G9dmElOaYAEttaj.jpg" /></p>
<h2>更新を前提とした予測モデルという位置づけ</h2>
<p>AI Futures Projectは、今回公開した最新モデルについて、現時点の技術動向や前提条件を反映したものであり、今後の研究開発の進展や実証結果に応じて見直しを行うとしている。AI 2027で提示された予測も、こうした更新の一部として位置づけられており、同プロジェクトは引き続き、モデルの前提や結果を公開しながら予測を更新していく方針を示している。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>現在の生成AI勢力図はどこからどのように生まれたのか？──Google起点の系譜を可視化した「AI Mafia」</title>
      <link>https://ledge.ai/articles/ai_mafia_google_ai_talent_network</link>
      <description><![CDATA[<p>生成AIを牽引する研究者や起業家たちは、どこから生まれ、どのようにつながってきたのか。
個人開発者のDipak Wani氏が公開したインタラクティブ可視化「<a href="https://dipakwani.com/ai-mafia/">AI Mafia</a>」は、Googleを起点とするAI人材の系譜をネットワーク図として可視化した試みとして注目を集めている。</p>
<p>「AI Mafia」は、人物、企業、研究組織などをノードとして配置し、それらの関係性を線で結んだインタラクティブなネットワーク図だ。閲覧者はノードを操作することで、特定の研究者がどの組織に所属し、どの企業や研究と結びついてきたのかを辿ることができる。生成AI分野を形作ってきた人材の流れを、構造として俯瞰できる点が特徴となっている。</p>
<h2>Googleに集結したAI人材──2010年代前半の「集中」</h2>
<p>この可視化の背景にあるのが、米国の人気ビジネス系ポッドキャスト「Acquired」が2025年秋に公開したエピソード「<a href="https://www.youtube.com/watch?v=lCEB7xHer5U">Google: The AI Company</a>」で整理されたAI史だ。同エピソードでは、現在の生成AI革命を担う人材の多くが、2010年代前半に一度Googleへ集結していたことが語られている。</p>
<p>@<a href="https://https://www.youtube.com/watch?v=lCEB7xHer5U">YouTube</a></p>
<p>2012年のImageNetコンペティションで勝利し、深層学習の転換点を作ったジェフ・ヒントン教授と、その教え子であるアレックス・クリジェフスキー、イリヤ・サツケヴァーらは、スタートアップ「DNN Research」を通じてGoogleに加わった。ほぼ同時期に、デミス・ハサビスらが創業したDeepMindも2014年にGoogleに買収され、優秀な研究者集団がGoogle傘下に入った。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/hinton_dnn_hassabis_deepmind_723429abc4/hinton_dnn_hassabis_deepmind_723429abc4.jpg" alt="hinton dnn hassabis deepmind.jpg" /></p>
<p>さらに、アンドリュー・ン氏やジェフ・ディーン氏らを中心に、Google内部では「Google Brain」が立ち上げられた。Acquiredではこの時期を、かつてIBMがコンピュータ時代初期にプログラマーを一手に集めていた状況になぞらえ、AI分野の人材と計算資源がGoogleに強く集中していた段階として位置づけている。</p>
<h2>OpenAIの設立と最初の人材流出</h2>
<p>こうした集中構造に変化をもたらしたのが、2015年前後のOpenAI設立だ。イーロン・マスク氏やサム・アルトマン氏らは、GoogleがAI研究と計算資源を事実上独占している状況に危機感を示し、対抗的な研究組織としてOpenAIを立ち上げた。
多くの研究者がGoogleに留まる中で、最初に大きな動きを見せたのがイリヤ・サツケヴァー氏だった。同氏はGoogleを離れ、OpenAIの創業メンバーとして参加し、後に同社の技術的中核を担う存在となった。この動きは、AI人材がGoogleから外部へ流出し始めた象徴的な出来事とされている。</p>
<h2>Transformer論文が加速させた分岐</h2>
<p>人材流動を決定的に加速させたのが、2017年にGoogleの研究者らが発表した論文「Attention Is All You Need」、いわゆるTransformer論文だ。現在の生成AIの基盤となるこの技術はGoogle内部で生まれたが、製品化には慎重な姿勢が取られた。</p>
<p>その結果、論文の共著者たちは数年以内に相次いでGoogleを退職し、スタートアップを創業したり、競合企業へ移籍したりしていった。主要著者の一人であるノーム・シャジール氏は、社内でのチャットボット公開が認められなかったことを背景に退職し、Character.AIを創業した例として知られる。</p>
<p><strong>Transformer論文著者たちのその後</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/after_transformer_6288345452/after_transformer_6288345452.jpg" alt="after transformer.jpg" /></p>
<h2>現在の生成AI勢力図へとつながる人材の流れ</h2>
<p>現在の生成AI業界を代表する企業の多くは、こうした流れの延長線上にある。OpenAIはサツケヴァー氏ら元Google研究者が技術基盤を築き、AnthropicはOpenAIからスピンアウトしたダリオ・アモデイ氏（元Google Brain）らによって設立された。DeepMindの共同創業者であるムスタファ・スレイマン氏はInflection AIを経て、現在はMicrosoftのAI部門を率いている。</p>
<p>Dipak Wani氏の「AI Mafia」は、これらの人材の集中と分岐の歴史を、一枚のインタラクティブな図として可視化したものだ。2025年10月下旬に<a href="https://news.ycombinator.com/item?id=45715819">Hacker News</a>上で作者本人によって公開され、年末から年始にかけてテック系コミュニティで共有が広がった。研究成果の優劣を示すものではないが、生成AI時代のエコシステムがどのような人材の流れによって形作られてきたのかを理解する手がかりとして参照されている。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>アリババ、新世代GUIエージェント「MAI-UI」を発表──AIエージェントによるスマートフォン操作で最高性能を記録</title>
      <link>https://ledge.ai/articles/alibaba_mai_ui_gui_agent_mobile_operation_benchmark</link>
      <description><![CDATA[<p>Alibaba GroupのAI研究組織であるTongyi Labは2025年12月26日、GUI（グラフィカル・ユーザー・インターフェース）を直接操作できる新たなAIエージェントモデル群「MAI-UI」を<a href="https://arxiv.org/abs/2512.22047">発表</a>した。</p>
<p>MAI-UIは、スマートフォンやPCの画面を人間と同様に認識・操作することを目的としたモデル群で、GUI要素の認識（グラウンディング）と操作（ナビゲーション）に関する複数のベンチマークにおいて、最高水準の性能を記録したとしている。</p>
<h2>GUIエージェントを前提に設計した統一アーキテクチャ</h2>
<p>MAI-UIの最大の特徴は、GUIエージェントに必要とされる複数の能力を、単一の統一アーキテクチャとして設計している点にある。
Tongyi Labによると、MAI-UIは以下の要素をネイティブに統合している。</p>
<ul>
<li>ユーザーとの対話による指示内容の補完</li>
<li>MCP（Model Context Protocol）を用いた外部ツール呼び出し</li>
<li>デバイス（端末）とクラウドの協調実行</li>
<li>オンライン強化学習による長期タスク対応</li>
</ul>
<p><strong>■ MAI-UIの実行フロー例。GUI操作に加え、ユーザー確認（Call User）やMCPツール呼び出しを組み合わせてタスクを完了する構成を示す</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/MAI_UI_x2_9b5d693101/MAI_UI_x2_9b5d693101.png" alt="MAI-UI x2.png" /></p>
<p>この設計により、画面操作だけに依存する従来のGUIエージェントと比べ、操作回数の削減やタスク成功率の向上を図っているという。</p>
<h2>デモで示されたスマートフォン操作能力</h2>
<p>Tongyi Labが公開したデモ動画では、MAI-UIが画面表示を直接認識し、複数のアプリをまたいで操作を行う様子が示されている。</p>
<p>MAI-UIは、アプリの内部APIに依存せず、画面上のボタンや入力欄を視覚的に把握し、タップ、入力、画面遷移を段階的に実行する。
条件が不足している場合には、即座に処理を進めるのではなく、ユーザーに確認を求めた上で操作を継続する挙動も確認できる。</p>
<p><strong>■ スマートフォンの鉄道予約アプリを操作するMAI-UIのデモ画面。左が実際の端末画面、右がエージェントの観測・判断・操作ログを示している</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/mai_ui2_c1213f283a/mai_ui2_c1213f283a.jpg" alt="mai-ui2.jpg" /></p>
<p>一部の処理では、MCPツールを併用することで、UI操作をすべて画面経由で行うのではなく、効率的にタスクを進める構成が採られている。</p>
<h2>デバイスとクラウドの役割分担</h2>
<p>MAI-UIは、端末側で動作するローカルエージェントと、クラウド側のエージェントが役割分担する構成を採用している。
基本的なGUI操作や進行管理は端末側で行い、タスク逸脱や高度な判断が必要な場合のみ、クラウド側が介入する設計だという。</p>
<p><strong>■ 物件情報の比較とメッセージ送信を行うMAI-UIのタスク実行例。GUI操作とMCPツール呼び出しを組み合わせて処理している</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x7_2_baee50d765/x7_2_baee50d765.png" alt="x7 (2).png" /></p>
<p>この構成により、処理遅延の抑制やプライバシーへの配慮を両立させる狙いがあるとしている。</p>
<h2>ベンチマークと実世界タスクへの対応</h2>
<p>MAI-UIは、既存のGUI操作ベンチマーク「AndroidWorld」に加え、複数アプリをまたぐ長期・複合タスクを想定した新ベンチマーク「MobileWorld」でも評価が行われた。</p>
<p><strong>■ MAI-UIのベンチマーク結果。ScreenSpot-Pro、AndroidWorld（SR）、MobileWorld（SR）において、既存モデルとの比較を示している</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/mai_ui_overview_b3fa56ca6b/mai_ui_overview_b3fa56ca6b.jpg" alt="mai-ui overview.jpg" /></p>
<p>MobileWorldでは、ユーザーとの対話、MCPツール活用、第三者への情報共有など、実際の利用環境に近いタスクが設定されている。</p>
<p><strong>■ 物件情報の比較とメッセージ送信を行うMAI-UIのタスク実行例。GUI操作とMCPツール呼び出しを組み合わせて処理している</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x8_4e2d87bf9f/x8_4e2d87bf9f.png" alt="x8.png" /></p>
<h2>公開モデルと今後の展開</h2>
<p>MAI-UIのモデル群のうち、2B（20億）および8B（80億）パラメータのモデルはオープンソースとして公開されており、公式リポジトリやモデル配布プラットフォームから利用可能となっている。</p>
<p>Tongyi Labは、MAI-UIを単一アプリ内の自動操作にとどまらない、実世界タスクに対応するGUIエージェントの基盤として位置付けている。</p>
<p>@<a href="https://www.youtube.com/watch?v=KTR18H-kEXU">YouTube</a></p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>AMD、CES 2026で“ヨタ級AI”を見据えた基盤「Helios」公開──次世代GPU「Instinct MI455X」とサーバーCPU「EPYC “Venice”」が中核、PC向けのRyzen AI 400も発表</title>
      <link>https://ledge.ai/articles/amd_ces2026_yotta_ai_helios_mi455x_ryzen_ai_400</link>
      <description><![CDATA[<p>米半導体大手のAMDは2026年1月6日（米国時間）、米ラスベガスで開催中のCES 2026の基調講演において、AIデータセンター向けの新たなラックスケール基盤「Helios」と、PC向け新プロセッサ「Ryzen AI 400シリーズ」などの新製品を<a href="https://ir.amd.com/news-events/press-releases/detail/1272/amd-and-its-partners-share-their-vision-for-ai-everywhere-for-everyone-at-ces-2026">発表</a>した。</p>
<p>基調講演はCEOのLisa Su氏が務め、同社が掲げる「AI Everywhere, for Everyone」のビジョンを軸に、AI計算需要の急拡大を見据えた製品戦略を示した。</p>
<p>@<a href="https://www.youtube.com/watch?v=UbfAhFxDomE&amp;list=TLGGBbam0h3MCckwNzAxMjAyNg">YouTube</a></p>
<h2>ラックスケールAI基盤「Helios」を公開</h2>
<p>AMDが新たに披露した「Helios」は、AIデータセンター向けに設計されたラックスケールの統合プラットフォームだ。次世代GPU「Instinct MI455X」と、サーバーCPU「EPYC “Venice”」を中核に、ネットワークおよびソフトウェアスタック（ROCm）を含めて最適化された構成を採る。</p>
<p><strong>■ AMDは、AI計算需要がゼタスケールからヨタスケールへ移行すると説明。今後のAI計算量は1万倍規模で増加するとの見通しを示した</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/amd_yotta_flop_7ac334c034/amd_yotta_flop_7ac334c034.jpg" alt="amd yotta flop.jpg" /></p>
<p>同社は、AIモデルの大規模化と計算需要の急増を背景に、将来的には“ヨタ級”の計算規模が求められるとの認識を示しており、Heliosはそうした長期的な需要を見据えた設計思想に基づくものだとしている。</p>
<p>今回の基調講演では、Instinct MI400シリーズの製品ポートフォリオを初めて体系的に提示するとともに、次世代となる「MI500シリーズ」を2027年に投入する計画にも言及した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/amd_instinct_48be29b62a/amd_instinct_48be29b62a.jpg" alt="amd instinct.jpg" /></p>
<h2>AIデータセンター向けGPU「Instinct MI455X」</h2>
<p><strong>■ Instinct MI455Xは、前世代MI355X比で最大10倍のAI性能向上をうたう。AMDは世代ごとの性能進化を強調した</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/amd_mi455x_b3f4643fe4/amd_mi455x_b3f4643fe4.jpg" alt="amd mi455x.jpg" /></p>
<h2>AI PC向け「Ryzen AI 400シリーズ」を発表</h2>
<p>PC向けでは、新たに「Ryzen AI 400シリーズ」および法人向けの「Ryzen AI PRO 400シリーズ」を発表した。最大60TOPSのAI処理性能を持つNPUを搭載し、クラウドに依存しないローカルAI処理を重視した設計が特徴となる。</p>
<p>Ryzen AI 400シリーズは2026年1月から出荷が開始され、主要PCメーカーから順次搭載モデルが投入される予定だ。AMDは、生成AIや各種AIアプリケーションをPC上で直接実行するユースケースを想定し、AI PC市場への本格展開を進めるとしている。</p>
<p><strong>■ Ryzen AI 400シリーズの概要。60TOPSのNPUやRDNA 3.5 GPUを備え、生成AIやコンテンツ制作などのローカルAI処理を想定する</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/amd_ryzen_ai_400series_3c8af1e013/amd_ryzen_ai_400series_3c8af1e013.jpg" alt="amd ryzen ai 400series.jpg" /></p>
<h2>データセンターからPCまでを貫くAI戦略</h2>
<p>AMDは今回のCES 2026で、データセンター向けの大規模AI基盤と、エンドユーザーに近いPC向けAIプロセッサの両面で新製品を打ち出した。HeliosとRyzen AI 400シリーズの発表により、同社はAIワークロード全体を視野に入れたエンドツーエンドの製品戦略を改めて明確にした形だ。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>AIは主役から基盤へ──CES 2026で示された「実装段階」に入ったテクノロジーの姿</title>
      <link>https://ledge.ai/articles/ces2026_ai_design_embedded_devices</link>
      <description><![CDATA[<p>世界最大級のテクノロジー見本市「CES 2026」は、2026年1月6日から9日まで、米国ネバダ州ラスベガスで<a href="https://www.ces.tech/press-releases/ces-2026-the-future-is-here">開催された</a>。主催するConsumer Technology Association（CTA）は、今回のCESを「The Future Is Here（未来はすでにここにある）」と位置づけ、先端技術が構想や実験段階を越え、現実の製品やサービスとして展開されるフェーズに入ったことを強調した。
展示会場では、AIが単独の主役として語られるのではなく、PC、デジタルヘルス、モビリティ、日用品など幅広い分野に組み込まれ、生活や仕事のあり方を具体的に変える基盤技術として提示された。</p>
<p>CES 2026で示されたのは、AIが主役として語られるフェーズを越え、製品や体験の設計思想そのものに組み込まれていく段階に入った姿だった。編集部では、こうした移り変わりを象徴するようなデバイスをいくつかピックアップした。いずれも「AI搭載」を強調するより、計算や推論が製品の内部に溶け込み、使い方そのものを形づくっている点が共通している。これらの製品を通じて、CES 2026で示された“実装段階”の姿を見ていく。</p>
<h2>キーボード一体型AI PC：HP EliteBoard G1a</h2>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/v2_2026_hpeliteboardg1anextgenaipc_hpeliteboard_primary_826008a117/v2_2026_hpeliteboardg1anextgenaipc_hpeliteboard_primary_826008a117.webp" alt="v2_2026_hpeliteboardg1anextgenaipc-hpeliteboard_primary.webp" /></p>
<p>HPは、キーボード一体型のAI PC「EliteBoard G1a」を<a href="https://www.ces.tech/ces-innovation-awards/2026/hp-eliteboard-g1a-next-gen-ai-pc/">発表</a>した。外観はフルサイズキーボードに近いが、内部にCPUに加えてNPUを含む演算基盤を搭載し、外付けディスプレイと接続して使用する構成となっている。本製品はローカルでのAI処理を前提に設計されており、クラウドに依存せず、生成AIや要約、分析といったAIタスクを端末側で実行する用途を想定している。画面を本体から切り離すことで、AI処理を担う計算ユニットを机上に集約する設計思想が示された。
本製品はCES Innovation Awards 2026にも選出されている。</p>
<p><strong>PC → 生活空間AIへの転換</strong>
PC領域でのローカルAI処理を前提とした設計が示された一方で、CES 2026では、AIがディスプレイや入力装置の枠を越え、生活空間そのものに組み込まれる例も複数見られた。</p>
<h2>AIコンパニオン：Lepro Ami</h2>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Lepro_Ami_70cd5a7695/Lepro_Ami_70cd5a7695.jpg" alt="Lepro_Ami.jpg" /></p>
<p>Leproは、デスクトップ型AIコンパニオン「Lepro Ami」を<a href="https://www.newswire.ca/news-releases/lepro-unveils-lepro-ami-at-ces-2026-a-desktop-ai-companion-that-feels-in-the-room--847670849.html">公開</a>した。公式説明によると、Amiは音声入力や周囲の環境情報をAIが解釈し、利用者との対話や反応を行うことを目的としたデバイスである。画面操作を中心とする従来のAIとは異なり、空間内での存在感や応答性を重視した設計が特徴とされる。AI処理の一部はローカルで行う構成が示されており、プライバシーへの配慮も設計要素として挙げられている。日常空間に常駐するAIの形を提示するデバイスとして位置づけられている。</p>
<p><strong>空間AI → 個人データ／ライフログ</strong>
空間に常駐するAIに加え、CES 2026では、個人の行動や状態を継続的に捉えるAIを想定したデバイスも提示された。AIを“使う”存在ではなく、“常にそばにある前提”で設計する動きが浮かび上がる。</p>
<h2>AIライフログ・ペンダント：Motorola Project Maxwell</h2>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Maxwell_Photo_7_1536x1024_36f46681ba/Maxwell_Photo_7_1536x1024_36f46681ba.jpg" alt="Maxwell-Photo-7-1536x1024.jpg" /></p>
<p>Motorolaは、AIライフログデバイス「Project Maxwell」を研究プロジェクトとして<a href="https://motorolanews.com/motorola-unveils-new-flagship-devices-and-ai-powered-innovation-at-lenovo-tech-world-2026/">紹介</a>している。ペンダント型デバイスとして構想されており、利用者が見聞きしている情報をAIが理解・整理することを目的とする。公式リリースでは、視覚や音声といった日常環境データをAIが解析し、個人に合わせた体験や支援につなげる構想が示されている。AIは常時稼働するアシスタントとして振る舞うことを想定しており、Motorola 312 Labsによる実験的な取り組みの一環として位置づけられている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/G_EPFK_2a_MAA_Vy_Iy_216a72ad1f/G_EPFK_2a_MAA_Vy_Iy_216a72ad1f.jpg" alt="G-EPFK2aMAAVyIy.jpg" /></p>
<p><strong>ライフログ → デジタルヘルス</strong>
こうした流れは、デジタルヘルス分野でも顕著だった。CES 2026では、ウェアラブルや医療機器に限らず、日常的な動作や映像を起点に健康状態を把握しようとするアプローチが広がりを見せた。</p>
<h2>ミラー型デバイス：NuraLogix「Longevity Mirror」</h2>
<p>デジタルヘルス技術企業NuraLogixは、ミラー型デバイス「Longevity Mirror」を<a href="https://www.linkedin.com/posts/nuralogix-corporation_ai-longevity-healthtech-activity-7414349139225788416-Ne-N">公開</a>した。製品では、30秒のセルフィービデオをAIが解析し、血流変化などの映像情報から健康関連指標を推定する。同社によると、AIは映像データをもとに「Longevity Index（LIX）」を算出し、心血管疾患リスクや代謝の健康、精神的ストレスなど5つの生理学的領域を統合的に評価するという。ウェアラブルや物理センサーを用いず、カメラ映像とAI解析のみで健康状態の把握を試みる点が特徴とされている。</p>
<p>@<a href="https://www.youtube.com/watch?v=dDem2hR4_1E">YouTube</a></p>
<p>CES 2026には、約14万8,000人が来場し、4,100社以上（うち約1,200社がスタートアップ）が出展した。CTAのGary Shapiro氏は、CESを「世界で最も強力なイノベーションの実証の場」と位置づけ、技術がビジネスや政策、社会と結びつく場であると説明している。また、Kinsey Fabrizio氏は、AIをはじめとする技術がビジョンから実用段階へと移行している点を強調した。
AIが話題の中心となった過去数年を経て、2026年はAIを含む先端技術が、実体あるプロダクトや産業の中で役割を担い始めた段階を示すCESとなった。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2026/1/10 [SAT]OpenAI、健康分野に特化した「ChatGPT Health」公開──医療情報を安全に整理</title>
      <link>https://ledge.ai/articles/chatgpt_health_launch</link>
      <description><![CDATA[<p>OpenAIは2026年1月7日（現地時間）、健康・ウェルネス分野に特化した新機能「ChatGPT Health」を<a href="https://openai.com/ja-JP/index/introducing-chatgpt-health/">発表</a>した。医療記録やウェルネスアプリの情報を活用し、利用者が自身の健康状態を理解しやすくすることを目的とした専用体験を提供する。</p>
<p>ChatGPT Healthは、健康に関する会話専用に設計された機能で、通常のChatGPTのチャットとは分離された空間で利用できる。OpenAIによると、近年ChatGPTには健康やウェルネスに関する質問が急増しており、こうしたニーズに対応する形で専用機能を開発したという。</p>
<p>同機能では、検査結果や診療内容といった医療記録を読み解く支援に加え、運動・食事・睡眠などのウェルネスアプリのデータを統合し、情報を整理することが可能とされている。これにより、医師に相談する際の質問を事前にまとめたり、生活習慣を見直すためのヒントを得たりする用途を想定している。</p>
<p>ChatGPT Healthは、まず少人数の初期ユーザーを対象に提供を開始し、利用状況を踏まえながら改善を続ける。OpenAIは、改善を進めながら数週間以内にウェブ版とiOSのすべてのユーザーへ提供を拡大する予定としている。一方で、電子健康記録（EHR）の連携および一部のアプリは米国のみで利用可能で、Appleヘルスケアの接続にはiOSが必要だとしている。</p>
<p><strong>ChatGPT Healthの画面イメージ：ChatGPT内の専用スペースとして『ヘルスケア』が用意される</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/OAI_Chat_GPT_Health_Wayfinding_16_9_444cbb0458/OAI_Chat_GPT_Health_Wayfinding_16_9_444cbb0458.webp" alt="OAI_ChatGPT_Health_Wayfinding_16-9.webp" /></p>
<p>一方でOpenAIは、ChatGPT Healthが診断や治療を行うものではない点を明確にしている。あくまで医療専門職によるケアを補完する存在であり、最終的な医療判断は医師などの専門家が担うとしている。</p>
<p>ChatGPT Healthは、まず個人向けの健康・ウェルネス支援を目的として提供される。プライバシー面にも配慮し、健康関連の会話は他のチャット履歴から分離して管理される。OpenAIは、こうしたデータが基盤モデルの学習には使用されないことも明らかにしており、利用者が接続するデータを管理できる仕組みを用意したという。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>DeepSeek、LLMの大規模化による不安定を抑制する新構造「mHC」提案──性能向上への新たな選択肢として期待</title>
      <link>https://ledge.ai/articles/deepseek_mhc_llm_training_stability</link>
      <description><![CDATA[<p>中国のAI企業DeepSeekの研究チームは2025年12月31日、大規模言語モデル（LLM）の訓練を安定化させる新たな構造設計「Manifold-Constrained Hyper-Connections（mHC）」を提案する論文を<a href="https://www.arxiv.org/abs/2512.24880">発表</a>した。</p>
<p>mHCは、モデルの大規模化に伴って生じやすい学習の不安定化を、残差接続の構造制約によって抑制する手法で、計算コストを抑えながら性能とスケーラビリティの両立を目指す。</p>
<h2>Hyper-Connectionsが抱える課題</h2>
<p>近年のLLMでは、学習の安定性を確保するために残差接続（residual connections）が広く用いられてきた。DeepSeekの研究チームは、残差接続を拡張した「Hyper-Connections（HC）」が性能向上に寄与する一方で、モデルの規模が大きくなるにつれて学習が不安定化する問題があると指摘する。</p>
<p>論文によると、HCでは残差ストリームが無制約に拡張されることで、恒等写像（identity mapping）が持つ安定化の性質が損なわれ、勾配の発散や損失の急激な増大が生じやすくなる。特に数百億パラメータ規模のモデルでは、この問題が顕在化するという。</p>
<h2>残差接続を「多様体」に制約するmHC</h2>
<p>mHCは、こうした課題に対し、残差接続の重み行列を特定の多様体（manifold）上に制約するという設計を導入する。具体的には、残差行列を「二重確率行列（doubly stochastic matrix）」の集合であるBirkhoff polytopeに射影することで、各層における残差が入力特徴の凸結合として振る舞うようにする。</p>
<p>この制約により、残差接続は再び恒等写像に近い性質を保ち、層を重ねても信号や勾配が過度に増幅・減衰しにくくなる。射影にはSinkhorn–Knoppアルゴリズムが用いられ、論文では反復回数を制限することで計算負荷と理論的性質のバランスを取っている。</p>
<p><strong>残差接続（左）、Hyper-Connections（中央）、mHC（右）の概念比較。mHCはHCのマッピングを多様体（manifold）上に制約することで学習安定性の改善を狙う</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x1_296ef66bec/x1_296ef66bec.png" alt="x1.png" /></p>
<h2>学習を支える構造設計としての位置づけ</h2>
<p>mHCは、新しい最適化アルゴリズムや損失関数を導入する「訓練手法」ではなく、学習時の挙動を規定する残差接続の構造設計として位置づけられている。推論時の計算グラフを大きく変えるものではなく、あくまで訓練過程における安定性と拡張性を高める点が特徴だ。</p>
<p>論文では、残差接続の構造を制御することで、モデルを大規模化しても学習が破綻しにくい状態を維持できると説明している。</p>
<h2>大規模モデルでの検証結果</h2>
<p>DeepSeekの研究チームは、同社のLLMアーキテクチャを用い、約30億、90億、270億パラメータ規模のモデルでmHCを検証した。その結果、従来のHCで見られた勾配や損失の不安定な挙動が大幅に抑えられ、学習が安定して進行することを確認したという。</p>
<p>また、複数のベンチマークにおいて、mHCを用いたモデルはHCや従来の残差接続と同等、あるいはそれを上回る性能を示したと報告している。</p>
<h2>計算コストを抑えるための工夫</h2>
<p>mHCは理論的提案にとどまらず、実装面での効率化も重視している。論文では、カーネル融合や再計算手法、通信と計算を重ね合わせるパイプライン最適化などを組み合わせることで、残差ストリームを拡張した場合でも学習時間の増加を限定的に抑えられるとしている。</p>
<p>これにより、モデルの拡張によって計算コストやメモリ使用量が急増することを防ぎ、実運用に近い条件での検証を可能にした。</p>
<h2>LLMスケーリングにおける新たな選択肢</h2>
<p>論文は、LLMの性能向上を「パラメータ数や学習データを増やす」以外の方法で支えるアプローチとして、構造設計の重要性を示している。mHCは、残差接続という基盤的な要素を見直すことで、大規模化に伴う不安定性を抑制する道筋を提示した。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2026/1/11 [SUN]イーロン・マスクのNeuralink、BCI：思考で操作する脳内チップを量産へ　2026年計画を示す</title>
      <link>https://ledge.ai/articles/elon_musk_neuralink_bci_mass_production_2026</link>
      <description><![CDATA[<p>脳と機械を直接つなぐブレイン・コンピューター・インターフェイス（BCI）を開発する米スタートアップ Neuralink が、BCIデバイスの量産を2026年に開始する計画を示した。創業者の イーロン・マスク 氏が2026年1月1日の同氏のXへの<a href="https://x.com/elonmusk/status/2006513491105165411">投稿</a>で明らかにした。</p>
<p>マスク氏は投稿で、NeuralinkがBCIデバイスの高ボリューム生産を2026年に開始し、手術工程を大幅に簡略化した、ほぼ完全に自動化された外科手術へ移行すると説明した。これまで同社のBCIは、主に臨床試験向けに限定的な規模で製造されてきたが、今後はより多くの患者への提供を想定した製造フェーズへの移行を見据える。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/elon_x_post_neuralink2026_87a80d4661/elon_x_post_neuralink2026_87a80d4661.jpg" alt="elon x post neuralink2026.jpg" /></p>
<h2>手術の自動化と侵襲性低減</h2>
<p>マスク氏はあわせて、BCIデバイスに用いられる電極スレッドが、脳を覆う硬膜（dura）を除去することなく貫通する設計になると述べた。従来の脳外科手術では硬膜の切開や除去を伴うケースが多く、これを不要とする構造は、手術の侵襲性を抑える点で重要な意味を持つとされる。同氏はこの点について「大きな進展だ（This is a big deal）」と強調した。</p>
<p>Neuralinkは、ロボットを用いた手術システムの開発を進めており、将来的には手術工程の標準化や効率化、医師の負担軽減を図る構想を示してきた。今回の投稿は、量産体制の構築と並行して、自動化された手術プロセスを実運用に移行させる方針を示したものとなる。</p>
<h2>臨床試験の進展と背景</h2>
<p>マスク氏がリポストした投稿によると、Neuralinkは2025年にかけて、米国以外の地域も含め臨床試験を拡大してきた。重度の発話障害を対象とした技術については、米食品医薬品局（FDA）からブレークスルー・デバイス指定を受けたほか、英国や中東、カナダでの臨床試験や手術も実施されたとされる。</p>
<p>これらの試験では、脊髄損傷などにより身体機能に制約のある患者が、思考によってコンピューターを操作する事例が報告されている。現時点でNeuralinkが想定する主な用途は、神経障害や麻痺を持つ患者の支援であり、一般向け利用についての具体的な計画は示されていない。</p>
<h2>量産フェーズへの転換点</h2>
<p>今回示されたBCIデバイスの量産計画と手術自動化の方針は、Neuralinkが研究・実証段階にとどまらず、実用化と提供規模の拡大を見据えた段階へ進もうとしていることを示すものとなる。今後は、安全性や有効性の検証に加え、各国の規制当局による承認が引き続き重要な要素となる。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2026/1/11 [SUN]『女性をビキニに』指示が問題化、XのAI画像生成・編集機能が有料会員限定に</title>
      <link>https://ledge.ai/articles/grok_bikini_image_generation_paid_only</link>
      <description><![CDATA[<p>「@Grok 女性をビキニにして」などの指示による画像生成が物議を醸していた、XのAI画像生成・編集機能について、X上での利用が有料会員に限定されていることが、2026年1月9日ごろから確認されている。この仕様変更は<a href="https://www.reuters.com/sustainability/boards-policy-regulation/musks-ai-bot-grok-limits-image-generation-x-paid-users-after-backlash-2026-01-09/">Reuters</a>など複数の国内外メディアが報じている。</p>
<p>対象となっているのは、GrokをX上でメンションし、返信欄から画像生成や編集を依頼する機能だ。現在、無料ユーザーが同様の操作を行うと、「画像の生成と編集は現在、有料会員限定です」といったメッセージが表示され、リプライ形式での画像生成・編集は実質的に利用できなくなっている。</p>
<h2>X上の「@Grok」経由のみが制限対象</h2>
<p>複数の報道やユーザー検証によると、今回の制限はGrokの画像生成・編集機能そのものを全面的に停止したものではない。
実際には、以下のような状態が確認されている。</p>
<ul>
<li>Xの返信欄で「@Grok」をメンションして画像生成・編集を依頼する方法は、無料会員ではブロック</li>
<li>一方で、Grokのスタンドアロン版アプリやウェブ版（grok.x.ai）では、無料ユーザーでも画像生成・編集が可能な状態が続いている</li>
<li>Xアプリやウェブ上で画像を直接操作する一部の編集機能についても、無料で利用できるケースが報告されている</li>
</ul>
<p>このため、現時点での変更は「X上のリプライ経由の画像生成・編集のみを有料限定とした」措置とみられている。</p>
<h2>背景に非同意の性的画像生成問題</h2>
<p>今回の仕様変更の背景として、海外メディアは、非同意による性的画像生成の急増を挙げている。
昨年末から年始にかけて、他人の写真を無断で加工し、衣服を除去したり、性的に強調した画像を生成するいわゆる「デジタル脱衣」型のディープフェイクが拡散。女性や未成年を対象とした事例も問題視された。</p>
<p>こうした状況を受け、英国やEU、インドなどで規制当局や政府関係者が懸念を示しており、Reutersなどは、国際的な批判や規制圧力が今回の制限につながった可能性を報じている。</p>
<p>XおよびGrokを開発するxAIは、現時点でこの仕様変更について正式なリリースや詳細な説明を公表していない。
今回の有料化措置については、無料での悪用が難しくなった点を評価する声がある一方、抜け道が残っており根本的な対策とは言えないとする批判も出ていると、複数メディアは伝えている。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>「写真が勝手にビキニ化」AI「Grok」画像編集が炎上──未成年を含む生成も焦点に、各国で規制圧力</title>
      <link>https://ledge.ai/articles/grok_image_editing_nonconsensual_sexualized_images</link>
      <description><![CDATA[<p>2025年の年末から2026年の年始にかけ、米実業家イーロン・マスク氏が設立したAI開発企業 xAI が提供するAI「Grok」をめぐり、実在の人物の写真が本人の同意なく加工され、性的に見える画像としてSNS「X（旧Twitter）」上で拡散する事例が相次いだ。年末年始のタイミングで海外メディアが一斉に報じ、国際的な問題として注目を集めた。</p>
<p><a href="https://www.reuters.com/technology/french-ministers-report-groks-sex-related-content-x-platform-prosecutors-2026-01-02/">Reuters</a> は2026年1月3日（現地時間）、年越し前後にX上でGrokの画像編集機能を用いた投稿が急増したと報道した。他人が投稿した写真に対し、テキストで指示を与えることで人物の服装を変更し、ビキニ姿など性的に見える形へ加工した画像が生成され、公開リプライなどを通じて広く拡散したという。</p>
<p>こうした投稿は特定の著名人に限らず、一般の女性や若年層とみられる人物の写真にも及んだ。生成された画像はX上で即座に表示され、年末年始の利用増加と相まって、短期間のうちに問題が可視化・拡大したとされる。</p>
<p>英紙<a href="https://www.theguardian.com/technology/2026/jan/02/elon-musk-grok-ai-children-photos">The Guardian</a>もこの問題を取り上げた。同紙は、Grokによって未成年を含む人物が「薄着」の状態に加工される画像が生成された事例が確認されたと報じ、非同意の性的表現や児童保護の観点から懸念が高まっていると伝えた。</p>
<p><strong>xAIはGrokをめぐり、安全対策の不備を認め、CSAMは違法で禁止されているとX上で表明した</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/grok_x_9767524026/grok_x_9767524026.jpg" alt="grok x.jpg" /></p>
<p>事態の深刻化を受け、フランスでは年始早々、閣僚がGrokによる性的コンテンツ生成を問題視し、検察当局に通報した。Reutersは、欧州連合（EU）のデジタルサービス法（DSA）との関係も含め、AIを組み込んだプラットフォームの責任を問う動きが各国で強まっていると報じている。</p>
<p>Grokを提供するxAI側は、安全対策に不備があったことを認め、是正措置を進めているとされる。一方、画像編集を含む生成AI機能が急速に一般ユーザーへ開放される中、年末年始に顕在化した今回の問題は、非同意の加工や悪用をどこまで防げるのかという課題を改めて浮き彫りにした。</p>
<p>こうした海外での一連の報道を受け、日本国内でも年末年始にかけてこの問題が紹介され、生成AIの利便性とリスクをめぐる議論が広がった。年の変わり目に一気に表面化したGrokをめぐる騒動は、生成AIとSNSが結びつくことで生じる影響の大きさを示す事例となっている。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>量子コンピュータとAIの「リアルな現在地」から「SF的未来」へ：第一人者・大関真之教授に聞く</title>
      <link>https://ledge.ai/articles/interview_prof_ohzeki</link>
      <description><![CDATA[<p>:::button
<a href="https://25to26.ledge.ai/">Ledge.ai年末年始特集サイトはこちら</a>
:::</p>
<p>空前の生成AIブームが世界を席巻する中、技術界の視線は次なる地平へと注がれている。その筆頭に挙げられるのが「量子コンピュータ」だ。メディアではその驚異的な計算能力が頻繁に報じられ、AIの進化を加速させる究極の鍵として、大きな期待が寄せられている。しかし、その輝かしい未来像と研究の最前線にある現実との間には、無視できない大きな隔たりが存在する。</p>
<p>この複雑な状況の真実を探るべく、量子コンピュータ研究の第一人者であり、東北大学・東京科学大学で教鞭をとる大関真之教授に話を伺った。量子アニーリング方式の研究で知られる大関教授だが、あまり知られていない事実がある。<strong>「ちなみに僕は最初はゲート（方式）なんですけどね。みんな知らないんですけど…アニーリングをやれって上司から命令されたからやるしかなかっただけなんです」</strong> と彼は明かす。この実務から生まれた両方式への深い知見は、分野全体を俯瞰する、他に類を見ない冷静かつ鋭い視点を与えている。</p>
<p>本記事は、単なる技術礼賛ではない。専門家の視点から、量子コンピュータが直面する「不都合な真実」、AIとの融合における「致命的な課題」、そしてそれらを超えた先に見える「SF的な未来」の可能性まで、客観的かつ深く掘り下げていく。</p>
<p>※インタビューは2025年11月5日にオンラインで行われた。</p>
<h2>1. 幻想を打ち砕く：量子コンピュータの「リアルな現在地」</h2>
<p>現在のメディア報道や世間の期待と、研究現場における量子コンピュータの実際の性能との間には、大きなギャップが存在する。まずその幻想を打ち砕き、専門家が見る「リアルな現在地」を明らかにすることから始めたい。大関教授の言葉は、この分野に漂う熱狂に冷や水を浴びせるかもしれないが、真の可能性を理解するためには不可欠な第一歩である。</p>
<h3>1.1. ゲート方式の厳しい現実：「ノイズだらけで使い物にならない」</h3>
<p>現在、量子コンピュータの主流として開発が進められている「ゲート方式」。しかし、大関教授はその現状を <strong>「ただの乱数生成器の域を超えない」</strong> と厳しく評価する。</p>
<p>メディアでは量子ビット数が飛躍的に増加しているというニュースが頻繁に流れるが、教授は <strong>「実際には積極的な利用にはかなりハードルがある」</strong> と一蹴する。問題の本質は、計算過程で発生するノイズ（エラー）にある。量子ビットを増やしても、それに伴ってエラーが積み重なり、最終的な計算結果は信頼できないものになってしまうのだ。</p>
<p>ユーザビリティも絶望的だ。これは学術的なツールに限った話ではない。大関教授は、ある企業の有料商用サービスにアクセスしようとした際の経験を語る。「<strong>数日とかジョブ待ち</strong> 状態」。研究開発のツールとしてすら実用レベルに達していないのが実情だ。ニュースで語られる華々しい成果と、研究者が直面する現実は、残念ながら大きく乖離している。念の為補足すると、その企業がまだ量子コンピュータを大量に並べて多くのジョブを捌ける状態にないのだから仕方のないことではある。</p>
<h3>1.2. アニーリング方式の可能性と限界</h3>
<p>ゲート方式とは異なるアプローチをとる「量子アニーリング方式」は、実用面で一定の評価を得ている。大関教授も <strong>「ジョブ遅延が全くない」「普通にAPIを叩くとちゃんと答えは返ってくる」</strong> と、その安定性を認める。これは量子アニーリングマシンについては早期に開発が進み、周りのシステムや多数のマシンでカバーをしていることによる。</p>
<p>しかし、これもまた「組合せ最適化問題」を劇的に速く解く銀の弾丸ではない。大関教授は、「何も考えずに実行して、<strong>組合せ最適化問題を早く解くなんていうのは基本ないですね</strong> 」と断言する。これらの問題は「NP困難」という性質を持ち、本質的に <strong>「どんな手段を使っても指数関数的に時間がかかる」</strong> からだ。量子アニーリングを使ったとしても、この原理を覆すことはできない。</p>
<p>それでも、教授はこの方式に悲観的なわけではない。CPUがクロック周波数を上げることで高速化してきたように、量子アニーリングマシンを動かすQPU（Quantum Processing Unit）の「1ステップ」そのものが質的に向上することで、性能が向上する”伸びしろ”に期待を寄せている。<strong>「技術としてはまだまだ伸びしろはあるだろうな」</strong> と、その可能性を冷静に見据えているのだ。</p>
<p>ここまでで明らかになったのは、量子コンピュータのハードウェアが依然として発展途上であり、ソフトウェアや応用を語る以前に、ノイズや計算原理といった根本的な課題を抱えているという事実である。この厳しい現実を踏まえた上で、次にAI分野で特に期待の高い「量子機械学習」の議論へと進みたい。</p>
<h2>2. 量子機械学習（QML）の落とし穴：見過ごされてきた2つの致命的課題</h2>
<p>ハードウェアが手強い物理的挑戦に直面しているとすれば、ソフトウェア、特に大きな期待を集める「量子機械学習（QML）」の分野は、それ自体が存亡に関わる危機に瀕している。大関教授によれば、この分野は歴史の繰り返しというべき隘路にはまり込み、実用化を阻む2つの致命的な課題から抜け出せずにいるという。</p>
<h3>2.1. 課題1：勾配消失問題という「歴史の再演」</h3>
<p>QMLが直面する一つ目の大きな壁は、古典的なディープラーニングがかつて乗り越えた「勾配消失問題」の再来だ。ディープラーニングの黎明期、ニューラルネットワークの層を深くすると学習が進まなくなるという問題に直面した。層を重ねるごとに勾配情報が失われることが原因だったが、「ReLU」という活性化関数の登場で劇的に改善された歴史がある。驚くべきことに、量子回路でも全く同じことが起きている。</p>
<p>量子回路における非線形変換は、本質的に三角関数（sinやcos）の組み合わせで表現される。これらの関数の微分値は1以下であるため、回路が複雑になる（層が深くなる）と、勾配情報は指数関数的にゼロに収束してしまう。これは <strong>「Barren Plateau（不毛な台地）」</strong> 問題として知られている。</p>
<p>さらに深刻なのは、古典的なニューラルネットワークと違い、<strong>「自分で勝手にその非線形変換部分を変えられない」</strong> ことだ。量子力学の原理に縛られるため、ReLUのような都合の良い解決策を自由に設計できない。いわば <strong>「量子回路を使うっていう縛りプレイ」</strong> の中で、歴史的な課題と再び向き合わなければならないのだ。</p>
<h3>2.2. 課題2：「くっそ遅い」データ入力のボトルネック</h3>
<p>理論的な課題以上に、より現実的で致命的なのがデータ入力のボトルネックだ。現代の機械学習は、大量のデータを高速に処理することを大前提としている。しかし、QMLの現実はその真逆にある。大関教授は、その遅さを「くっそ遅い」と一言で表現する。</p>
<p>データを一つ処理するたびに、QPUに情報を書き込み、その応答を待つというプロセスを繰り返さなければならない。
<strong>「データ数1個分に対してパラメータの変更があるので量子回路を調整して再入力する必要がある。計算時間よりもそのボトルネックが大きい」</strong>
これは、ミニバッチ学習どころか、1つのデータで勾配を1回計算するのに途方もない時間がかかることを意味する。ビッグデータを扱う現代の機械学習の手法とは、全く相容れないのが現状だ。</p>
<p>これらの課題により、QMLの研究は <strong>「大きくは進化していない」</strong> と大関教授は指摘する。しかし、この行き詰まりは、視点を変えることで新たなパラダイムへの扉を開くきっかけにもなる。</p>
<h2>3. 新たなパラダイム：「次世代GPU」としての量子コンピュータ</h2>
<p>これまでの厳しい指摘から一転し、大関教授は量子コンピュータの新たな捉え方を提示する。それは、単なる「万能で高速な計算機」という幻想から脱却し、その本質を見極めることで見えてくる未来像だ。この視点の転換は、将来のコンピューティングアーキテクチャに大きなインパクトを与える可能性を秘めている。
　この結論は、行き詰まりの中から生まれた「原点回帰」の思考から導かれた。教授は、最も根源的な問いを自らに投げかけたという。<strong>「そもそも、量子コンピュータとは何をするマシンなんだろう？」</strong></p>
<ul>
<li>
<p><strong>本質の再定義</strong>　その答えは、量子力学の基礎方程式である「シュレーディンガー方程式」のシミュレーションにある。数学的に言えば、それは <strong>「巨大な行列とベクトルの掛け算を実行するマシン」</strong> に他ならない。</p>
</li>
<li>
<p><strong>GPUとの対応関係</strong>　この本質は、現代のAIを支えるGPUの役割と驚くほど酷似している。GPUもまた、その中核機能は膨大な線形代数演算（行列計算）の実行だ。ここから、<strong>「将来的にはGPUとちょうど対応する置き換えが行われる部分は結構あるんじゃないか」</strong> という未来像が導き出される。量子コンピュータは、特定のタスクに特化した「次世代のGPU」、あるいはGPUと相補的に機能する新しいプロセッサになり得るのだ。</p>
</li>
<li>
<p><strong>最大のメリット省電力性</strong>　なぜGPUの代替が必要なのか？その答えこそ、このパラダイムにおける量子コンピュータの最大の武器、すなわち圧倒的な電力効率にある。<strong>「QPUベースの量子コンピューターはやっぱり省電力性に優れているっていうことが最大のメリットですよね」</strong> と大関教授は強調する。AIモデルの巨大化に伴い、その消費電力が社会的な課題となりつつある今、同じ線形代数演算を根本的に異なる原理で、より効率的に実行できるアーキテクチャは、「あれば便利なもの」から「必要不可欠なもの」へと変わる可能性がある。</p>
</li>
<li>
<p><strong>未来のアプリケーション</strong>　この視点に立つと、有望な応用分野も見えてくる。特に、金融市場の株価変動のように、時間と共に変化し、<strong>「非局所的な相関」</strong> を持つシステムのシミュレーションに威力を発揮する可能性がある。<strong>「今までの常識的な力学だと近くのものしか変化しないんですが、（量子力学では）全然違う場所のところに『俺はこんな状況なんだけど、あなたはどんな状況？』という関係を扱うことが重要になる」</strong> と教授は説明する。一見無関係に見える遠くの事象が連動する複雑なダイナミクスを捉える能力は、従来のモデルでは不可能だった予測を可能にするかもしれない。</p>
</li>
</ul>
<p>量子コンピュータを「何でもできる魔法の箱」ではなく、特定のタスクに特化した「新しいアーキテクチャのプロセッサ」として捉え直すこと。この視点こそが、現実的な応用への道を切り拓く鍵となるのかもしれない。そしてこの思索は、さらに根源的な問いへと繋がっていく。</p>
<h2>4. SF的未来への展望：量子が拓く「真のAI」への道筋</h2>
<p>ここからは、より長期的で思索的なテーマへと足を踏み入れる。現在のLLM（大規模言語モデル）が持つ本質的な限界と、その先にある「意識」や「自己」を持つAIの実現に、量子コンピュータはどう関わるのか。大関教授と共に、根源的な問いを探求する。</p>
<h3>4.1. LLMに「足りないピース」と量子の役割</h3>
<p>現在のLLMは、人間が一生かかっても触れられないほどの膨大なデータを学習し、驚くほど人間らしい対話を行う。しかし、多くの人が直感的に「何かが足りない」と感じているのも事実だ。その正体は何なのか。</p>
<p>大関教授は、問題はデータ量ではなく「モデルが悪い」ことにあると指摘する。<strong>「人間の思考の結果として表出した言語や画像を確率分布で表現したのが生成モデルですが、その先、到達できない何かがあるはずなんです」</strong> 。現在のAIモデル（アーキテクチャ）では表現しきれない、何か根本的な構造が存在するのではないか、という問いだ。</p>
<p>ここで、量子の役割が浮上する。Googleが「量子超越性」を実証した際に見せたのは、あるタスク、すなわち特殊な確率分布からのサンプリングが <strong>「量子コンピュータだと簡単にできるけど、スーパーコンピュータだと（事実上不可能なくらい）大変」</strong> という事実だった。</p>
<p>もし、人間の思考や意識、あるいは共感といった複雑な現象が、現在のAIモデルでは近似できない「特殊な確率分布」によって支配されているとしたらどうだろうか。その「足りないピース」を表現する能力を、量子コンピュータだけが持っているのかもしれない。大関教授は、このSF的な可能性について <strong>「もちろんその可能性は否定するものではないですよね」</strong> と、開かれた姿勢を見せる。</p>
<h3>4.2. なぜ巨大テックは投資するのか：「アポロ計画」に見る本質</h3>
<p>実用化にはまだ30年かかるとも言われる量子コンピュータに、なぜGoogleやNVIDIAのような巨大テック企業は巨額の投資を続けるのか。その答えを、大関教授は「アポロ計画」とのアナロジーで鮮やかに解説する。
<strong>「『月に行くことに何の意味があるんだ』という話で。だけど月に行くためにロケットを開発し、ロケット燃料を開発し、宇宙服を開発して…。目標の周りでいろんなチャレンジが進むんですよね。」</strong></p>
<p>「月に行く」という壮大で、それ自体は直接的な利益を生まないかもしれない目標が、ロケット素材、生命維持技術、燃料技術といった数多くの副次的な技術革新を生み、産業全体を押し上げた。</p>
<p>同様に、「量子コンピュータを作る」という挑戦的な目標が、超電導技術、極低温冷却技術、材料科学、精密測定技術といった、幅広い周辺分野全体の進歩を牽引する、健全な研究開発のドライバーとなっているのだ。巨大テック企業の投資は、単一の成果だけでなく、この技術的波及効果全体に向けられている。それは、未来のコンピューティングの覇権をかけた、壮大な布石なのである。</p>
<h2>5. 未来を創るために：研究者の信念とビジネスパーソンへの提言</h2>
<p>インタビューの最後に、我々は技術論を超え、未来を創り出す当事者としての大関教授個人の研究哲学と、これからの時代を生きるビジネスパーソンへのメッセージを伺った。</p>
<h3>5.1. 今ある技術で価値を創出する</h3>
<p>大関教授の研究者としての姿勢は、一つの哲学に貫かれている。それは <strong>「今あるもので、多少しょぼくても価値を創造する」</strong> という信念だ。</p>
<p>その象徴的なエピソードが、東日本大震災の際の取り組みだ。当時、まだ黎明期にあった量子アニーリングマシンを使い、津波からの避難経路を最適化する問題に取り組んだ。多くの研究者がマシンのベンチマークテストに終始する中、不完全な技術であっても社会課題の解決に繋げようとするその姿勢は、理論だけでなく実践を重んじる研究者としての矜持を示している。
<strong>「今あるもので多少しょぼくても価値を創造するっていうのを率先してやるっていうのが、昔も変わらないし、これからも続けてやっていこうかなっていうのは思いますね。」</strong></p>
<h3>5.2. 10年先を見据える思考法</h3>
<p>最後に、大関教授から、未来を見据えるビジネスパーソンへ力強いアドバイスが送られた。</p>
<p><strong>「生成AIとかLLMとか、今の時代はもうある意味あらゆるものがやればできる、やればできそうというか、調べたりやってみたりすればあらかた分かるように、できるようになった。だけどこの先のことは創り続ける必要があるわけですから、自分たちがそのリードに立つとか、先駆者になるためには、もう少し先のこと（10年、20年先）を想像しよう、創造しようと考えるようにするといいのかなと思いますね。」</strong></p>
<p>現在のトレンドを追いかけるだけでは、真の先行者にはなれない。誰も答えを知らない、不確実で困難な未来の領域にこそ、次の時代の勝機がある。厳しい現実を直視しつつも、その先にある壮大な可能性を信じて考え続けること。それが、不確実な未来を切り拓くための唯一の方法なのかもしれない。</p>
<h2>おわりに</h2>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/25to26_thumb_ced6fcf9a3/25to26_thumb_ced6fcf9a3.png" alt="25to26_thumb.png" /></p>
<p>Ledge.ai年末年始特集では、読者に向けて、2025年のAI関連の重要トレンドを振り返り、また2026年以降のAIの展望について発信している。新しい年へ動き出すための情報が詰まっているので、ぜひ以下ボタンより特集サイトをご覧いただきたい。</p>
<p><strong>【コンテンツ情報】</strong>
無料登録を行うと、これらすべての記事を閲覧できるようになる。
■ 特別インタビュー
京都賞受賞のAIのパイオニア甘利俊一先生をはじめ、量子コンピュータ/量子機械学習からロボット基盤モデル、話題の”PLURALITY”、NVIDIA、AMDなど、”いま読んでおくべき”インタビューが満載
■ 2025年のAI動向総ざらい
■ 厳選注目記事49本</p>
<p>:::button
<a href="https://25to26.ledge.ai/">Ledge.ai年末年始特集サイトはこちら</a>
:::</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>生成AIは「実装フェーズ」へ──ディープラーニング協会・松尾豊理事長が年頭所感で示した2026年のAI論点</title>
      <link>https://ledge.ai/articles/jdla_new_year_message_2026</link>
      <description><![CDATA[<p>2026年1月5日、日本ディープラーニング協会（JDLA）は年始にあたり、理事長で東京大学大学院工学系研究科教授の 松尾豊氏による<a href="https://www.jdla.org/news/260105001/">年頭所感</a>を公表した。生成AIの社会実装が急速に進む中、同所感では2025年の技術的動向を振り返るとともに、2026年に向けたAI活用、人材育成、制度整備の方向性が整理されている。</p>
<p>年頭所感では、2025年を「生成AIが研究や実証の段階を越え、実践的な活用フェーズに入った年」と位置づけた。企業活動や教育、行政など、幅広い分野で生成AIの導入が進みつつあり、特定用途にとどまらない汎用的な技術基盤としての役割が強まっているという。</p>
<p>生成AIはもはや一部の先進的な現場だけの技術ではなく、社会全体を支えるインフラに近い存在になりつつあるとの認識が示された。</p>
<h2>AIエージェントとフィジカルAIの進展</h2>
<p>技術面では、AIエージェントが業務プロセスに組み込まれ始めている点や、実世界と連動するフィジカルAIの進展に言及した。モデル性能の向上に加え、AIが人の業務や現場環境とどのように結びつくかという「使われ方」の変化が顕在化していると整理している。</p>
<p>あわせて、大規模投資やインフラ整備の動きにも触れ、データセンター整備などを含む産業基盤の強化が進んでいる現状を示した。</p>
<h2>生成AIを巡る制度と国際環境の変化</h2>
<p>生成AIの普及に伴い、著作権や倫理などの社会的課題が顕在化している点にも触れられている。海外では新興AI企業の台頭や市場環境の変化が見られ、国際競争が激化しているとした。</p>
<p>国内ではAI関連法制の整備が進み、イノベーションの促進とリスク対応の両立を図る枠組みが整いつつあることが紹介されている。</p>
<h2>AI人材育成と資格制度の役割</h2>
<p>人材面では、JDLAが実施するG検定やE資格といった資格制度に言及した。これらを通じてAIに関わる基礎的・専門的知識を持つ人材の裾野が広がっており、高専DCONなどの実践的な教育施策も含め、人材育成基盤が拡充しているとした。</p>
<p>AI技術の社会実装を支えるためには、技術者だけでなく、AIを理解し活用できる多様な人材の育成が不可欠であるとの認識が示されている。</p>
<h2>2026年に向けて</h2>
<p>年頭所感の締めくくりでは、2026年に向けて「学びと信頼の循環」をさらに広げていく方針が示された。AIと共に成長できる社会の実現を目指し、引き続き産業界・教育機関・行政との連携を進めていくとしている。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>JKT48、生成AIによるメンバー画像の性的加工に公式警告──48時間以内の削除要請、法的措置も示唆</title>
      <link>https://ledge.ai/articles/jkt48_ai_generated_image_sexual_abuse_warning</link>
      <description><![CDATA[<p>年末年始にかけて、生成AIを使い実在人物の画像を本人の同意なく性的に加工・拡散する事例が相次ぐ中、秋元康氏が総合プロデューサーを務めるインドネシアのアイドルグループ JKT48 の運営事務局は2026年1月5日（現地時間）、同様の被害がメンバーに及んでいるとして公式声明を<a href="https://x.com/officialJKT48/status/2007856963942748375">発表</a>した。運営は、不適切なAI生成コンテンツの即時削除と拡散停止を求め、応じない場合には法的措置を取る可能性があると明らかにした。</p>
<h2>生成AIによる不適切コンテンツの作成・流通を確認</h2>
<p>公式Xに掲載された声明によると、運営は複数の報告を受け、メンバーの顔やアイデンティティを用いた不適切、あるいはポルノ的なAI生成コンテンツが作成・拡散されている事例を確認したという。声明では、こうした行為が名誉毀損や侮辱に該当する可能性があると指摘している。</p>
<h2>48時間以内の削除要請と法的対応の可能性</h2>
<p>運営は、声明公開後 2×24時間（48時間）以内 に、該当コンテンツの掲載停止、削除、ならびに恒久的な削除対応を行うよう関係者に要請した。期限内に対応がなされず、コンテンツの流通が継続した場合には、影響を受けたメンバーが法的手続きを取る判断を支持し、運営として法的支援を行う方針も示している。</p>
<h2>対象となり得る行為を具体的に列挙</h2>
<p>声明では、法的対応の対象となり得る行為として、AIを用いたポルノ的コンテンツの制作に加え、その配布・拡散・宣伝、さらにそれらを支持・助長・拡散する投稿やコメントも含まれると明記された。運営は、管理下にあるすべての関係者の安全、尊厳、権利を守ることへの強い姿勢を示している。</p>
<p>JKT48運営は、ファンや関係者に対し、健全で相互に尊重し合う環境づくりへの協力を呼びかけた。生成AIの普及が進む中、著名人やアイドルを対象としたAI悪用への対応について、運営レベルで明確な方針が示された形となる。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/jkt48_x_d95c00ae48/jkt48_x_d95c00ae48.jpg" alt="jkt48 x.jpg" /></p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>感情を削ると推論力も落ちる：AIの成績低下が示した「感情」の役割──中国・天津大学、感情を組み込む新世界モデル「LEWM」を提案</title>
      <link>https://ledge.ai/articles/large_emotional_world_model_emotion_reasoning_ai</link>
      <description><![CDATA[<p>AIから感情に関する情報を取り除くと、感情理解タスクだけでなく、知識問題や推論問題といった一見無関係に見える課題でも成績が低下することが明らかになった。</p>
<p>中国・天津大学の研究チームは、感情が人間の行動や意思決定を左右する重要な要素である点に着目し、感情を明示的に扱う新たな世界モデル「<a href="https://arxiv.org/abs/2512.24149">Large Emotional World Model（LEWM）</a>」を提案した。研究成果は2025年12月30日、論文「Large Emotional World Model」としてarXivに公開された。</p>
<h2>感情を無視すると「合理的だが現実的でない」予測になる</h2>
<p>研究チームはまず、既存の大規模世界モデルに感情を除去するモジュールを組み込み、モデル性能への影響を検証した。その結果、感情や感情分析タスクでは最大8％の精度低下が確認された一方、常識推論ベンチマーク「HellaSwag」や知識理解ベンチマーク「MMLU」でも1〜3％程度の成績低下が見られた。</p>
<p>論文では、落ち込んだ気分の人が衝動買いをする例を挙げ、物理法則や合理性だけを前提とした世界モデルでは、人間の実際の行動を正確に予測できないと指摘している。
感情はノイズではなく、推論全体を調整する「文脈的な信号」として機能していることが示唆された。</p>
<h2>感情を検出・中和する「感情フィルタリング」実験</h2>
<p>研究チームは予備実験として、感情表現を検出・除去する「感情フィルタリングモジュール」を設計した。
このモジュールは、</p>
<ul>
<li>文に感情が含まれるかを判定する<strong>Affect Classifier</strong></li>
<li>感情語を中立表現に置き換える<strong>Affect Adapter</strong></li>
</ul>
<p>から構成され、感情認識と感情中和を同時に学習する二段階方式で訓練されている。</p>
<p>感情を中和したモデルでは、感情タスクの成績が大きく低下するだけでなく、非感情タスクにも影響が及ぶことが確認され、感情情報が広範な認知処理に関与していることが実証されたという。</p>
<p><strong>■ 感情フィルタリングモジュール（予備実験）</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Large_Emotional_World_Model2_d384c0ddce/Large_Emotional_World_Model2_d384c0ddce.png" alt="Large Emotional World Model2.png" /></p>
<h2>感情・行動・状態を結ぶEWHデータセット</h2>
<p>研究では、新たに「Emotion-Why-How（EWH）」と呼ばれるマルチモーダルデータセットも構築された。
EWHは、以下の要素を1つの因果構造として結び付ける点が特徴だ。</p>
<ul>
<li>状態（映像・音声・画像）</li>
<li>感情状態</li>
<li>行動</li>
<li>行動後の状態と感情変化</li>
</ul>
<p>例えば、
「落ち込んだ状態 → 服を買う → 一時的に気分が良くなる」
といった感情を介した状態遷移を、映像・音声・テキスト情報とともに学習できる構造になっている。</p>
<p><strong>■ EWH データセットの概要</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Large_Emotional_World_Model3_72a2e46f4c/Large_Emotional_World_Model3_72a2e46f4c.png" alt="Large Emotional World Model3.png" /></p>
<h2>感情を内包する世界モデル「LEWM」</h2>
<p>LEWMは、従来の世界モデルに感情状態の予測と遷移を組み込んだ点が特徴だ。現在の状態・行動・感情を同時に入力し、次の世界状態と感情の両方を予測する。</p>
<p>論文では、</p>
<ol>
<li>まず感情がどう変化するかを推定</li>
<li>その感情を前提に次の世界状態を予測</li>
</ol>
<p>という因果順序を明示的に設計している。</p>
<p><strong>■ LEWMモデルアーキテクチャの図解</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Large_Emotional_World_Model4_4c9f9b4c2e/Large_Emotional_World_Model4_4c9f9b4c2e.png" alt="Large Emotional World Model4.png" /></p>
<p>実験の結果、LEWMは感情に左右される人間行動の予測精度を向上させつつ、基本的な世界モデルタスクでは従来モデルと同等の性能を維持した。</p>
<h2>感情は「付加情報」ではなく、世界理解の一部</h2>
<p>研究チームは、感情を世界モデルに組み込むことで、より人間に近い行動予測や社会的シミュレーションが可能になると結論付けている。感情を排除した合理的なモデルではなく、感情を含めて世界を理解するモデルが、今後のAIに求められる方向性であることを示す研究といえる。</p>
<p>:::box
[関連記事：AIの\</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>NVIDIA、フィジカルAIと自動運転AIを統合展開──ロボットと車両向けに新モデル群</title>
      <link>https://ledge.ai/articles/nvidia_physical_ai_autonomous_driving_integrated_models</link>
      <description><![CDATA[<p>NVIDIA は2026年1月5日（現地時間）、米ラスベガスで開催中のCES 2026に際し、現実世界で動作する人工知能「フィジカルAI」に向けた新たなAIモデル群を<a href="https://blogs.nvidia.co.jp/blog/nvidia-releases-new-physical-ai-models-as-global-partners-unveil-next-generation-robots/">発表</a>した。</p>
<p>ロボット向けの基盤モデルに加え、安全な推論に基づく自動運転車両開発を支援するオープンソースAI「<a href="https://nvidianews.nvidia.com/news/alpamayo-autonomous-vehicle-development">Alpamayo</a>」ファミリーも同時に公開し、ロボットと車両の双方を対象としたAI基盤の統合展開を進める。</p>
<h2>フィジカルAIを中核に据えたAI基盤を拡充</h2>
<p>NVIDIAが掲げるフィジカルAIは、ロボットや車両といった物理的な存在が、周囲の環境を認識し、状況に応じて判断・行動するためのAI技術を指す。今回の発表では、視覚認識、行動生成、推論といった要素を統合した複数の基盤モデルが示され、現実世界での多様なタスクに対応する汎用的なAI基盤の整備を進める姿勢を打ち出した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/physical_ai_models_1280x680_1_960x510_9fb39ada11/physical_ai_models_1280x680_1_960x510_9fb39ada11.jpg" alt="physical-ai-models-1280x680-1-960x510.jpg" /></p>
<h2>自動運転向けオープンAI「Alpamayo」を公開</h2>
<p>こうしたフィジカルAIの重要な応用分野として位置づけられるのが自動運転だ。NVIDIAは今回、自動運転車両向けのオープンソースAIモデルとツール群からなる「Alpamayo」ファミリーを<a href="https://nvidianews.nvidia.com/news/alpamayo-autonomous-vehicle-development">発表</a>した。Alpamayoは、安全性が強く求められる自動運転開発を想定し、周囲の状況を理解したうえで判断を行う「リーズニング（推論）」能力を重視した設計が特徴とされる。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/nvidia_alpamayo_d1b1006f40/nvidia_alpamayo_d1b1006f40.jpg" alt="nvidia alpamayo.jpg" /></p>
<p>Alpamayoファミリーには、学習済みモデルに加え、シミュレーション環境やデータセットが含まれており、開発者や企業が共通基盤上で検証や改良を進められるよう設計されている。想定される用途には、レベル4自動運転の研究・開発や、現実の走行環境で発生し得る多様なシナリオへの対応などが含まれる。</p>
<p>@<a href="https://www.youtube.com/watch?v=KGCTwoAlhsM">Youtube</a></p>
<p>今回の発表は、ロボット向けAIと自動運転向けAIを個別に展開するものではなく、共通するフィジカルAI基盤のもとで横断的に提供する点に特徴がある。NVIDIAは、モデル、ソフトウェア、シミュレーション環境を含む包括的なAI基盤を通じて、ロボットと車両の両分野における開発を同時に支援していく方針を示した。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>NVIDIA、次世代AI基盤「Rubin」をCESで正式発表──GPU「Rubin」と新CPU「Vera」を含む6チップ構成、推論コストは最大10分の1に</title>
      <link>https://ledge.ai/articles/nvidia_rubin_vera_ces_2026</link>
      <description><![CDATA[<p>NVIDIAは2026年1月5日（米国時間）、CESの開催に際し、AIデータセンター向けの次世代プラットフォーム「Rubin」を<a href="https://nvidianews.nvidia.com/news/rubin-platform-ai-supercomputer">発表</a>した。</p>
<p>新たに設計したCPU「Vera」とGPU「Rubin」を中核に、6種類の半導体を極めて緊密に統合した構成が特徴で、同社は従来世代「Blackwell」と比べ、AIの学習および推論にかかるコストを大幅に削減できるとしている。</p>
<p>Rubinプラットフォームは、NVIDIA Vera CPU、NVIDIA Rubin GPU、NVLink 6 Switch、ConnectX-9 SuperNIC、BlueField-4 DPU、Spectrum-6 Ethernet Switchの6つの新チップで構成される。チップ単体の性能向上に依存するのではなく、CPU、GPU、ネットワーク、ストレージ、セキュリティまでを含めた“プラットフォーム全体”を同時に設計する「エクストリーム・コードザイン」を採用した点が特徴だ。</p>
<p><strong>■ CESの基調講演で、NVIDIAのCEOであるジェンスン・フアン氏が次世代AI基盤「Rubin」を構成する主要コンポーネントを紹介した</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/jensen_vera_rubin2_467cfbac7b/jensen_vera_rubin2_467cfbac7b.jpg" alt="jensen vera rubin2.jpg" /></p>
<p>Rubinという名称は、銀河の回転速度を観測し、ダークマターの存在を示した米国の天文学者ヴェラ・ルービン氏に由来する。NVIDIAは、同氏が宇宙観を大きく変えた存在であることになぞらえ、AIコンピューティングの新たな基盤となるプラットフォームにその名を冠したとしている。</p>
<p><strong>■ CESの基調講演で示された、天文学者ヴェラ・ルービン氏と銀河回転曲線の図。新プラットフォーム「Rubin」は、同氏にちなんで命名された</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/vera_rubin2_0fca679ec3/vera_rubin2_0fca679ec3.jpg" alt="vera rubin2.jpg" /></p>
<p>Rubin GPUは、第3世代のTransformer Engineを搭載し、推論処理に最適化された演算性能を特徴とする。NVIDIAは、演算精度を動的に調整する仕組みにより、推論性能と効率の両立を図っていると説明している。</p>
<p><strong>■ NVIDIAがCESで示した「Rubin GPU」の主な仕様。推論性能やメモリ帯域などで、Blackwell世代からの大幅な向上が示されている</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/nvidia_rubin_cpu_a40d212a8b/nvidia_rubin_cpu_a40d212a8b.jpg" alt="nvidia rubin cpu.jpg" /></p>
<p>一方、新CPU「Vera」は、大規模AIファクトリー向けに設計されたNVIDIA独自のCPUだ。GPUとの高帯域接続を前提とした構成を採用し、従来の汎用CPUとは異なる役割を担うとしている。</p>
<p><strong>■ NVIDIAが新たに設計した「Vera CPU」の概要。88基のカスタムOlympusコアと、GPUとの高帯域接続を前提とした構成が示されている</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/nvidia_vera_cpu_c2ba958e3c/nvidia_vera_cpu_c2ba958e3c.jpg" alt="nvidia vera cpu.jpg" /></p>
<p>システム形態としては、72基のRubin GPUと36基のVera CPUを1ラックに統合した「Vera Rubin NVL72」と、8基のGPUを搭載する「HGX Rubin NVL8」を用意する。いずれもNVLink 6による高帯域接続を特徴とし、ラック単位でAIスーパーコンピューターとして機能する設計だ。</p>
<p>また、Rubinでは冷却や運用面の刷新も図られている。100％液体冷却を前提とした設計により、高密度化を進めつつ、組み立てや保守作業の効率化を実現したという。加えて、CPU、GPU、相互接続を横断してデータを保護するコンフィデンシャル・コンピューティング機能や、信頼性を高めるRAS（信頼性・可用性・保守性）機構も強化されている。</p>
<p>Rubinプラットフォームはすでに量産段階にあり、2026年後半からOEMやクラウド事業者を通じて提供される予定だ。Microsoftの次世代AIデータセンターや、CoreWeaveのAIクラウドなどでの採用も計画されており、NVIDIAはRubinを通じて、学習・推論の両面でAIインフラの次の世代を切り開くとしている。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>LLMは「同じ質問を2回」入力すると精度が上がる──Google研究者ら、プロンプト反復の効果を短報で報告</title>
      <link>https://ledge.ai/articles/prompt_repetition_improves_llm_accuracy</link>
      <description><![CDATA[<p>Googleの研究者らは、同一の質問文を2回連結して入力するだけで、大規模言語モデル（LLM）の回答精度が向上するとする研究成果を発表した。論文は短報「Prompt Repetition Improves Non-Reasoning LLMs」として2025年12月17日に arXivに<a href="https://arxiv.org/abs/2512.14982v1">公開</a>されており、推論（reasoning）を用いない設定において、主要LLMと複数のベンチマークで広範な改善が観測されたという。</p>
<h2>質問文を「そのまま2回」繰り返すだけ</h2>
<p>研究で提案された手法は、質問文を変更・補足するのではなく、同一のクエリをそのまま2回連結して入力するというものだ。例えば、通常はQと入力するところをQQとする。特別な指示文や追加のプロンプト設計は必要としない。論文では、この操作を \u003CQUERY\u003E を \u003CQUERY\u003E\u003CQUERY\u003E に変換するものとして説明している</p>
<p>LLMは因果言語モデルとして学習されており、トークンの並び順が注意（attention）の届き方に影響する。このため、質問文と文脈や選択肢の配置順（question-first / options-first）によって性能差が生じることが知られている。プロンプト反復は、各トークンが他のすべてのトークンを参照しやすくすることで、この差を緩和すると説明されている。</p>
<h2>非推論設定で顕著な改善、70条件中47で「有意に向上」</h2>
<p>実験は、各AIモデルの公式APIを用いて実施され、2025年2月から3月にかけて評価された。対象には、Gemini、GPT、Claude、DeepSeekといった複数の主要LLMが含まれている。具体的には、Gemini 2.0 Flash／Flash Lite、GPT-4o／GPT-4o-mini、Claude 3 Haiku／Claude 3.7 Sonnet、DeepSeek V3が評価対象となった。</p>
<p>論文では、7つのモデルと7つのベンチマークなどを組み合わせた計70条件で比較を行った。その結果、統計検定（McNemar検定、p\u003C0.1）の基準で47条件において性能が有意に改善し、性能が低下した条件はなかったとしている。</p>
<p><strong>推論を用いない設定におけるPrompt Repetitionとベースラインの精度比較。星印は統計的に有意な改善（p\u003C0.1）を示す。70条件中47で改善、悪化は確認されなかった</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/figure1_big2_202ffaa00c/figure1_big2_202ffaa00c.jpg" alt="figure1_big2.jpg" /></p>
<h2>生成トークン数・レイテンシは原則増えず</h2>
<p>論文では、回答精度が向上した一方で、生成トークン数やレイテンシ（応答時間）は多くの条件で増加しなかったと報告されている。反復は並列化可能なprefill（前処理）段階で完結するためだという。これは、生成プロセスそのものではなく、入力を読み込む段階で処理が完結するためだと説明されている。</p>
<p>ただし例外として、非常に長い入力や反復×3などの条件では、Claude系モデルでprefillが重くなり、レイテンシが増える場合がある点も明記されている。</p>
<h2>推論（step-by-step）を有効にした場合は「中立〜わずかに正」</h2>
<p>推論を促す設定（think step by step）では、プロンプト反復の効果は中立からわずかに正にとどまった。28条件中5勝・1敗・22引き分けで、研究者らは「推論モデルはそもそも推論過程の中で入力の再読・反復を行うため」と説明している。</p>
<p>一方で、入力文が非常に長い場合や、同じ質問を3回以上繰り返す設定では、モデルや条件によっては応答時間が増加する可能性も指摘されている。</p>
<h2>追加学習なしで性能を引き出す可能性</h2>
<p>この手法は、追加学習や外部ツールを必要とせず、出力形式も変えないため、既存システムにドロップインで導入可能だとされる。研究チームはこれを「多くのタスクにおけるデフォルト手法の候補」と位置付けている。研究チームは今後の方向性として、反復部分の最適化やKVキャッシュの扱い、非テキストモダリティへの応用などを挙げている。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>5億点の商品群の楽天市場にAIコンシェルジュ導入　エージェント型AI「Rakuten AI」を提供開始</title>
      <link>https://ledge.ai/articles/rakuten_ichiba_ai_concierge_rakuten_ai_launch</link>
      <description><![CDATA[<p>楽天グループは2026年1月5日、同社が運営する楽天市場のスマートフォンアプリに、エージェント型AIツールRakuten AIを搭載したと<a href="https://corp.rakuten.co.jp/news/press/2026/0105_02.html">発表</a>した。ユーザーとの対話を通じてニーズを理解し、商品選びを支援するAIコンシェルジュとして提供する。</p>
<p>導入により「楽天市場」のAIコンシェルジュがユーザーとの対話を通じてニーズを把握し、商品選定のサポートと新たな商品との出会いを生む「ディスカバリーショッピング体験」を提供するという。</p>
<h2>テキスト・音声・画像で条件を入力、対話しながら商品を探索</h2>
<p>ユーザーは、「楽天市場」スマートフォンアプリのホーム画面右下に表示されるアイコンから「Rakuten AI」にアクセスできる。希望予算、購入目的、活用シーンなどを、テキスト、音声、画像を用いて入力することで、欲しい商品を手軽に検索することが可能だ。</p>
<p>AIコンシェルジュからの質問に答えながら対話を進めることで、潜在的なニーズが明らかになりやすくなり、商品提案の精度が高まるとしている。これにより、「楽天市場」に出品されている約5億点の商品群の中から、条件や目的に合った商品を見つけやすくなる。</p>
<h2>商品情報に加え、トレンド情報も反映</h2>
<p>「Rakuten AI」が提案する商品情報には、「楽天市場」内の商品情報や価格比較情報に加え、気候や流行、社会情勢などのトレンドも反映される。これらの情報はウェブの自然検索結果をもとに取り込まれ、ユーザーが買い物をする際に必要な情報を包括的に提示するとしている。</p>
<p>今後は、楽天が運営するEコマースサービスから得られるマーケティングデータを活用し、ユーザーのニーズにより合致した商品提案を行う予定だ。提案精度の継続的な向上を通じて、一人ひとりにパーソナライズされた買い物体験の提供を目指す。</p>
<h2>「AI-nization」戦略の一環として位置付け</h2>
<p>楽天は、AI活用を意味する造語「AI-nization（エーアイナイゼーション）」を掲げ、ビジネスのあらゆる領域でAI活用を推進している。「Rakuten AI」は、楽天エコシステムにおける顧客体験の向上と日常生活のサポートを目的に設計されたエージェント型AIツールだ。</p>
<p>AIエージェントを通じてユーザーを楽天エコシステム内のさまざまなサービスにつなぎ、よりパーソナルな体験を提供することを目指しており、今回の「楽天市場」アプリへの搭載は、その展開における重要な一歩と位置付けられている。</p>
<p>楽天は今後も、「楽天市場」におけるAI活用の推進や「Rakuten AI」との連携を通じて、利便性が高く、楽しい買い物体験の提供を目指すとしている。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>Roborock、階段を上り下りできるロボット掃除機「Saros Rover」発表──CES 2026で公開</title>
      <link>https://ledge.ai/articles/roborock_saros_rover_stair_climbing_robot_vacuum_ces2026</link>
      <description><![CDATA[<p>中国のスマート家電メーカー Roborock は2026年1月6日（現地時間）、米ラスベガスで開催された CES 2026 において、階段を上り下りできるロボット掃除機 Roborock Saros Rover を<a href="https://newsroom.roborock.com/gl/news/ces-2026-roborock-releases-the-world-s-first-robotic-vacuum-with-wheel-leg-architecture-as-it-joins-hands-with-real-madrid-football-club-">発表</a>した。脚と車輪を組み合わせた独自構造を採用し、これまでロボット掃除機が対応できなかった階段環境での清掃を想定したモデルとして紹介されている。</p>
<p>Saros Roverは、脚の動きと車輪走行を組み合わせることで段差を乗り越える構造を特徴とする。Roborockはこの仕組みを「wheel-leg architecture」と呼び、階段や複雑な段差を含む住環境での移動と清掃を可能にすると説明している。従来のロボット掃除機では階段は落下リスクのある障害物として回避されてきたが、本モデルでは階段そのものを清掃対象として扱う点が特徴だ。</p>
<p>@<a href="https://www.youtube.com/watch?v=SuyBti4YC-Y">YouTube</a></p>
<p>同社は公式YouTubeチャンネルで、Saros Roverの動作を示すデモ動画も公開した。動画では、本体が階段の段差を認識し、脚部を使って一段ずつ姿勢を調整しながら上り下りする様子が示されている。移動と同時に階段表面を清掃する動作も確認でき、Roborockが想定する利用シーンを視覚的に示す内容となっている。</p>
<p>RoborockはSaros RoverをCES 2026ではコンセプトモデルとして紹介しており、現時点で発売時期や価格、量産計画などの詳細は明らかにしていない。階段環境での移動と清掃を想定したロボット掃除機として、その技術的方向性を示す展示となった。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>Samsung、Gemini連携で家電すべてをAI前提に再設計──CES 2026で「Family Hub」進化を発表</title>
      <link>https://ledge.ai/articles/samsung_gemini_ai_appliances_family_hub_ces_2026</link>
      <description><![CDATA[<p>Samsung Electronicsは2026年1月5日（現地時間）、世界最大級の家電見本市CES 2026の関連イベント「The First Look」において、同社のAI家電戦略を<a href="https://news.samsung.com/global/samsung-presents-your-companion-to-ai-living-at-the-first-look-during-ces-2026">発表</a>した。その中で、AI冷蔵庫「Bespoke AI Refrigerator Family Hub（以下、Family Hub）」のアップグレードを明らかにし、生成AI「Gemini」との連携を前面に打ち出した。冷蔵庫にとどまらず、AIを基盤とした家電・サービス全体の再設計を進める方針を示している。</p>
<p>@<a href="https://www.youtube.com/watch?v=hPdEMp1fOA4">YouTube</a></p>
<h2>CES 2026で示した「全家電AI前提」戦略</h2>
<p>SamsungはCES 2026で、「Your Companion to AI Living」というビジョンを掲げ、家庭内のさまざまな家電やサービスをAIを基盤とした形で再定義する構想を示した。家電を個別に高度化するのではなく、生活全体を支援するAI基盤として再設計することを目指す。
Family Hubは、その戦略を象徴する存在として位置付けられている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/samsung_your_companion_to_620a47a6ee/samsung_your_companion_to_620a47a6ee.jpg" alt="samsung your companion to.jpg" /></p>
<h2>Geminiと連携するAI Visionを強化</h2>
<p>刷新されたFamily Hubでは、Googleの生成AI「Gemini」と連携した「AI Vision（AI Food Manager）」機能を強化した。冷蔵庫内のカメラで食材を認識し、出し入れの状況を自動で把握することで、在庫管理をAIが担う。
Samsungは、こうした認識・判断・提案といったAIの役割を、冷蔵庫に限らず他の家電にも広げていく考えだ。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/AI_Vision_build_with_google_gemini_bc2a52f89f/AI_Vision_build_with_google_gemini_bc2a52f89f.jpg" alt="AI Vision build with google gemini.jpg" /></p>
<h2>食材管理から献立提案までをAIが自動化</h2>
<p>AI Visionで取得した食材データをもとに、Family Hubは利用可能な食材に応じたレシピや献立を提案する。あわせて、食材の使用傾向や消費状況をまとめる「FoodNote」機能も提供される。
家電がユーザーの行動を理解し、次の行動を提案するという設計思想が、冷蔵庫にも取り入れられた。</p>
<h2>家族単位で最適化されるUIと音声操作</h2>
<p>Family Hubは音声認識による「Voice ID」に対応し、家族それぞれを識別する。これにより、カレンダーやニュース、天気、健康関連情報などを、利用者ごとに最適化して表示できる。
また、音声アシスタント「Bixby」を用いた操作も強化され、冷蔵庫のドアを音声で開閉するハンズフリー操作にも対応する。</p>
<h2>冷蔵庫を起点に広がるAI家電の再設計</h2>
<p>Samsungは、今回のFamily Hubの進化を、全家電をAI前提で再設計する取り組みの一例と位置付けている。冷蔵庫を家庭内の情報と行動の起点とすることで、家電同士やサービスとの連携を拡張し、AIが日常生活を支援する環境の構築を進めるとしている。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>AGIは来ない、バブルは続かない──スタンフォード大 HAI研究所が示す2026年のAI、過剰期待の時代は終わり、評価フェーズへ</title>
      <link>https://ledge.ai/articles/stanford_hai_ai_2026_evaluation_phase</link>
      <description><![CDATA[<p>巨額投資と急速な技術進展が続いてきたAI分野は、2026年に転機を迎える可能性がある。Stanford Human-Centered AI Institute（HAI）は2025年12月15日（米国時間）、同研究所に所属する研究者らの予測をまとめた記事を<a href="https://hai.stanford.edu/news/stanford-ai-experts-predict-what-will-happen-in-2026">発表</a>し、AIをめぐる議論は「できるかどうか」から「どの程度役に立つのか」を問う評価フェーズへ移行するとの見方を示した。</p>
<p>同記事では、計算機科学、医学、法学、経済学など複数分野の研究者が共通して、過剰な期待や宣伝が先行してきたAI開発のあり方に転換点が訪れていると指摘している。</p>
<h2>AGIは2026年にも実現しない、AI主権が主要テーマに</h2>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/James20_Fall202022_aefa4ec674/James20_Fall202022_aefa4ec674.webp" alt="James20Fall202022.webp" /></p>
<p>HAI共同ディレクターで計算機科学教授のJames Landay氏は、2026年に汎用人工知能（AGI）が実現することはないと明言した。その上で、各国が自国のデータや計算資源を管理する「AI主権（AI Sovereignty）」への関心が急速に高まると予測している。</p>
<p>AI主権の形は一様ではなく、自国で大規模言語モデル（LLM）を構築するケースもあれば、他国が開発したモデルを自国内のGPU上で運用し、データを国外に出さない方式も含まれる。HAIでは、こうした複数の主権モデルを整理・分析する研究にも取り組んでいるという。</p>
<p>一方で、世界各地で進む大規模データセンター投資については、投機的な側面も指摘されている。Landay氏は、AI関連投資が無制限に拡大し続けるわけではなく、バブル的な様相が意識される局面に入るとの見解を示した。</p>
<h2>生産性向上は限定的、失敗するAIプロジェクトが増加</h2>
<p>2026年には、AIがもたらす生産性向上について、より冷静な評価が広がるとみられている。プログラミング支援やコールセンター業務など一部の領域では効果が確認される一方、多くのAI導入プロジェクトは期待した成果を上げられない可能性があると指摘された。</p>
<p>その結果、企業や組織は「AIをどこに適用すべきか」という選別を迫られ、成功確率の高い用途に資源を集中させる動きが強まるとみられる。</p>
<h2>巨大モデルの限界と、高品質データ重視への転換</h2>
<p>HAIの研究者らは、モデルの巨大化が必ずしも性能向上につながらなくなりつつある点にも言及している。データの量的枯渇や品質低下が課題となる中、より小規模でも高品質なデータセットを用いたモデル開発への関心が高まると予測されている。</p>
<p>この流れは、計算資源や環境負荷への懸念とも結びつき、AI開発の効率性を重視する方向性を後押しする可能性がある。</p>
<h2>医療・科学分野で進む「ブラックボックス」の解体</h2>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/russ_altman_1_be03daceca/russ_altman_1_be03daceca.webp" alt="russ_altman_1.webp" /></p>
<p>科学・医療分野では、AIモデルの予測精度だけでなく、なぜその結論に至ったのかを説明できることが強く求められるようになる。HAI上級フェローのRuss Altman氏は、高性能なニューラルネットワーク内部を解析し、重要な特徴や判断根拠を明らかにする研究が進展すると見ている。</p>
<p>医療分野では、自己教師あり学習の進展により、大規模かつ高品質な医療データを用いた基盤モデルが登場し、診断精度の向上や希少疾患への応用が広がる可能性も示された。</p>
<h2>法務、経済分野でも「測るAI」へ</h2>
<p>法務分野では、「文章を書けるか」ではなく、正確性やリスク、業務効率への寄与といった具体的な成果を評価する指標が重視される見通しだ。複数文書を横断して推論する高度なタスクに対応するAIの評価手法も整備されつつある。</p>
<p>また、経済分野では、AIが雇用や生産性に与える影響を職種・タスク単位で可視化する「AI経済ダッシュボード」が登場し、政策立案や企業経営に活用される可能性があるとされている。</p>
<h2>人間中心のAI設計が問われる段階へ</h2>
<p>HAIの研究者らは、AIが人間の思考力や判断力、長期的な成長に与える影響にも目を向ける必要があると指摘する。短期的な利便性や満足度ではなく、人間の能力をどのように補完し、育てるのかを前提とした設計思想が、今後のAI開発で重要になると結論づけている。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>都営バス、AI翻訳透明ディスプレイを実装へ──浅草で多言語対応と運転士負担軽減を検証</title>
      <link>https://ledge.ai/articles/toei_bus_ai_translation_transparent_display_pilot</link>
      <description><![CDATA[<p>東京都は2026年1月6日、都営バスの運転席周辺に「AI翻訳透明ディスプレイ」を設置する導入実証を、同年1月から浅草エリアで開始すると<a href="https://www.kotsu.metro.tokyo.jp/pickup_information/news/bus/2026/bus_p_2026010612338_h.html">発表</a>した。</p>
<p>訪日外国人の増加に伴う多言語対応ニーズの高まりや、バス運転士の担い手不足を背景に、多様な利用者とのコミュニケーションにおける有効性・利便性を検証するとともに、運転士の負担軽減や今後の導入に向けた課題整理を行う。</p>
<p>同実証では、バスの運転席周辺にAI翻訳透明ディスプレイを設置し、利用者と運転士のやり取りをリアルタイムで翻訳・表示する。外国人利用者に加え、聴覚・言語障害者を含む、誰もが安心して利用できる公共交通環境の実現を目指す。</p>
<p>今回の取り組みは、2025年11月に渋谷エリアで実施した導入実証に続くものとなる。訪日外国人利用者が特に多い浅草エリアを対象とすることで、より幅広い利用者からの意見を収集し、利便性や運用上の課題をさらに検証する。</p>
<p>実証実験の実施期間は2026年1月15日から29日まで。対象路線は、都02（大塚駅～錦糸町駅）、草63（池袋駅東口～浅草寿町）、草64（池袋駅東口～浅草雷門南）の3系統。AI翻訳透明ディスプレイは、民間事業者として<strong>TOPPAN株式会社</strong>が提供する。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/bus_p_2026010612338_h_05_5d029b5d44/bus_p_2026010612338_h_05_5d029b5d44.png" alt="bus_p_2026010612338_h_05.png" /></p>
<p>実証を通じて東京都は、多言語対応の実効性や現場での運用負荷を検証し、公共交通へのAI技術の本格導入に向けた検討を進めるとしている。なお、本取り組みは「2050東京戦略」における「インフラ・交通」分野の施策の一環として位置づけられている。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>TSMC、最先端2nm半導体の量産を開始──次世代プロセスが本格稼働</title>
      <link>https://ledge.ai/articles/tsmc_2nm_mass_production_start</link>
      <description><![CDATA[<p>半導体受託生産（ファウンドリー）世界最大手の台湾積体電路製造（TSMC）は、回路線幅2ナノメートルの最先端半導体について、計画通り2025年第4四半期に、量産を<a href="https://www.tsmc.com/english/dedicatedFoundry/technology/logic/l_2nm">開始した</a>。TSMCは2025年12月31日付で更新した公式サイトの2nm（N2）技術ページで、N2が2025年4Qに量産を開始したと記載している。</p>
<p>公式サイトで公開している2ナノ（N2）技術の説明によると、同プロセスは従来世代からトランジスタ構造を刷新した次世代プロセスに位置づけられている。微細化だけでなく、電流制御の方式そのものを改めることで、性能と消費電力の両面で世代全体としての改善を図る点が特徴だ。</p>
<p>量産開始は、研究開発や試験的な生産段階を終え、実際の製品向けに本格的な製造が始まったことを意味する。これにより、半導体メーカーやシステム開発企業は、2ナノ世代を前提とした製品設計や供給計画を具体的に進められる段階に入った。</p>
<p>TSMCは2ナノ技術について、密度とエネルギー効率の両面で業界最先端の水準になるとしている。性能向上に加え、消費電力の効率性が重視される計算分野を中心に、次世代プロセスの重要性は高まっている。</p>
<p>2ナノプロセスの生産は、台湾の先端製造拠点で進められており、新竹のFab 20や高雄のFab 22が主要な製造施設とされている。TSMCは今後も、先端プロセスへの投資と技術開発を継続し、次世代半導体の量産体制を段階的に強化していく方針だ。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>ユニバーサル・ミュージック、NVIDIAとAI分野で提携──数百万曲の公式カタログを基盤に「責任ある音楽AI」を共同開発</title>
      <link>https://ledge.ai/articles/umg_nvidia_responsible_music_ai_partnership</link>
      <description><![CDATA[<p>Universal Music Group（UMG）は2026年1月6日（現地時間）、NVIDIAと、音楽分野における「責任あるAI（Responsible AI）」の開発を目的とした協業を<a href="https://www.universalmusic.com/universal-music-group-to-transform-music-experience-for-billions-of-fans-with-nvidia-ai/">発表</a>した。</p>
<p>NVIDIAのAIインフラと、数百万曲規模に及ぶUMGの公式音楽カタログを組み合わせ、音楽の発見、創造、ファンエンゲージメントを拡張する。</p>
<p>UMGによると、今回の協業は、世界中で音楽を楽しむ数十億人のリスナー体験を高度化することを目的とするものだ。両社は共同で研究開発を進め、音楽創作の発展と、権利者への正当な対価還元を両立させることを共通の目標に掲げている。</p>
<h2>音楽理解AI「Music Flamingo」を拡張</h2>
<p>協業の中核となるのが、NVIDIAが開発する音楽理解AIモデル Music Flamingo だ。Music Flamingoは、最大15分に及ぶフルトラックを処理でき、ハーモニーや楽曲構造、音色、歌詞、文化的背景、感情の流れといった要素を総合的に理解する。</p>
<p>UMGとNVIDIAは、このモデルをUMGの膨大な公式カタログに対応させることで、ジャンルやタグといった従来の分類を超え、楽曲の文脈や感情的ナラティブに基づいた新しい音楽発見体験の実現を目指すとしている。</p>
<h2>ファン体験とアーティスト支援の両立</h2>
<p>AI技術は、アーティストとファンの関係性を拡張する用途にも活用される。アーティストは自身の楽曲をより深く分析・表現できるツールを活用でき、ファン側は単なる検索やプレイリストを超えた、よりインタラクティブな音楽体験を得られるという。</p>
<p>また、既存アーティストのエンゲージメント強化に加え、新進アーティストが適切なリスナーに発見される機会を広げる狙いもある。</p>
<h2>アーティスト主導のAI開発体制</h2>
<p>両社は、アーティスト、作曲家、プロデューサーが直接関与する専用のAIインキュベーターを設立する。実際の制作現場に即した形でAIツールを共同設計・検証し、独創性や真正性を重視した活用を進めることで、画一的なAI生成物を避ける方針だ。</p>
<h2>著作権と帰属を重視した「責任あるAI」</h2>
<p>UMGとNVIDIAは、AIの活用と同時に、著作権保護や楽曲の正確な帰属を重視する姿勢を強調している。AIによる音楽利用において、アーティストの権利を守りつつ、透明性と信頼性を確保することを協業の前提条件と位置付けた。</p>
<h2>スタジオ資産も活用した研究体制</h2>
<p>UMGのMusic &amp; Advanced Machine Learning Lab（MAML）は、これまでもNVIDIAのAIインフラを活用してきた。今回の協業では、Abbey Road Studios や Capitol Studios といった世界的スタジオも研究・制作環境として活用し、レーベルや出版社を含む幅広い関係者の知見を反映させる。</p>
<p>UMGとNVIDIAは、公式音楽カタログを基盤としたAI活用を通じ、音楽産業における技術革新と権利保護の両立を図るとしている。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>Boston Dynamics、Google DeepMindとAI研究で提携──Atlasを軸にロボット知能を共同開発</title>
      <link>https://ledge.ai/articles/xboston_dynamics_google_deepmind_ai_partnership_202601</link>
      <description><![CDATA[<p>米ロボット企業の Boston Dynamics は2026年1月5日（米太平洋時間）、人工知能（AI）研究機関 Google DeepMind と新たなAIパートナーシップを締結したと<a href="https://bostondynamics.com/blog/boston-dynamics-google-deepmind-form-new-ai-partnership/">発表</a>した。両社は、ロボットの知覚・判断・動作を統合するAI技術の研究開発を共同で進める。</p>
<p>今回の提携は、Boston Dynamicsが開発するロボットプラットフォームと、Google DeepMindが長年取り組んできた強化学習やロボティクス向けAI研究を結びつけることを目的とする。発表によると、研究は実世界で動作するロボットを前提に進められ、シミュレーションと実機を往復する形で学習手法の高度化を図るという。</p>
<p>両社は、電動化された二足歩行ロボット「Atlas」を主要な研究対象の一つとして位置付けている。Atlasは、人間に近い可動域と運動能力を備えたプラットフォームとして、現実環境での動作学習や制御アルゴリズムの検証に用いられる。DeepMindはこれまで、仮想環境を活用したロボット制御や強化学習の研究を進めてきたが、今回の提携では実機ロボットを用いた検証が本格化する形となる。</p>
<p>発表では、ロボットが周囲の環境を認識し、状況に応じて行動を調整するための知能の実装が重視されている。単一のタスクに特化した制御ではなく、複数の条件や変化に対応できる汎用的な能力の獲得を目指すとしている。</p>
<p>Boston Dynamicsはこれまでも、ロボットの知能に関する研究において、外部の技術や研究機関と協力してきた。例えば、同社の四足歩行ロボット「Spot」は、外部のAI技術と組み合わせた研究・実験のプラットフォームとして活用されてきたほか、2024年には Toyota Research Institute と、Atlasを用いた汎用ヒューマノイド研究での提携を発表している。同社は、ロボットの用途や研究目的に応じて、知能部分を柔軟に組み合わせるアプローチを取ってきた。</p>
<p>今回のパートナーシップは研究および技術開発を目的としたもので、企業買収や資本関係の変更を伴うものではない。Boston Dynamicsは、引き続き自社のロボット開発を主導しつつ、DeepMindのAI研究成果を取り入れる形で協力を進める。</p>
<p>ロボット工学とAI研究を統合する動きは近年加速しており、身体を持つAI、いわゆる「Physical AI」への関心も高まっている。Boston DynamicsとGoogle DeepMindは、今回の提携を通じて、現実世界で機能するロボット知能の研究を前進させるとしている。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
    <item>
      <title>LLMは「大きいほど脳に近い」？──Meta研究、脳活動との対応がスケールと文脈長で強まると報告</title>
      <link>https://ledge.ai/articles/xllm_brain_alignment_scaling_and_context_meta_study</link>
      <description><![CDATA[<p>Metaの研究チームは、人間が物語を聞いている最中の脳活動と大規模言語モデル（LLM）の内部表現を比較した結果、モデルの規模や入力文脈の長さに応じて、脳の言語処理と対応する計算構造が現れるとする研究成果を<a href="https://arxiv.org/abs/2512.01591">発表</a>した。論文「Scaling and context steer LLMs along the same computational path as the human brain」は、2025年12月にarXivで公開されている。</p>
<h2>脳活動とモデル内部表現をどう対応づけたのか</h2>
<p>研究では、被験者3人が約10時間にわたってオーディオブックを聴取する際の脳活動を「脳磁図（MEG）」を用いて計測した。
取得した脳信号は、単語の出現タイミングに同期させて解析され、言語刺激に対する時間分解能の高い反応として整理された。</p>
<p>一方、同じテキストを22種類の言語モデルに入力し、各層の内部表現を抽出。脳信号からモデル内部表現を予測する線形写像を学習し、その予測精度をもとに、どのモデル層が、脳のどの時間帯の反応と対応するかを評価した。</p>
<p><strong>図：脳活動とLLM内部表現の対応付け手法の概要</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/method_figure_tmax_923f1bc517/method_figure_tmax_923f1bc517.png" alt="method_figure_tmax.png" /></p>
<h2>層の深さと脳反応の時間順序に見られた対応関係</h2>
<p>解析の結果、多くのモデルにおいて、浅い層ほど脳の早い反応と、深い層ほど遅い反応と対応する傾向が確認されたという。これは、言語処理における計算の進行順序が、LLMと人間の脳で部分的に一致している可能性を示すものとされる。</p>
<p>研究では、この層の深さと脳反応のピーク時間との対応を「temporal alignment（時間的整合）」と呼び、単なる相関ではなく、計算の段階構造が揃っているかどうかを捉える指標として位置づけている。</p>
<h2>モデル構造の違いが示した共通点と相違点</h2>
<p>こうした時間的整合は、Transformer系モデルだけでなく、状態空間モデル（Mamba）や再帰型モデル（RecurrentGemma）でも観測された。</p>
<p>一方で、BERTやRoBERTa、wav2vec 2.0といった双方向モデルでは、脳活動との対応自体は一定程度見られるものの、計算の時間順序に関する整合は弱く、統計的に有意ではないケースが多かったという。</p>
<p>研究チームは、モデルが「未来の単語も参照できる」双方向構造を持つことが、時間順序の対応を弱める可能性を指摘している。</p>
<h2>モデル規模の拡大で現れた“対応の立ち上がり”</h2>
<p>モデルサイズの影響については、パラメータ数のみを段階的に変化させたPythiaモデル群を用いて検証された。その結果、最小規模の14Mパラメータモデルでは時間的整合は有意に確認されなかった一方、12Bパラメータモデルでは非常に強い整合が示された。</p>
<p>この対応の強まりは、モデルサイズの増加に対して対数的に進み、一定規模を超えると伸びが緩やかになる傾向も見られたという。</p>
<h2>文脈情報が計算対応に与える影響</h2>
<p>研究では、入力する文脈の長さも重要な要因として検証された。Llama-3.2（3B）を用いた実験では、文脈をほとんど与えない条件では時間的整合は弱く、文脈長を1000語程度まで拡張すると、脳活動との対応が大きく強まった。</p>
<p>この効果はMambaモデルでも同様に確認されており、文脈情報の蓄積が、脳に近い計算順序を形成する要因になっている可能性が示唆されている。</p>
<h2>単語予測の容易さだけでは説明できない点</h2>
<p>研究チームは、こうした対応が単に「次の単語を予測しやすいかどうか」によって生じている可能性も検討した。その結果、予測可能性の高低で単語を分けても、時間的整合の傾向は維持されており、単純な次トークン予測の難易度だけでは説明できないと結論づけている。</p>
<h2>研究が示唆することと、残された課題</h2>
<p>研究は、LLMのスケールや文脈処理能力の拡張が、人間の脳における言語処理の計算構造と対応する方向へ作用する可能性を示した。一方で、被験者数が限られている点や、MEGの空間分解能の制約、感覚入力を伴わないテキストモデル中心の分析である点など、今後の検証課題も挙げられている。</p>
<p>研究チームは、今後さらに多様なモデルや条件での比較を通じて、言語モデルと人間の認知過程の関係を精査していくとしている。</p>
]]></description>
      <pubDate>Wed, 07 May 2025 01:46:44 GMT</pubDate>
    </item>
  </channel>
</rss>