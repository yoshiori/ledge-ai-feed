<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
  <channel>
    <title>Ledge.ai 新着記事</title>
    <link>https://ledge.ai/</link>
    <description>Ledge.ai の最新テクノロジー記事</description>
    <item>
      <title>Gemini 3 Flashに高精度な画像理解「Agentic Vision」追加──Python実行で画像を再検査、品質5〜10%向上</title>
      <link>https://ledge.ai/articles/gemini_3_flash_agentic_vision_image_understanding</link>
      <description><![CDATA[<p>Googleは2026年1月27日、同社のAIモデル「Gemini 3 Flash」に、高精度な画像理解機能「Agentic Vision」を追加したと<a href="https://blog.google/innovation-and-ai/technology/developers-tools/agentic-vision-gemini-3-flash/">発表</a>{target=”_blank”}した。画像を一度解析して回答を返す従来型の手法とは異なり、モデル自身が処理手順を組み立て、必要に応じて画像を拡大・切り出ししながら検証を重ねることで、視覚的根拠に基づく回答を可能にするという。</p>
<p>Agentic Visionは、Google AI StudioおよびVertex AIのGemini APIで提供され、現時点ではGemini 3 Flash専用機能として位置付けられている。</p>
<h2>画像理解を「一度きりの認識」から「段階的な検証プロセス」へ</h2>
<p>Agentic Visionの特徴は、画像理解を静的な認識処理ではなく、段階的な検証プロセスとして扱う点にある。
モデルは画像を見て即座に答えを出すのではなく、</p>
<ul>
<li>どの部分を確認すべきかを判断</li>
<li>必要な操作を実行</li>
<li>結果を再度観察し、次の行動を決める</li>
</ul>
<p>といったループを繰り返しながら推論を進める。Googleはこのアプローチを「Think → Act → Observe」の循環として説明している。</p>
<h2>視覚推論とPython実行を組み合わせた設計</h2>
<p>技術的な中核となるのが、視覚推論とPythonコード実行の統合だ。</p>
<p><strong>図）Agentic Visionの処理フロー</strong> ：ユーザー入力（画像＋テキスト）を受けたAIエージェントが、「Think（計画）→Act（コード実行）→Observe（結果確認）」の循環を回しながら、画像を拡大・切り出し・注釈付けして分析する。Gemini 3 Flashでは、Pythonによる画像操作が推論プロセスに組み込まれている。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/agentic_vision_gemini_3_flash_bl_width_1000_format_webp_z5u5_Yj_Z_f25520272d/agentic_vision_gemini_3_flash_bl_width_1000_format_webp_z5u5_Yj_Z_f25520272d.webp" alt="agentic-vision-gemini-3_flash_bl.width-1000.format-webp_z5u5YjZ.webp" /></p>
<p>Agentic Visionでは、モデルが自律的にPythonコードを生成・実行し、以下のような操作を行う。</p>
<ul>
<li>画像の一部を切り出して拡大表示</li>
<li>特定領域を再解析</li>
<li>数値データを抽出して計算処理</li>
</ul>
<p>こうした処理をコード実行環境で行うことで、推論過程をより厳密にし、誤認識を減らす狙いがある。Googleによると、この仕組みにより多くの視覚系ベンチマークで5〜10%の品質向上が確認されているという。</p>
<p><strong>図）Agentic Visionによる視覚ベンチマーク性能の変化</strong> ：Gemini 3 Flashにコード実行を組み合わせた構成（with code execution）は、画像理解系ベンチマークの多くで、従来構成を5〜10%上回るスコアを示した。Googleは、画像の再検査や注釈付けを推論過程に組み込んだ点が精度向上につながったとしている</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/agentic_vision_gemini_3_flash_bl_width_1000_format_webp_CO_Ee0g_Z_9ff68e3d0e/agentic_vision_gemini_3_flash_bl_width_1000_format_webp_CO_Ee0g_Z_9ff68e3d0e.webp" alt="agentic-vision-gemini-3_flash_bl.width-1000.format-webp_COEe0gZ.webp" /></p>
<h2>ズーム、注釈、可視化──Agentic Visionで可能になる操作</h2>
<p>Agentic Visionが提供する主な機能は、次の3点に整理できる。</p>
<ul>
<li><strong>ズームと再検査：</strong> 小さな文字や遠景の対象など、初回解析では不十分な要素を検知し、拡大・再分析する</li>
<li><strong>画像への直接注釈：</strong> 境界ボックスやラベルを画像上に描画し、対象物の位置や数を明示することで、回答の根拠を可視化する</li>
<li><strong>視覚情報の計算とグラフ化：</strong> 画像内の表や数値を読み取り、計算処理を行ったうえでグラフ出力することも可能</li>
</ul>
<p>これらはいずれも、モデルが自律的に「必要」と判断した場合に実行される点が特徴となっている。</p>
<h2>Google AI StudioとVertex AIで提供、Flash専用機能として展開</h2>
<p>Agentic Visionは、Google AI StudioおよびVertex AIのGemini APIで利用できる。開発者は、コード実行機能を有効化することで、画像を対象とした高度な検証プロセスをアプリケーションに組み込める。</p>
<p>なお、公式ドキュメントでは、本機能はGemini 3 Flashに限定して提供されると明記されており、他のGeminiモデルでは利用できない。</p>
<h2>業務利用を意識した高精度画像理解へ</h2>
<p>GoogleはAgentic Visionについて、単なる画像認識精度の向上にとどまらず、業務利用に耐える信頼性の確保を目的とした機能強化だと位置付けている。
計器の読み取り、画像化された表データの解析、数量確認など、正確性が求められる場面での活用を想定しているという。</p>
]]></description>
      <pubDate>Tue, 03 Feb 2026 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>学術＆研究2026/2/2 [MON]“プロンプトで歩ける世界”が現実に──Google、世界生成AIの実験プロトタイプ「Project Genie」米国提供　Google DeepMindの世界モデル「Genie 3」を搭載</title>
      <link>https://ledge.ai/articles/google_project_genie_interactive_virtual_world_us_release</link>
      <description><![CDATA[<p>Googleは2026年1月29日（現地時間）、テキストや画像から対話型の仮想世界を生成・探索できる実験的なプロトタイプ「Project Genie」を<a href="https://blog.google/innovation-and-ai/models-and-research/google-deepmind/project-genie/">公開</a>した。まずは米国の「Google AI Ultra」ユーザーを対象に提供する。</p>
<h2>テキストや画像から対話型の仮想世界を生成</h2>
<p>Project Genieは、ユーザーが自然言語の指示や画像を入力すると、AIが仮想空間を生成し、その中を移動・操作しながら探索できる点が特徴だ。生成された世界は静的な3Dモデルではなく、視点移動や操作に応じて周囲の環境がリアルタイムに構築される。</p>
<p>同プロトタイプでは、体験は三つの中核機能で構成されている。テキストや生成・アップロードした画像を用いて環境を作成する「World sketching」、生成された世界の中を歩行や飛行、乗り物での移動などを通じて体験する「World exploration」、既存の世界を基に新たな解釈を加える「World remixing」だ。
作成前には世界の見た目をプレビューし、視点を一人称・三人称から選択することもできる。</p>
<p>@<a href="https://youtu.be/YxkGdX4WIBE">YouTube</a></p>
<h2>世界モデル「Genie 3」を中核に据えた構成</h2>
<p>技術面では、Google DeepMindが開発した世界モデル「Genie 3」を中核に据える。ユーザーの行動に応じて進行方向の環境を生成する仕組みを採用し、物理挙動や相互作用を含む動的な世界をシミュレーションする。プロトタイプはGenie 3に加え、画像生成モデル「Nano Banana Pro」や対話型AI「Gemini」を組み合わせたWebアプリとして提供されている。</p>
<h2>米国のGoogle AI Ultraユーザー向けに限定提供</h2>
<p>Project GenieはGoogle Labsにおける実験的研究プロトタイプとして提供される。現時点では米国在住の18歳以上で、「Google AI Ultra」に加入しているユーザーに限定されている。生成した世界や探索の様子は動画としてダウンロードすることも可能だ。</p>
<p>Googleは、現段階では生成結果が必ずしも現実世界の物理や入力内容に完全に一致しない場合があるほか、操作時の遅延や生成時間が最大60秒に制限されている点など、いくつかの制約があるとしている。一部のGenie 3の機能も本プロトタイプには含まれていない。今後はユーザーからのフィードバックを基に改良を進め、提供地域を段階的に拡大していく方針だ。</p>
]]></description>
      <pubDate>Mon, 02 Feb 2026 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>AIが高次元幾何学の難問に挑む──中国研究チーム、接吻数問題で複数次元の記録更新</title>
      <link>https://ledge.ai/articles/ai_kissing_number_problem_game_theoretic_rl</link>
      <description><![CDATA[<p>中国の北京大学や上海科学智能研究院などの研究グループが2026年1月26日、AIを用いて、高次元幾何学の難問として知られる「接吻数問題」に取り組み、複数の次元で既存の下界を更新する成果を報告した。研究成果は論文「<a href="https://arxiv.org/abs/2511.13391">Finding Kissing Numbers with Game-theoretic Reinforcement Learning</a>」として発表された。</p>
<p>接吻数問題は、同じ大きさの球が1つの球の周囲に互いに重ならないよう最大でいくつ接触できるかを問う問題で、1694年にアイザック・ニュートンらが議論して以来、長年にわたり研究が続けられてきた。次元が高くなるにつれて幾何構造が複雑化し、解析や探索が極めて困難になることが知られている。</p>
<h2>球の配置を「行列」として扱うAI手法</h2>
<p>研究チームは、接吻数問題をGram行列（内積行列）の補完問題として定式化し、ゲーム理論と強化学習を組み合わせたAIシステム「PackingStar」を開発した。従来のように高次元空間上の座標を直接最適化するのではなく、球同士の内積関係のみを行列として扱うことで、数値的不安定性を抑えつつ大規模な並列探索を可能にしたとしている。</p>
<p>PackingStarでは、2つのAIエージェントが協調的に動作する。一方のエージェントが行列を拡張して配置候補を追加し、もう一方が全体構造を考慮して不適切な要素を削除・修正する。この「追加」と「修正」を繰り返すことで、より大きな配置、すなわちより高い接吻数に対応する構造を探索する仕組みだ。</p>
<p><strong>【図：PackingStarの3段階（シミュレーション→行列初期化→2プレイヤー行列補完ゲーム）を示した模式図】</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x2_1_12e03bdc99/x2_1_12e03bdc99.png" alt="x2 (1).png" /></p>
<h2>25〜31次元で既存の下界を更新</h2>
<p>論文によると、PackingStarは25次元から31次元までのすべての次元で、これまでに知られていた最良の下界を上回る接吻配置を発見した。特に25次元では、得られた配置が高次元格子として知られるLeech格子の部分構造と対応する明確な幾何パターンを示しており、最適構造である可能性を示唆しているという。ただし、厳密な数学的証明は現時点では示されていない。</p>
<p>研究チームは、2011年や2016年に提案されていた構成テンプレートを超える配置が得られたとし、高次元における探索能力の拡張を成果として位置づけている。</p>
<p><strong>【図：次元ごとの接吻数と、本研究による更新点を示したグラフ】</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x1_76eb8d2f20/x1_76eb8d2f20.png" alt="x1.png" /></p>
<h2>13次元で「合理構造」を刷新</h2>
<p>また13次元では、1971年以来更新されていなかった「合理構造」と呼ばれる構成を刷新し、接吻数1146の配置を発見した。合理構造とは、球同士の内積がすべて有理数で表される配置を指す。現在知られている13次元での最高記録は非合理構造による1154だが、合理構造は厳密な解析が可能である点から、理論的価値が高いとされる。</p>
<p>論文では、今回の成果が高次元幾何学や球面符号、情報理論分野での研究に資する可能性にも言及している。</p>
<h2>一般化接吻数でも新記録</h2>
<p>さらに、球同士の角度制約を変更した「一般化接吻数」においても成果を報告した。12次元（内積制約1/4）、14次元および17次元（内積制約1/3）で既存記録を更新し、数千件規模の新たな配置を発見したとしている。</p>
<h2>AIによる数学探索の新たな事例に</h2>
<p>研究チームは、PackingStarが既存の構造を単に最適化するのではなく、新しい幾何構造を体系的に発見できる点を特徴として挙げている。一方で、得られた配置が真に最適であることを示す証明は今後の課題とした。</p>
]]></description>
      <pubDate>Mon, 02 Feb 2026 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>AmazonがOpenAI出資を検討、最大500億ドル（約7.6兆円）規模と報道　今ラウンドでの資金調達は1000億ドル超も</title>
      <link>https://ledge.ai/articles/amazon_openai_investment_50b_funding_100b_reported</link>
      <description><![CDATA[<p>AmazonがOpenAIに対し、最大500億ドル（約7兆6000億円）規模の出資を検討していると、<a href="https://www.reuters.com/business/retail-consumer/amazon-talks-invest-up-50-billion-openai-wsj-reports-2026-01-29/">Reuters</a>や<a href="https://www.cnbc.com/2026/01/29/amazon-openai-investment-jassy-altman.html">CNBC</a>など複数の米メディアが2026年1月29日に報じた。協議は進行中で、最終的な出資額や条件は流動的だとしている。</p>
<h2>最大500億ドル規模、交渉はなお初期段階</h2>
<p>関係者の話として伝えられているところによると、AmazonはOpenAIに数百億ドル規模の投資を協議しており、その金額は最大500億ドルに達する可能性がある。ただし、協議はまだ初期段階にあり、最終的な条件や金額は確定していないとされる。Amazonはコメントを控え、OpenAIも現時点で公式な見解を示していない。</p>
<h2>条件書は数週間以内に署名の可能性も</h2>
<p>報道では、関係者によると条件書（term sheet）が数週間以内に署名される可能性があるとも伝えられている。今回の資金調達は、Amazonのほか、MicrosoftやNVIDIAといった戦略投資家を先行させ、その後に他の投資家が参加する2段階構成となる可能性があるという。</p>
<h2>OpenAI、最大1000億ドル調達・評価額8300億ドルの可能性</h2>
<p>OpenAIは今回のラウンドで、最大1000億ドルの資金調達を目指しているとされ、評価額は約8300億ドルに達する可能性がある。報道では、SoftBankや中東の政府系ファンドなども投資家候補として名前が挙がっている。</p>
<p>交渉には、AmazonのCEOであるAndy Jassy氏と、OpenAIのCEOのSam Altman氏が直接関与しているとされる。米紙<a href="https://www.wsj.com/tech/ai/amazon-in-talks-to-invest-up-to-50-billion-in-openai-43191ba0">The Wall Street Journal</a>は、この協議について先行して報じていた。</p>
<p>Amazonは、OpenAIの競合であるAnthropicにもこれまでに巨額投資を行ってきた。今回の動きについては、大手テック企業が複数のAI企業に同時に投資する構図が鮮明になりつつあるとの見方も出ている。</p>
]]></description>
      <pubDate>Mon, 02 Feb 2026 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Anthropic、Claudeで外部業務ツール連携を拡充　SlackやAsana、Figmaなどを会話内で操作可能に</title>
      <link>https://ledge.ai/articles/anthropic_claude_interactive_tools_slack_asana_figma</link>
      <description><![CDATA[<p>米AI企業の Anthropic は2026年1月26日、同社のAIサービス Claude において、外部業務ツールを会話内でインタラクティブに利用できる機能を拡充したと<a href="https://claude.com/blog/interactive-tools-in-claude">発表</a>した。今回のアップデートにより、Slack、Asana、Figma、Canva、Box などのツールを、Claudeのチャット画面内で直接操作できるようになる。</p>
<h2>会話の流れでツールを操作、画面切り替え不要に</h2>
<p>対応したツールでは、単なる情報参照にとどまらず、各サービスの操作をClaudeの会話の流れの中で行える。</p>
<p>例えばSlackではメッセージの下書きや送信、Asanaではタスクやプロジェクトの更新、FigmaやCanvaではデザインや資料の作成・確認といった作業を、別タブに移動することなく進められるという。</p>
<p>これにより、複数の業務ツールを行き来しながら作業する従来のワークフローを簡略化し、AIとの対話を起点に業務を進める形を想定している。</p>
<p>@<a href="https://www.youtube.com/watch?v=bluAmTHoEow">YouTube</a></p>
<h2>Model Context Protocol（MCP Apps）を活用</h2>
<p>こうしたインタラクティブな連携は、Anthropicが推進するオープン標準プロトコル「Model Context Protocol（MCP）」の拡張仕様である「MCP Apps」によって実現されている。
MCP Appsは、外部ツール側が提供するインターフェースをAIクライアント上で扱えるようにする仕組みで、Claudeはこの仕様に対応することで、外部サービスの操作画面を会話の文脈に沿って表示・利用できるようになった。</p>
<p>Anthropicは、MCPを通じてClaudeと業務ツールの接続性を高め、開発者や企業が独自のツールを統合できる環境整備も進めている。</p>
<h2>対象プランと今後の展開</h2>
<p>今回のインタラクティブツール連携は、Claudeの有料プラン（Pro、Team、Enterpriseなど）を中心に提供される。対応ツールは今後も拡大予定としており、Claudeを業務のハブとして活用するユースケースを広げていく方針だ。</p>
]]></description>
      <pubDate>Mon, 02 Feb 2026 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Genspark、日本市場に本格参入──音声操作と自律ワークフローを備えた「AI Workspace 2.0」発表</title>
      <link>https://ledge.ai/articles/genspark_ai_workspace_2_japan_launch</link>
      <description><![CDATA[<p>米AIスタートアップの Genspark は2026年1月28日、日本市場での法人展開を本格化すると<a href="https://prtimes.jp/main/html/rd/p/000000002.000176655.html">発表</a>した。あわせて、音声入力アプリ「Speakly」やAI受信トレイのワークフローなどを含む「AI Workspace 2.0」を公開した。</p>
<h2>日本市場を重要拠点に位置づけ</h2>
<p>Gensparkは、日本市場を米国やアジアの主要市場と並ぶ重要拠点の一つと位置づけ、業務利用を前提としたAIワークスペースの提供を本格化する。日本語環境での業務利用を想定し、法人向けの導入を進める方針だ。</p>
<h2>業務全体を担う「AI Workspace 2.0」</h2>
<p>「Genspark AI Workspace 2.0」は、情報収集、情報処理、成果物作成までを単一の環境で担う業務特化型AIワークスペースとして設計されている。ユーザーの指示をもとに、AIがタスクを分解し、複数の工程を自律的に実行する点が特徴だ。</p>
<p><strong>Genspark AI Workspace 2.0は、情報収集・処理・成果物作成までを単一環境で担う業務向けAIワークスペースとして設計されている</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/sub2_86e094679d/sub2_86e094679d.png" alt="sub2.png" /></p>
<h2>音声入力とカスタムワークフローを新搭載</h2>
<p>新バージョンでは、音声入力による操作に対応した。キーボード操作を介さず、話しかけるだけで指示を出せるため、業務のスピード向上が期待される。
また、ユーザー自身が業務内容に応じたカスタムワークフローを構築できる機能も追加された。定型業務や複数ステップにまたがる処理をAIに任せることで、業務の自動化を進められる。</p>
<h2>メール処理を自動化する「AI受信トレイ」</h2>
<p>AI Workspace 2.0には、メール処理を自動化する「AI受信トレイ」も搭載されている。受信箱のトリアージや転送、優先順位付けなどを自律的に処理する仕組みとしている。</p>
<h2>複数AIモデルを統合、モデル選択は不要</h2>
<p>Gensparkは、ChatGPTやGemini、Claudeなど複数の大規模言語モデルを内部で活用している。ユーザーは個別のモデルを意識する必要はなく、タスク内容に応じて最適なモデルが裏側で使い分けられる仕組みだ。
同社は、ChatGPTやGemini、Claudeなどを含む70以上のAIモデルを統合し、指示をタスクに分解したうえで最適なモデルを選定すると説明している。</p>
<p><strong>Gensparkは、複数のAIモデル、ツール、データを組み合わせる独自のエージェントエンジンによって業務実行を行う</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/sub1_c8285ce386/sub1_c8285ce386.png" alt="sub1.png" /></p>
<p>こうしたGenspark AI Workspace 2.0の動作イメージについて、同社は以下の公式デモ動画で紹介している。</p>
<p>@<a href="https://www.youtube.com/watch?v=KtOZNvFhV8c">YouTube</a></p>
<h2>日本企業での導入も進展</h2>
<p>Gensparkは、日本国内において広告、IT、製造、金融など複数業界の企業で導入が進んでいると説明する。今後は、日本市場向けの機能拡張や導入支援を通じて、業務現場でのAI活用をさらに広げていく考えだ。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/sub3_2ec073c221/sub3_2ec073c221.png" alt="sub3.png" /></p>
]]></description>
      <pubDate>Mon, 02 Feb 2026 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>学術＆研究2026/2/2 [MON]100万文字DNAを一度に解析──Google DeepMind、11種のゲノム過程を統合予測するAI「AlphaGenome」</title>
      <link>https://ledge.ai/articles/google_deepmind_alphagenome_nature_1mb_genome_prediction</link>
      <description><![CDATA[<p>Google DeepMindの研究チームは、最大100万塩基（約100万文字）に及ぶDNA配列を一度に解析し、遺伝子発現やスプライシングなど11種類の主要なゲノムプロセスを高精度で予測できるAIモデル「AlphaGenome」を開発した。研究成果は<a href="https://www.nature.com/articles/s41586-025-10014-0">Nature</a>に2026年1月28日付で掲載され、研究コミュニティ向けにソースコードとモデルの重みも公開されている。</p>
<p>Nature掲載とともに「DNA配列の理解と遺伝的変化の分子レベルでの影響予測を支援するモデル」として紹介している。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/deepmind_x_67d9e3d161/deepmind_x_67d9e3d161.jpg" alt="deepmind x.jpg" /></p>
<h2>最大100万塩基を「文脈」として扱い、11のプロセスを同時予測</h2>
<p>従来のゲノム解析AIにおける入力配列長は、数万〜数十万塩基程度が限界とされていた。AlphaGenomeはこの制約を大幅に拡張し、最大100万塩基（1M bp）という長大なDNA配列を単一の入力として処理することを可能にした。</p>
<p>特筆すべきは、単一のモデルアーキテクチャでありながら、多岐にわたる予測タスクをこなす点だ。遺伝子発現（RNA-seq、CAGE、PRO-cap等）やスプライシングに加え、クロマチンアクセシビリティ、ヒストン修飾、転写因子結合、さらには3次元的なクロマチン接触パターンに至るまで、計11種類の主要なゲノムプロセスを同時に、かつ高精度で予測できるよう設計されている。</p>
<h2>既存SOTAと同等以上、長距離相互作用の解析で強み</h2>
<p>Nature論文によれば、AlphaGenomeは遺伝子発現量やスプライシング予測といった主要タスクにおいて、既存の最先端（SOTA）モデルと同等、あるいは一部でそれらを上回る性能を示した。</p>
<p>とりわけ優位性が示されたのは、DNA配列上で遠く離れた領域同士が影響し合う「長距離調節」の解析だ。最大100万塩基という広い受容野（コンテキスト）を前提とすることで、従来の短い入力長では捉えきれなかったエンハンサーとプロモーター間の相互作用など、遠隔制御に関わる情報を扱える設計となっている。</p>
<h2>「非コード領域」の変異影響を多角的に評価</h2>
<p>ヒトゲノムの大部分を占めながら、その機能解釈が難しいとされてきた「非コード領域（non-coding region）」。AlphaGenomeは、この領域に生じた単一塩基変異が、細胞内の分子プロセスにどのような影響を及ぼすかを、多面的に予測・スコアリングできる。</p>
<p>具体的には、ある変異が遺伝子発現、スプライシング、クロマチン状態といった複数の層に同時に与える変化を評価可能だ。論文では、臨床的に関連する既知の変異を対象に、複数の分子過程をまたいだ予測が既存の実験知見と整合する例も示されている。</p>
<h2>1次元の配列から3次元クロマチン接触パターンを予測</h2>
<p>AlphaGenomeの特徴の一つとして、1次元のDNA配列情報のみから、Hi-C実験などで観測される「3次元クロマチン接触パターン（コンタクトマップ）」を予測できる点が挙げられる。</p>
<p>遺伝子の発現調節は、DNAが核内でどのように折り畳まれているかという立体構造と密接に関係している。配列情報から空間的な接触傾向を推定できるこの機能は、ゲノム制御機構の理解に資する可能性がある。</p>
<h2>Nature掲載に合わせ、コードと重みを研究者向けに公開</h2>
<p>Google DeepMindは本研究成果の発表と同時に、GitHubを通じてAlphaGenomeのソースコードおよび学習済みモデルの重みを公開した。利用は非商用の研究用途に限定されている。</p>
<p>これにより、世界中の研究者が再現実験や手法検証を行えるほか、独自データを用いた解析や予測パイプラインの拡張が可能となる。研究チームは、論文掲載とオープンな公開を通じて、ゲノム研究コミュニティでの幅広い活用を促している。</p>
]]></description>
      <pubDate>Mon, 02 Feb 2026 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>車限定だった「Geminiナビ」が徒歩・自転車にも拡大──Google マップで利用可能に</title>
      <link>https://ledge.ai/articles/google_maps_gemini_navigation_walking_cycling</link>
      <description><![CDATA[<p>地図アプリ「Google マップ」のナビゲーション中に利用できるAIアシスタント「Gemini」について、これまで自動車の運転時に限定していた対応を拡大し、徒歩および自転車でのナビゲーション中でも利用可能にした。同社が公式ブログで<a href="https://blog.google/products-and-platforms/products/maps/gemini-navigation-biking-walking/">発表</a>した。機能はGemini提供地域において、AndroidおよびiOS向けに順次提供される。</p>
<h2>自動車ナビ限定から、徒歩・自転車へ拡大</h2>
<p>Geminiは、Google マップのナビゲーション中に音声で呼び出し、目的地までの案内を続けながら各種操作や質問に対応するAIアシスタントとして提供されてきた。従来は自動車でのナビゲーション時に限られていたが、今回のアップデートにより、徒歩ナビおよび自転車ナビでも同様の体験が可能となった。</p>
<h2>ナビ中に使える主な機能</h2>
<p>ナビゲーション中、ユーザーは音声による自然言語でGeminiに指示を出すことができる。たとえば、</p>
<ul>
<li>到着予定時刻（ETA）の確認</li>
<li>ルート周辺の飲食店や施設の検索</li>
<li>簡単なメッセージ送信</li>
</ul>
<p>などを、ナビを中断せずに行える。Googleは、移動中に画面操作を最小限に抑える設計である点を強調している。
徒歩・自転車利用での意味合い</p>
<p>徒歩での移動中には、周辺情報を把握しながら目的地まで案内を受けられる点が特徴となる。また自転車での利用では、走行中に画面へ触れることなく音声で操作できるため、ハンズフリーによる利便性の向上が想定されている。</p>
<h2>提供条件と対応環境</h2>
<p>この機能は、Geminiが提供されている地域で利用可能となる。対応端末はAndroidおよびiOSで、最新版のGoogle マップが必要となる。提供は段階的に行われるため、利用可能になる時期はユーザーや地域によって異なる。</p>
<h2>導入の背景</h2>
<p>Googleは、ナビゲーション中の音声アシスタントを従来のGoogle アシスタントからGeminiへと順次移行してきた。今回の対応拡大は、自動車以外の移動手段にもAIナビ体験を広げる取り組みの一環と位置づけられる。</p>
]]></description>
      <pubDate>Mon, 02 Feb 2026 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google、ピクサー出身監督と制作した短編アニメをサンダンス映画祭でプレビュー上映へ──AIを制作工程に組み込む新たな表現手法を提示</title>
      <link>https://ledge.ai/articles/google_sundance_ai_animation_workflow</link>
      <description><![CDATA[<p>Googleは2026年1月26日（現地時間）、同社が制作した短編アニメ映画を、米国で開催されるサンダンス映画祭のプログラム「Sundance Institute Story Forum」でプレビュー上映すると<a href="https://blog.google/innovation-and-ai/models-and-research/google-deepmind/dear-upstairs-neighbors">発表</a>した。作品は、Google DeepMindの研究チームと、ピクサー出身の監督が協働して制作したもので、AIをアニメーション制作工程に組み込む新たな表現手法を示す事例として紹介される。</p>
<p>なお、Sundance Film Festivalは、インディペンデント映画を中心に、新しい表現手法や制作アプローチを積極的に紹介してきた国際的な映画祭として知られている。上映作品だけでなく、物語の作り方や制作プロセスを議論する場も設けられており、映画表現の変化をいち早く取り上げる場として注目されている。</p>
<p>@<a href="https://www.youtube.com/watch?v=eCk5VFKKz08">YouTube</a></p>
<h2>ピクサー出身監督とGoogle DeepMindが共同制作</h2>
<p>今回プレビュー上映される短編アニメは、ピクサー出身の監督 Connie He 氏と、Google DeepMindの研究者・エンジニアが共同で制作した。同作は、研究成果のデモンストレーションではなく、完成した短編アニメ作品として制作された点が特徴だ。Googleは、AI研究の成果を実際のクリエイティブ制作に適用する試みとして位置づけている。</p>
<h2>AIを“主役”にしない制作ワークフロー</h2>
<p>制作では、Google DeepMindが開発する画像生成モデル「Imagen」や動画生成モデル「Veo」などが用いられた。ただし、テキストプロンプトのみを入力して映像を生成する一般的な生成AIの使い方は採られていない。</p>
<p><strong>Imagenをファインチューニングして生成された主人公「Ada」のビジュアル例。単一のプロンプト生成ではなく、制作者の素材や意図を反映させながら表現の幅を広げたとしている。</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/WM_Ada_T2_I_1080p_max_1080x1080_format_webp_abc27ecd3f/WM_Ada_T2_I_1080p_max_1080x1080_format_webp_abc27ecd3f.webp" alt="WM_AdaT2I_1080p.max-1080x1080.format-webp.webp" /></p>
<p>Googleによると、制作工程ではまずアーティストが手描きのスケッチや絵コンテ、ラフアニメーションを作成し、それらのビジュアル素材をもとにAIモデルを調整・活用する手法が取られたという。Veoは既存のアニメーションや映像素材を入力として受け取り、動きや質感を保ったまま表現を拡張する用途で使われ、Imagenはキャラクターや背景のビジュアル表現を補完する役割を担ったとしている。</p>
<p>このように、AIは制作工程の各段階で補助的に組み込まれており、物語構成や演出、最終的な表現の判断は人間の制作者が担う設計となっている。Googleは、AIを「自動生成の主体」ではなく、アーティストの意図を反映するための制作ツールとして位置づけている。</p>
<h2>「Story Forum」でのプレビュー上映という位置づけ</h2>
<p>上映が行われる「Sundance Institute Story Forum」は、完成作品の優劣を競うコンペティションではなく、物語表現や制作手法、創作プロセスそのものを共有・議論することを目的としたプログラムだ。</p>
<p>今回の短編アニメも、AIを活用した新しい制作工程の事例として紹介され、制作の背景や手法についての説明とあわせて上映される予定となっている。</p>
<h2>AI研究成果を創作の現場へ</h2>
<p>Googleは同作品について、AI研究の成果を実験段階にとどめず、映画やアニメーションといった創作の現場でどのように活用できるかを示す試みだとしている。表現の幅を広げることや、制作工程の選択肢を増やすことを目的としており、現時点で商用配信や一般公開の予定については明らかにしていない。</p>
]]></description>
      <pubDate>Mon, 02 Feb 2026 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>JAXA、地球観測データを「生成AIから呼び出し」可能に──Python API v0.1.5でClaude Desktop向けMCPサンプル追加</title>
      <link>https://ledge.ai/articles/jaxa_earth_api_claude_mcp</link>
      <description><![CDATA[<p>宇宙航空研究開発機構（JAXA）は2026年1月27日、地球観測データを生成AIから呼び出せるようになったと、<a href="https://x.com/satellite_jaxa/status/2015981999564198052">公式X</a>{target=”_blank”}（旧Twitter）で明らかにした。あわせて、地球観測データ活用向けの公式ライブラリ「JAXA Earth API for Python」の最新版v0.1.5において、Claude Desktopで動作するMCP（Model Context Protocol）のサンプルコードを追加したことを公表している。</p>
<p>JAXAは投稿の中で、「ITエンジニアの皆さん、ぜひ遊んでみてください！」と呼びかけており、生成AIと宇宙・地球観測データを組み合わせた実験的な活用を後押しする姿勢を示した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/jaxa_x_2cc93d539e/jaxa_x_2cc93d539e.jpg" alt="jaxa x.jpg" /></p>
<h2>Claude Desktopから衛星データを直接取得──生成AIがJAXA Earth APIを操作</h2>
<p>今回のアップデートにより、AnthropicのAIアシスタント「Claude Desktop」から、JAXA Earth APIを外部ツールとして直接呼び出すことが可能になる。具体的には、Claude Desktopが対応するMCP（Model Context Protocol）を介し、ローカル環境で動作するMCPサーバーとしてJAXA Earth APIを接続する構成がサンプルコードとして示されている。</p>
<p>これにより、ユーザーは会話形式の操作を通じて、JAXAが提供する地球観測データを取得・参照する実験を行えるようになる。</p>
<h2>JAXA Earth API、生成AI連携を前提に拡張</h2>
<p>JAXA Earth API for Pythonは、JAXAが保有する各種地球観測データを扱うための公式Pythonライブラリだ。
衛星やセンサーごとの細かな仕様を意識せずにデータ取得や処理を行える点が特徴で、研究用途だけでなく、教育や試作的なアプリケーション開発にも利用されてきた。</p>
<p>最新版のv0.1.5では、こうした既存のAPI機能に加え、生成AIとの連携を想定したMCPサンプルが新たに追加された。</p>
<h2>生成AIが外部データを扱うための仕組み「MCP」</h2>
<p>MCPは、生成AIが外部のツールやデータソースを安全に利用するためのプロトコルだ。
Claude Desktopでは、ローカル環境で起動したMCPサーバーと接続することで、AIが外部APIを「ツール」として呼び出す構成を取ることができる。</p>
<p>JAXA Earth API for Python v0.1.5では、この仕組みを前提とした設定例やサンプルコードが公式ドキュメント上で公開されている。
<a href="https://data.earth.jaxa.jp/api/python/v0.1.5/en/">https://data.earth.jaxa.jp/api/python/v0.1.5/en/</a>{target=”_blank”}</p>
<h2>会話で地球観測データを扱う実験環境を公開</h2>
<p>JAXAによる今回の対応は、地球観測データを単にAPIとして提供するだけでなく、生成AIと組み合わせた新たな利用形態を試せる環境を提示した点が特徴だ。公式ドキュメントでは、仮想環境の構築からClaude Desktop側の設定までが整理されており、個人開発者や研究者が比較的容易に検証を始められる構成となっている。</p>
<p>生成AIを介した地球観測データの利活用が、今後どのような形で広がっていくのか注目される。</p>
]]></description>
      <pubDate>Mon, 02 Feb 2026 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI、GPT-5.2搭載の研究執筆AI「Prism」を無料で公開──人数制限なしの共同作業ワークスペース</title>
      <link>https://ledge.ai/articles/openai_prism_gpt5_2_research_writing_workspace</link>
      <description><![CDATA[<p>OpenAIは2026年1月27日、科学研究向けのAI執筆・共同作業ワークスペース「Prism」を<a href="https://openai.com/index/introducing-prism/">発表</a>した。GPT-5.2を中核に据えたAIネイティブ設計を採用し、論文執筆や修正、共同編集といった研究の日常業務を単一のクラウド環境に統合する。Prismは無料で利用でき、ChatGPTアカウントを持つユーザーであれば、人数制限なく共同作業を行える。</p>
<h2>研究執筆に残る分断を解消</h2>
<p>OpenAIは、AIは数学や生物学などの分野で研究の加速に寄与し始めている一方、論文執筆や推敲、数式管理、参考文献整理、共同編集といった日常業務は、依然として複数のツールに分断されたままだと述べる。研究者はエディタ、PDF、数式コンパイラ、文献管理ツール、チャットを行き来する必要があり、文脈の断絶が作業効率を下げてきた。Prismは、こうした断片化を解消する最初の取り組みとして位置付けられている。</p>
<h2>GPT-5.2を統合したAIネイティブ環境</h2>
<p>Prismは、OpenAIの最新モデルであるGPT-5.2を研究執筆のワークフローに直接組み込んだ点が特徴だ。数式や参考文献、論文全体の構造を文脈として理解したうえで、AIがドラフト作成や修正、推敲を支援する。研究者は、執筆環境の外にあるチャットツールを行き来することなく、同一のプロジェクト内でAIと対話しながら作業を進められる。</p>
<p><strong>OpenAIが公開したPrismの紹介動画。論文執筆とAI支援が同一ワークスペース内でどのように統合されているかを示している。</strong></p>
<p>@<a href="https://www.youtube.com/watch?v=xnInEsaaj9c">YouTube</a></p>
<h2>クラウド型・専門記法対応のワークスペース</h2>
<p>Prismは、学術論文で広く使われる数式や専門記法に対応したクラウドベースの執筆環境として設計されている。OpenAIが取得したクラウドLaTeX基盤「Crixet」を発展させたもので、既存の成熟した執筆・共同編集機能を土台に、AIを自然に統合したという。数式や図表、引用関係を横断的に理解しながら編集できる点を強みとする。</p>
<h2>研究チームでの共同作業を前提に設計</h2>
<p>科学研究は本質的に共同作業で進められる。Prismは、プロジェクト数や共同編集者数に制限を設けず、研究チーム全体での利用を前提に設計されている。クラウド型のため、ローカル環境の構築や専門ソフトのインストールは不要で、編集内容はリアルタイムに反映される。これにより、版管理や手動での統合作業にかかる負担を減らし、研究内容そのものに集中できるとしている。</p>
<h2>無料提供と今後の展開</h2>
<p>Prismは無料で利用でき、ChatGPTの個人アカウントを持つユーザーであれば、すぐに執筆や共同作業を開始できる。購読契約や席数の制限は設けられていない。OpenAIは今後、ChatGPT Business、Enterprise、Education向けにも提供を予定しており、より高度なAI機能については有料プランで順次展開するとしている。</p>
<p>OpenAIは、2025年にAIがソフトウェア開発の在り方を大きく変えたのに続き、2026年には科学研究でも同様の変化が起きると見ている。Prismは、研究者の日常業務に伴う摩擦を減らすことで、科学の進展を加速させるための第一歩と位置付けられている。</p>
]]></description>
      <pubDate>Mon, 02 Feb 2026 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ギネス認定の人型ロボット「Pepper」、AI接客で進化──ソフトバンクロボティクスが新モデル「Pepper+」提供開始</title>
      <link>https://ledge.ai/articles/softbank_robotics_pepper_plus_guinness</link>
      <description><![CDATA[<p>ロボット開発を手掛ける ソフトバンクロボティクスは2026年2月2日、人型ロボット「Pepper」が「世界初の量産型ヒューマノイド」としてギネス世界記録に認定されたと<a href="https://www.softbankrobotics.com/jp/news/press/20260202a/">発表</a>した。あわせて、AIによる接客機能などを強化した新モデル「Pepper+（ペッパープラス）」の提供を同日から開始した。</p>
<h2>AI接客を軸に刷新した新モデル「Pepper+」</h2>
<p>Pepper+は、同社が2014年に発表した人型ロボット「Pepper」の後継モデルにあたる。新モデルでは、生成AIを活用した対話機能をはじめとするAIエージェント機能を搭載し、来訪者との自然な会話や状況に応じた案内・接客を可能にした。</p>
<p>胸部に搭載するタブレットも刷新し、アプリケーション開発や機能拡張の柔軟性を高めている。人物認識や映像解析といった機能と組み合わせることで、小売店や商業施設、イベント会場などでの業務利用を想定した設計とした。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/pepper_b7bef26d34/pepper_b7bef26d34.jpg" alt="pepper +.jpg" /></p>
<h2>業務利用を前提にした提供形態</h2>
<p>Pepper+は、業務用途での利用を前提とした形で提供される。ソフトバンクロボティクスは、AI接客エージェントを中心に、案内業務やエンターテインメント用途など複数のソリューションを用意しており、既存のPepper向けサービスとの統合や移行も進めるとしている。</p>
<h2>「世界初の量産型ヒューマノイド」──ギネス認定の対象は初代Pepper</h2>
<p>今回発表されたギネス世界記録は、Pepper+そのものではなく、2014年に誕生した初代Pepperに対するものだ。ギネス世界記録では、Pepperを「世界初の量産型ヒューマノイド（サービスロボット）」として認定している。</p>
<p>初代Pepperは、研究用途にとどまらず、商用・量産モデルとして実際の現場に導入された点が評価対象となった。今回の発表は、その実績を改めて公式に位置づけるものとなる。</p>
<p>@<a href="https://www.youtube.com/watch?v=JgHKqOec_Mk">YouTube</a></p>
]]></description>
      <pubDate>Mon, 02 Feb 2026 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>SpaceX、AI企業xAIを買収──イーロン・マスク、AI・ロケット・衛星通信・モバイル通信を一体化した垂直統合型の技術基盤を構築へ</title>
      <link>https://ledge.ai/articles/spacex_acquires_xai_orbital_ai_data_center</link>
      <description><![CDATA[<p>衛星通信「Starlink」などを展開する SpaceX は2026年2月2日（米国時間）、人工知能（AI）開発企業 xAI を買収したと<a href="https://www.spacex.com/updates#xai-joins-spacex">発表</a>した。両社はいずれも起業家の イーロン・マスク 氏が率いており、今回の買収により、AI、ロケット、衛星通信、モバイル通信を一体化した垂直統合型の技術基盤を構築する。</p>
<h2>AIと宇宙インフラを統合する「次の本」</h2>
<p>SpaceXは今回の統合について、単なる事業拡張ではなく「次の章ではなく、次の“本”にあたる取り組み」だと位置づけた。AI、ロケット、宇宙ベースのインターネット、モバイル端末への直接通信、リアルタイム情報プラットフォームを横断的に結びつけ、人類の未来を加速させる基盤を構築するとしている。</p>
<h2>地上データセンターの限界を問題視</h2>
<p>発表では、近年のAIの進展が巨大な地上データセンターに依存している点を課題として挙げた。AIの学習や推論には膨大な電力と冷却が必要であり、世界的な電力需要の増大は、地域社会や環境に負担を与えかねないと指摘している。</p>
<p>SpaceXは、こうした制約を地上だけで解決することは困難であり、長期的には宇宙空間にAI計算基盤を移行する必要があるとの見解を示した。</p>
<h2>太陽エネルギーを活用する「軌道上AI」</h2>
<p>同社が描く構想の中核が、宇宙空間に展開する「軌道上データセンター」だ。宇宙ではほぼ常時太陽光を利用でき、運用や保守にかかるコストも限定的だとする。SpaceXは、100万基規模の衛星を打ち上げ、これらをAI計算基盤として機能させる構想を明らかにした。</p>
<p>この取り組みは、恒星のエネルギーを本格的に活用する文明段階を示す「カルダシェフ・スケール」にも言及し、人類が次の段階へ進む第一歩になると位置づけている。</p>
<h2>Starshipが支える大規模展開</h2>
<p><strong>SpaceXの次世代大型ロケット「Starship」。同社は、Starshipの打ち上げ能力を前提に、AI計算基盤となる衛星群を大規模に軌道へ展開する構想を示している。</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Starship_Deploy_91b8acf776_317a6581b0/Starship_Deploy_91b8acf776_317a6581b0.jpg" alt="Starship_Deploy_91b8acf776.jpg" /></p>
<p>SpaceXは、こうした構想を実現する前提として打ち上げ能力の飛躍的な向上を挙げた。これまでのFalconロケットによる打ち上げでは、年間の軌道投入量は数千トン規模にとどまっていたが、Starshipの本格運用により状況は大きく変わるとしている。</p>
<p>2026年には、Starshipによる次世代「V3 Starlink」衛星の投入が始まり、1回の打ち上げで従来の20倍以上の通信容量を追加できるという。また、次世代の直接モバイル通信衛星も投入し、地球上のあらゆる場所で携帯通信を可能にする構想も示された。</p>
<h2>年間100GW、将来は1TW規模へ</h2>
<p>SpaceXは試算として、年間100万トンの衛星を打ち上げ、1トンあたり100kWの計算能力を持たせた場合、年間100ギガワット（GW）のAI計算能力を軌道上に追加できると説明した。将来的には、年間1テラワット（TW）規模まで拡張する道筋もあるとしている。</p>
<p>同社は、今後2〜3年以内に、AI計算能力を最も低コストで生み出せる手段が「宇宙になる」との見通しも示した。</p>
<h2>月・火星、さらに深宇宙へ</h2>
<p>今回の構想は地球周回軌道にとどまらない。Starshipの能力により、月面への大量輸送や恒久拠点の構築が可能になるとし、月の資源を活用した衛星製造や、深宇宙への展開にも言及した。これにより、AI計算基盤が月や火星での活動、さらには人類の多惑星文明化を支える中核インフラになると位置づけている。</p>
<p>SpaceXは、宇宙ベースのAIデータセンターが、科学研究や技術革新を加速させるだけでなく、将来的な月面基地や火星都市の建設を資金面・技術面から支える基盤になるとしている。</p>
]]></description>
      <pubDate>Mon, 02 Feb 2026 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>SpaceX、AI向け「宇宙データセンター」構想をFCC提出──太陽光を直接使う“軌道上AIインフラ”、衛星100万基規模</title>
      <link>https://ledge.ai/articles/spacex_fcc_orbital_ai_data_center_1m_satellites</link>
      <description><![CDATA[<p>SpaceXが、AI向け計算基盤の新たな構想を打ち出した。<a href="https://www.reuters.com/business/aerospace-defense/spacex-seeks-fcc-nod-solar-powered-satellite-data-centers-ai-2026-01-31/">Reuters</a>によると、SpaceXは2026年1月30日、米連邦通信委員会（FCC）に対し、将来的に最大100万基規模へ拡大する衛星コンステレーション計画を申請した。これらの衛星は、従来の通信用途にとどまらず、「軌道上データセンター」としてAIの計算・データ処理を担う構想が盛り込まれているという。</p>
<h2>地上の電力制約を回避する「宇宙」という選択肢</h2>
<p>Reutersによれば、今回申請された低軌道（LEO）衛星群は、衛星同士をネットワークで接続し、高度な計算機能を担うことを想定している。宇宙空間でほぼ連続的に得られる太陽光を直接電力源として活用することで、地上データセンターが直面する電力供給の制約を回避できる可能性があると報じている。</p>
<h2>FCCの審査と「Starlink Gen2」の判断例</h2>
<p>FCCへの申請手続き上、こうした構想を含む衛星システムも通信衛星として扱われ、周波数利用の適格性などが審査対象となる。FCCは2026年1月9日、SpaceXが申請していた次世代通信衛星「Starlink Gen2」を正式に承認しており、公式文書（<a href="https://docs.fcc.gov/public/attachments/DA-26-36A1.pdf">DA-26-36A1</a>）では、大規模な低軌道衛星コンステレーションであっても、既存の通信枠組みに基づき審査・判断が行われることが示されている。</p>
<p>今回の申請は、提出時点で承認されたものではなく、今後FCCによる審査を経て判断される。</p>
]]></description>
      <pubDate>Mon, 02 Feb 2026 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Waymoの自動運転車が学校近くで子どもと接触、当局が調査開始──同社「17mph→6mph未満まで減速」</title>
      <link>https://ledge.ai/articles/waymo_school_zone_child_collision_nhtsa_probe</link>
      <description><![CDATA[<p>Waymoは2026年1月28日、米カリフォルニア州サンタモニカで自社の自動運転車が若い歩行者と接触した事故について、公式ブログで詳細を<a href="https://waymo.com/blog/2026/01/a-commitment-to-transparency-and-road-safety">公表</a>した。事故は学校から2ブロック以内の場所で発生しており、米運輸省道路交通安全局（NHTSA）は同件について予備調査を開始している。</p>
<h2>学校近くで発生、登校時間帯の事故</h2>
<p>Waymoによると、事故が起きたのは1月23日午前8時前後。場所はSanta Monica市内の学校近くで、周囲には路上駐車されたSUVがあり、視界が制限されていたという。
歩行者の子どもが車両の死角から進路に入ったため、同社の自動運転システムはブレーキ操作を行い、速度を約17マイル（約27km）から6マイル（約10km）未満まで減速したものの、完全には停止できず接触に至ったとしている。</p>
<h2>接触後は911に通報、車両は現場待機</h2>
<p>接触後、歩行者は自力で歩道へ移動できる状態だったとされる。Waymoの車両は自動的に911へ通報し、警察が現場に到着するまでその場に待機。警察の許可を得た後に現場を離れたという。
同社は事故当日にNHTSAへ報告を行い、当局の調査に全面的に協力する姿勢を示している。</p>
<h2>NHTSAが予備調査を開始</h2>
<p>NHTSAの事故調査部門（ODI）は、本件について予備調査（Preliminary Evaluation）を開始した。公開文書によると、調査では以下の点が主な焦点とされている。</p>
<ul>
<li>学校周辺・登校時間帯における自動運転システムの挙動</li>
<li>制限速度および注意義務の順守状況</li>
<li>衝突後の対応や安全手順</li>
</ul>
<p>事故が学校ゾーンに近接した環境で発生した点を踏まえ、同様の条件下での安全性が検証対象となる。</p>
<h2>人間運転との比較も提示</h2>
<p>Waymoは公式ブログの中で、同社が参照する査読済みモデルに基づき、「同様の状況下で注意深い人間ドライバーであれば、約14マイル（約22km）程度の速度で接触していた可能性がある」との比較も示した。ただし、同社はこの比較が事故の正当化を目的とするものではなく、安全性能評価の文脈で提示したものだと説明している。</p>
<h2>調査結果は今後公表へ</h2>
<p>現時点で事故原因や法的責任についての結論は出ておらず、NHTSAの調査結果が待たれる状況だ。Waymoは「透明性と道路安全への責任」を強調し、調査の進展に応じて追加情報を公開するとしている。</p>
]]></description>
      <pubDate>Mon, 02 Feb 2026 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>xAI、動画生成AI「Grok Imagine 1.0」公開──10秒・720p動画と音声品質を大幅強化</title>
      <link>https://ledge.ai/articles/xai_grok_imagine_1_0_video_generation_release</link>
      <description><![CDATA[<p>米AI企業のxAIは2026年2月2日、動画生成AIの新モデル「Grok Imagine 1.0」を公開した。公式Xで<a href="https://x.com/xai/status/2018164753810764061">発表</a>した。</p>
<p>xAIは今回のリリースを「これまでで最大の進化（our biggest leap yet）」と位置づけており、同社の生成AIサービスにおける動画生成機能を本格的に拡張するものとなる。新モデルでは、最大10秒の動画生成や720p解像度への対応に加え、音声品質やプロンプト追従性を大幅に向上させたとしている。</p>
<h2>最大10秒・720p対応、音声とプロンプト理解を刷新</h2>
<p>Grok Imagine 1.0では、生成できる動画の長さが最大10秒に拡張され、解像度も720pに対応した。xAIは、映像のディテールがより鮮明になり、モーションも滑らかになったことで、従来よりもシーン全体が自然に展開する映像表現が可能になったと説明している。</p>
<p>あわせて音声生成も大幅に改善され、キャラクターの発話に感情や表現力を持たせることができるほか、場面に応じた音楽を自動生成し、映像と同期させることも可能だという。プロンプト理解も向上しており、最初の指示だけでなく、フォローアッププロンプトを通じて内容を調整するなど、対話的な動画生成が行える点を特徴として挙げている。</p>
<h2>直近30日で12億本超を生成、日常用途も想定</h2>
<p>xAIによると、Grok Imagineは直近30日間で約12.45億本（1.245 billion）の動画を生成したという。具体的なユーザー数や地域別の内訳は明らかにしていないものの、短期間で大量の動画が生成されている点から、同社はサービスの利用が急速に広がっていることを示唆している。用途としては、古い家族写真やペットの写真をアニメーション化するほか、その日のニュースを題材にミームや短編クリップを作成するといった日常的な使い方も想定しており、専門的な映像制作に限らない幅広い利用シーンを見込んでいる。</p>
<h2>APIモデルは外部ベンチマークで高評価と説明</h2>
<p>あわせてxAIは、Grok ImagineのAPIモデルが、第三者によるAIモデル評価サイトのArtificial Analysisの動画生成ベンチマークにおいてトップ評価を獲得したと<a href="https://x.ai/news/grok-imagine-api">説明</a>している。同社は、こうした外部評価を、Imagine 1.0を支える基盤モデルの品質を示す材料の一つとして位置づけている。xAIは今後の改善に向けてユーザーからのフィードバックも重視するとしており、生成した作品をX上で共有し、@xaiをタグ付けして投稿するよう呼びかけている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Artificial_Analysis_9de29a7743/Artificial_Analysis_9de29a7743.jpg" alt="Artificial Analysis.jpg" /></p>
]]></description>
      <pubDate>Mon, 02 Feb 2026 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>技術の思春期に入ったAI──Anthropic CEOダリオ・アモデイ氏、国家安全保障・経済・民主主義へのリスクに警鐘</title>
      <link>https://ledge.ai/articles/ai_technology_adolescence_amodei_democracy_risk</link>
      <description><![CDATA[<p>AI開発企業AnthropicのCEOである ダリオ・アモデイ 氏は2026年1月27日、強力なAIが社会にもたらすリスクについて論じた長文エッセイ「The Adolescence of Technology（技術の思春期）」を<a href="https://www.darioamodei.com/essay/the-adolescence-of-technology">公開</a>した。同氏は、AIが急速に能力を高める一方で、制度や社会の成熟が追いついていない「危険な過渡期」にあると指摘し、国家安全保障、経済、民主主義の3領域で深刻な影響が生じうると警告している。</p>
<h2>「技術の思春期」とは──能力が先行する危うい段階</h2>
<p>アモデイ氏は現在のAIを、人間の成長過程になぞらえて「技術の思春期（Adolescence）」にあると表現する。この段階では、技術的な能力が急速に拡大する一方で、それを安全かつ安定的に運用するためのガバナンスや社会制度が十分に整っていない。</p>
<p>強い影響力を持ち始めながらも、判断力や自制が成熟していない――同氏は、こうした「能力の増大」と「未成熟さ」が同居する状態こそが、現在のAIを最も不安定で危険な存在にしていると論じている。</p>
<h2>国家安全保障：AIが不安定化させる軍事・サイバー空間</h2>
<p>国家安全保障の領域では、AIの高度化が軍事、諜報、サイバー分野に新たな不安定要因をもたらす可能性があると指摘した。
サイバー攻撃や情報操作の高度化により、従来の抑止や防御の枠組みが機能しにくくなる恐れがあるという。</p>
<p>また、国家間の技術競争が激化する中で、安全対策や慎重な検証が後回しにされる構造的リスクにも言及しており、AIは一度広く普及すると制御が難しくなる点を強調している。</p>
<h2>経済：生産性向上の裏で進む急激な再編</h2>
<p>経済面では、AIが生産性を大きく押し上げる可能性を認めつつ、その恩恵が均等に分配されないリスクを挙げた。雇用構造の急変や、特定の企業や国家への権力集中が進めば、社会的な不安定化につながりかねないとしている。</p>
<p>技術進歩のスピードに制度的・社会的な調整が追いつかない場合、短期間で大きな歪みが生じる可能性があるとし、経済的影響を過小評価すべきではないとの認識を示した。</p>
<h2>民主主義：情報環境の脆弱化と「共通の現実」の喪失</h2>
<p>民主主義への影響について、アモデイ氏は特に強い懸念を示している。AIによる情報生成・拡散能力の向上は、世論操作や偽情報の高度化を招き、民主主義が前提としてきた「共通の現実」や情報の信頼性を損なう恐れがあるという。</p>
<p>同氏は、こうした情報環境の変化が、有権者の判断基盤そのものを弱体化させる可能性がある点を、深刻なリスクとして位置づけている。</p>
<h2>米ミネソタ州での出来事が浮き彫りにした「国内の民主主義」</h2>
<p>アモデイ氏は、このエッセイが主にAIと将来を論じたものであるとしつつも、公開時点で米ミネソタ州をめぐって起きている出来事に触れ、民主主義的価値と権利を国内で守る重要性が一層切実になっていると述べている。</p>
<p>同州では連邦当局の強制執行をめぐる死亡事件などをきっかけに、権利や法の在り方を巡る議論と抗議が広がっており、アモデイ氏はこうした現実の緊張が、エッセイで論じた「民主主義の脆弱性」を現実の問題として浮き彫りにしていると示唆した。</p>
<p>なお、ミネソタ州での出来事をめぐっては、Reutersが、OpenAIの サム・アルトマン CEOが従業員向けの内部メッセージで、移民当局ICEの対応を「行き過ぎ」と批判したと報じている。</p>
<h2>前作「Machines of Loving Grace」との関係</h2>
<p>アモデイ氏は今回のエッセイを、約1年前に発表した「<a href="https://www.darioamodei.com/essay/machines-of-loving-grace">Machines of Loving Grace</a>」（強力なAIを正しく活用できた場合に何が実現できるかを論じたエッセイ）の“対”に位置づけている。</p>
<p>前作では、AIがもたらしうる可能性や理想像に焦点が当てられていたのに対し、今回のエッセイでは、その裏側にあるリスクや、社会が備えるべき課題が正面から論じられている。</p>
<h2>「止める」か「進める」かではなく、どう向き合うか</h2>
<p>アモデイ氏は、AI開発を止めるべきだと主張しているわけではない。一方で、無条件に加速させる姿勢にも警鐘を鳴らしている。必要なのは、ガバナンス、制度設計、国際的な協調を含め、社会全体でAIと向き合う枠組みを整えることだとし、「技術の思春期」にある今の判断が、将来の安定性を大きく左右すると結論づけている。</p>
]]></description>
      <pubDate>Sat, 31 Jan 2026 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>AI研究の主要国際会議「NeurIPS 2025」採択論文でハルシネーション多数──GPTZero、「vibe citing（雰囲気引用）」を中心概念として定義し分析を公開</title>
      <link>https://ledge.ai/articles/neurips_2025_hallucinated_citations_gptzero</link>
      <description><![CDATA[<p>AI生成コンテンツの検出を手がけるGPTZeroは2026年1月21日、AI研究の主要国際会議NeurIPS 2025の採択論文を分析した結果、51本の論文にまたがる計100件の「ハルシネーション（幻覚引用）」を確認したと<a href="https://gptzero.me/news/neurips/">発表</a>{target=”_blank”}した。対象は、NeurIPS 2025で採択された論文のうち4,841本で、いずれも既に採択され、会議で発表済みの論文だとしている。</p>
<h2>採択論文4,841本を分析、51本で「確認済み」100件</h2>
<p>GPTZeroによると、今回確認された「幻覚引用」とは、実在しない、または事実と一致しない参考文献が、もっともらしい書誌情報として記載されている状態を指す。具体的には、実在論文に似たタイトルや著者名を組み合わせたもの、存在しないDOIやURL、別論文を指すarXiv IDなどが含まれるという。
同社は、専用ツール「Hallucination Check」でオンライン照合できない引用を抽出したうえで、人手による確認を経て“幻覚”と判断した例のみを集計したとしている。</p>
<h2>「vibe citing（雰囲気引用）」と定義、誤検出の可能性にも言及</h2>
<p>GPTZeroは、生成AIが実在文献を下敷きにしながら、著者名・タイトル・掲載先・年次などを混在させて作る引用を「vibe citing（雰囲気引用）」と呼び、今回の分析の中心概念として定義した。
一方で、同社は「照合不能＝即ハルシネーションではない」とも説明しており、未公開資料やアーカイブ文献などがオンラインで見つからない場合もあるとして、最終判断には人の確認が必要だと注意を促している。</p>
<h2>投稿数急増で査読負荷が拡大、公式統計も</h2>
<p>NeurIPS運営側の公式発表によると、NeurIPS 2025の有効投稿数は21,575本、採択数は5,290本、採択率は24.52％だった。投稿数は近年大きく増加しており、査読体制への負荷が高まっている状況が示されている。</p>
<p>GPTZeroは、今回の結果について、特定の著者や査読者を批判するものではなく、投稿規模の拡大と生成AIの普及が、従来の査読プロセスに新たな課題を突きつけていると説明している。</p>
<h2>LLM利用ポリシーでは、問題があれば措置の可能性も</h2>
<p>NeurIPSは、論文作成における大規模言語モデル（LLM）の利用について<a href="https://neurips.cc/Conferences/2025/LLM">公式ポリシー</a>{target=”_blank”}を設けており、内容の正確性は著者が責任を負うと明記している。科学的整合性に重大な問題が認められた場合、受理後であっても調査や措置の対象となり得るとしている。</p>
<p>GPTZeroは、引用確認を含むチェック工程を、著者・査読者・編集・運営の各段階で支援するツールの活用が有効だとし、今後も学術出版プロセスの透明性向上を目指すとしている。</p>
]]></description>
      <pubDate>Sat, 31 Jan 2026 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>GPT-4oが再び退場へ──OpenAI、ChatGPTで旧モデル整理　2月13日に提供終了、利用の大半はGPT-5.2に移行</title>
      <link>https://ledge.ai/articles/gpt-4o_retirement_chatgpt_feb_13</link>
      <description><![CDATA[<p>OpenAIは2026年1月29日、ChatGPTで提供している「GPT-4o」「GPT-4.1」「GPT-4.1 mini」「OpenAI o4-mini」の4モデルを、2026年2月13日をもって提供終了（retire）すると<a href="https://openai.com/index/retiring-gpt-4o-and-older-models/">発表</a>した。同日には、すでに発表されている「GPT-5（InstantおよびThinking）」のChatGPTでの提供終了も実施される。なお、APIについては現時点で変更はないとしている。ChatGPT上で利用可能な旧世代モデルを整理し、より新しいモデルにリソースを集中させる狙いだ。</p>
<h2>2月13日にChatGPTから4モデルを除外</h2>
<p>提供終了の対象となるのは、以下の4モデルだ。</p>
<ul>
<li>GPT-4o</li>
<li>GPT-4.1</li>
<li>GPT-4.1 mini</li>
<li>OpenAI o4-mini</li>
</ul>
<p>これらのモデルは2月13日以降、ChatGPTのモデル選択肢から削除され、同サービス上では利用できなくなる。一方で同社は、APIについては「現時点では変更はない」と明記している。</p>
<h2>GPT-4oは一度引退後に復活、再び退場へ</h2>
<p>OpenAIは今回の告知の中で、GPT-4oについて特別に背景を説明している。</p>
<p>GPT-4oは、過去に一度ChatGPTから非推奨（deprecated）となった後、GPT-5のリリース時にアクセスが復活した経緯がある。復活の背景には、PlusおよびProユーザーの一部から寄せられたフィードバックがあった。具体的には、創造的なアイデア出しなどの用途で移行に時間が必要だったことや、GPT-4oの「会話のスタイルや温かみ」を評価する声があったとしている。</p>
<p><strong>「GPT-5切替で “4oロス” 広がる──ChatGPT界隈を席巻した「#keep4oムーブ」と期間限定 “里帰りモード”」</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Chat_GPT_Image_2025_8_12_18_58_22_d73354f1a7/Chat_GPT_Image_2025_8_12_18_58_22_d73354f1a7.jpg" alt="ChatGPT Image 2025年8月12日 18_58_22.jpg" /></p>
<h2>利用の大半はGPT-5.2に移行、GPT-4o選択は0.1％</h2>
<p>OpenAIは、今回あらためてGPT-4oを提供終了とする理由について、「必要な改善がすでに整った」ことと、「利用状況の変化」を挙げた。</p>
<p>同社によると、現在のChatGPT利用の大半はすでにGPT-5.2に移行しており、日常的にGPT-4oを選択しているユーザーは全体の0.1％にとどまっているという。この状況を踏まえ、GPT-4oの役割は事実上、新世代モデルに引き継がれたと判断した。</p>
<h2>GPT-5.1／5.2で人格や創造性を強化</h2>
<p>OpenAIは、GPT-4oに寄せられたフィードバックが、GPT-5.1およびGPT-5.2の改良に直接反映されたと説明している。</p>
<p>新世代モデルでは、人格や創造性の改善に加え、ChatGPTの応答スタイルを調整できる仕組みを拡充。ベースとなるスタイルやトーンの選択に加え、温かみや熱量といった要素をユーザーが調整できるようになっている。</p>
<h2>過剰な拒否や説教的応答の是正にも言及</h2>
<p>OpenAIは今後の方向性として、ChatGPTの人格や創造性のさらなる改善に加え、「不要な拒否」や「過度に慎重、あるいは説教的な応答」への対処を進めていく方針も示した。これらについては、近くアップデートを予定しているという。</p>
<p>また、18歳以上の成人ユーザー向けに、適切な安全策の範囲内で選択肢と自由度を拡張したChatGPT体験を目指す考えも示している。この取り組みの一環として、OpenAIは18歳未満と推定される利用者を対象に、年齢に応じた体験を提供するための年齢推定（age prediction）を、多くの市場で展開していることにも触れた。</p>
<h2>「苦渋の判断」としつつ改善に集中</h2>
<p>OpenAIは、GPT-4oの提供終了について「一部のユーザーにとっては不満や不便を感じる決定であることは理解している」としつつ、「モデルの引退は決して容易ではないが、より良いモデルの改善に集中するために必要な判断だ」と説明している。</p>
]]></description>
      <pubDate>Fri, 30 Jan 2026 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>日本発ヒューマノイド「cinnamon 1」初公開　ドーナッツロボティクス、無言で操作できる特許技術も発表</title>
      <link>https://ledge.ai/articles/donut_robotics_cinnamon1_humanoid_silent_gesture</link>
      <description><![CDATA[<p>ヒューマノイド開発に取り組むスタートアップ、ドーナッツロボティクスは2026年1月21日、日本ブランドのヒューマノイド『cinnamon 1（シナモン ワン）』を<a href="https://prtimes.jp/main/html/rd/p/000000041.000057944.html">発表</a>した。二足歩行が可能な量産型ヒューマノイドとして、同社は2026年内の市場投入を目指す。</p>
<h2>二足歩行の量産ヒューマノイド、年内投入を目標に</h2>
<p>「cinnamon 1」は、同社が保有する特許技術を搭載した二足歩行のヒューマノイドロボットだ。現時点では海外企業から提供された機体をベースに、独自開発のAIを搭載している。将来的には、機体も含めた国産ヒューマノイドの実現を目指すとしている。</p>
<p>AIには、視覚情報・言語理解・行動を統合する「Vision-Language-Action（VLA）」の概念を取り入れる予定で、人の指示を理解し、自律的に行動するロボットの開発を進める。</p>
<h2>声を出さずに指示できる「サイレント ジェスチャー コントロール」</h2>
<p>同社はあわせて、手振りや指の動きだけでロボットに指示を伝える特許技術「サイレント ジェスチャー コントロール」を発表した。音声を使わずに操作できる点が特徴で、同社はこの技術を搭載したヒューマノイドとして「世界初」としている。</p>
<p>騒音の大きい工場や建設現場、声を出しにくい家庭環境などでの利用を想定しており、難聴者が多い環境でも使いやすい技術として位置づけている。</p>
<h2>工場・建設現場での作業代替を想定</h2>
<p>ドーナッツロボティクスは、2026年内に工場内や建設現場での作業代替を進める計画を示している。2025年10月には、建築関連事業を手がける株式会社エムビーエスと資本業務提携を発表しており、建設業界での活用を見据える。</p>
<p>また、VLA開発を支える国内データセンターの設立構想にも言及しており、ヒューマノイド開発を軸にしたAI基盤づくりを進める方針だ。</p>
<p>YouTube動画を引用する場合　　　
@<a href="https://www.youtube.com/watch?v=1vBI0GYjAKM">YouTube</a></p>
]]></description>
      <pubDate>Sun, 25 Jan 2026 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>フィールズ賞のテレンス・タオ氏、「GPT-5.2 Proが数学の未解決問題をほぼ自律的に解き切った」と評価──エルデシュ問題#728で示されたAIの新たな到達点</title>
      <link>https://ledge.ai/articles/ai_autonomous_solution_erdos_problem_728</link>
      <description><![CDATA[<p>AIが数学の未解決問題を「ほぼ自律的に解き切った」と、数学者が評価した。著名な数学者であるテレンス・タオ氏が2026年1月8日、分散型SNS「Mathstodon」への投稿で、エルデシュ問題の一つである #728 が、AIツールによって「more or less autonomously（ほぼ自律的に）」解かれたと<a href="https://mathstodon.xyz/@tao/115855840223258103">述べた</a>。</p>
<p>この成果についてタオ氏は「私たちの知る限り、既存の文献では再現されていない」としたうえで、近年のAIツールの能力向上を示す「節目（milestone）」だと位置づけた。</p>
<h2>エルデシュ問題とは</h2>
<p>エルデシュ問題は、20世紀を代表する数学者ポール・エルデシュが提起した数多くの問題をもとに整理された、数学の未解決問題群を指す。現在も多くの問題が未解決のまま残されており、世界中の数学者が長年にわたって取り組んできた。</p>
<p>今回タオ氏が言及した エルデシュ問題#728 は、その中の一つで、問題の内容や背景はエルデシュ問題の<a href="https://www.erdosproblems.com/728">公式サイト</a>で公開されている。</p>
<p>未解決問題は、数学界において特別な位置づけを持つ。解決に至れば理論的に重要な意味を持つだけでなく、長年の研究の蓄積を塗り替える可能性があるためだ。</p>
<p><strong>■ ポール・エルデシュ（左）と、当時10歳のテレンス・タオ。1985年撮影。</strong> エルデシュ問題は、エルデシュ自身が提起した未解決問題群に由来する。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Paul_Erdos_with_Terence_Tao_952867730e/Paul_Erdos_with_Terence_Tao_952867730e.jpg" alt="Paul_Erdos_with_Terence_Tao.jpg" /></p>
<h2>GPT-5.2 Proと人間の役割分担</h2>
<p>証明には、OpenAIの大規模言語モデル GPT-5.2 Pro が用いられた。人間側は、計算環境の整備や初期試行へのフィードバック、結果の検証、形式的な証明への整理といった役割を担ったとされる。</p>
<p>AIが完全に単独で問題を解決したわけではないものの、思考の主導権がどこにあったのかが、今回の評価における重要なポイントとなった。</p>
<p><strong>■ 「ほぼ自律的に」解いた、という表現</strong>
従来、AIは数学分野において計算の高速化や文献探索、発想の補助といった役割を担ってきた。一方で今回のケースでは、問題の解釈から解法の構築、証明に至るまでの過程をAIが主導したと評価された。</p>
<p>タオ氏によれば、AIは初期の試行に対する一定のフィードバックを受けた後、問題の趣旨に沿った形で解決に到達したという。この点が、「ほぼ自律的」という表現につながった。</p>
<h2>「解決以上に興味深い点」</h2>
<p>タオ氏は今回の投稿で、解決結果そのものに加え、「それ以上に興味深い点がある」とも述べている。投稿では、</p>
<p>「より興味深いのは、解法の説明（exposition）を迅速に書き、書き直し、再構成するAI主導の能力が現れつつある点だ」
と指摘し、証明文書の作成や改稿をめぐるAIの能力向上に言及した。</p>
<p><strong>■ テレンス・タオ氏がMathstodonに投稿したエルデシュ問題#728に関する発言。AIが「ほぼ自律的に」問題を解いたと評価している。</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/terencetao_mathstodon_b05b8af4d5/terencetao_mathstodon_b05b8af4d5.jpg" alt="terencetao mathstodon.jpg" /></p>
<p>従来、数学論文の執筆や大幅な改稿には多大な時間と労力が必要だったが、AIによる文章生成と形式証明ツールを組み合わせることで、説明文を用途や読者に応じて柔軟に作り直すことが可能になりつつあるという。</p>
<h2>他のAI解決事例との位置づけ</h2>
<p><strong>■ エルデシュ問題を巡るAI活用事例を整理した公式Wikiの一部。#728は、AIによる解決後に類似文献が確認されたケースとして位置づけられている。</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/github_erdos_problem_058007f757/github_erdos_problem_058007f757.jpg" alt="github erdos problem.jpg" /></p>
<p>タオ氏自身も、AIによるエルデシュ問題の解決例の多くでは、後に類似の結果が既存文献で確認されてきたと説明している。#728についても、問題文の再構成が比較的最近まで行われていなかったことが、先行研究が見当たらなかった理由の一つだとされている。</p>
<h2>数学とAIの関係に生じた変化</h2>
<p>今回の発言は、AIが数学者の役割を代替したことを示すものではない。一方で、未解決問題を対象とした成果について、数学者がAIの関与を明示的に評価し、その能力の到達点を具体的に言及した点は確認できる事実である。エルデシュ問題#728をめぐる今回の事例は、AIの活用がどの段階まで進んでいるのかを示す一例として位置づけられ、今後、同様の評価が他の問題や分野でも示されるかどうかが注目される。</p>
]]></description>
      <pubDate>Sat, 24 Jan 2026 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>LLMは「同じ質問を2回」入力すると精度が上がる──Google研究者ら、プロンプト反復の効果を短報で報告</title>
      <link>https://ledge.ai/articles/prompt_repetition_improves_llm_accuracy</link>
      <description><![CDATA[<p>Googleの研究者らは、同一の質問文を2回連結して入力するだけで、大規模言語モデル（LLM）の回答精度が向上するとする研究成果を発表した。論文は短報「Prompt Repetition Improves Non-Reasoning LLMs」として2025年12月17日に arXivに<a href="https://arxiv.org/abs/2512.14982v1">公開</a>されており、推論（reasoning）を用いない設定において、主要LLMと複数のベンチマークで広範な改善が観測されたという。</p>
<h2>質問文を「そのまま2回」繰り返すだけ</h2>
<p>研究で提案された手法は、質問文を変更・補足するのではなく、同一のクエリをそのまま2回連結して入力するというものだ。例えば、通常はQと入力するところをQQとする。特別な指示文や追加のプロンプト設計は必要としない。論文では、この操作を \u003CQUERY\u003E を \u003CQUERY\u003E\u003CQUERY\u003E に変換するものとして説明している</p>
<p>LLMは因果言語モデルとして学習されており、トークンの並び順が注意（attention）の届き方に影響する。このため、質問文と文脈や選択肢の配置順（question-first / options-first）によって性能差が生じることが知られている。プロンプト反復は、各トークンが他のすべてのトークンを参照しやすくすることで、この差を緩和すると説明されている。</p>
<h2>非推論設定で顕著な改善、70条件中47で「有意に向上」</h2>
<p>実験は、各AIモデルの公式APIを用いて実施され、2025年2月から3月にかけて評価された。対象には、Gemini、GPT、Claude、DeepSeekといった複数の主要LLMが含まれている。具体的には、Gemini 2.0 Flash／Flash Lite、GPT-4o／GPT-4o-mini、Claude 3 Haiku／Claude 3.7 Sonnet、DeepSeek V3が評価対象となった。</p>
<p>論文では、7つのモデルと7つのベンチマークなどを組み合わせた計70条件で比較を行った。その結果、統計検定（McNemar検定、p\u003C0.1）の基準で47条件において性能が有意に改善し、性能が低下した条件はなかったとしている。</p>
<p><strong>推論を用いない設定におけるPrompt Repetitionとベースラインの精度比較。星印は統計的に有意な改善（p\u003C0.1）を示す。70条件中47で改善、悪化は確認されなかった</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/figure1_big2_202ffaa00c/figure1_big2_202ffaa00c.jpg" alt="figure1_big2.jpg" /></p>
<h2>生成トークン数・レイテンシは原則増えず</h2>
<p>論文では、回答精度が向上した一方で、生成トークン数やレイテンシ（応答時間）は多くの条件で増加しなかったと報告されている。反復は並列化可能なprefill（前処理）段階で完結するためだという。これは、生成プロセスそのものではなく、入力を読み込む段階で処理が完結するためだと説明されている。</p>
<p>ただし例外として、非常に長い入力や反復×3などの条件では、Claude系モデルでprefillが重くなり、レイテンシが増える場合がある点も明記されている。</p>
<h2>推論（step-by-step）を有効にした場合は「中立〜わずかに正」</h2>
<p>推論を促す設定（think step by step）では、プロンプト反復の効果は中立からわずかに正にとどまった。28条件中5勝・1敗・22引き分けで、研究者らは「推論モデルはそもそも推論過程の中で入力の再読・反復を行うため」と説明している。</p>
<p>一方で、入力文が非常に長い場合や、同じ質問を3回以上繰り返す設定では、モデルや条件によっては応答時間が増加する可能性も指摘されている。</p>
<h2>追加学習なしで性能を引き出す可能性</h2>
<p>この手法は、追加学習や外部ツールを必要とせず、出力形式も変えないため、既存システムにドロップインで導入可能だとされる。研究チームはこれを「多くのタスクにおけるデフォルト手法の候補」と位置付けている。研究チームは今後の方向性として、反復部分の最適化やKVキャッシュの扱い、非テキストモダリティへの応用などを挙げている。</p>
]]></description>
      <pubDate>Sat, 03 Jan 2026 23:50:00 GMT</pubDate>
    </item>
  </channel>
</rss>