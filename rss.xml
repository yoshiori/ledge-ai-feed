<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
  <channel>
    <title>Ledge.ai 新着記事</title>
    <link>https://ledge.ai/</link>
    <description>Ledge.ai の最新テクノロジー記事</description>
    <item>
      <title>Amazon、倉庫労働の自動化を加速──内部文書で「50万人置き換え」計画浮上、同社は否定</title>
      <link>https://ledge.ai/articles/amazon_robotics_automation_500k_plan</link>
      <description><![CDATA[<p>Amazonが、物流センターにおける自動化を大幅に拡大し、最大50万人の従業員をロボットに置き換える計画を進めていることが、<a href="https://www.nytimes.com/2025/10/21/technology/inside-amazons-plans-to-replace-workers-with-robots.html">ニューヨーク・タイムズ</a>が入手した社内文書により明らかになったと報じた。</p>
<p>同紙によると、文書はAmazon Robotics部門が作成したもので、2033年までに50万〜60万人分の人的業務をロボットやAIによって自動化するという長期計画が記されている。短期的にも2027年までに約16万人分の作業を代替する見通しが示されており、対象となるのは倉庫内での仕分け、ピッキング、梱包、搬送といった物理的業務だという。</p>
<p>また、文書では自動化により出荷1件あたり約0.30ドルのコスト削減が見込まれ、2025〜2027年の3年間で約126億ドル（約1兆9,000億円）の経費削減を達成できると試算しているという。</p>
<p>これに対し、Amazonは<a href="https://www.theverge.com/news/803257/amazon-robotics-automation-replace-600000-human-jobs">The Verge</a>の取材に対し、報道内容を否定した。広報担当者は「この文書は特定チームの分析をまとめたものであり、全社的な人員計画や雇用戦略を示すものではない」と述べ、さらに「当社は依然として多数の雇用を創出しており、ロボット導入が雇用喪失を意味するわけではない」とコメントしている。</p>
<p>Amazonは2012年に物流ロボット企業Kiva Systemsを買収して以降、自社倉庫の自動化を中核戦略として進めてきた。近年では、AIによる在庫管理やロボット搬送に加え、配送現場でもAIスマートグラスを活用するなど、エンドツーエンドでの自動化を進めている。</p>
<p>一方、労働組合や雇用政策の専門家からは「労働コスト削減の裏で雇用機会が失われる可能性がある」と懸念の声も上がっている。Amazonが今後、自動化と雇用創出の両立をどのように図るかが注目される。</p>
]]></description>
      <pubDate>Sat, 25 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Amazon、配達ドライバー向けAIスマートグラスを発表──荷物スキャンから配達証明までハンズフリーで</title>
      <link>https://ledge.ai/articles/amazon_ai_smart_glasses_for_delivery_drivers</link>
      <description><![CDATA[<p>Amazonは2025年10月22日（現地時間）、配達ドライバーが安全かつ効率的に業務を行えるよう支援するAI搭載スマートグラス<a href="https://www.aboutamazon.com/news/transportation/smart-glasses-amazon-delivery-drivers">発表</a>した。荷物のスキャンからルート案内、配達証明の撮影までをハンズフリーで行える新デバイスで、同社の物流ネットワークにおける最新のイノベーションとして位置づけられている。</p>
<p>このスマートグラスは、ドライバーがスマートフォンを操作することなく、視界内で作業情報を確認し、音声操作で指示を実行できるよう設計されている。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/amazon_ai_smart_glass_fb85f5e064/amazon_ai_smart_glass_fb85f5e064.jpg" alt="amazon ai smart glass.jpg" /></p>
<p>AIアシスタント機能を備え、目的地までのルート案内、荷物バーコードのスキャン、配達証明の撮影など、一連の業務を統合的にサポートする。安全面にも配慮し、運転中は自動的に無効化される設計を採用。ドライバーが「前方から目を離さずに」作業できる環境を整えた。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/_c8592cf9a3/_c8592cf9a3.gif" alt="ダウンロード.gif" /></p>
<p>このスマートグラスは現場ドライバー（Delivery Associates, DAs）のフィードバックを基に開発されており、コントロールベストや交換式バッテリー、緊急ボタンなど、配送現場のニーズを反映した構造となっている。度付きレンズや調光レンズにも対応しており、長時間の着用や屋外での使用にも配慮がなされている。</p>
<p>現在、北米地域の数百名のドライバーが試験運用を実施中で、ユーザーフィードバックを踏まえた改良が進められている。今後は配送サービスパートナー（Delivery Service Partners, DSPs）を含む広範な導入を予定しているが、商用展開の時期などは明らかにされていない。</p>
<p>Amazonは本取り組みを、倉庫内ロボットやAI物流最適化技術と並ぶ「次世代配送体験」の一環と位置づけており、配達の安全性と作業効率の両立を目指すとしている。</p>
]]></description>
      <pubDate>Sat, 25 Oct 2025 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI、「日本のAI経済ブループリント」を公開──生成AIで支える包摂的成長と新国家戦略</title>
      <link>https://ledge.ai/articles/openai_japan_economic_blueprint_2025</link>
      <description><![CDATA[<p>OpenAIは2025年10月22日、日本におけるAIの経済的・社会的潜在力を最大限に活かすための政策提言書「日本のAI：OpenAIの経済ブループリント」を<a href="https://openai.com/ja-JP/index/japan-economic-blueprint/">発表</a>した。日本がAI時代において経済成長と社会的包摂を両立するための具体的な国家戦略を示すものであり、教育、医療、行政、産業、エネルギーなど多岐にわたる分野での活用を提言している。</p>
<p>同文書は、OpenAIの政策・パートナーシップ担当である大久保和也氏が序文を執筆。AIを電気やインターネットと並ぶ汎用技術（General Purpose Technology）と位置づけ、「AIは日本の生産性を高め、より包摂的で持続可能な社会を支える」と述べた。AIを日本の国家成長の中核に据えるための「生きた提案書」として策定されたという。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/AI_in_Japan_Open_AI_c85874e918/AI_in_Japan_Open_AI_c85874e918.jpg" alt="AI in Japan OpenAI.jpg" /></p>
<h2>国家戦略の三本柱──包摂・インフラ・教育を軸に</h2>
<p>ブループリントは、AIを経済と社会の成長エンジンに据えるための三本の柱を掲げる。</p>
<h3>1. 包摂的な参加型社会基盤の構築</h3>
<p>AIをすべての人に届け、誰もが開発と活用に参加できる社会へ。日本の柔軟な著作権制度を基盤に、国際ルール形成を主導する「日本モデル」を確立する方針を示した。</p>
<h3>2. 戦略的インフラ投資</h3>
<p>AIの中核を担う半導体・データセンター・再生可能エネルギーを一体的に整備し、「ワット（電力）」と「ビット（情報）」を連携させる。政府の「GX2040ビジョン」との連動を通じ、地方へのデータセンター誘致や再エネ電源開発を官民協働で推進する。</p>
<h3>3. 教育・リスキリングによる人的資本投資</h3>
<p>初等教育からAIリテラシーを育成し、生涯学習をAIで支援。ChatGPT Eduなどのツールを「思考のパートナー」として活用し、批判的思考や創造性を育む教育を全国に広げるとした。</p>
<h2>経済効果の試算──AIが日本のGDPを最大140兆円押し上げ</h2>
<p>同文書は、AIの経済的インパクトを複数の独立分析に基づいて示している。
AIを最大限に活用した場合、日本のGDPを累計140兆円押し上げる可能性がある（みずほリサーチ＆テクノロジーズ）。また、生成AIだけでも実質GDPを16.2%増加させ得る（大和総研）。さらに、AI利用企業の生産性は非利用企業より8.8%高い（経済産業研究所）とされ、OpenAIはこれらを「日本全体の生産性への投資効果」と位置づけている。</p>
<h2>各分野での波及効果──製造・医療・教育・行政・科学・金融</h2>
<p>ブループリントでは、AIがもたらす変革を6分野で具体的に示した。</p>
<ul>
<li><strong>製造業</strong> ：中小企業336万社を支えるAI需要予測・検査システムの導入事例を紹介。品質向上やコスト削減、熟練工の技能継承を支援するツールとしての効果を示した。</li>
<li><strong>医療・介護</strong> ：AI画像診断や見守りセンサーによる人手不足解消や医療費削減効果を提示。骨粗鬆症の予防だけでも年間1.5兆円の介護費削減が可能と試算している。</li>
<li><strong>教育</strong> ：AIチューターによる個別最適化学習の導入を推進し、AIを「批判的思考を磨くツール」として位置づけた。</li>
<li><strong>行政</strong> ：さいたま市や福岡市、東京都のAI活用事例を挙げ、文書作成や住民対応の自動化に加え、地域文化振興への応用も紹介。</li>
<li><strong>科学</strong> ：AIによる創薬支援が臨床開発時間を最大60%短縮し、治験成功率の向上にも寄与。</li>
<li><strong>金融</strong> ：生成AIによるパーソナライズ投資提案やAML（マネーロンダリング防止）対策への応用を紹介。金融庁の「AIディスカッションペーパー」を“成長ガイド”と評価した。</li>
</ul>
<h2>AIインフラとエネルギー政策の一体化</h2>
<p>AI経済を支える物理的基盤として、データセンター市場は2028年に5兆円規模へ拡大すると予測。経産省によると、AIと半導体工場の増設により2034年度までに電力需要が約5.8%増加すると見込まれている。</p>
<p>OpenAIは、日本政府が掲げる「GX2040ビジョン」との連携を通じて、再生可能エネルギーが豊富な地域へのデータセンター誘致を提言。GX（グリーン成長）とDX（デジタル変革）を統合した「GX×DXモデル」が、持続的な経済発展の鍵になるとした。</p>
<h2>AIをすべての人の豊かさにつなげる設計図</h2>
<p>OpenAIはこのブループリントを「AIという変革の力を日本のすべての国民の豊かさに繋げるための設計図」と位置づけた。
「AIによって日本は経済成長と人間中心社会の両立を実現できる。今こそ官民一体でこの“日本モデル”を世界に先駆けて示すときだ」と締めくくっている。</p>
<p><a href="https://cdn.openai.com/global-affairs/f9d1cd88-506e-48f9-b34b-6ff63655434e/openai-japan-economic-blueprint-jp.pdf">「​​日本のAI：​​Open AIの経済ブループリント」全文</a></p>
]]></description>
      <pubDate>Sat, 25 Oct 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>850人超の著名人がAI「スーパーインテリジェンス」開発禁止を要請──ヒントン氏やメーガン妃らが署名</title>
      <link>https://ledge.ai/articles/ai_superintelligence_ban_850_signatories_fli_20251022</link>
      <description><![CDATA[<p>AIが人間の知能を超える「スーパーインテリジェンス（超知能）」の開発を巡り、世界の著名人や研究者らが開発の停止を求める声明を出した。</p>
<p>Future of Life Institute（FLI）は2025年10月22日、超知能AIの開発を禁止するよう求める声明「The Statement on Superintelligence」を<a href="https://superintelligence-statement.org/">公開</a>した。署名には、AI研究の第一人者ジェフリー・ヒントン氏やアップル共同創業者のスティーブ・ウォズニアック氏、英王室のヘンリー王子夫妻（ハリー王子とメーガン妃）など850人以上が名を連ねている。</p>
<h2>「安全・制御・公共合意」が条件</h2>
<p>声明の内容は極めてシンプルだ。FLIは、AIの超知能開発について、
「安全かつ制御可能な方法で行われるという幅広い科学的コンセンサス」と
「社会（国民）の強い支持」が得られるまで、
開発を禁止するよう求めている。</p>
<p>声明では、制御不能なAIがもたらすリスクとして、社会秩序の崩壊や経済格差の拡大、人間の自由の喪失、さらには人類そのものの存続への脅威を挙げている。背景には、OpenAIやMeta、Google DeepMindなどが進める「人間を超える知能の実現」競争があり、開発速度に対して安全性や統治の議論が追いついていないという問題意識がある。</p>
<h2>科学者から文化人まで、多様な署名者</h2>
<p>署名には、AI研究者や起業家だけでなく、政治家や芸能人など幅広い分野の人物が名を連ねた。</p>
<ul>
<li>Geoffrey Hinton 氏（AI研究者、「AIのゴッドファーザー」）</li>
<li>Yoshua Bengio 氏（モントリオール大学教授、チューリング賞受賞者）</li>
<li>Steve Wozniak 氏（Apple共同創業者）</li>
<li>Richard Branson 氏（Virgin Group創業者）</li>
<li>Prince Harry 氏、Meghan Markle 氏（英王室夫妻）</li>
<li>Steve Bannon 氏（元米ホワイトハウス首席戦略官）</li>
<li>Joseph Gordon-Levitt 氏（俳優）</li>
</ul>
<p>署名者の幅広さは、AI開発における倫理・安全への懸念が学術界にとどまらず、社会全体の関心事になりつつあることを示している。</p>
<p>声明は10月22日に発表され、同日よりオンラインで署名を受け付けた。署名受付の正確な時刻や締切は明示されていないが、発表から数日で850人超の署名が集まったとされる。記事執筆時点（10月24日）では署名が一般公開され、3万件を超えており、今後も追加署名が受け付けられる見通しだ。</p>
<h2>モラトリアムから「禁止」へ</h2>
<p>Future of Life Instituteは、2023年3月にもGPT-4以降のAI開発を6か月停止するよう求める書簡を発表しており、今回の声明はその延長線上にある。当時は一時停止（moratorium）を呼びかけたが、今回は「prohibition（禁止）」というより強い表現が使われており、AI開発競争への懸念が一層強まっていることがうかがえる。</p>
<h2>今後の行方</h2>
<p>この声明は法的拘束力を持たないものの、AI業界や政策立案者へ少なからず影響があるとみられている。各国政府はすでにAI規制法の整備を進めており、声明で示された「安全・制御・公共合意」という３つの条件が、今後の国際的なAI開発ガイドラインに組み込まれるかどうかが焦点となる。</p>
]]></description>
      <pubDate>Fri, 24 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>AIによる価格操作に州が規制──ニューヨーク州、家主向けアルゴリズム家賃設定ツールを禁止</title>
      <link>https://ledge.ai/articles/ai_rent_pricing_ban_newyork_oct2025</link>
      <description><![CDATA[<p>ニューヨーク州は2025年10月16日、キャシー・ホウクル知事が家主や物件管理会社がAIなどを活用したアルゴリズムで賃料を設定する行為を禁じる法案に署名したことを<a href="https://www.governor.ny.gov/news/governor-hochul-signs-legislative-package-bolster-homeownership-and-strengthen-protections">発表</a>した。州として、住宅市場におけるAIによる価格操作を直接規制するのは初めてとなる。</p>
<h2>アルゴリズムによる家賃設定を禁止</h2>
<p>この日ホウクル知事が署名した、住宅所有を促進し、賃借人保護を強化するための法案の中で、新法（S7882／A1417）は、複数の家主が同一のアルゴリズム価格設定ソフトウェアを用いることで事実上「賃料の共謀的引き上げ」が行われる構造を問題視したもの。対象となるのは、需要データや周辺相場をもとに家賃を自動調整するAIツールで、いわゆる「アルゴリズム価格設定（algorithmic pricing）」の利用を禁じる。</p>
<p>知事府の公式発表によれば、今回の法案は「住宅の公平性を守るための包括的な立法パッケージ」の一部として署名された。ホウクル知事は声明の中で、「ニューヨーカーは、公平で透明な家賃を支払う権利を持つ」と述べ、AIの悪用を防ぐ姿勢を強調した。</p>
<h2>背景に“AI賃料カルテル”への懸念</h2>
<p>米国ではここ数年、RealPage社の「YieldStar」や「RENTmaximizer」など、AIによって賃料を算出・最適化するソフトウェアの利用が急速に広がっていた。これらのツールは、入居需要、周辺相場、空室率などを解析して「最大利益を得られる家賃」を自動的に提案する仕組みを持つ。</p>
<p>しかし複数の家主が同一のAIモデルやデータを共有して賃料を設定すると、事実上の価格協調（カルテル）に当たるのではないかとの批判が高まっていた。2023年には、同ソフトの利用をめぐり連邦レベルで独占禁止法違反の集団訴訟も提起されている。</p>
<p>今回のA1417法は、そうした構造的な“AI共謀”を防ぐ狙いがある。米経済自由団体のAmerican Economic Liberties Project
は「AIを介した価格共謀の初の州レベル禁止」として歓迎の声明を<a href="https://www.economicliberties.us/press-release/economic-liberties-applauds-new-yorks-landmark-statewide-ban-on-rent-collusion-software/">発表</a>した。</p>
<h2>全米での議論に波及も</h2>
<p>新法は署名から60日後に発効予定で、違反が確認された場合は行政処分や罰金の対象となる。
今後、他州が同様の法整備に踏み出す可能性も指摘されている。AIによる価格設定はホテル、航空券、交通など他産業でも広く導入されており、今回の規制が「アルゴリズム経済」全体にどのような影響を与えるか注目が集まる。</p>
]]></description>
      <pubDate>Fri, 24 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>日立、AIネイティブな基幹システム刷新を支援──「モダナイゼーション powered by Lumada」提供開始</title>
      <link>https://ledge.ai/articles/hitachi_modernization_powered_by_lumada_launch</link>
      <description><![CDATA[<p>日立製作所は2025年10月21日、AIを前提とした業務・IT・組織の変革を一体的に支援する新サービス「モダナイゼーション powered by Lumada（モダナイゼーション パワード バイ ルマーダ）」の提供開始を<a href="https://www.hitachi.co.jp/New/cnews/month/2025/10/1021.html">発表</a>した。企業の基幹システムをAIネイティブな構造へ刷新し、デジタルセントリック企業への転換を後押しする。</p>
<p>少子高齢化による労働人口減少や、熟練エンジニアの減少、システムのサイロ化などが企業経営の課題となる中、日立は従来の基幹システム更新支援にとどまらず、AI活用を前提とした業務改革・組織改革までを含めたモダナイゼーションを推進する。</p>
<p>同サービスは、日立のデジタル事業基盤「Lumada」に蓄積されたAI技術とドメインナレッジを活用し、次の2つのメニューで構成される。</p>
<h3>1. グランドデザイン策定サービス</h3>
<p>GlobalLogicの「CAST Imaging」「Highlight」などを活用して現行システムの構造を可視化し、投資優先度やリスク評価を踏まえたロードマップを策定する。AWSの「Blu Age」や「Transform」、Figma Makeなどのテクノロジーパートナーソリューションも組み合わせ、実効性の高い変革計画を設計する。</p>
<h3>2. 業務・ITモダナイゼーションサービス</h3>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/67590_517_e86efbc37a9fcedccdf1cbfdd26877a8_1345x631_2a1d20325c/67590_517_e86efbc37a9fcedccdf1cbfdd26877a8_1345x631_2a1d20325c.webp" alt="67590-517-e86efbc37a9fcedccdf1cbfdd26877a8-1345x631.webp" /></p>
<p>経営から現場まで200種以上のAIエージェントを活用して業務の自動化を推進し、重要データの特定や品質維持を行うデータマネジメント基盤を提供。さらに、ITシステムのアーキテクチャーを領域ごとに最適化し、AIを活用したコード生成などで刷新を加速させる。あわせて、AI・アジャイル開発を担う人材育成や組織文化の変革も支援する。</p>
<p>サービスは、日立グループの実践を通じて検証された「カスタマーゼロ」の成果も取り入れる。ビルシステム事業では、AIソリューション「HMAX for Building : BuilMirai（ビルミライ）」の進化に向けて先行適用し、ベテラン技術者の暗黙知をAI化して設備メンテナンス業務の自動化を進めている。
また、大同生命保険株式会社システム開発二部 次世代システム開発室の黒川智也室長は、「AIの一層の活用を強力に推進するものであり、当社のIT活用の方向性と合致している」とコメントしている。</p>
<p>「モダナイゼーション powered by Lumada」は、Lumada 3.0ビジョンを体現するAIソリューション「HMAX」を展開する際にも活用される予定。日立はGlobalLogicのグローバル人材を活用し、迅速かつ持続的なデジタル変革支援を行うとしている。</p>
]]></description>
      <pubDate>Fri, 24 Oct 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google、量子チップ「Willow」で“検証可能な量子優位”を実証──Nature掲載論文でスーパーコンピューターを13,000倍上回る性能</title>
      <link>https://ledge.ai/articles/google_quantum_willow_verifiable_advantage_nature2025</link>
      <description><![CDATA[<p>Googleは2025年10月22日、科学誌『Nature』において、同社の量子コンピューター用チップ「Willow（ウィロー）」が、既存のスーパーコンピューターを13,000倍上回る速度で演算を実行し、「検証可能な量子優位（verifiable quantum advantage）」を達成したと<a href="https://research.google/blog/a-verifiable-quantum-advantage/">発表</a>した。</p>
<p>論文「<a href="https://www.nature.com/articles/s41586-025-09526-6">Observation of constructive interference at the edge of quantum ergodicity</a>」によると、Willowチップは105個の超伝導キュービットを備え、量子情報の拡散を解析する手法「アウト・オブ・タイム・オーダー相関（OTOC）」を用いて実験を実施した。結果、古典スーパーコンピューターでは約32年を要する計算を、わずか2時間で完了したという。</p>
<p>@<a href="https://www.youtube.com/watch?v=mEBCQidaNTQ">YouTube</a></p>
<p>A Verifiable Quantum Advantage：研究チームがWillowチップ、量子エコーアルゴリズム、NMR分光法応用などを解説</p>
<h2>Quantum Echoes──量子エコー・アルゴリズムの仕組み</h2>
<p>Googleの研究チームは、量子システム内部の相互作用を可視化する新しいアルゴリズム「Quantum Echoes（量子エコー）」を開発した。この手法は、量子回路を順方向と逆方向の両方で実行し、干渉パターンを比較することで誤差を自己検証できる。</p>
<p>この仕組みにより、量子システムの「見えない構造」をエコー信号として観測可能にする点が特徴で、これまでノイズに埋もれていた量子状態の動的相関を明確に測定できるようになった。</p>
<h2>量子干渉と「Constructive Interference」──新しい優位性の原理</h2>
<p>論文では、量子系内部のパウリ演算子の干渉（constructive interference）が、OTOCの高次成分（OTOC(2)など）において初めて実験的に観測されたことを報告。これにより、量子干渉の複雑性が古典シミュレーションでは再現不能であることが確認された。</p>
<p>実際、Googleの試算では、今回の65キュービット実験を古典スーパーコンピューター「Frontier」でシミュレートするには約3.2年を要し、Willowチップの実行時間（2.1時間）と比べて13,000倍以上の差がある。これにより、「Beyond classical（古典を超えた）」領域に踏み入ったと位置づけている。</p>
<p><strong>Quantum Echoesアルゴリズムは、世界最速のスーパーコンピューターを13,000倍上回る速度で計算を実行</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/13000times_faster_3aa42ccb1d/13000times_faster_3aa42ccb1d.jpg" alt="13000times faster.jpg" /></p>
<h2>実世界応用への一歩──分子構造解析とHamiltonian Learning</h2>
<p>Google Quantum AIは、Quantum Echoesの応用として、ハミルトニアン学習（Hamiltonian learning）を実証した。
これは量子システムの測定データ（OTOC）を実際の物理系と照合し、未知のパラメータを最適化して学習する手法である。</p>
<p>研究チームは、UCバークレーと連携し、Willowチップ上で2種類の分子構造を予測し、NMR分光法でその正確性を確認した。論文では、この方法が分子シミュレーション、薬剤設計、エネルギー材料開発などへの応用に発展する可能性が示唆されている。</p>
<p><strong>量子研究施設を視察するGoogle CEOのサンダー・ピチャイ氏「科学そのものを行う力が量子計算に宿り始めている」と述べた</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Quantum_Sundar_Inline_width_1000_format_webp_5a6124c1fb/Quantum_Sundar_Inline_width_1000_format_webp_5a6124c1fb.webp" alt="QuantumSundar_Inline.width-1000.format-webp.webp" /></p>
<p>Google Quantum AIのディレクターであるHartmut Neven氏は、「量子コンピューターが理論段階を超え、実際の科学的課題を解くためのツールになりつつある」とコメント。共同リーダーのJulian Kelly氏も「Quantum Echoesを通じて、結果を検証可能な形で量子優位を確認できたのは初めて」と述べた。</p>
<p><strong>Google Quantum AIが開発した次世代量子チップ「Willow」。105個の超伝導キュービットを搭載し、誤差率0.15％の高精度ゲート制御を実現した</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Willow_Chip_4k_Render_02_width_1000_format_webp_2ad189fa0b/Willow_Chip_4k_Render_02_width_1000_format_webp_2ad189fa0b.webp" alt="WillowChip_4k_Render_02.width-1000.format-webp.webp" /></p>
]]></description>
      <pubDate>Thu, 23 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>全国7400館の蔵書をChatGPTなどのAIアシスタントから検索──「カーリル for AI（カーリル図書館MCP）」ベータ版を公開</title>
      <link>https://ledge.ai/articles/calil_for_ai_library_mcp_beta</link>
      <description><![CDATA[<p>株式会社カーリルは2025年10月9日、ChatGPTやClaudeなどのAIアシスタントから全国の図書館蔵書を横断検索できる新サービス「カーリル for AI（カーリル図書館MCP）」のベータ版を公開したことを<a href="https://blog.calil.jp/2025/10/forai.html">発表</a>した。</p>
<p>同社が提供する図書館検索プラットフォーム「カーリル」は、全国7400館以上の公共図書館を対象に蔵書と貸出状況をリアルタイムで検索できるサービスで、今回の発表はそのAI連携版にあたる。</p>
<p>「カーリル for AI」は、OpenAIやAnthropicの提供するAIアシスタントに拡張機能として組み込むことで、自然な対話を通じて図書館の蔵書を調べられる仕組み。ユーザーがChatGPTやClaudeに「近くの図書館で『深夜特急』を借りられる？」といった質問をすると、AIがカーリルのデータベースを参照し、所蔵館や貸出可否などを回答する。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/calil_for_ai1_54f04abd1a/calil_for_ai1_54f04abd1a.jpg" alt="calil_for_ai1.jpg" /></p>
<p>この仕組みは、AIモデルが外部データソースへ安全にアクセスするための新しい通信規格「Model Context Protocol（MCP）」を採用している。カーリル図書館MCPは、各地の図書館OPAC（蔵書検索システム）と連携し、最新の貸出情報をリアルタイムに取得する。従来のウェブ検索とは異なり、ユーザーはAIとの自然な会話の中で図書館情報にアクセスできるのが特徴だ。</p>
<p>利用には、ChatGPTまたはClaudeの設定画面で「カーリル図書館MCP」を有効化するだけでよい。設定後は、会話の中で書名や著者名を指定すれば、AIが自動的に対応する図書館データを検索する。</p>
<p>カーリルによると、現在は公共図書館を中心に対応しているが、今後は大学図書館や専門図書館への拡大も予定しているという。同社は「AIを通じて図書館の情報資源をより多くの人に届ける」ことを目指しており、AIと公共データをつなぐ新しいアクセスモデルの実証段階と位置付けている。</p>
]]></description>
      <pubDate>Thu, 23 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>広島銀行、営業準備に生成AI導入──「無意識に使える」業務フローへ　面談時間を7割削減見込み</title>
      <link>https://ledge.ai/articles/hirogin_ai_assistant_sales_poc_2025</link>
      <description><![CDATA[<p>ひろぎんホールディングス傘下の広島銀行は、営業業務に生成AIを導入し、行員の業務効率化を進めている。</p>
<p>ひろぎんホールディングスは2025年9月29日付で、融資稟議書作成機能を内製開発し全営業店に導入したほか、個人顧客との面談準備を支援するAIの概念実証（PoC）を実施し、有用性を確認したと<a href="https://www.hirogin-hd.co.jp/news/__icsFiles/afieldfile/2025/09/29/20250929_news.pdf">発表</a>した。</p>
<p>この取り組みは、ひろぎんホールディングスが掲げる「無意識に生成AIを利用する業務フローの構築」の一環として進められている。行員がAIを意識的に操作することなく、日常業務の中で自然に支援を受けられる環境を目指しているという。</p>
<p>広島銀行では、企業情報や営業記録をもとに融資稟議書の一部（申込経緯、資金使途など）を生成AIが自動でドラフト化する仕組みを開発。内容を確認・修正して活用することで、年間約5,200時間の業務削減効果を見込む。若手行員にとっては、AIが作成した草案を通じて稟議書の書き方を学べる機会にもなる。</p>
<p>さらに、個人顧客との面談準備の効率化に向け、行内の複数システムに分散する顧客情報をAIが自動で整理し、提案内容や会話のアイディアを生成するPoCを実施。検証の結果、準備時間を従来比で7割削減できる見込みが立った。これを受け、同社は内製による正式開発を進め、2026年2月から営業担当者による運用を開始する予定だ。</p>
<p>ひろぎんホールディングスは、2024年にMicrosoft Azure環境上でグループ専用生成AI「AI Assistant」を構築しており、現在は業務システムとの連携を進める“ステージ2”に位置づけている。将来的にはAIエージェントの活用による価値創造と顧客体験の向上を目指し、非金融領域を含む「総合力の高いソリューション」提供へと展開を広げていく方針だ。</p>
]]></description>
      <pubDate>Thu, 23 Oct 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>生成AIが拓く新しい設計と創造 ー「GENEX #1」開催レポート</title>
      <link>https://ledge.ai/articles/genex1-report</link>
      <description><![CDATA[<p>国内最大級のAI専門メディア「Ledge.ai」を運営するレッジは、業界横断カンファレンスシリーズ「GENEX」の第1回を9月30日に開催した。テーマは「3D CAD × Generative AI」。産業・エンタメ・教育の第一線で活躍するクリエイターと、CADソフトウェアベンダー／ハードウェアベンダー／助成金活用支援で業界をリードする企業が集い、生成AIとものづくりの未来について語っていただいた。本記事では、当日の各セッションの内容を紹介する。</p>
<p>なお、本イベントの模様は期間限定でアーカイブ配信を実施している。
セッションをご覧になりたい方は、以下より視聴が可能である。
:::button
<a href="https://zfrmz.com/VhuUWyN8c4eSEqAmIFRr">視聴申込みはこちら</a>{target=_blank}
:::</p>
<h2>パネルディスカッション『生成AIによって変革する設計のプロトタイピング』</h2>
<p>本セッションでは、デジタルコンテンツクリエイターの小畑 正好 氏、dots in space代表取締役の橋本 和幸 氏が登壇し、モデレーターはLedge.ai編集長の落合 研次が務めた。</p>
<h3>生成AIの潮流：エンタメから産業利用へ</h3>
<p>セッションは、落合による画像・動画生成AIの進化に関する3つの大きな潮流の解説から始まった。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/genex_report_image4_8ea7d919be/genex_report_image4_8ea7d919be.png" alt="genex-report-image4.png" /></p>
<ul>
<li><strong>エンターテイメント用途</strong>: MidjourneyやStable Diffusionに代表される、生成されたアウトプットそのものを楽しむ潮流。</li>
<li><strong>リアル用途</strong>: 「Text-to-CAD」技術のように、生成物を設計図や工業製品のプロトタイプとして実世界で活用する潮流。</li>
<li><strong>Edit（編集）</strong>: 既存のデータや画像に指示を加えて編集・加工する潮流。</li>
</ul>
<p>特に、自然言語の指示から高精度な3D CADモデルを生成する「CADFusion」のような技術が紹介され、設計の初期段階におけるプロトタイピングが大きく変革される可能性が示唆された。</p>
<h3>現実を再構築する新技術「Gaussian Splatting」</h3>
<p>パネルディスカッションの中では、注目される技術の一つとして「Gaussian Splatting（ガウシアンスプラッティング）」が紹介された。従来の3D点群データとは異なり、ガウス関数（正規分布）を使って非常にリアルな3Dシーンを生成する技術である。既存の3Dモデルとは異なり、光の反射や質感などを極めて忠実に再現できるのが特徴である。データ生成の際に人力での対応に膨大な時間を要するため、生成AIの活用が不可欠だ。</p>
<p>またセッションの中では、長年CG業界の最前線で活躍してきた小畑氏と橋本氏に、自身の経験を踏まえ、生成AIとの向き合い方についても語っていただいた。</p>
<p>橋本氏は、かつてエヌビディアでディープラーニングの黎明期を経験した視点から、「AIの登場は、人間がアルゴリズムを組まなくても、入力と出力のパターンを学習させることで答えを導き出せるようになった点で革命的」と指摘。その上で、生成AIを「電卓のような道具」と捉え、「どんなデータを入力し、どう活用するかという人間の発想こそが重要になる」と強調した。</p>
<p>小畑氏も、40年にわたるCG制作の経験を振り返り、「技術の進化は常にあり、その度に人間の役割は変わってきた」と述べた。大河ドラマのオープニング映像制作を例に挙げ、実写からCGへと表現手法が変化した歴史に触れつつ、生成AIの登場で「個人のクリエイターが高品質な作品を生み出せる時代が来る」との期待を寄せた。</p>
<h2>株式会社 日本HP『オンプレで実現する3D CAD×生成AI：RAG導入と設計検索の実践ポイント』</h2>
<p>続いて、株式会社 日本HP エンタープライズ営業統括 ソリューション営業本部 ワークステーション営業部 AI／データサイエンス市場開発担当部長の勝谷 裕史 氏が登壇。同社のAIワークステーションを活用し、機密性の高い3D CADデータを安全かつ効率的に活用するための具体的な手法が、事例やデモを交えて解説された。</p>
<h3>生成AIの導入状況：普及の谷を越え「事例」を求める段階へ</h3>
<p>勝谷氏は、様々な企業の生成AI導入フェーズを見ていく過程で、普及の谷（キャズム）を越え、2025年の前半くらいからアーリーマジョリティの段階に入ったという実感を持ち始めたという。</p>
<p>企業での活用が進む中で、クラウドAIの利便性が注目される一方、勝谷氏はセキュリティリスクについてを懸念点として挙げた。実際に大手AIサービスの会話履歴が検索可能になった事例などを示し、機密性の高い設計データを外部に出すことの危険性を強調した。その解決策として、データを手元のマシンで完結させる「オンプレミスAI」での運用も組み合わせていくことが重要であると述べた。</p>
<h3>AIワークステーションのメリット：GPUサーバーよりも「安く、手軽で、静か」</h3>
<p>オンプレミス環境の構築ではGPUサーバーも選択肢となるが、セッションではAIワークステーションの優位性について紹介された。
AIワークステーションは、コストを半分以下に抑えながら、より高い性能を発揮する。さらに、特別な電気工事が不要で、オフィスエアコンより静かな静音性（60dB以下）も実現しているため、開発者のデスクサイドに設置して手軽にAI開発を始めることが可能になるというのもGPUサーバーとの違いとして挙げられた。</p>
<p>またセッションの中では、日本HPが提供する様々なワークステーションのラインナップも紹介され、その中でも最新のインテル® Core™ Ultra 9 プロセッサー 285Kを搭載した「HP Z2 Tower G1i Workstation」は、デスクトップサイズでありながら大規模なAI処理を可能にするその性能が強調された。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/genex_report_image11_f8653818b5/genex_report_image11_f8653818b5.png" alt="genex-report-image11.png" /></p>
<h3>3Dモデルを認識して回答するデモも実演</h3>
<p>セッションの最後には、単一GPUで3Dモデル（点群データ）を直接認識するローカルLLM「PointLLM」のデモが披露された。AIが3Dモデルのデータを読み込み、その形状や特徴を「これはロボットのフィギュアです」といったように自然言語で説明する様子を実演。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/genex_report_image12_dbaca1b50c/genex_report_image12_dbaca1b50c.png" alt="genex-report-image12.png" /></p>
<p>AIワークステーションを活用することで、企業内に眠っている膨大な過去の3D設計データをAIに自動で解析・タグ付けさせることで、新たなデータ活用の可能性が示された。</p>
<h2>オートデスク株式会社『デザインと創造をパワフルにサポートする Autodesk AI』</h2>
<p>3つ目のセッションでは、オートデスク株式会社 日本地域営業統括 技術営業本部 本部長の加藤 久喜 氏が登壇。「デザインと創造をパワフルにサポートする Autodesk AI」と題し、製造業をはじめとする各業界のワークフローを革新するAI技術の現在地と未来について語った。</p>
<h3>短時間で無数の選択肢を生成を可能にする「ジェネレーティブデザイン」</h3>
<p>設計領域でのAI活用の具体的なアプローチとして強調されたのが「ジェネレーティブデザイン」である。これは、設計者が設定した強度、材料、製造方法などの条件に基づき、AIが最適な形状デザインを数百通りも自動生成する技術である。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/image7_8c1fc0c653/image7_8c1fc0c653.png" alt="image7.png" /></p>
<p>セッションの中で紹介された車椅子部品設計事例では、人間が3.5時間かけて3案しか出せなかった設計を、AIはわずか20分で100以上の検証可能な設計案を生成できるという。これにより、設計プロセスが「単一の最適解を探す」作業から「多様な選択肢の中から最良のトレードオフを見つける」戦略的な意思決定へと変化していく可能性が示された。</p>
<h3>信頼性と革新性：「Autodesk AI」の多角的な取り組み</h3>
<p>Autodesk AIは、単一の技術ではなく、同社の製品群全体にわたる包括的な取り組みとして紹介された。</p>
<p><strong>信頼性へのコミットメント</strong>
オートデスクは、AIの倫理的で責任ある利用を担保するため、AIマネジメントシステムの国際規格である「ISO/IEC 42001」を世界で初めて取得した企業の一つであることを発表。AIの学習データや判断プロセスを公開する「AI Transparency Card」などの取り組みを通じて、ユーザーが安心してAI技術を活用できる基盤を構築している。</p>
<p><strong>アイデアを即座に立体化する「Project Bernini」</strong>
最新の研究プロジェクトとして、テキストや簡単な2Dスケッチから3Dモデルを自動生成する「Project Bernini」が披露された。この技術は、コンセプトデザインの初期段階でアイデアを即座に可視化し、試行錯誤を加速させる。将来的には、生成されたモデルを直接編集可能なCADデータへ変換する研究も進められており、アイデアから製造までをシームレスに繋ぐことを目指している。</p>
<p><strong>自動車業界での実践例</strong>
自動車のデザイン開発において、従来は膨大な時間とコストを要した空力シミュレーションをAIが代替。数秒で衝突シミュレーションの結果を予測するなど、開発リードタイムを劇的に短縮する事例が紹介された。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/genex_report_image5_adb16088c3/genex_report_image5_adb16088c3.png" alt="genex-report-image5.png" /></p>
<h3>対話ベースで3Dモデルの生成を可能にする「Autodesk Fusion」の新機能</h3>
<p>セッションのデモで特に注目を集めたのが、クラウドベースの3D CAD/CAM/CAEツール「Autodesk Fusion」に搭載されたAIアシスタント機能だ。</p>
<p>ユーザーが「エアフライヤーを生成して」と自然言語で指示するだけで、AIが3Dモデルを生成。さらに「このソリッドを分割して、2mmのシェルを追加して」といった対話形式の指示で、複雑な設計変更も自動で実行する。最終的には、生成したモデルのレンダリング画像やマーケティングプランを含むPowerPointのプレゼンテーション資料まで自動作成する能力も示され、設計からビジネス提案までの一連のワークフローがAIとの対話で完結する未来像が提示された。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/genex_report_image6_fb0462812d/genex_report_image6_fb0462812d.png" alt="genex-report-image6.png" /></p>
<h2>トランステップ株式会社『補助金活用支援セッション 公的資金活用した、最新技術の導入』</h2>
<p>最後のセッションでは、本イベント共催のトランステップ株式会社 代表取締役社長 岡島 礼 氏が登壇し、補助金を利用して最新技術を導入するための具体的なノウハウを解説した。</p>
<h3>国が後押しする「DX投資」、今こそ補助金活用の好機</h3>
<p>セッションで岡島氏はまず、「補助金」と「助成金」の違いを明確にした。特に企業の設備投資や事業成長を後押しするのは、経済産業省などが管轄する「補助金」である。そして今、国はデジタルトランスフォーメーション（DX）分野への投資を強力に推進しており、関連する補助金予算も増加傾向にある。</p>
<p>岡島氏は、人手不足に悩む中小企業の省力化投資を支援する「中小企業省力化投資補助金」などを例に挙げ、3DCADソフトウェアやワークステーションの導入がこれらの制度の対象となり得ることを具体的に示した。これは、最新技術への投資を検討する企業にとって絶好の追い風と言える。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/image10_7b2f65c46b/image10_7b2f65c46b.png" alt="image10.png" /></p>
<h3>補助金トータルソリューション「トレテル」</h3>
<p>しかし、補助金の活用には大きな壁が立ちはだかる。一つは、年間数万件も公募される膨大な情報の中から自社に最適な補助金を「見つけられない」こと。もう一つは、申請手続きが複雑で「使いこなせない」ことである。</p>
<p>この課題に対し、岡島氏は同社が提供するAIを活用した補助金トータルソリューション「トレテル」を紹介。このツールを使えば、キーワード検索やAIとの対話を通じて、全国の膨大な補助金情報から自社に最適なものを瞬時に探し出し、申請に関する疑問も解消できる。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/genex_report_image1_c60cbac109/genex_report_image1_c60cbac109.png" alt="genex-report-image1.png" /></p>
<p>複雑で縁遠いと思われがちな補助金だが、正しい知識と便利なツールを活用すれば、企業の成長を加速させるための強力な武器となる。今回のセッションは、多くの企業が最新技術導入への一歩を踏み出すための、価値ある道しるべとなったはずだ。</p>
<h2>まとめ —— 生成AIがものづくりに拓く新たな可能性</h2>
<p>GENEX #1では、生成AIがものづくりの現場にいかなる変化をもたらすのかについて、多角的な視点から議論が展開された。冒頭のパネルディスカッションでは、生成AIが「エンタメからリアルな空間へ」と進化していく潮流が示され、発想力を持つ人間が活用してこそ真価を発揮するという本質的な視点が提示された。
そのうえで、ハードウェアの観点では日本HPがオンプレミスAIワークステーションによる安全かつ効率的な環境の可能性を示し、ソフトウェアの観点ではオートデスクがジェネレーティブデザインやProject Berniniを通じて設計プロセスの革新を描いた。さらに制度の観点ではトランステップが補助金活用の具体策を提示し、導入を現実のものとするための道筋を明確にした。</p>
<p>次回「GENEX #2」では、テーマをロボティクスに移し、生成AIとリアルなものづくりの接点をさらに掘り下げる予定である。生成AIとロボティクスが融合する未来に注目いただきたい。</p>
<p>本イベントの模様は期間限定でアーカイブ配信をご覧になりたい方は、以下より視聴が可能である。
:::button
<a href="https://zfrmz.com/VhuUWyN8c4eSEqAmIFRr">視聴申込みはこちら</a>{target=_blank}
:::</p>
]]></description>
      <pubDate>Thu, 23 Oct 2025 00:50:00 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI共同創設者のAndrej Karpathy氏、ChatGPTを一から構築できるオープンソース「nanochat」を公開──約100ドル・4時間で独自LLMを訓練可能</title>
      <link>https://ledge.ai/articles/andrej_karpathy_releases_nanochat_open_source_chatgpt_clone</link>
      <description><![CDATA[<p>OpenAIの創設メンバーであり、元TeslaのAIディレクターとしても知られるAndrej Karpathy（アンドレイ・カルパティ）氏は2025年10月19日（現地時間）、ChatGPTのようなAIチャットボットを一から構築できるオープンソースプロジェクト「nanochat」を<a href="https://x.com/karpathy/status/1977755427569111362">発表</a>し、GitHub上にリポジトリを<a href="https://github.com/karpathy/nanochat">公開</a>した。</p>
<h2>4時間・約100ドルでChatGPT風モデルを構築</h2>
<p>Karpathy氏は投稿で、「わずか4時間・100ドルでChatGPTのような会話モデルを訓練できる」と説明。
クラウドGPU（8×H100構成）上で単一スクリプトを実行するだけで、LLMの事前学習から推論、WebUIまでを一括構築できる点が特徴だ。</p>
<p>コードは約8,000行と比較的コンパクトで、依存関係を最小限に抑えた「フルスタック実装」。
前作「nanoGPT」が事前学習フェーズに特化していたのに対し、今回の「nanochat」はトークナイザーの訓練から強化学習（RL）、推論エンジン、WebUIまでを統合している。</p>
<h2>Rust製トークナイザーと一貫した学習パイプライン</h2>
<p>nanochatでは、Karpathy氏が自作したRust実装のトークナイザーを用いてFineWebデータセットで事前学習（pretraining）を行う。
その後、SmolTalkデータによる会話形式の中間学習（mid-training）を経て、指示追従（SFT）や数学・コード・世界知識のベンチマーク評価（ARC-E/C、MMLU、GSM8K、HumanEval）を実施。
さらに、GSM8Kタスクに対する強化学習（GRPO）にも対応している。</p>
<p>推論時には、KVキャッシュを用いた効率的なデコードと、軽量Pythonサンドボックスでのツール使用（コード実行）機能を備える。CLIおよびChatGPT風のWebUIから利用でき、学習結果はMarkdown形式の「レポートカード」として自動出力される。</p>
<h2>性能とスケーラビリティ</h2>
<p>約4時間の訓練で「GPT-2を上回るCOREスコア」を達成し、12〜24時間の訓練では、MMLUで40点台、ARC-Easyで70点台、GSM8Kで20点台を記録。Karpathy氏は「1000ドル規模の訓練まで拡張すれば、数倍の一貫性と応答精度が得られる」としている。</p>
<h2>LLM教育「LLM101n」シリーズの集大成</h2>
<p>Karpathy氏は、nanochatを自身が開発中の教育プログラム「LLM101n」の“集大成プロジェクト（capstone project）”と位置づけている。
リポジトリは教育・研究者コミュニティ向けに「最大限フォーク可能」であり、LLMの構造理解や再実装の教材としても活用できる。
同氏は投稿の締めくくりで「これからが本番。チューニングとヒルクライミングを始める」と述べ、今後の最適化と拡張を予告した。</p>
<ul>
<li>GitHubリポジトリ：<a href="https://github.com/karpathy/nanochat">karpathy/nanochat</a></li>
<li>技術解説スレッド：<a href="https://github.com/karpathy/nanochat/discussions/1">Discussions #1 – nanochat speedrun walkthrough</a></li>
</ul>
]]></description>
      <pubDate>Wed, 22 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ドイツのNeura Robotics、「NEURA Gym」を公開──物理空間でAIを鍛える“世界初のAIジム”</title>
      <link>https://ledge.ai/articles/neura_robotics_neura_gym_physical_ai_training_center</link>
      <description><![CDATA[<p>ドイツ発のロボティクス企業 Neura Robotics（ニューラ・ロボティクス） は2025年10月14日、物理空間におけるAI訓練センター 「NEURA Gym（ニューラ・ジム）」 を公開した。同社の公式動画「<a href="https://www.youtube.com/watch?v=hUNujYlRmZU">NEURA Gym: The First Physical AI Training Center for Robots</a>」では、数百台のロボットが現実世界でタスクを学習する様子が紹介されている。</p>
<p>@<a href="https://www.youtube.com/watch?v=hUNujYlRmZU">YouTube</a></p>
<p>Neura Roboticsは、「テキストや画像、シミュレーションだけの学習では不十分であり、現実世界での経験こそが真の知能を生む」と強調。NEURA Gymはその考えに基づき、実世界の環境でAIモデルをトレーニングするための物理的インフラとして設計された。</p>
<h2>実世界でAIを鍛える“ジム”</h2>
<p>NEURA Gymは、数百台のロボットが常時稼働する広大な物理的空間で構成されている。ロボットは、仮想世界プラットフォーム「Neuraverse（ニューラバース）」と連携し、現実とシミュレーションの両方でデータを生成・共有する。</p>
<p>データ収集は、以下のサイクルで進む。まずシミュレーション上でタスクを学習し、その後、物理環境での挙動を検証。テレイグザキューション（遠隔操作）を通じて、ロボット自身の精密な感覚値や関節データを取得し、モデルを改善する。これを繰り返すことで、AIは実世界での「誤差」や「摩擦」を学び、より汎用的なスキルを身につけていく。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/neura_gym1_2d2e19081a/neura_gym1_2d2e19081a.jpg" alt="neura gym1.jpg" /></p>
<p>同社はこの仕組みを、人間の学習に例えて説明する。「水泳を覚えるには、水の中に入らなければならない」。つまり、AIも実際の物理環境に“飛び込む”ことで、真に応用可能な知能を獲得できるという考えだ。</p>
<h2>Neuraverse：スキル共有の中枢</h2>
<p>NEURA Gymで収集されたデータは、Neura Roboticsのグローバルデータ基盤「Neuraverse」に統合される。Neuraverseは、世界中のロボットから集めた現実データを共有・再利用する仕組みで、同社はこれを「世界最大かつ最速で拡張する物理訓練データリポジトリ」と呼んでいる。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/neura_gym2_a42849d8d4/neura_gym2_a42849d8d4.jpg" alt="neura gym2.jpg" /></p>
<p>学習済みスキルは即座に他のロボットへ転送可能であり、「一体が学べば全体が賢くなる」というネットワーク効果を生む。Neura Roboticsは、これを「ロボット版の“共有知能」と位置づけている。</p>
<h2>企業にも開放、世界展開を計画</h2>
<p>NEURA GymはNeura Robotics社内だけでなく、外部企業にも開放されている。
企業はスペースを予約し、自社のAIアプリケーションを訓練することが可能。Neura Roboticsのトレーナーによる支援を受けるほか、独自のエンジニアチームを持ち込むこともできる。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/neura_gym3_2cdb6aea39/neura_gym3_2cdb6aea39.jpg" alt="neura gym3.jpg" /></p>
<p>同社はすでに、単一施設の運営にとどまらず、世界各地でNEURA Gymの展開を進める計画を明らかにしている。</p>
<p>Neura Roboticsは2025年6月の発表で、「2030年までに500万台のロボットを普及させる」目標を掲げている。
NEURA GymとNeuraverseは、その基盤を支える“知能インフラ”であり、AIロボットが現実の環境を理解し、適応するための訓練場といえる。</p>
]]></description>
      <pubDate>Wed, 22 Oct 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google DeepMind、次世代核融合炉「SPARC」開発にAI導入──CFSと協働しプラズマ制御を最適化</title>
      <link>https://ledge.ai/articles/google_deepmind_fusion_energy_ai_cfs_sparc</link>
      <description><![CDATA[<p>Google DeepMindは2025年10月16日、米マサチューセッツ州の核融合エネルギー企業Commonwealth Fusion Systems（CFS）と提携し、次世代核融合炉「SPARC」の開発にAIを導入すると<a href="https://deepmind.google/discover/blog/bringing-ai-to-the-next-generation-of-fusion-energy/">発表</a>した。AIを用いてプラズマ挙動のシミュレーションや制御を最適化し、試験運転の効率化と安全性向上を図る。</p>
<p>CFSは同日、両社がAI研究基盤「TORAX（Tokamak Research Accelerator）」を活用し、複数のAIモデルを連携させてプラズマ物理を再現すると<a href="https://blog.cfs.energy/with-ai-alliance-google-deepmind-and-cfs-take-fusion-to-the-next-level/">公式ブログ</a>で説明。燃料供給、RF加熱、磁場コイルの電流制御など、数百に及ぶパラメータを強化学習によって同時最適化するという。これにより、実機での試験条件の探索を高速化し、SPARC運転時のプラズマ安定性やエネルギー効率を高めることを目指す。</p>
<p><strong>SPARCトカマク炉の断面可視化映像。磁場で閉じ込められたプラズマの挙動をAIがシミュレーションしている。左は実際のプラズマ、右はTORAXによるプラズマパルスの解析例で、圧力変化と制御コマンドによる性能差を示す。</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Deep_Mind_partnering_with_Commonwealth_Fusion_Systems_2c672bf3e4/Deep_Mind_partnering_with_Commonwealth_Fusion_Systems_2c672bf3e4.gif" alt="DeepMind- partnering with Commonwealth Fusion Systems.gif" /></p>
<p>Google DeepMindは2022年に、AIがトカマク型核融合炉の磁場をリアルタイム制御できることを示した論文をNature誌に<a href="https://deepmind.google/discover/blog/accelerating-fusion-science-through-learned-plasma-control/">発表</a>している。今回の提携はその研究を実際の炉運転に応用する段階であり、「AIをエネルギーの消費側から、生成側の推進力へと拡張する取り組み」として位置づけられる。</p>
<p>CFSは2026年にSPARCの初期稼働を予定しており、AIはその試運転や運転最適化を支援する主要技術となる見込みだ。将来的には商用核融合炉「ARC」でもAI制御を導入し、長期安定運転や自動最適化を実現する構想も示している。</p>
<p>Googleは2025年6月、CFSの商用核融合炉「ARC」から200メガワットの電力を供給する契約を<a href="https://ledge.ai/articles/google_fusion_power_cfs_ppa">締結</a>しており、今回のAI提携はその延長線上に位置づけられる。グループとしてクリーン電力調達とAI技術開発を両輪で推進し、AIが電力を“消費する”だけでなく、“創り出す”側にも関与し始めている。</p>
]]></description>
      <pubDate>Tue, 21 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI、キング牧師の肖像生成を一時停止──遺族と協議し、歴史的人物保護を強化</title>
      <link>https://ledge.ai/articles/openai_pauses_sora_mlk_generation_with_king_estate</link>
      <description><![CDATA[<p>OpenAIは2025年10月19日（現地時間）、動画生成AI「Sora」において、マーティン・ルーサー・キング・ジュニア博士（キング牧師）の肖像生成を一時停止したと<a href="https://x.com/OpenAINewsroom/status/1979005850166648933">発表</a>した。キング牧師の遺族団体「The Estate of Martin Luther King, Jr., Inc.（King, Inc.）」の要請を受けた措置であり、歴史的人物の扱いに関する倫理的ガードレールを強化する方針を示した。</p>
<h2>King Estateの要請で肖像生成を一時停止</h2>
<p>声明によると、一部のユーザーがキング牧師の肖像を不敬な形で生成していたことを受け、King, Inc.の要請によりOpenAIが対応した。
OpenAIは「歴史的人物の表現には強い表現の自由の利益があるが、公人とその家族は最終的に自身の肖像の扱いを管理すべき」との立場を表明。今後は、遺族や公式代理人が希望すれば、肖像をSoraでの生成対象から除外できる仕組みを導入するという。</p>
<p>OpenAIは、King, Inc.を代表して連絡を取ったバーニス・A・キング氏（Bernice A. King）と、AI倫理評議会（AI Ethics Council）のJohn Hope Bryant氏に感謝を示した。</p>
<h2>バーニス・キング氏「父の肖像は公共財ではない」</h2>
<p>キング牧師の娘でKing, Inc.の代表を務めるバーニス・A・キング氏は、SNS上で共同声明全文を<a href="https://www.facebook.com/photo/?fbid=1355219049297091">掲載</a>し、遺族側の見解を示した。
同氏は「父は公人であったが、選挙で選ばれた公職者ではなく、肖像権は公共ドメインに属さない」と述べ、米国の半数の州で死後の肖像権を50〜100年保護している点に言及した。</p>
<p>さらに、「多くのAI生成は表現の自由ではなく愚行だった」と強い言葉で批判し、「OpenAIが父の肖像の扱いに協力してくれたことを感謝する。AIを責任ある方法で活用し、倫理的なガードレールを整備する契機となることを望む」とコメントした。
同氏は投稿の中で、「父の言葉と遺産は、人権と社会正義の象徴として保護されるべきだ」とも述べている。</p>
<h2>倫理委員会も「人権保護の転換点」と評価</h2>
<p>AI倫理評議会を運営する非営利団体「Operation HOPE」も<a href="https://operationhope.org/operation-hope-hope-ai-ethics-council-on-openais-pause-of-sora-generations-depicting-dr-king/">声明</a>を発表し、今回の対応を「テクノロジーと人権保護のバランスを取る重要な転換点」と評価した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/operation_hope_2b5e07e137/operation_hope_2b5e07e137.jpg" alt="operation hope.jpg" /></p>
<h2>今後の展開</h2>
<p>OpenAIは2025年9月30日に公表した公式文書「<a href="https://openai.com/ja-JP/index/launching-sora-responsibly/">Launching Sora responsibly</a>」で、実在人物の肖像生成について「本人または権利者の同意に基づく」ことを原則としていた。今回の措置は、その方針をさらに実践的に強化するものとなる。</p>
<p>今回の決定は、AIが歴史的人物や著名人をどのように扱うべきかという国際的な倫理・法的議論を加速させる可能性がある。OpenAIは今後も、他の遺族団体や文化的機関と協議を進め、生成AI時代における肖像の扱いと責任の在り方を再定義していくとみられる。</p>
]]></description>
      <pubDate>Tue, 21 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>世界のAIチップを“毎晩測る”──SemiAnalysis、リアルタイム性能追跡プラットフォーム「InferenceMAX」始動</title>
      <link>https://ledge.ai/articles/semianalysis_inferencemax_real_time_benchmark</link>
      <description><![CDATA[<p>米調査・分析メディアのSemiAnalysisは2025年10月9日（現地時間）、AIチップの推論性能をリアルタイムで追跡するオープンベンチマークプラットフォーム「InferenceMAX（インファレンスマックス）」を<a href="https://newsletter.semianalysis.com/p/inferencemax-open-source-inference">公開</a>した。GPUやNPU、CPUを含む数百種類のチップを対象に、主要な推論フレームワークやAIモデルの性能を毎晩自動で再測定し、その結果をリアルタイムで可視化する。</p>
<h2>AIチップ性能をライブで監視</h2>
<p>InferenceMAXは、AI推論の実運用環境に近い形で各種ハードウェアを検証するオープンソースベンチマークだ。
SemiAnalysisによると、このシステムは「毎晩、数百種類のAIチップ上でベンチマークを実行し、オープンソースの推論フレームワークやモデルを継続的に再測定する」仕組みを持つ。
ソフトウェアスタックやドライバの更新に伴う性能の向上や退行をリアルタイムで記録・反映し、推論性能の進化を“生きた指標”として提示する。</p>
<p>ベンチマーク結果は無料で公開されており、InferenceMAX公式<a href="https://inferencemax.semianalysis.com/">ダッシュボード</a>{target=\</p>
]]></description>
      <pubDate>Tue, 21 Oct 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google DeepMind、AIに“造語”を教えて振る舞いを制御──Geminiが自ら意味を説明する能力も確認</title>
      <link>https://ledge.ai/articles/deepmind_neologism_learning_for_ai_controllability</link>
      <description><![CDATA[<p>Google DeepMindの研究チームは2025年10月9日、AIに新しい架空の言葉（造語）を学習させることで、その振る舞いを精密に制御できる手法を<a href="https://arxiv.org/abs/2510.08506">発表</a>した。論文「Neologism Learning for Controllability and Self-Verbalization」は、AIが学習した造語の意味を自然言語で説明できる“自己言語化（self-verbalization）”という現象も初めて報告している。</p>
<p>この研究はarXiv上で公開されたプレプリント（査読前論文）で、AIの内部表現を「言葉」で理解・制御する新しいアプローチとして注目を集めている。</p>
<h2>造語でAIをコントロール</h2>
<p>従来、AIの出力傾向を操作するには、プロンプト設計や外部ツール（例：steering vector、autoencoderなど）による内部操作が必要だった。今回の手法では、モデル本体のパラメータを一切変更せず、造語に対応する新しい単語埋め込み（embedding）だけを学習する。</p>
<p>たとえば「Give me a lack answer.」と指示すると、AIは短い回答を返すようになり、別の造語では「誤った回答」「お世辞」「拒否」など異なる挙動を誘発できる。</p>
<p>研究チームはこの方法を「ネオロジズム学習（Neologism Learning）」と呼び、言語による“行動パラメータ”の追加と位置づけている。</p>
<p><strong>ネオロジズム学習のプロセス。左から「造語による概念の学習」「AIによる自己言語化（Verbalization）」「説明文を使った再検証（Plug-In Evaluation）」の流れを示す</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x1_2e7a14d15f/x1_2e7a14d15f.png" alt="x1.png" /></p>
<h2>AIが自ら意味を説明</h2>
<p>研究チームは、AIが学習した造語の意味を英語で説明できることを確認した。
たとえば、短文回答を誘発する造語を学習したモデルに「What does lack mean?（lackとは何を意味しますか？）」と尋ねると、
「It means to give a shorter response.（短い回答をすることを意味します）」と答える。</p>
<p>このようにAI自身が学習した内部概念を自然言語で記述する能力を、研究チームは“self-verbalization（自己言語化）”と定義。
さらに、造語をその説明文に置き換えても同様の挙動が再現されるかを検証する「plug-in evaluation」を導入し、自己説明の信頼性を評価した。</p>
<h2>モデル間で“機械語”が通じる</h2>
<p>DeepMindの実験では、Gemma-3-4B-ITが学んだ造語を別のモデル――Gemini 2.5 Flash――に入力したところ、意味が通じ、ほぼ同じ制御効果を示した。
たとえば“lack”という語を用いた場合、Gemmaでは回答の平均文数が42.9から15.8に減少し、Geminiでも中央値が37から4に減少した。</p>
<p>研究チームはこの現象を「machine-only synonym（機械専用類義語）」と呼び、
人間には直感的に理解できないが、AI同士では通じ合う“共通語彙”が形成される可能性を指摘している。</p>
<h2>複合的な概念も制御可能</h2>
<p>ネオロジズム学習は、単純な行動特性だけでなく、複数の概念を組み合わせた複合的制御にも対応する。
たとえば「短く・数値を含む・高確率」といった3つの条件を、それぞれに対応する造語を同時に指定することで達成できるという。</p>
<p>研究では、「短文」「誤答」「お世辞」「拒否」など7種類の単純概念や、言語的特徴を扱うベンチマークAxBenchにおいても、高い制御性能が確認された。</p>
<h2>AIの「内なる言葉」への道</h2>
<p>著者のひとりであるジョン・ヒューイット（John Hewitt）氏は、
「私たちは既存の語彙だけではAIの内部概念を十分に理解できない」と述べ、造語学習をその橋渡しと位置づけている。</p>
<p>研究は、AIの制御可能性（controllability）と説明可能性（explainability）を同時に高める新しい方向性を示すものであり、
将来的にはAI間通信や人間との協調学習に応用できる可能性があるとみられる。</p>
]]></description>
      <pubDate>Mon, 20 Oct 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Walmart、OpenAIと提携──ChatGPTに「Instant Checkout」登場　チャットで完結する購買体験を提供</title>
      <link>https://ledge.ai/articles/walmart_openai_instant_checkout_chatgpt</link>
      <description><![CDATA[<p>米小売最大手のWalmartは2025年10月14日（現地時間）、OpenAIとの提携を<a href="https://corporate.walmart.com/news/2025/10/14/walmart-partners-with-openai-to-create-ai-first-shopping-experiences">発表</a>した。この提携により、顧客やSam’s Clubの会員は、ChatGPTの「Instant Checkout（インスタントチェックアウト）」機能を通じてチャットで完結する買い物が可能になる。Walmartは“AI-First Shopping”を掲げ、反応型から予測型へと購買体験の転換を図る。</p>
<h2>チャットで完結する購買体験を提供</h2>
<p>新機能「Instant Checkout」は、ChatGPTのチャット画面上で商品提案から決済までをチャットで完結できる仕組みだ。
ユーザーが「Walmartで子どものランチボックスを探して」と入力すれば、ChatGPTが該当商品を提示し、そのまま少ない手順で購入まで案内する。</p>
<p>WalmartとSam’s Clubの商品が順次対象となる予定で、提供は「近日中（soon）」としており、詳細時期は非公表。</p>
<h2>「Agentic Commerce」──AIが先読みする買い物へ</h2>
<p>Walmartは今回の取り組みを「agentic commerce（自律型コマース）」と位置づける。
これまでのように顧客が検索して商品を選ぶ“反応型”から、AIがニーズを学習し、必要になる前に提案する“予測型”ショッピングへ転換する構想だ。リリースでは、「AIが顧客を理解し、日常の買い物を支援する時代に入った」と述べている。</p>
<h2>AI活用の知見を消費者体験へ拡張</h2>
<p>WalmartはこれまでもAIを活用し、商品カタログの改善や在庫最適化、アソシエイトのAIリテラシー教育を進めてきた。
今回のOpenAIとの提携は、そうしたAI活用の成果を消費者向けのチャット購買体験へ拡張するものとなる。</p>
<p>Doug McMillon CEOは、「長らくECの買い物は検索バーと長い商品リストが中心だったが、それは変わる」と述べ、Sparky（同社AI）とOpenAIとの連携で、より楽しく便利な未来へ進むと強調した。OpenAIのSam Altman氏も「日々の買い物を少し簡単にする取り組みの一つ」とコメントしている。</p>
]]></description>
      <pubDate>Mon, 20 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/10/19 [SUN]Amazon、次世代原子炉「小型モジュール炉（SMR）」開発支援を発表──AI需要増に対応し、脱炭素電源を強化</title>
      <link>https://ledge.ai/articles/amazon_smr_clean_energy_ai_infrastructure</link>
      <description><![CDATA[<p>Amazonは2025年10月16日（米国時間）、AI技術やクラウドサービスの拡大に伴う電力需要の急増に対応するため、次世代原子炉「小型モジュール炉（SMR）」の開発支援を<a href="https://www.aboutamazon.com/news/sustainability/amazon-smr-nuclear-energy">発表</a>した。米国のエネルギー企業Dominion Energy、X-energy、Energy Northwestの3社と協力し、先進原子力技術の導入を進める。</p>
<p>Amazonは「再生可能エネルギーに加え、信頼性の高いゼロカーボン電源が必要」と説明し、原子力を再エネを補完する選択肢として位置づける。</p>
<p>今回の取り組みでは、Dominion Energyとバージニア州でのSMR開発可能性の評価（MOU）を行い、X-energyとは高温ガス炉「Xe-100」商業化の支援で連携、Energy Northwestとはワシントン州での先進原子力プロジェクトを検討する。いずれも初期段階で、建設開始時期は現時点で未定としている。</p>
<p><strong>X-energy社のXe-100運転訓練シミュレーター</strong>：開発中の小型モジュール炉（SMR）「Xe-100」の制御システムを再現した施設で、オペレーターが安全運転や緊急対応を訓練する
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/1_0bbe5f5462/1_0bbe5f5462.jpg" alt="ダウンロード (1).jpg" /></p>
<p>Amazonは世界で500件を超える再生可能エネルギープロジェクトを展開しており、クリーン電力の拡大を継続している。長期目標の2040年ネットゼロ（二酸化炭素排出実質ゼロ）の達成に向け、SMRを含むクリーン電源の多様化を進める方針だ。</p>
]]></description>
      <pubDate>Sat, 18 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Anthropic、「Claude Skills」を発表──資料を読み込み専門ワークを自動化する新機能</title>
      <link>https://ledge.ai/articles/claude_skills_release_oct2025</link>
      <description><![CDATA[<p>AI開発企業のAnthropicは2025年10月16日（現地時間）、AIアシスタント「Claude」に新機能「Agent Skills」（以下、Claude Skills）を導入したと<a href="https://www.anthropic.com/news/skills">発表</a>した。ユーザーが自らの業務資料や手順書をスキルとして登録すると、Claudeがそれらを読み込み、専門的なワークフローを自動実行できるようになる。企業ごとのナレッジやスクリプトをAIに統合し、業務効率化をさらに推し進める狙いだ。</p>
<p>@<a href="https://www.youtube.com/watch?v=IoqpBKrNaZI">YouTube</a></p>
<h2>フォルダ単位で“教え込む”仕組み</h2>
<p>Claude Skillsは、指示文やスクリプト、関連資料をまとめたフォルダをClaudeに読み込ませる仕組みだ。各フォルダには「SKILL.md」という定義ファイルが含まれ、タスク内容や使用条件などが記述される。Claudeは必要に応じてこれらのスキルを呼び出し、指示に従って処理を実行する。</p>
<p>この設計は「進行開示（progressive disclosure）」と呼ばれ、必要な情報だけを段階的に読み込むことで効率と安全性を両立する。Anthropicは、Claude Skillsを「プロンプトの再利用性を高め、AIが現実的な作業単位で動けるようにする構造」と説明している。</p>
<h2>企業導入例：Box、Canvaなど</h2>
<p>発表では、企業向けの活用例も紹介された。
クラウドストレージ大手のBoxでは、文書を自動的に要約・変換し、PowerPointやWord形式に整理するワークフローをスキルとして実装。Notionは「複雑なタスクでのプロンプト調整を減らし、より予測可能な結果につながる」とコメントしている。</p>
<p>さらにデザインプラットフォームのCanvaは、ブランドガイドをClaude Skillsに登録し、AIが自動でデザイン案を生成する活用を計画しているという。これにより、社内のスタイルガイドや手順書をAIに“教え込む”ことで、誰でも同じ品質で成果物を作れる環境を整備できる。</p>
<h2>エンジニア向けにはコード実行にも対応</h2>
<p>Anthropicの<a href="https://www.anthropic.com/engineering/equipping-agents-for-the-real-world-with-agent-skills">エンジニアリングブログ</a>では、Claude Skillsの内部構造が詳細に解説されている。Claude Skillsは単なるプリセットではなく、自己記述的モジュール構造を持つ。スキルはフォルダ単位で管理され、初期段階ではメタ情報のみを読み込み、関連タスクが発生した際に全体を展開する。</p>
<p>スクリプトやPythonコードを含めることで、データ分析や自動レポート生成といった専門処理も可能だ。開発者は /v1/skills エンドポイントを通じてスキルを登録・管理でき、実行にはCode Execution Tool（ベータ）が必要となる。</p>
<p>従来の「プロンプト＋RAG（検索）」のように文脈を都度読み込む手法に比べ、Claude Skillsでは情報の再利用が容易で、処理速度や一貫性が大幅に向上するという。</p>
<h2>今後の展開</h2>
<p>Anthropicは今後、スキルの作成・共有・管理を容易にするツールやチーム配布機能の提供を予定している。また、セキュリティ面での検証や公開スキルストアの構想も進行中だ。</p>
<p>今回の発表は、Claudeを“対話AI”から“実行AI”へ進化させる試みの一環であり、Anthropicが目指すエージェント時代の布石といえる。</p>
]]></description>
      <pubDate>Sat, 18 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google、AIがPCを操作する「Gemini 2.5 Computer Use model」を開発者向けに公開──ClaudeやOpenAIモデルを上回る性能を実証</title>
      <link>https://ledge.ai/articles/google_gemini_2_5_computer_use_release</link>
      <description><![CDATA[<p>Google DeepMind は2025年10月7日（米国時間）、AI が実際のコンピューター画面を理解し、クリックや入力などの操作を実行できる新モデル「Gemini 2.5 Computer Use model」を開発者向けにプレビュー提供したと<a href="https://blog.google/technology/google-deepmind/gemini-computer-use-model/">発表</a>した。</p>
<p>Gemini API を通じて利用でき、AI が人間と同様にブラウザやアプリのUI（ユーザーインターフェース）を操作することを可能にする。</p>
<h2>Gemini API に“computer_use”ツールを追加</h2>
<p>今回発表された新モデルは、Gemini 2.5 の機能拡張として API に追加された「computer_use」ツールを用いて動作する。</p>
<p>AI はユーザーからの指示に加え、スクリーンショットと直近の操作履歴を入力として受け取り、次に取るべきアクション（クリック・入力・スクロールなど）を出力。実行結果を再び画面キャプチャとして取得し、目標達成までループ処理を行う。これにより、設定変更やフォーム入力、情報検索など、複数ステップを自律的に完了できる。</p>
<p>Google は公式ブログで、「このモデルはユーザー許可を前提に、安全性と透明性を重視して設計されている」と強調している。</p>
<p><strong>Computer Use model の処理ループ。AI がスクリーンショットと操作履歴をもとに次の行動を生成し、クライアント環境で実行 → 状況を再取得して次の判断へとつなげる</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/CTU_Diagram_RD_4_V01_width_1000_format_webp_b3415f41ee/CTU_Diagram_RD_4_V01_width_1000_format_webp_b3415f41ee.webp" alt="CTU-Diagram-RD4-V01.width-1000.format-webp.webp" /></p>
<h2>プレビュー提供と利用方法</h2>
<p>開発者は Google AI Studio および Vertex AI を通じて Computer Use model にアクセスできる。プレビュー版の段階では主にブラウザ操作に最適化されており、今後はより広範なアプリやデスクトップ環境への対応も検討されているという。</p>
<p>Google は、操作範囲やデータアクセスを制御する仕組みを組み込み、「責任ある自動化（Responsible Automation）」の実現を掲げている。</p>
<h2>ベンチマーク性能：Claude Sonnet 4.5 を上回る</h2>
<p>Google DeepMind は、Gemini 2.5 Computer Use model の性能を複数の標準ベンチマークで検証した。
Browserbase による Online-Mind2Web テストでは 65.7 % の精度を記録し、Claude Sonnet 4.5 や OpenAI Computer-Using Model を上回った。
さらに WebVoyager や AndroidWorld でも高スコアを達成し、実行速度（レイテンシ）でも優位性を示している。</p>
<p><strong>Gemini 2.5 Computer Use model は、Claude Sonnet 4.5 や OpenAI Computer-Using Model に比べ、低レイテンシかつ高精度を示した。</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/CTU_Scatterplot_RD_7_width_1000_format_webp_7e4e545c1c/CTU_Scatterplot_RD_7_width_1000_format_webp_7e4e545c1c.webp" alt="CTU-Scatterplot-RD7.width-1000.format-webp.webp" /></p>
<p><strong>複数ベンチマークで高い精度を記録。特に WebVoyager と AndroidWorld で際立ったスコアを達成した。</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/CTU_Benchmark_Chart_RD_5_V01_width_1000_format_webp_e0982edd33/CTU_Benchmark_Chart_RD_5_V01_width_1000_format_webp_e0982edd33.webp" alt="CTU-Benchmark_Chart-RD5_V01.width-1000.format-webp.webp" /></p>
<h2>動作デモ：AI がブラウザを自律操作</h2>
<p>公式ブログでは、実際の操作デモ動画も公開されている。
動画では AI が画面を認識し、ブラウザ上でリンクをクリックしたり、テキストを入力してタスクを完了する様子が確認できる。</p>
<p>@<a href="https://www.youtube.com/watch?v=_lu-FcPUIfM">YouTube</a></p>
<h2>AI による“手の届く自動化”へ</h2>
<p>今回の発表は、AI が人間の指示をもとに実際のUI を操作できる「エージェント時代」の幕開けを示す。
Google は Computer Use を “次世代の AI アシスタント” 開発の基盤と位置づけており、将来的には業務支援やウェブ操作、アプリ間連携など、より幅広い自動化領域への展開が期待される。</p>
]]></description>
      <pubDate>Sat, 18 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>エンタメ＆アート2025/10/17 [FRI]Google、動画生成AI「Veo 3.1」を発表──1分超のシーン拡張「Extend」搭載、Flowと統合強化</title>
      <link>https://ledge.ai/articles/google_veo3_1_flow_integration</link>
      <description><![CDATA[<p>Googleは現地時間2025年10月15日、動画生成AI「Veo」の最新版となるVeo 3.1を<a href="https://blog.google/technology/ai/veo-updates-flow/">発表</a>した。</p>
<p>新バージョンでは、AI映像制作ツール「Flow」に新機能を追加し、その中核としてVeo 3.1を統合。照明・構図・音声をAIが自動的に制御できるようになり、リッチなオーディオ、物語制御（narrative control）の強化、質感のリアリズム向上を図ったアップデートとなっている。</p>
<p>@<a href="https://www.youtube.com/watch?v=I06Ef8alr2Y">YouTube</a></p>
<h2>Flowとの統合で生成から編集まで一体化</h2>
<p>Googleは今回、AI映像制作ツール「Flow」へのアップデートを発表した。Flowは5か月前の導入以降、すでに2億7,500万本以上の動画を生成しており、Veo 3.1の搭載によって生成から編集までのプロセスがさらに統合された。</p>
<p>Flowは、テキスト・画像・音声・映像素材といった“ingredients”を組み合わせて動画を構築できるツールである。
既存の「Ingredients to Video」「Frames to Video」「Extend」機能に加え、今回は音声統合を拡張。ユーザーは複数の素材をもとに、AIがシーン構成やカメラワーク、トーンを自動的に最適化した一貫性のある映像を生成できる。</p>
<p>新しいExtendでは、直前のクリップの終端1秒を手がかりに、1分以上の連続ショットとして自然に拡張することも可能。Googleはこれにより、「映像制作をより直感的で対話的な体験へと変える」としている。</p>
<p>@<a href="https://www.youtube.com/watch?v=B78BJuPxmBU">YouTube</a></p>
<h2>照明や構図、音声もAIが自動編集</h2>
<p>Veo 3.1では、照明・陰影・カメラ構図をAIが自動的に制御し、シーン全体のトーンや一貫性を高める。
また、AIによる音声生成と映像への同期統合にも対応し、環境音や効果音を含む“音響的なリアリティ”を再現できるようになった。</p>
<p>Flow内には、シーンに要素を追加する「Insert」と、不要な物体を背景ごと削除する「Remove（近日提供）」の編集機能も加わった。影や照明の整合を自動で処理することで、合成感を抑えた自然な編集を可能にしている。
Googleは、こうした機能群を通じて「richer audio」「more narrative control」「enhanced realism」の実現を掲げている。</p>
<h2>Gemini APIで提供、Standard／Fastモデルを展開</h2>
<p>Veo 3.1は、「Standard」モデルと「Fast」モデルの2種類を用意し、Gemini API経由で開発者向けに提供が開始された。生成速度を優先するワークフローにはFastモデル、品質を重視する制作用途にはStandardモデルが推奨される。</p>
<p>さらに、Veo 3.1はVertex AIおよびGeminiアプリからも利用可能。新機能はGemini API／Vertex AIの双方で順次展開される予定で、API向けの「Scene extension」機能も今後提供される見込みだ。
Googleは「AIによる創造的表現の民主化をさらに進める」とし、プロフェッショナルから一般ユーザーまで、誰もが高品質な映像制作にアクセスできる環境の構築を目指している。</p>
<h2>Veoシリーズの進化</h2>
<p>Veoシリーズは、2024年12月の「Veo 2」、2025年春の「Veo 3」に続く最新バージョン。
今回のVeo 3.1では、「AI任せの自動生成」から「人とAIが協働して映像を作る」方向へと進化した。
Flowとの連携により、テキストによる指示だけでなく、素材・音声・カメラ指示などを含めた多層的なプロンプト設計が可能になり、AI映像生成の精度と自由度が大幅に向上している。</p>
<p>Googleは今後、VeoをGeminiエコシステムの中核技術として位置づけ、AIを活用したクリエイティブツールの拡充を進める方針を示している。</p>
]]></description>
      <pubDate>Sat, 18 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/10/20 [MON]NTT、次世代純国産LLM「tsuzumi 2」発表──フルスクラッチ設計でGPT-5級の日本語性能を軽量モデルで実現</title>
      <link>https://ledge.ai/articles/ntt_tsuzumi2_fullscratch_gpt5_level_japanese_llm</link>
      <description><![CDATA[<p>NTT2025年10月20日、フルスクラッチで開発した純国産の大規模言語モデル（LLM）「tsuzumi」の次世代版「tsuzumi 2」を<a href="https://group.ntt/jp/newsrelease/2025/10/20/251020a.html">発表</a>した。
ChatGPTなど海外製LLMの普及が進む一方、電力消費や機密情報の取り扱いといった課題が顕在化するなか、NTTは1GPU環境で動作可能な軽量設計と高い日本語理解力を両立。日本語に最適化された高セキュアな生成AIとして提供を開始した。</p>
<h2>tsuzumi 2の進化ポイント</h2>
<p>NTTは、tsuzumi 2を「日本の企業DXを支える高性能・高セキュア・低コストな純国産LLM」と位置づけ、次の3つの進化点を挙げている。</p>
<ol>
<li>日本語性能のさらなる向上</li>
<li>特化型モデル開発効率の向上</li>
<li>低コスト・高セキュアの維持、国産AI</li>
</ol>
<p>これらの改良により、NTTは軽量ながらも世界トップクラスの性能を達成したと説明している。</p>
<h2>GPT-5級の日本語性能を実現</h2>
<p>「tsuzumi 2」は同クラス帯（約30Bパラメータ）のモデルとして世界トップクラスの日本語性能を持つ。
知識・解析・指示遂行・安全性の各評価において、数倍以上大きなフラッグシップモデル（GPT-5など）に匹敵する水準を記録した。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/tsuzumi2_3_88e14a3c64/tsuzumi2_3_88e14a3c64.jpg" alt="tsuzumi2 3.jpg" /></p>
<p>同社は、独自トークナイザーによって日本語の単語分割を最適化。文法構造に沿った分割を学習させることで、自然で読みやすい文の生成と高い効率性を両立した。
また、英語やソースコードに対しても効率的な生成が可能で、トークン当たり文字数ではGPT-5の約1.5倍を出力できるという。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/tsuzumi2_1_8855a698bd/tsuzumi2_1_8855a698bd.jpg" alt="tsuzumi2 1.jpg" /></p>
<h2>金融・医療・公共分野に特化した開発効率を強化</h2>
<p>tsuzumi 2では、RAG（検索拡張生成）とFine Tuningを組み合わせた特化モデル開発が可能になった。金融・医療・公共分野の知識を重点的に強化し、少量データでも高精度な応答を実現。NTT社内では、財務システムに関する問い合わせ応答タスクにおいて、他社先進モデルと同等以上の性能を確認した。また、FP2級試験を用いた評価では、他モデルの約10分の1の追加学習データで合格基準に到達したという。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/tsuzumi2_6_f1c925969d/tsuzumi2_6_f1c925969d.jpg" alt="tsuzumi2 6.jpg" /></p>
<h2>軽量・高セキュア設計で企業利用を想定</h2>
<p>tsuzumi 2は、1GPUでの推論が可能な軽量設計を維持し、推論コストを約10〜20分の1に削減。16bit・8bit・4bitの量子化モードを備え、用途に応じて精度や速度を調整できる。また、オンプレミスやプライベートクラウドでも動作可能で、機密情報を含む業務データも安全に処理できる。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/tsuzumi2_7_d5c23fc249/tsuzumi2_7_d5c23fc249.jpg" alt="tsuzumi2 7.jpg" /></p>
<h2>学習データを自社管理、著作権と文化への配慮</h2>
<p>NTTは、海外製オープンモデルに依存せず、学習データ・開発プロセス・品質を完全に自社管理している。新聞社データなどの権利保護にも配慮し、学習データから自主的に削除。40年以上にわたる日本語研究の成果を基盤に、「日本語・文化・慣習を理解するAI」を目指している。</p>
<h2>教育・企業現場での導入事例も拡大</h2>
<p>すでに東京通信大学が学内LLM基盤としてtsuzumi 2を採用。授業Q&amp;Aや教材作成支援、履修相談などで運用を開始する。
また、NTTドコモビジネスと富士フイルムビジネスイノベーションは、契約書や提案書などの非構造化データを安全に構造化・分析できる生成AIソリューションの共同開発を進めている。</p>
<h2>今後の展開</h2>
<p>NTTは、tsuzumi 2をグループ各社のAIソリューションに順次組み込み、産業ごとの特化モデルを展開予定。
さらに、AI間の自律的な議論を行う「AIコンステレーション」構想や、サイバーセキュリティ分野での応用開発も進める。
11月に開催される「NTT R&amp;Dフォーラム 2025（IOWN Quantum Leap）」では、tsuzumi 2を活用した最新ソリューションを披露するという。</p>
]]></description>
      <pubDate>Sat, 18 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>NVIDIA、「AI Day Tokyo 2025」全26セッションを無料公開──「ソブリンAI」「フィジカルAI」など最新動向をオンデマンドで視聴可能に</title>
      <link>https://ledge.ai/articles/nvidia_ai_day_tokyo_2025_ondemand_sessions</link>
      <description><![CDATA[<p>NVIDIAは10月21日までに、9月に東京ミッドタウン（東京都港区）で開催したイベント「<a href="https://blogs.nvidia.co.jp/blog/ai-day-tokyo/?ncid=so-twit-279362&amp;linkId=100000387789084">NVIDIA AI Day Tokyo 2025</a>」で実施した全26セッションの<a href="https://www.nvidia.com/ja-jp/on-demand/playlist/playList-4fa34d64-8fc9-487f-819a-18e06511216b/">オンデマンド配信</a>を開始した。生成AI、ロボティクス、医療AI、ソブリンAIなどをテーマとした講演を、同社公式サイトで無料視聴できる。</p>
<p>このイベントは、産業・研究・行政分野におけるAI活用の現状と今後の展望を共有する目的で開催され、約900名が参加した。主催はNVIDIA Japan。</p>
<p>主要テーマのひとつである「ソブリンAI（Sovereign AI）」では、日本国内のデータセンターやインフラ環境内でAI開発・運用を完結させる取り組みを紹介。政府や産業界における「データ主権」の確立に関する議論が行われた。</p>
<p>また、「フィジカルAI（Physical AI）」では、物理法則を理解し、ロボット制御や自動運転など現実世界の動作を最適化するAI技術が取り上げられた。AIが現実空間で学習・推論を行う応用例が紹介されたという。</p>
<p>その他のセッションでは、生成AIモデル「NIM」や「NeMo」を用いたアプリケーション開発、医療・創薬分野でのAI活用、ロボティクスシミュレーション、GPUクラウド基盤の構築、日本企業による導入事例などが扱われている。</p>
<p>NVIDIAは同ブログ内で、日本のAI演算需要が2030年までに2020年比で約320倍に増加すると予測しており、日本市場をアジア地域の主要拠点の一つとして位置づけている。</p>
<p>オンデマンド配信は、NVIDIA公式サイトの「NVIDIA On-Demand」上で<a href="https://www.nvidia.com/ja-jp/on-demand/playlist/playList-4fa34d64-8fc9-487f-819a-18e06511216b/">公開</a>されており、全26セッションが日本語で視聴可能。視聴登録のみでアクセスできる。</p>
]]></description>
      <pubDate>Sat, 18 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>アルトマン氏「エロティックばかり注目されたけど」──ChatGPT、成人ユーザーの自由拡大へ</title>
      <link>https://ledge.ai/articles/openai_chatgpt_adult_mode_update_oct2025</link>
      <description><![CDATA[<p>OpenAIのサム・アルトマンCEOは10月14日（現地時間）、X（旧Twitter）上で、ChatGPTの安全制限を一部緩和し、成人認証済みユーザーに対してエロティックな会話を許可する方針を<a href="https://x.com/sama/status/1978129344598827128">発表</a>
した。</p>
<p>投稿は瞬く間に注目を集め、「エロティック解禁」が大きな話題となったが、アルトマン氏は翌日に「その部分ばかり注目されてしまったが」と<a href="https://x.com/sama/status/1978539332215681076">補足</a>し、実際には“より人間らしいAI体験”を実現するための包括的な方針変更であることを強調した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/gpt5_erotica_7cea863241/gpt5_erotica_7cea863241.jpg" alt="gpt5 erotica.jpg" /></p>
<h2>安全を優先してきたChatGPTの制限</h2>
<p>ChatGPTはこれまで、性的表現や親密な会話を含むコンテンツを厳しく制限してきた。
アルトマン氏は「メンタルヘルス問題に慎重を期すためだった」と説明し、精神的に不安定なユーザーに配慮した措置であったと振り返った。
「深刻な危機状態にあるユーザーは別扱いとし、他者に害を与える行為は依然として許可しない」と述べ、ポリシーの根幹は維持されるとしたうえで、「リスクのない成人ユーザーにはより多くの自由を与える」と明言した。</p>
<h2>“4oらしさ”を再導入──人間的なAI体験へ</h2>
<p>アルトマン氏は同日、「数週間以内に“4oで好まれた振る舞い”に近い人格（パーソナリティ）を選べる新バージョンのChatGPTを提供する」と投稿した。
GPT-4oは会話の自然さや表情豊かな応答で人気を集めたモデルであり、今後はユーザーが望む場合に、フレンドリーな口調や絵文字を多用した“人間らしい”対話スタイルを選べるようになる。
アルトマン氏は「これは利用時間を増やすためではなく、ユーザーが自分の望む形でAIと関わる自由を得るための設計だ」と述べている。</p>
<h2>「成人は大人として扱う」──12月に年齢認証を本格導入</h2>
<p>アルトマン氏は、12月に年齢認証を本格導入し、認証済み成人ユーザーに対してはエロティック会話なども許可する方針を示した。
一方で、ティーンエイジャーに対しては「安全をプライバシーや自由より優先する」と述べ、メンタルヘルス関連ポリシーは緩めないと強調している。</p>
<p>アルトマン氏は、「社会がR指定映画で境界を設けるように、AIにも適切な年齢境界を設けたい」と例え、「我々は選挙で選ばれた道徳警察ではない」と付け加えた。</p>
<h2>倫理と自由の境界線</h2>
<p>アルトマン氏の発言は、AIにどこまで人間的な自由を与えるかという議論を再燃させた。</p>
<p>OpenAIは今後、成人向け表現やAIの人格設計に関するガイドラインをさらに明確化するとみられる。今回の方針転換は、「AIをどう設計し、どう育てるか」という人間社会全体のテーマに踏み込む第一歩となりそうだ。</p>
]]></description>
      <pubDate>Sat, 18 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI、「ChatGPT Atlas」を正式公開──AIブラウザとしてmacOS版をリリース</title>
      <link>https://ledge.ai/articles/openai_chatgpt_atlas_release_oct2025</link>
      <description><![CDATA[<p>OpenAIは米国時間10月21日、AIブラウザ「ChatGPT Atlas」を<a href="https://openai.com/index/introducing-chatgpt-atlas/">発表</a>した。</p>
<p>同社によると、AtlasはChatGPTを中心に構築されたmacOS向けアプリで、チャットとWebブラウジングを融合した新しいAI体験を提供するという。発表が行われたライブ配信では、AIがWebの使い方そのものを再定義する「10年に一度の機会」と位置づけ、従来のブラウザの概念を刷新する設計思想が語られた。</p>
<p>@<a href="https://www.youtube.com/watch?v=8UWKxJbjriY">YouTube</a></p>
<h2>ChatGPTが心臓部にあるブラウザ</h2>
<p>ライブ配信で同社は、Atlasの開発が「ブラウザとチャットできたらどうなるか？」という問いから始まったと説明。
従来の複雑なWeb体験を、シンプルな会話で置き換えることを目指したと語った。Atlasは単にAI機能を追加した既存ブラウザではなく、「ChatGPTがその鼓動する心臓（beating heart）」として常にユーザーのそばで支援するよう設計されているという。</p>
<h2>コア機能：3つの柱</h2>
<p>Atlasを支えるコア機能として以下の3点が挙げられた。</p>
<ol>
<li><strong>Chat Anywhere</strong> ：ユーザーがどのWebページを閲覧していても、チャット機能を呼び出すことでページの文脈を理解した支援が受けられる。コピー＆ペーストやタブ移動の必要がなく、メール作成や文書作業中でもAIが内容を把握して提案を行う。</li>
<li><strong>ブラウザ・メモリ</strong> ：ChatGPTの記憶機能をブラウザー全体に拡張。Atlasはユーザーの閲覧や検索の履歴を理解し、過去に見たドキュメントを「人間が話すような言葉」で検索できる。利用者ごとにAIがパーソナライズされていく仕組み。</li>
<li><strong>エージェント</strong> ：ChatGPTがユーザーの代わりに操作を実行する機能。予約やドキュメント編集などを支援する際に、AIが小さなカーソルを出してクリック動作を行う。ログイン情報やブラウザー履歴にアクセスでき、まるで「ユーザー自身の自然な延長」として働く。</li>
</ol>
<h2>その他の主要機能</h2>
<ul>
<li><strong>Ask ChatGPT（サイドバー）</strong> ：ウェブページ右上の「Ask ChatGPT」ボタンから起動。表示中ページの要約、製品比較、プルリクエストやSlackチャンネルの要約などを行える。</li>
<li><strong>検索体験の刷新</strong> ：検索結果はニュース・画像などのタブでも絞り込め、ページ遷移後もChatGPTとの対話を並行して続けられる。</li>
<li><strong>カーソル操作（Use cursor）</strong> ：フォーム入力欄でテキストを選択し、ChatGPTを呼び出して文法チェックやトーン調整などを行う。</li>
</ul>
<h2>安全性と提供状況</h2>
<p>OpenAIは、Agentモードがユーザーのタブ上でのみ動作し、外部ファイルやコード実行にはアクセスしないよう安全設計が施されていると説明。ブラウザ・メモリ機能も完全オプションで、設定やシークレットウィンドウで制御可能とした。</p>
<p>AtlasはmacOSユーザー向けに全世界で提供開始されており、Plus／Pro／Businessユーザーを対象にプレビュー提供されている。今後、Windowsおよびモバイルへの展開を計画しているとのことだ。</p>
<h2>AIとWebを再定義する「次の10年」</h2>
<p>OpenAIはブログ「<a href="https://openai.com/index/introducing-chatgpt-atlas/">Introducing ChatGPT Atlas</a>」の中で、Atlasを「チャット、ブラウジング、ワークスペースを統合する新しい形のAI体験」と表現。AIがユーザーを理解し、インターネット上で必要な情報を能動的に探し出す未来像を描いている。同社はこのプロジェクトを「early days（初期段階）」と位置づけ、今後さらに機能を拡張していく方針だ。</p>
]]></description>
      <pubDate>Sat, 18 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>政府、OpenAIに著作権侵害防止を要請──「Sora 2」問題で平デジタル相は“オプトイン方式”を提言</title>
      <link>https://ledge.ai/articles/openai_sora2_government_copyright_request_oct2025</link>
      <description><![CDATA[<p>OpenAIの動画生成AI「Sora 2」による日本のアニメ作品に酷似した映像がSNS上で拡散している問題を受け、政府が対応に乗り出した。城内実内閣府特命担当大臣（知的財産戦略・クールジャパン戦略担当）は10月10日の<a href="https://www.gov-online.go.jp/press_conferences/minister_of_state/202510/video-303104.html">記者会見</a>で、OpenAIに対し著作権侵害となる行為を行わないよう要請したと明らかにした。</p>
<p>城内大臣は「アニメや漫画は世界の人々を魅了し続ける、我が国が世界に誇る宝」と述べ、知的財産権の保護を重視する姿勢を強調。要請は内閣府の知的財産戦略推進事務局からオンラインで直接行われたという。記者質問の内容から、実施時期は10月上旬で、Sora 2による“酷似動画”が相次いだ直後とみられる。</p>
<p>一方、平将明デジタル大臣は10月12日、TBS番組でAIの学習段階における権利処理の在り方について言及した。「OpenAIには、きちんと権利処理をしていただくようお願いしている」と述べたうえで、「AIの学習データについても、事前の同意を得るオプトイン方式が望ましい」と発言。AI事業者に対し、無断利用ではなく同意制に基づくデータ利用の仕組みを導入するよう求めた。</p>
<p>これに先立つ10月3日、OpenAIのCEOであるサム・アルトマン氏は、動画生成AI「Sora 2」に関連する著作権保護と収益分配制度に関する方針をブログで明らかにしていた。同氏は「試行錯誤を重ねながら早期に開始する」と述べ、経済的な利益と新しい関係構築の双方を実現したい考えを示している。</p>
<p>政府は今後もAI事業者に対し、著作権および文化的資産の保護を重視した対応を求める方針を示している。AIによる創作支援が拡大するなかで、学習データの扱いと権利保護の両立が国際的な課題となりつつある。</p>
]]></description>
      <pubDate>Sat, 18 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/10/18 [SAT]「見て、聞いて、考えるCopilot」──Windows 11が“AIネイティブOS”へ進化</title>
      <link>https://ledge.ai/articles/windows11_copilot_vision_ai_pc_strategy</link>
      <description><![CDATA[<p>マイクロソフトは2025年10月16日（米国時間）、Windows 11における次世代AI機能の拡充方針を発表した。公式ブログ「<a href="https://blogs.windows.com/windowsexperience/2025/10/16/making-every-windows-11-pc-an-ai-pc/">Making every Windows 11 PC an AI PC</a>」で、すべてのWindows 11搭載端末を「AI PC」として再定義し、Copilotを中核に据えたAI体験をOS全体に統合していく構想を明らかにした。</p>
<h2>新しいウェイクワード「Hey Copilot」で話しかけるだけ</h2>
<p>@<a href="https://www.youtube.com/watch?v=7Nbf1fqxcCM">YouTube</a></p>
<p>同社は「AIはWindows体験の中心になる」として、音声・視覚・行動理解を備えた次世代のCopilot機能を順次展開する。音声による起動「Hey Copilot」、カメラや画像から状況を読み取る「Copilot Vision」、アプリ操作や設定変更などを文脈的に実行する「Copilot Actions」などが含まれるという。</p>
<p>たとえば、Copilot Visionは開いているグラフを要約したり、エラー画面を読み取って修正案を提示したりすることが可能。さらに音声起動「Hey Copilot」と組み合わせれば、「この表をPowerPointにまとめて」「この画像を説明して」といった自然な音声操作にも対応する。AIが“見て・聞いて・動く”ことで、ユーザーとのインタラクションがより直感的なものに変わる。</p>
<p>技術面では、NPU（Neural Processing Unit）を搭載した「AI PC」でオンデバイスAI処理を実行。これにより応答速度の向上とプライバシー保護を両立する。マイクロソフトは、AIをクラウド依存ではなくOSレベルに組み込む方針を明確にし、「AIネイティブOS」への移行を本格化させた。</p>
<p>同日公開された別の公式ブログ「<a href="https://blogs.windows.com/windowsexperience/2025/10/16/new-experiences-currently-rolling-out-for-windows-11">New experiences currently rolling out for Windows 11</a>」では、新しいCopilot体験の段階的な提供が開始されていることを明らかにした。ユーザーは自然言語でシステム設定やファイル操作、スクリーンショット分析、スケジュール調整などを行えるようになる。</p>
<p>さらに、開発者向けには外部サービスと連携できる「Copilot Extensions」を提供し、Microsoft Storeを通じてAI対応アプリの配信を拡充する方針も示された。</p>
<p>Windows 10のサポート終了（2025年10月）を目前に控え、マイクロソフトはWindows 11をAIネイティブOSとして再設計する姿勢を強調している。同社は「AIがPC体験そのものを再定義する」として、SurfaceシリーズやOEMメーカーと連携し、AI PC時代の普及を進めていく考えだ。</p>
]]></description>
      <pubDate>Sat, 18 Oct 2025 04:50:00 GMT</pubDate>
    </item>
  </channel>
</rss>