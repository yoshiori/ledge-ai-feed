<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
  <channel>
    <title>Ledge.ai 新着記事</title>
    <link>https://ledge.ai/</link>
    <description>Ledge.ai の最新テクノロジー記事</description>
    <item>
      <title>日本の「なぞなぞ」でAIの思考力をテスト──人間並みの正答率はGPT-5のみ。JAISTの研究チーム</title>
      <link>https://ledge.ai/articles/jaist_nazonazo_gpt5_benchmark</link>
      <description><![CDATA[<p>2025年9月18日、北陸先端科学技術大学院大学（JAIST）の研究チームは、日本の子ども向け「なぞなぞ（Nazonazo）」を活用し、大規模言語モデル（LLM）の洞察的推論能力を評価する新ベンチマークを開発したことを<a href="https://arxiv.org/abs/2509.14704">発表</a>した。実験の結果、GPT-5のみが人間に匹敵する正答率を示し、他のモデルは大きく下回った。</p>
<h2>飽和する既存ベンチマーク</h2>
<p>AIの能力を測る代表的なベンチマーク（MMLU、GSM8K、HumanEvalなど）は、最先端モデルが80〜90％の高スコアを記録するようになり、モデル間の性能差を明確に測りにくくなっている。OpenAI共同創業者のアンドレイ・カルパシー氏も「評価危機（evaluation crisis）」を指摘していた。</p>
<h2>日本の「なぞなぞ」はハイレベル？</h2>
<p>研究チームは、この「評価危機」を打開する手段として、日本の伝統的な言葉遊びである「なぞなぞ」を採用した。
なぞなぞは短文形式で低コストに新規作成が可能なうえ、専門知識を必要とせず、純粋な洞察力を試せる。また日本語特有の「漢字の分解」「語呂合わせ」「外来語表記」などにより、多様で難度の高い問題を作れる。</p>
<p>例として有名な「パンはパンでも食べられないパンは、なーんだ？」（答え：フライパン）が紹介されているほか、論文では「侍から“人偏”を取ると寺になる」という仕掛けのなぞなぞ（添付図参照）が示されている。</p>
<p><strong>漢字分解を利用したなぞなぞの例。「侍」から「人偏」を取ると「寺」となり、答えは「寺」</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Nazo_Nazo_Benchmar_7a284b9f17/Nazo_Nazo_Benchmar_7a284b9f17.jpg" alt="NazoNazo Benchmar.jpg" /></p>
<h2>英語 “リドル（riddle）” との差異</h2>
<p>英語圏では「RiddleSense」「BRAINTEASER」などのリドル系ベンチマークが存在するが、既に学習データに取り込まれており、GPT-4が98％超の精度で人間を上回るケースもある。
これに対し、日本語なぞなぞは人間でも平均正答率が52.9％にとどまり、AIモデルはさらに苦戦した。論文は「英語リドルはAIにとって容易になりすぎたが、日本語なぞなぞは汚染リスクが小さく、モデルの純粋な推論力を測るのに適している」と位置づけている。</p>
<h2>実験結果</h2>
<p>研究チームは38種類のLLM（GPT-4o、Claude、Gemini、Grok、Llama、DeepSeekなど）と成人126人を比較。</p>
<ul>
<li>人間の平均正答率は52.9％</li>
<li>GPT-5のみが人間平均と同等のスコアを記録</li>
<li>他のモデルは20〜30％台にとどまり、人間の半分程度にすぎなかった</li>
</ul>
<h2>AIが苦手な「最後のひと押し」</h2>
<p>多くのモデルは正解候補を途中で生成するものの、最終的に選べず「検証失敗」に陥るケースが頻発した。人間が持つ「Aha!（ひらめき）」や「これは正しい」という確信度がAIには弱く、洞察課題に特有の“最後のひと押し”が欠けていると指摘される。
また、モデルのパラメータ数の大きさと正答率には相関がなく、「推論型モデル」であることが成績の向上につながっていた。</p>
<h2>今後の展望</h2>
<p>論文は「GPT-5が例外的に人間並みの成績を示したが、他の最先端モデルは依然として人間に及ばない」と結論づけている。研究チームは、さらに難易度を高めた「Nazonazoベンチマーク2」の準備を進めており、今後はAIの“メタ認知的感覚”──正しいと感じる力──の強化が研究の焦点になる見通しだ。</p>
]]></description>
      <pubDate>Fri, 03 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google「Gemini for Home」を発表──Nest Cam刷新と新Google Homeスピーカーで “家庭のAI体験” を一新</title>
      <link>https://ledge.ai/articles/google_gemini_for_home_nest_cam_home_speaker</link>
      <description><![CDATA[<p>Googleは2025年10月1日（現地時間）、会話型AI「Gemini」を家庭向けに導入する新戦略「Gemini for Home」を<a href="https://blog.google/products/google-nest/gemini-for-home-launch/">発表</a>した。従来のGoogle Assistantに代わり、Geminiがスマートホームの中核を担う。あわせて「Nest Cam Indoor（有線・第3世代）」「Nest Cam Outdoor（有線・第2世代）」と新しい「Google Homeスピーカー」を公開し、Google Homeアプリのデザインも刷新した。Nest Camは順次各国で販売開始、Google Homeスピーカーは2026年春に発売予定とされる。</p>
<h2>Gemini for Homeとは</h2>
<p>「Gemini for Home」は、家庭内のデバイス横断でより自然な会話と文脈理解に基づく操作を可能にする。照明・センサー・カメラなどの制御に加え、家族ごとのパーソナライズや要約も担う。これによりGoogle Assistantはスマートスピーカー／ディスプレイ上で順次Geminiへ移行する。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Google_Home_Speaker_with_Light_R_width_1000_format_webp_c9825d2c87/Google_Home_Speaker_with_Light_R_width_1000_format_webp_c9825d2c87.webp" alt="Google_Home_Speaker_with_Light_R.width-1000.format-webp.webp" /></p>
<h2>新しいNestデバイス</h2>
<p>GoogleはGemini対応を前提とした新しいNestシリーズを発表した。</p>
<ul>
<li><strong>Nest Cam Indoor（第3世代・有線）／Nest Cam Outdoor（第2世代・有線）</strong> ：2K HDR、低照度性能、広視野などを強化。一部地域では発表当日から販売が始まっており、価格・展開国が案内されている。</li>
<li><strong>Google Homeスピーカー</strong> ：Gemini前提の会話体験と360度オーディオ、サラウンド対応をうたう。発売は2026年春予定（ティーザー段階）。</li>
</ul>
<p>@<a href="https://www.youtube.com/watch?v=xFNVratdz7w">YouTube</a></p>
<h2>Google Homeアプリの刷新</h2>
<p>同時にGoogle Homeアプリも再設計された。新デザインにより、家庭内のすべてのデバイスやカメラ映像を一元管理できる。特に注目されるのは「Gemini Live」で、ユーザーはアプリ内でGeminiと会話しながらデバイスを操作できる。カメラ映像の要約や通知整理など、AIを活用した新しい管理体験が提供される。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/GHA_Hero_Image_Social_width_1200_format_webp_5b687e9699/GHA_Hero_Image_Social_width_1200_format_webp_5b687e9699.webp" alt="GHA_Hero-Image_Social.width-1200.format-webp.webp" /></p>
<h2>今後の展望</h2>
<p>新しいNest製品やGoogle Homeスピーカーの提供開始時期は、各国で順次案内される予定だ。既存のNest製品への対応範囲やアップデート方針も注目点となる。Googleが家庭内AIの新標準としてGeminiを根付かせられるかが、今後の焦点となる。</p>
]]></description>
      <pubDate>Fri, 03 Oct 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ソフトバンクと理研、AI計算基盤と量子コンピュータを相互接続──SINETを介し2025年10月に開始</title>
      <link>https://ledge.ai/articles/softbank_riken_ai_quantum_connection</link>
      <description><![CDATA[<p>2025年9月29日、ソフトバンクと国立研究開発法人理化学研究所（以下、理研）は、学術情報ネットワーク「SINET（サイネット）」を活用し、ソフトバンクのAI計算基盤と理研が運用する量子コンピュータの相互接続を2025年10月に開始すると<a href="https://www.softbank.jp/corp/news/press/sbkk/2025/20250929_01/">発表</a>した。量子コンピュータと大規模AI計算基盤を直結させる国内初の試みであり、研究開発や産業応用における新たな可能性が広がると期待されている。</p>
<h2>相互接続の概要</h2>
<p>今回の取り組みでは、ソフトバンクが構築するAI計算基盤と、理研の計算科学研究センター（R-CCS）が運用する量子コンピュータを、SINETを経由して接続する。これにより、AIによる大規模データ処理と、量子コンピュータ特有の高速かつ効率的な計算を組み合わせた研究が可能になる。</p>
<h2>プロジェクト背景：「JHPC-Quantum」構想</h2>
<p>この相互接続は、日本政府が推進する「JHPC-Quantum（Japan High Performance Computing and Quantum）」プロジェクトの一環として実施される。スーパーコンピュータ「富岳」や先端的なAI計算基盤、量子コンピュータといった計算資源を連携させ、次世代の研究基盤を構築することが狙いだ。</p>
<h2>今後の展望</h2>
<p>両者は、今回の相互接続によって、AI学習と量子計算を融合させた新しい研究手法の実証を進める方針を示している。具体的には、材料科学や創薬、組合せ最適化といった産業応用への活用が想定されている。
ソフトバンクは自社のAIインフラを提供し、理研は量子計算の知見を活かすことで、学術と産業の双方に資する先端計算環境を構築していく考えだ。</p>
]]></description>
      <pubDate>Fri, 03 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Cloudflare、非営利団体と報道機関をAIクローラから防御──Project Galileoに新機能追加</title>
      <link>https://ledge.ai/articles/cloudflare_project_galileo_ai_crawler_protection</link>
      <description><![CDATA[<p>Cloudflareは米国時間2025年9月23日、非営利団体や独立系メディアを対象とするセキュリティ支援プログラム「Project Galileo」を拡張し、AIクローラからのアクセスを監視・制御できる新機能を無償提供すると<a href="https://www.cloudflare.com/press/press-releases/2025/cloudflare-helps-protect-independent-journalism-and-non-profits-from-ai/">発表</a>した。対象は約750の団体で、AI時代におけるコンテンツ保護と収益確保を後押しする狙いがある。</p>
<h2>Project Galileoとは</h2>
<p>Project Galileoは2014年に開始された取り組みで、独立系メディアや人権団体、ジャーナリストらに無償のセキュリティ対策を提供してきた。サイバー攻撃や情報操作から公共性の高い情報源を守ることを目的とし、これまでに数多くの組織を支援している。Cloudflareによれば、2024年5月から2025年3月の間に参加団体への攻撃を1,089億件以上遮断した実績があり、その規模の大きさが示されている。</p>
<h2>新たに追加された機能</h2>
<ul>
<li><strong>Bot Management</strong> ：AIクローラを含むボットを識別し、アクセス状況を可視化。</li>
<li><strong>AI Crawl Control</strong> ：どのAIクローラを許可するか、または拒否するかを団体ごとに制御可能。</li>
</ul>
<p>この拡張により、参加団体はAIサービスによる利用実態を把握し、必要に応じて制限をかけることができる。</p>
<p>Cloudflare共同創設者兼CEOのMatthew Prince氏は発表の中で、「私たちはジャーナリズムを信じている。独立系やローカルニュースの健全性は、インターネット全体、そして社会の健全性と不可分だ」と述べた。</p>
<p>また、メディア支援団体のLION（Local Independent Online News）やInternews Europeなどが、この拡張を歓迎し、持続可能なニュース配信に資するとの期待を表明している。</p>
<h2>今後の展望</h2>
<p>同社は、AIクローラの利用状況を把握し制御できるようにすることで、参加団体がコンテンツ利用に関して交渉力を高められると説明している。また、今回の拡張によって、独立系ニュースや非営利団体が持続可能な活動を続けられるよう支援し、インターネット上の公共性を守ることを目指すとしている。さらに、AI Crawl ControlやBot Managementを無償提供することで、AI時代における報道機関や市民団体の役割を強化し、透明で健全な情報流通を促進するとしている。</p>
]]></description>
      <pubDate>Fri, 03 Oct 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>イーロン・マスク、xAIで“Grokipedia”構築中　『Wikipediaを大幅に上回る』と表明</title>
      <link>https://ledge.ai/articles/elon_musk_xai_grokipedia_vs_wikipedia</link>
      <description><![CDATA[<p>イーロン・マスク氏は9月30日（現地時間）、自身のXアカウントで「<a href="https://x.com/elonmusk/status/1972992095859433671">xAIが“Grokipedia”を構築中</a>」と明かした。Wikipediaを「大幅に上回る改善」と表現し、xAIの目標である「宇宙の理解」に向けた必要な一歩だと位置づけた。</p>
<h2>マスク氏「Wikipediaを大幅に上回る」</h2>
<p>マスク氏は「We are building Grokipedia @xAI. Will be a massive improvement over Wikipedia. Frankly, it is a necessary step towards the xAI goal of understanding the Universe.」（私たちはxAIで「Grokipedia」を構築中です。これはWikipediaを大幅に上回る改善となるでしょう。率直に言って、xAIの目標である“宇宙の理解”に向けた必要な一歩なのです）と投稿。
この発言は、ベンチャー投資家デビッド・サックス氏が「Wikipediaは左派活動家によって偏向している」と批判した投稿に返信する形で行われた。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/grokipedia_5727bb7339/grokipedia_5727bb7339.jpg" alt="grokipedia.jpg" /></p>
<h2>背景にあるWikipedia偏向論争</h2>
<p>Wikipediaの編集方針や中立性をめぐる議論は長年続いてきた。直近ではタッカー・カールソン氏が共同創設者ラリー・サンガー氏を招いたインタビューを公開し、偏向の問題が再び注目を集めた。
マスク氏は今月のAll-In Podcastでも「GrokがWikipediaの誤りを正し、欠けている文脈を補う」と発言しており、今回の「Grokipedia」構想はその延長線上にあるとみられる。</p>
<h2>xAIの目標との関わり</h2>
<p>xAIの公式サイトには「宇宙の理解（Understand the Universe）」を掲げるミッションが明記されている。マスク氏は「Grokipedia」を、この壮大な目標の達成に不可欠な取り組みと強調した。</p>
<h2>「Grokipedia」名義のサイトも登場</h2>
<p>「grokipedia.com」や「grokipedia.fun」といったドメイン名を持つサイトもすでに存在している。しかし、これらは第三者が取得したものであり、xAI公式とは無関係とみられる。現時点で公式なサイトやプロジェクト詳細は公表されておらず、運用体制や公開時期については不明。</p>
<p>マスク氏の投稿は数百万回以上閲覧され、大きな関心を集めている。構想の詳細が明かされていない段階ながら、その動向に多くの注目が集まっている。</p>
]]></description>
      <pubDate>Thu, 02 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Microsoft 365 Copilotに「Agent Mode」「Office Agent」を導入 ～ “vibe working” でAIによるWord・Excelの自動化を推進</title>
      <link>https://ledge.ai/articles/microsoft365_agent_mode_office_agent_vibe_ai</link>
      <description><![CDATA[<p>Microsoftは2025年9月29日（米国時間）、同社の生成AI搭載ツール「Microsoft 365 Copilot」に、新機能「Agent Mode」および「Office Agent」を導入すると<a href="https://www.microsoft.com/en-us/microsoft-365/blog/2025/09/29/vibe-working-introducing-agent-mode-and-office-agent-in-microsoft-365-copilot/">発表</a>した。これらは「vibe working」と呼ばれる新しい作業体験を掲げ、WordやExcelでの文書作成・データ分析をAIが支援・自動化することを目的としている。</p>
<h2>Agent Mode：Officeアプリ内でのAI自動化</h2>
<p>Agent Modeは、WordやExcelなどのOfficeアプリケーションに組み込まれ、複数ステップにわたる作業をAIと対話しながら進められる機能。</p>
<p>Excelでは「Excel Labs」アドインを通じてプレビュー提供が開始され、数値の分析やグラフ化をAIに任せられる。Wordでは、文書の構成提案や修正作業をAIが継続的に補助する機能が実装され、まずはWeb版から展開される。</p>
<p>@<a href="https://youtu.be/nSqCy-7Qabk">YouTube</a></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Excel_benchmark_FINAL_7b6722975b/Excel_benchmark_FINAL_7b6722975b.webp" alt="Excel-benchmark-FINAL.webp" /></p>
<h2>Office Agent：Copilotチャットから文書やプレゼン生成</h2>
<p>Office Agentは、Copilotのチャット環境で稼働するエージェントで、Anthropicのモデルを搭載している。ユーザーが「レポートをまとめて」「会議資料を作成して」といった意図を伝えると、AIがWord文書やPowerPoint資料を生成・編集する。従来の単発的な応答にとどまらず、業務プロセス全体を遂行する“作業型エージェント”としての役割を担う。</p>
<p>@<a href="https://www.youtube.com/watch?v=NPSnD8-TZjY">YouTube</a></p>
<h2>“vibe working”のコンセプト</h2>
<p>Microsoftはこれらの新機能を総称して「vibe working」と表現している。簡潔な指示を入力するだけでAIが作業を補完し、文書作成やデータ分析の完成度を高めることを狙う。ユーザーはAIを相棒のように扱い、業務をより効率的に進められるという。</p>
<h2>提供条件と展開予定</h2>
<p>新機能は「Microsoft 365 Copilot」ライセンスを持つユーザーに順次展開される。Frontierプログラム参加者向けに先行提供されるケースもあり、初期段階では英語やWeb版が中心。今後は地域やアプリケーションの拡大が予定されている。</p>
]]></description>
      <pubDate>Thu, 02 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ChatGPTに「Instant Checkout」登場──ECサイトの商品をチャット内で直接購入可能に</title>
      <link>https://ledge.ai/articles/chatgpt_instant_checkout_launch</link>
      <description><![CDATA[<p>OpenAIは9月29日（現地時間）、ChatGPT内でECサイトの商品を直接購入できる新機能「Instant Checkout」を<a href="https://openai.com/index/buy-it-in-chatgpt/">発表</a>した。従来は外部リンクに移動していた購入手続きが、ChatGPT上で完結できるようになるという。</p>
<p>@<a href="https://www.youtube.com/watch?v=C6qcZdtIv54">YouTube</a></p>
<h2>機能概要</h2>
<p>「Instant Checkout」を利用すると、ChatGPTの会話画面内で商品購入が完結する。ユーザーはカート追加や外部ブラウザへの遷移を必要とせず、決済までをシームレスに行える。</p>
<p>同社ではChatGPTを「情報検索から購入までをワンストップで実現する場」へと進化させる方針を掲げている。今回の新機能により、生成AIがユーザーの消費行動に直結するハブとなることを狙う。</p>
<h2>仕組みと安全性</h2>
<p>提携するECサイトの商品情報はChatGPT内に直接表示され、購入操作が可能となる。決済処理はChatGPTのセキュアな仕組みを通じて行われ、ユーザーのプライバシーとセキュリティ確保が重視されている。</p>
<p><strong>「Instant Checkout」を支えるエージェント型コマースプロトコルの流れ</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/ACP_POR_1_47d3b99b8f/ACP_POR_1_47d3b99b8f.jpg" alt="ACP_-POR__1.jpg" /></p>
<h2>今後の展開</h2>
<p>OpenAIは、対応するECサイトの拡大を予定している。まずは一部のパートナーから導入を開始し、順次拡大する方針だ。また、開発者や事業者に向けた連携方法の提供も検討されている。</p>
]]></description>
      <pubDate>Thu, 02 Oct 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Anthropic、次世代 AI コーディングモデル『Claude Sonnet 4.5』を発表 — 30時間集中モードが復活、強化されたエージェント性能</title>
      <link>https://ledge.ai/articles/claude_sonnet_4-5_ai_coding_model</link>
      <description><![CDATA[<p>Anthropicは2025年9月30日、最新の大規模言語モデル「Claude Sonnet 4.5」を<a href="https://www.anthropic.com/news/claude-sonnet-4-5">発表</a>した。同社は「世界最高のコーディングモデル」と位置づけ、複雑なエージェントの構築やコンピュータ操作能力で大幅な性能向上を示したと説明している。内部テストでは30時間を超える自律的な作業継続が確認され、推論や数学のベンチマークでも著しい改善が見られた。</p>
<h2>コーディング性能の飛躍</h2>
<p>Sonnet 4.5は、ソフトウェアエンジニアリングのベンチマーク「SWE-bench Verified」で77.2%の正答率を記録し、並列計算を用いた高負荷環境では82.0%に達した。従来モデルのClaude Sonnet 4（72.7%）や競合のGPT-5（72.8%）を大きく上回っている。</p>
<p><strong>ソフトウェアエンジニアリングにおけるベンチマーク「SWE-bench Verified」での比較。Sonnet 4.5が最も高い精度を示した。</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/6421e7049ff8b2c4591497ec92dc4157b2ac1b30_3840x2160_13a7b3e290/6421e7049ff8b2c4591497ec92dc4157b2ac1b30_3840x2160_13a7b3e290.webp" alt="6421e7049ff8b2c4591497ec92dc4157b2ac1b30-3840x2160.webp" /></p>
<h2>幅広い領域での強化</h2>
<p>Claude Sonnet 4.5は、コーディングだけでなく、コンピュータ操作や数学、言語理解など幅広い分野で性能を向上させた。特にOSWorldベンチマークでは61.4%を達成し、前世代の42.2%から大幅に改善。金融や法務、医療、STEMといった専門分野でも、専門家による評価で大きな知識・推論力の進歩が確認されている。</p>
<p><strong>主要ベンチマークでの各モデル比較。Claude Sonnet 4.5は複数分野でトップクラスの性能を示した。</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/67081be1ea2752e2a554e49a6aab2731b265d11b_2600x2288_30c09323a7/67081be1ea2752e2a554e49a6aab2731b265d11b_2600x2288_30c09323a7.webp" alt="67081be1ea2752e2a554e49a6aab2731b265d11b-2600x2288.webp" /></p>
<h2>エージェント開発と新機能</h2>
<p>さらに同社は今回「Claude Agent SDK」を公開。長期タスクのメモリ管理、ユーザー権限の制御、複数エージェントの協調といった仕組みを開発者に提供する。さらに以下の新機能が追加された：</p>
<ul>
<li>Claude Codeへのチェックポイント機能</li>
<li>VS Code拡張と刷新されたターミナル</li>
<li>Claudeアプリでのコード実行・ファイル生成機能</li>
<li>Chrome拡張の一般公開（Maxユーザー向け）</li>
</ul>
<p>これにより、開発者は独自のエージェントやツールを構築できる環境が整備された。</p>
<p>@<a href="https://youtu.be/OZ-aLrJ0oVg">YouTube</a></p>
<h2>安全性とアラインメントの改善</h2>
<p>Sonnet 4.5はAnthropic史上「最もアラインメントが取れた」モデルとして公開された。危険な挙動（虚偽、迎合、権力志向、妄想助長など）を大幅に低減。AI Safety Level 3（ASL-3）の保護レベルで運用され、特に化学・生物・放射線・核（CBRN）分野に関連するリスク低減が強化されている。</p>
<p><strong>各モデルにおける「ミスアラインメント行動スコア」。Sonnet 4.5は最も低い数値を示し、安全性が向上している。</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/33efc283321feeff94dd80973dbcd38409806cf5_3840x2160_f4be449515/33efc283321feeff94dd80973dbcd38409806cf5_3840x2160_f4be449515.webp" alt="33efc283321feeff94dd80973dbcd38409806cf5-3840x2160.webp" /></p>
<h2>提供条件と研究プレビュー</h2>
<p>Claude Sonnet 4.5は即日利用可能で、価格は従来のSonnet 4と同じく入力100万トークンあたり3ドル、出力100万トークンあたり15ドル。
また研究プレビューとして「Imagine with Claude」も公開され、期間限定でリアルタイムにコードを生成するデモを体験できる。</p>
<p>@<a href="https://youtu.be/dGiqrsv530Y">YouTube</a></p>
<p>Anthropicは「Claude Sonnet 4.5は、より安全で強力なフロンティアモデルであり、開発者やビジネスユーザーにとって即戦力となる」としており、今後のAIエージェント活用の加速が期待される。</p>
]]></description>
      <pubDate>Wed, 01 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>日立、「NVIDIA AI Factory」 活用の集約型インフラ「AI Factory」を構築　Physical AI 開発を加速</title>
      <link>https://ledge.ai/articles/hitachi_ai_factory_physical_ai_acceleration</link>
      <description><![CDATA[<p>日立製作所は9月26日、NVIDIAのAIプラットフォーム「NVIDIA AI Factory」のリファレンスアーキテクチャを活用し、集約型インフラ「AI Factory」を構築したと<a href="https://www.hitachi.co.jp/New/cnews/month/2025/09/0926b.html">発表</a>した。この基盤は、同社が注力するPhysical AIソリューションの開発・導入を加速することを目的としている。</p>
<h2>OTとNVIDIA技術を融合した新基盤</h2>
<p>「AI Factory」は、日立が持つOT（制御・運用技術）分野の知見と、NVIDIAのアクセラレーテッドコンピューティングおよびAIソフトウェアスタックを融合させて設計された。現実世界と相互作用するAIを迅速に開発し、社会実装につなげる狙いだ。活用分野としては、モビリティ、エネルギー、産業、テクノロジーなどが想定されている。</p>
<h2>高性能GPUとネットワークによる構成</h2>
<p>システム構成は、NVIDIA Blackwell GPUを搭載した「HGX B200」システムを採用した「Hitachi iQ」、NVIDIA RTX PRO 6000 Server Edition GPUを搭載した「Hitachi iQ M Series」、そしてネットワーク基盤となる「NVIDIA Spectrum-X Ethernet」で構成される。またソフトウェアには、エンタープライズ向けの「NVIDIA AI Enterprise」や、産業規模の正確なデジタルツインを構築できる「NVIDIA Omniverse」が含まれる。</p>
<h2>Physical AIの開発を加速</h2>
<p>このインフラを用いることで、カメラやセンサーから得た情報を認識・推論し、次の行動を決定・実行する高度なPhysical AIモデルを迅速に開発・学習できる。デジタルツインの構築や、社会インフラを含む物理アセットの最適化を支援し、生産性向上など新たな可能性を拓くという。</p>
<h2>グローバル展開と共同開発体制</h2>
<p>「AI Factory」は米国、EMEA（欧州・中東・アフリカ）、日本に戦略的に配置され、相互接続されたネットワークによってグローバルなエンジニアチームが低遅延で協力できる環境を整える。これにより、多様なPhysical AIソリューションの共同開発を促進し、効率性・生産性・安全性の向上を実現する。</p>
<h2>経営陣とNVIDIAのコメント</h2>
<p>日立製作所 執行役社長兼CEOの德永俊昭氏は「NVIDIA RTX PROサーバーを基盤としたHitachi iQの活用により、AI推論やPhysical AIが高速化され、デジタルツインや社会インフラの最適化が強化される」とコメントした。また、執行役副社長兼デジタルシステム＆サービス統括本部長の阿部淳氏は「グローバルにAI Factoryを構築することで、『真のOne Hitachi』として運営が可能になり、Physical AIのイノベーションを加速させる」と述べた。</p>
<p>さらに、NVIDIAのEnterprise AI Products担当バイスプレジデントであるJustin Boitano氏も「企業データをソフトウェアと物理世界の両方で活用する革新的基盤となる」と評価している。</p>
<h2>Lumada 3.0への布石</h2>
<p>今回の取り組みは、日立が掲げる「Lumada 3.0」ビジョン実現に向けた重要な一歩と位置付けられている。IT、OT、プロダクトを融合したインダストリアルAI分野でのソリューション提供強化を通じ、社会とビジネスのイノベーションを推進する構えだ。</p>
]]></description>
      <pubDate>Wed, 01 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>AIロボット協会、経産省・NEDO事業に採択──ロボティクス分野の生成AI基盤モデル開発を推進</title>
      <link>https://ledge.ai/articles/airoa_post5g_robotics_ai_platform</link>
      <description><![CDATA[<p>一般社団法人AIロボット協会（AI Robot Association、以下AIRoA）は、経済産業省とNEDO（新エネルギー・産業技術総合開発機構）が公募した「ポスト5G情報通信システム基盤強化研究開発事業」において、ロボティクス分野の生成AI基盤モデルの開発に向けたデータプラットフォーム構築の採択事業者に決定したと<a href="https://www.airoa.org/ja/updates/20250926">発表</a>した。</p>
<h2>採択事業の概要</h2>
<p>AIRoAが採択されたのは、「ロボティクス分野の生成AI基盤モデルの開発に向けたデータプラットフォーム」に関する研究開発で、ポスト5G時代に対応した情報通信システムの基盤強化を目的とし、経済産業省およびNEDOが推進している。</p>
<h2>事業の目的</h2>
<p>ロボティクス分野においては、生成AIを活用した基盤モデルの開発が期待されている。AIモデルに学習させるための多様なデータの収集と共有を可能にすることで、ロボットの高度な知能化と産業競争力の強化につなげる狙いがある。</p>
<h2>AIRoAの役割</h2>
<p>AIRoAは、事業の中心的な実施主体として、参画する企業や研究機関とともにロボティクス分野のデータプラットフォームを整備する。これにより、研究者や開発者が利用できる共通基盤を形成し、効率的な技術開発を後押しする。</p>
<h2>今後の展望</h2>
<p>今回の採択を通じ、AIRoAは生成AIを活用したロボット開発と社会実装を加速させる方針だ。協会は公式発表の中で、「産業界や研究機関と連携し、日本から世界に通用するAIロボティクスの基盤を築いていく」と述べている。</p>
]]></description>
      <pubDate>Wed, 01 Oct 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>学術＆研究2025/9/30 [TUE]AIは専門家にどこまで迫ったか──OpenAI「GDPval」が検証する “現実の仕事力”</title>
      <link>https://ledge.ai/articles/openai_gdpval_ai_job_benchmark</link>
      <description><![CDATA[<p>OpenAIは2025年9月25日、最新の大規模言語モデル（LLM）が「現実の経済価値を持つタスク」でどの程度人間に迫っているかを測定する新しい評価指標「GDPval」を<a href="https://openai.com/index/gdpval/">発表</a>した。実際の業務成果物をもとに44職種・1,320件のタスクで性能を比較した結果、最先端モデルは専門家に近い水準に達していることが示された。</p>
<p>GDPvalは、米国GDPに寄与する9産業・44職種を対象に設計された新しい評価ベンチマークである。法律文書や設計図、動画編集、カスタマーサポートの対応記録など、実際の成果物をタスク化し、AIモデルと業界専門家の成果を比較する。</p>
<p><strong>「GDPval」に含まれる実務タスク例。設計図や看護報告、財務分析からカスタマーサポートまで幅広い領域をカバーしている。</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Example_GD_Pval_tasks_from_full_set_1f598d2776/Example_GD_Pval_tasks_from_full_set_1f598d2776.jpg" alt="Example GDPval tasks from full set.jpg" /></p>
<h2>幅広い産業と職種をカバー</h2>
<p>評価対象は、不動産、製造、政府、金融、医療、情報サービスなど主要な産業を網羅。ソフトウェア開発者や弁護士、看護師、金融アナリストなど、幅広い知識労働が調査された。</p>
<p><strong>「GDPval」で評価対象となった9産業・44職種。米国経済に大きく寄与する分野から抽出されている。</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/GD_Pval_includes_real_world_work_from_44_occupations_fc303d69cd/GD_Pval_includes_real_world_work_from_44_occupations_fc303d69cd.jpg" alt="GDPval includes real-world work from 44 occupations.jpg" /></p>
<h2>厳格なレビューと評価方法</h2>
<p>各タスクは平均5回の専門家レビューを経て整備され、最終的に人間とAIの成果物をブラインドで比較。220件のタスクは「ゴールドサブセット」として公開され、自動採点サービスも提供されている。</p>
<p><strong>タスクは複数段階の専門家レビューを経て現実性と品質を担保している。</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Tasks_undergo_multiple_rounds_of_review_to_ensure_realism_and_quality_7fb88c6b59/Tasks_undergo_multiple_rounds_of_review_to_ensure_realism_and_quality_7fb88c6b59.jpg" alt="Tasks undergo multiple rounds of review to ensure realism and quality.jpg" /></p>
<p><strong>人間専門家によるペアワイズ比較。AIの成果物と人間の成果物を並べ、どちらが優れているかを評価する。</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/GD_Pval_uses_pairwise_expert_comparisons_for_grading_69709621f2/GD_Pval_uses_pairwise_expert_comparisons_for_grading_69709621f2.jpg" alt="GDPval uses pairwise expert comparisons for grading.jpg" /></p>
<h2>成果と課題</h2>
<p>評価の結果、Claude Opus 4.1は文書やプレゼン資料のレイアウトで優れ、GPT-5は指示理解や計算精度に強みを示した。一方で、モデルの失敗要因として最も多かったのは「指示を正しく理解できていない場合」であると報告されている。</p>
<p>また、推論時間を増やす、プロンプトを工夫するなどの対応により性能はさらに改善可能であることも確認された。</p>
<h2>今後の展望</h2>
<p>現時点のGDPvalは知識労働に限定されるが、将来的には対話型や現場対応を含むタスクへ拡張する計画だ。OpenAIは220件のタスクを公開し、研究者コミュニティによる継続的な評価を促進することで、AIと人間の協働のあり方を探る。</p>
]]></description>
      <pubDate>Tue, 30 Sep 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>九州電力・IIJなど5社、「九州版ワット・ビット連携」実証──APN＋光NICで分散DCを直結</title>
      <link>https://ledge.ai/articles/kyuden_iij_qtnet_1finity_nautilus_wattbit_apn_opticalnic</link>
      <description><![CDATA[<p>九州電力、インターネットイニシアティブ（IIJ）、QTnet、1FINITY、ノーチラス・テクノロジーズの5社は2025年9月24日、地域分散型デジタルインフラを構築・検証する実証プロジェクトを開始すると<a href="https://www.kyuden.co.jp/press/2025/h250924-1.html">発表</a>した。プロジェクトは2025年10月から始まり、九州における「九州版ワット・ビット連携」の実現を目指す。（実証期間は2025年10月〜2026年3月）</p>
<h2>「ワット・ビット連携」に基づく取り組み</h2>
<p>政府が推進する「ワット・ビット連携」の考え方に基づき、九州の再生可能エネルギーを活用しつつ、地域に分散したデータセンター（DC）を連携させる。これにより、電力とIT処理の最適なバランスを図り、持続可能なデジタル基盤の実装を目指す。</p>
<h2>世界初、光NICによる分散DCの直結</h2>
<p>同プロジェクトの特徴は、APN（All-Photonics Network）を利用して、複数の小規模DCを光信号のまま接続し、一つのシステムのように機能させる点にある。
さらに、ネットワークには従来の電気信号ではなく、光信号を直接扱う光ネットワークインタフェースカード（光NIC）を導入。これにより、サーバ間を光信号で直接接続し、従来必要だった伝送装置や電気的変換を省略する。1FINITYの調査によれば、分散DCを光NICで直結する取り組みは世界初（2025年9月24日現在、1Finity調べ）となる。</p>
<h2>AI処理と分散DBの検証</h2>
<p>各DCにはAI処理向けのGPUサーバを配置。複数のDCに分散保存されたデータへアクセスし、AI処理を行うための分散データベース技術の検証も進める。これにより、地域ごとに異なる電力供給状況を踏まえ、昼夜や需給状況に応じて柔軟に処理を振り分ける仕組みを模索する。光信号の直結によりネットワーク装置の削減と省電力化も図る。</p>
<h2>今後の展望</h2>
<p>5社は、実証を通じて得られた技術的成果をもとに、再生可能エネルギーとデジタルインフラを組み合わせた「九州版ワット・ビット連携」のモデルケースを確立したい考えだ。</p>
]]></description>
      <pubDate>Tue, 30 Sep 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Dify協会を設立──LangGenius、NTTデータ、日本電子計算がAIアプリ普及を推進</title>
      <link>https://ledge.ai/articles/dify_association_established_langgenius_nttdata_jip</link>
      <description><![CDATA[<p>LangGeniusは2025年9月24日、NTTデータ、日本電子計算と共同で、2025年9月1日付で「一般社団法人Dify協会」を設立したと<a href="https://prtimes.jp/main/html/rd/p/000000010.000166429.html">発表</a>した。AIアプリケーション開発プラットフォーム「Dify」を中核に据え、オープンなエコシステムを形成し、日本におけるAI活用の普及を後押しする。</p>
<h2>設立の背景</h2>
<p>生成AIの社会実装が加速するなか、アプリケーションの導入や運用をめぐる課題は多岐にわたる。協会は、開発者や企業がAIを安心して活用できる環境を整備し、技術標準化やガイドライン策定を通じて信頼性の高いエコシステムを目指す。</p>
<h2>協会の目的と活動内容</h2>
<ul>
<li>協会は以下の活動を推進する。</li>
<li>会員間でのノウハウ・事例共有</li>
<li>技術標準や認証制度の検討</li>
<li>勉強会やイベントを通じた人材・企業間のネットワーキング</li>
<li>学術界・産業界・行政との連携支援</li>
</ul>
<p>設立時社員は、LangGenius、NTTデータ、日本電子計算の3社。
役員には、亀茲マルダン氏、新井貴博氏、湯澤元彦氏が理事として名を連ね、監事には戸田邦昭氏が就任した。主たる事務所は東京都中央区に置かれる。</p>
<h2>今後の展開</h2>
<p>協会は2025年10月24日に東京で開催されるイベント「IF Con Tokyo 2025」で活動方針を発表し、翌25日には開発者向け「Dify Studio ハッカソン」を実施する予定。オープンかつ中立的な場を提供することで、幅広いステークホルダーの参加を促し、日本のAIアプリ市場の成長を支えていく構えだ。</p>
]]></description>
      <pubDate>Tue, 30 Sep 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/9/29 [MON]ChatGPTに「共有プロジェクト」モード導入──OpenAI、セキュリティ強化や外部アプリ連携も拡充</title>
      <link>https://ledge.ai/articles/chatgpt_shared_projects_security_update</link>
      <description><![CDATA[<p>OpenAIは2025年9月25日（現地時間）、ChatGPTにおいて、チームでの共同作業やセキュリティ機能を強化する新機能を<a href="https://openai.com/index/more-ways-to-work-with-your-team/">発表</a>した。新たに「共有プロジェクト」モードを導入するほか、外部アプリとの接続機能やエンタープライズ向けの管理機能を拡充している。</p>
<h2>共有プロジェクトモードで共同作業を効率化</h2>
<p>今回のアップデートの中心となるのが「共有プロジェクト」モードだ。これにより、同じワークスペース内のメンバーがチャット、ファイル、カスタムインストラクションを一元的に共有できる。権限は「Chat」と「Edit」の2種類があり、後者ではファイル追加やメンバー招待などが可能となる。さらにプロジェクト専用のメモリが搭載され、長期的なタスクでも文脈を保持したまま進行できるという。</p>
<p>同機能はChatGPT Business、Enterprise、Eduプランに即日提供され、Free／Go／Plus／Proプランには近日中に展開予定。EnterpriseとEduでは既定でオフとなっており、管理者が制御できるとのこと。</p>
<h2>サードパーティアプリとの接続拡大</h2>
<p>OpenAIはまた、外部アプリとChatGPTを直接つなぐ「コネクタ」機能を強化した。現在はGmail、Google Calendar、Microsoft Outlook、Microsoft Teams、SharePoint、GitHub、Dropbox、Boxなどに対応し、情報の取り込みや操作をChatGPTから直接実行できる。</p>
<p>コネクタは用途に応じて自動的に選択され、回答の速度と正確性が向上。さらに、GitHubやSharePointなどでは同期型コネクタによる事前取り込みにも対応する。既存のアクセス権限は尊重され、Business以上のプランではデータが学習に利用されない。EnterpriseとEduでは既定でオフとなり、管理者が利用可否を制御できる。</p>
<p><strong>ChatGPTが対応する外部アプリの例（Google Drive、Gmail、GitHub、Notionなど）。今後さらに拡大予定</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Connector_logo_wall_16x9_96f5c1f71e/Connector_logo_wall_16x9_96f5c1f71e.webp" alt="Connector_logo_wall_16x9.webp" /></p>
<h2>セキュリティとコンプライアンスを強化</h2>
<p>エンタープライズ利用を想定し、セキュリティ・コンプライアンス機能も拡充された。新たにISO/IEC 27001、27017、27018、27701の認証を取得したほか、SOC 2の適用範囲をSecurity／Confidentiality／Availability／Privacyに拡大。</p>
<p>また、RBAC（カスタムロール・グループ権限管理）に対応し、アクセス制御を柔軟に設定可能となった。シングルサインオン（SSO）は既存のSAMLに加え、OIDCにも対応。さらにEnterpriseとEduではIP許可リストの設定も可能となり、組織全体での安全性が強化されている。</p>
<h2>今後の展開</h2>
<p>新機能はChatGPTにおけるチーム利用を本格化させる第一歩と位置づけられている。OpenAIはブログで「本リリースは、ChatGPTにおけるチームコラボレーションの初期ステップ」と述べ、今後も職場全体での安全な導入を支えるため、機能拡張を続けていく考えを示した。</p>
]]></description>
      <pubDate>Mon, 29 Sep 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>DeepSeek R1の詳細がNatureに掲載、初の査読付き著名LLMに──開発コストと学習手法を初公開、トレーニング費用はわずか30万ドル</title>
      <link>https://ledge.ai/articles/deepseek_r1_nature_training_cost_300k</link>
      <description><![CDATA[<p>科学誌Natureは2025年9月17日、中国の深究科技（DeepSeek）が開発した推論特化モデルDeepSeek R1を取り上げ、同モデルの開発費用が約30万ドルにとどまったことを<a href="https://www.nature.com/articles/d41586-025-03015-6">報じた</a>。同日にNature誌に掲載された論文では、推論力を「純粋強化学習（RL）」で引き出す独自の訓練パイプラインと、その性能評価の詳細が示された。</p>
<h2>30万ドルの低コスト開発、初の査読付きLLMに</h2>
<p>Natureによると、DeepSeek R1の開発に必要だったトレーニング費用は約30万ドルであった。他の大規模AIモデル開発が数億ドル規模に及ぶことと比べ、低コストである点が指摘された。</p>
<p>同誌はまた、DeepSeek R1の論文が「査読を経て掲載された初の著名大規模言語モデル（LLM）研究」であると伝えている。従来のLLM研究の多くはプレプリント段階にとどまっていたが、今回の掲載により、学術誌の査読を通過した形で研究成果が公式に公開された。</p>
<h2>学習パイプラインの詳細</h2>
<p>論文によれば、DeepSeek R1は以下の三段階で訓練された。</p>
<ul>
<li><strong>拒否サンプリング</strong> ：初期データから推論過程を一定基準で選別。</li>
<li><strong>強化学習（RL）</strong> ：正答率や形式に基づくルールベース報酬を導入し、推論力を強化。</li>
<li><strong>教師あり微調整（SFT）</strong> ：非推論データも含め、応答品質を人間の嗜好に整合させる。</li>
</ul>
<p>この設計により、前段階モデル「R1-Zero」で獲得した推論行動を維持しながら、言語一貫性や応答の明瞭性を改善した。</p>
<h2>技術的成果</h2>
<p>論文では、DeepSeek R1が複数のベンチマークで高い性能を記録したことが報告されている。数学コンテスト（AIME 2024）では、人間参加者の平均を大幅に上回る精度を達成。また、プログラミング課題（Codeforces、LiveCodeBenchなど）でも既存モデルを上回る性能を示した。さらにSTEM分野の高度問題に対しても、推論力の改善が確認されたという。</p>
<p>訓練過程で「思考時間」が自然に延び、応答が長文化しつつ自己反省や再検証といった高度な推論行動が自律的に現れたことが示されたとしている。</p>
<h2>公開リソースと安全性</h2>
<p>研究チームは、DeepSeek R1本体や蒸留モデルを公開しており、研究者が再利用できる形を整備している。一方で、Natureニュース記事は、強化された推論力が有害応答を精緻化するリスクに触れている。論文でも、ツール利用未対応や言語混在などの課題が指摘され、安全性評価の詳細が補足資料に掲載されている。</p>
<p>論文の著者らは、「複雑な推論力を引き出すために必要なのは、人手による大規模な注釈ではなく、難度の高い課題、信頼できる検証機構、そして十分な計算資源である」と記している。また、強化学習の過程で自己検証や内省といった高度な推論行動が自律的に現れたことを示し、純粋強化学習が大規模言語モデルの推論能力を促進する有力な手段になり得ると結論づけた。</p>
]]></description>
      <pubDate>Mon, 29 Sep 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>学術＆研究2025/9/29 [MON]Google、ロボット操作向けAI基盤モデル「Gemini Robotics 1.5」を発表──行動前に思考し、複雑タスクを実行</title>
      <link>https://ledge.ai/articles/google_gemini_robotics_15_release</link>
      <description><![CDATA[<p>Googleは2025年9月25日、ロボットの操作向けに新たなAIモデル「Gemini Robotics 1.5」を<a href="https://blog.google/intl/ja-jp/company-news/technology/gemini-robotics-15-ai">発表</a>した。Gemini 1.5 Proを基盤としたこのモデルは、ロボットが「行動を起こす前に考える」能力を備えており、従来よりも複雑なマルチステップの作業を遂行できる点が特徴だという。</p>
<h2>新モデル「Gemini Robotics 1.5」とは</h2>
<p>Googleが発表した「Gemini Robotics 1.5」は、同社の大規模言語モデル「Gemini 1.5 Pro」を拡張し、視覚・言語・行動を統合した「Vision-Language-Action（VLA）」モデルとして設計されている。ロボットは環境を理解し、人間の指示に基づいて複数ステップにわたるタスクをこなせるようになった。</p>
<p>@<a href="https://www.youtube.com/watch?v=AMRxbIO04kQ&amp;t=1s">YouTube</a></p>
<h2>思考してから行動する仕組み</h2>
<p>従来のロボティクスAIは与えられた動作を逐次実行することが多かったが、「Gemini Robotics 1.5」は行動前に計画を立てる能力を持つ。これにより、作業の失敗を減らし効率的に遂行できる。たとえば散らかった部屋で物を拾う場合、ロボットは最適な順序や手順を考えてから実行に移す。</p>
<p><strong>「Gemini Robotics 1.5」の仕組み。ロボットはゴミの分別を例に、行動前に思考・計画を立てるプロセスを経て動作する</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Gemini_Robotics_Agentic_System_width_1000_format_webp_75fe64eb0b/Gemini_Robotics_Agentic_System_width_1000_format_webp_75fe64eb0b.jpg" alt="GeminiRobotics-Agentic_System.width-1000.format-webp.jpg" /></p>
<p>@<a href="https://www.youtube.com/watch?v=eDyXEh8XqjM&amp;t=3s">YouTube</a></p>
<h2>応用例と成果</h2>
<p>公開されたデモでは、洗濯物の分類やごみ分別など、家庭やオフィスで想定される作業を自律的に行う様子が披露された。ロボットは周囲を観察しながら判断を下し、マルチステップのタスクを適切に処理することができる。</p>
<p><strong>Gemini Robotics-ER 1.5」は従来モデルよりも学術ベンチマークで高い性能を示した</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Gemini_Robotics_blog_figure_2_250_width_1000_format_webp_a4b168e1a6/Gemini_Robotics_blog_figure_2_250_width_1000_format_webp_a4b168e1a6.jpg" alt="GeminiRobotics-blog-figure-2-250.width-1000.format-webp.jpg" /></p>
<h2>研究開発の背景</h2>
<p>このモデルは、Google DeepMindとGoogle Researchの共同開発による成果だ。Geminiシリーズの能力を物理世界に応用する取り組みの一環であり、従来のロボットAIよりも抽象的な推論や柔軟な対応が可能になった点が強調されている。</p>
<p><strong>物体検出や軌道予測など、多様な認識・推論能力を備える「Gemini Robotics 1.5」</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Screenshot_2025_09_25_at_14_53_4_width_1000_format_webp_c5fae86c2b/Screenshot_2025_09_25_at_14_53_4_width_1000_format_webp_c5fae86c2b.webp" alt="Screenshot_2025-09-25_at_14.53.4.width-1000.format-webp.webp" /></p>
<h2>今後の展望</h2>
<p>Googleは「Gemini Robotics 1.5」を、物理世界における汎用人工知能（AGI）の実現に向けた重要な一歩と位置づけている。単にコマンドに反応するモデルではなく、自ら推論し、計画を立て、ツールを使いこなし、未知の状況にも対応できる自律的なシステムの構築を目指す。</p>
<p>ロボットが知性と器用さを兼ね備え、物理世界の複雑さを乗り越えて人間の生活により役立つ存在となるための基盤づくりであるとし、Googleは研究コミュニティとの協力を継続する方針を示している。また、ロボティクス分野の研究者らが最新の「Gemini Robotics-ER」モデルを活用し、どのような未来を切り拓くのか大いに期待していると結んでいる。</p>
]]></description>
      <pubDate>Mon, 29 Sep 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google、生成AIで教科書を再構築する「Learn Your Way」を公開──無作為化比較試験で学習成果の向上を確認</title>
      <link>https://ledge.ai/articles/google_learn_your_way_ai_textbook</link>
      <description><![CDATA[<p>Googleは2025年9月16日、生成AIを活用して教科書を再構築する新システム「Learn Your Way」を<a href="https://research.google/blog/learn-your-way-reimagining-textbooks-with-generative-ai/">発表</a>した。学習者の学年や関心に合わせて教材をリライトし、音声・スライド・マインドマップなど多様な形式で提示する仕組みを採用。シカゴ地域の高校生を対象とした実証実験では、従来型の教材よりも学習成果が有意に向上することが確認されたという。</p>
<p>従来の教科書は「すべての学習者に一律の内容を提供する媒体」であり、学年や興味関心に応じた最適化は難しかった。Google Researchのチームは、生成AIを用いることでこの制約を克服し、個別化と多様な表現を可能にするアプローチを開発した。</p>
<h2>Learn Your Wayの仕組み</h2>
<p>新システム「Learn Your Way」は、Googleの学習用大規模モデル「LearnLM」（Gemini 2.5 Proに統合）を基盤とする。教材の元テキストを入力し、以下のステップで再構築する。</p>
<ul>
<li><strong>パーソナライズ</strong> ：学年レベル（Flesch-Kincaid指標に基づく）や興味関心（スポーツ、音楽など）に合わせて文章をリライト。</li>
<li><strong>複数表現への変換</strong> ：没入型テキスト、スライド＋ナレーション、オーディオレッスン、マインドマップなど。</li>
<li><strong>学習支援機能</strong> ：埋め込み質問や小テストを生成し、学習者に即時のフィードバックを提供。</li>
</ul>
<p>これにより、学習者は自分に合った形式を選びながら、能動的に学習を進めることができるという。</p>
<p><strong>ニュートンの第三法則を学年や関心に応じてリライトした例。左は高校生向けにバスケットボールを題材に、右は小学生向けに美術を題材に説明している。</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x2_79d6efaf7b/x2_79d6efaf7b.jpg" alt="x2.jpg" /></p>
<h2>実証実験（無作為化比較試験）</h2>
<p>研究チームは、シカゴ地域の15〜18歳の学生60名を対象に無作為化比較試験（RCT）を実施した。教材は「思春期の脳発達」（LibreTexts）を使用。
<strong>■ 比較対象</strong> ：
・「Learn Your Way」利用群（30名）
・従来のPDFリーダー（Adobe Acrobat）利用群（30名）
<strong>■ 学習時間</strong> ：20〜40分
<strong>■ 評価方法</strong> ：
・即時テスト（15分）
・3日後の保持テスト（約10分）</p>
<h2>結果</h2>
<ul>
<li>即時テスト：Learn Your Way群が有意に高得点（p=0.03）。</li>
<li>保持テスト：Learn Your Way群が有意に高得点（p=0.03）。</li>
<li>アンケート：Learn Your Way群は「楽しく学べた」「理解が深まった」と回答する割合が高く、将来の利用意欲も強かった。</li>
</ul>
<p><strong>無作為化比較試験の結果</strong> ：「Learn Your Way」を使用した学生は、従来のデジタルリーダー利用者に比べ、即時テスト・保持テストのいずれも有意に高い得点を示した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Learn_Your_Way_5_width_1250_458379e0a2/Learn_Your_Way_5_width_1250_458379e0a2.png" alt="Learn-Your-Way-5.width-1250.png" /></p>
<h2>教育現場での意義</h2>
<p>この研究は、生成AIが教科書を「一律の媒体」から「学習者に応じて変化する教材」へと変革し得ることを示している。Googleは、教育科学の知見とAI技術を組み合わせることで、学習成果の向上と学習者の主体性強化につなげられると強調している。</p>
<p><strong>教育専門家による評価結果。特にナレーション付きスライドやオーディオレッスンは、学習者の関心を引きやすいと高評価を得た。</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Learn_Your_Way_4_width_1250_8bf77aa53c/Learn_Your_Way_4_width_1250_8bf77aa53c.png" alt="Learn-Your-Way-4.width-1250.png" /></p>
<h2>今後の展望</h2>
<p>研究チームは論文内で次の課題を指摘している。</p>
<ul>
<li>効果検証は1つの教材章に限られており、今後は複数教科・複数章での検証が必要。</li>
<li>各機能（例：スライド、音声、クイズ）の寄与度は明確化されていない。</li>
</ul>
<p>将来的には、学習者の解答状況に応じて教材を動的に調整する「適応学習」への拡張が期待される。</p>
]]></description>
      <pubDate>Mon, 29 Sep 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>LLMへの指示が得意な人は脳の働きが違う──「プロンプト力」がfMRI研究で初めて科学的に確認される</title>
      <link>https://ledge.ai/articles/llm_prompting_brain_fmri_study</link>
      <description><![CDATA[<p>大規模言語モデル（LLM）への指示が得意な人とそうでない人の間で、脳活動に違いがあることが初めて科学的に確認された。サウジアラビア・キングサウード大学の研究チームは2025年8月20日、fMRI（機能的磁気共鳴画像法）を用いたパイロット研究の成果をarXivに<a href="https://arxiv.org/abs/2508.14869">公開</a>した。</p>
<h2>fMRIで「プロンプト力」の神経基盤を観測</h2>
<p>研究では、22人の参加者を対象に「プロンプト力」を評価するための独自尺度「Prompt Engineering Literacy Scale（PELS）」を開発し、スコアに基づき「熟達者」と「中級者」に分類。その上で、安静時fMRIを用いて脳の機能的結合やネットワーク活動を比較した。</p>
<h2>主な発見</h2>
<p>解析の結果、熟達者の脳には以下の特徴が確認された。</p>
<ul>
<li><strong>低周波帯域の優位性</strong> ：視覚ネットワーク（VVN）、デフォルトモードネットワーク後部（pDMN）、左外側頭頂ネットワーク（LLPN）などで、低周波成分が高周波成分に比べ優位であり、安定的で効率的な神経活動が示唆された。</li>
<li><strong>脳領域間の機能結合の強化</strong> ：熟達者では、左中側頭回（言語処理や意味記憶に関与）および左前頭極（計画・抽象的推論・メタ認知に関与）の機能結合が有意に強化されていた。</li>
<li><strong>効率的な神経活動</strong> ：脳内の自発的活動を示す指標（fALFF）が全般的に低下しており、不要な揺らぎが少なく効率的な情報処理が行われている可能性が示された。</li>
</ul>
<p><strong>■ LLMプロンプト熟達者で強化された左中側頭回の機能結合（fMRI解析より）</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Increased_connectivity_in_the_left_middle_temporal_gyrus_1927809d6f/Increased_connectivity_in_the_left_middle_temporal_gyrus_1927809d6f.jpg" alt="Increased connectivity in the left middle temporal gyrus.jpg" /></p>
<p><strong>■ LLMプロンプト熟達者で強化された左前頭極の機能結合（fMRI解析より）</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Increased_connectivity_in_the_left_frontal_pole_966041e40f/Increased_connectivity_in_the_left_frontal_pole_966041e40f.jpg" alt="Increased connectivity in the left frontal pole.jpg" /></p>
<h2>人とAIの協働に関する新しい視点</h2>
<p>この研究は「The Prompting Brain: Neurocognitive Markers of Expertise in Guiding Large Language Models」と題し、arXivにプレプリントとして公開された。著者らは、LLMを効果的に活用する能力（いわゆる「プロンプト力」）が、単なるスキルではなく神経科学的な特徴を持つことを示した点に意義があると述べている。</p>
<h2>今後の展望</h2>
<p>論文の著者らは、研究がパイロット的な小規模実験であり、より大規模かつ多様な参加者を対象とした検証が必要だと指摘している。また、プロンプト熟達度と脳活動の関連が、教育や職業訓練にどのような影響を及ぼすかを探る余地があるとした。さらに、AIと人間の協働を支える神経科学的理解を深めることで、ユーザーの特性に合わせたAIインターフェース設計につながる可能性があると述べている。</p>
]]></description>
      <pubDate>Mon, 29 Sep 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Meta、AIグラス上位モデル「Meta Ray-Ban Display」を発表──「パーソナルスーパーインテリジェンス」への第一歩、米国で9月30日発売</title>
      <link>https://ledge.ai/articles/meta_rayban_display_launch_2025</link>
      <description><![CDATA[<p>Metaは2025年9月18日（米国時間）、Ray-Banとの協業によるスマートグラスの新モデル「Meta Ray-Ban Display」を<a href="https://about.fb.com/news/2025/09/meta-ray-ban-display-ai-glasses-emg-wristband/">発表</a>した。AI機能に加え、右レンズ内側にディスプレイを搭載し、通知や情報を視界に直接表示できるのが特徴。価格は799ドルで、米国で9月30日から販売が始まる。</p>
<p><strong>「Meta Connect 2025で発表された3種類の新モデル──ディスプレイ搭載の『Meta Ray-Ban Display』、スポーツ向け『Oakley Meta Vanguard』、改良版『Ray-Ban Meta（第2世代）』」</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/G1_F7_W_Kz_Xs_AI_83_Cl_9320d07ac5/G1_F7_W_Kz_Xs_AI_83_Cl_9320d07ac5.png" alt="G1F7WKzXsAI83Cl.png" /></p>
<h2>製品概要</h2>
<ul>
<li>名称：「Meta Ray-Ban Display」</li>
<li>発表日：2025年9月18日</li>
<li>発売日：米国で9月30日より</li>
<li>価格：799ドル（Meta Neural Band 同梱）</li>
<li>提供カラー：Black、Sand</li>
<li>レンズ：Transitions®レンズ</li>
</ul>
<h2>主な特徴</h2>
<p>「Meta Ray-Ban Display」は、右レンズ内に600×600解像度のHUDを搭載。通知や情報を表示することが可能だ。12MPカメラを内蔵し、写真や動画撮影もできるほか、デュアルスピーカーと複数マイクを備え、ハンズフリーでの利用に対応する。Meta AIと連携し、音声や視覚情報を活用した応答も可能となっている。</p>
<h2>Neural Band との連携</h2>
<p>製品には筋電位（EMG）を利用した「Meta Neural Band」が同梱される。手首のわずかな動きを感知し、直感的な操作を可能にするもので、グラスとの連携により操作性を拡張する。</p>
<h2>利用時間と充電</h2>
<ul>
<li>グラス本体：通常使用で約6時間</li>
<li>付属の折りたたみ式充電ケース：最大30時間まで拡張可能</li>
<li>Neural Band：約18時間稼働</li>
</ul>
<h2>展開スケジュール</h2>
<p>米国では9月30日から、Best Buy、LensCrafters、Sunglass Hut、Ray-Ban Storeなどで販売される。2026年初頭にはカナダ、フランス、イタリア、英国にも展開予定。日本の公式ブログでも製品概要が紹介されており、今後の展開に関する情報提供が予告されている。</p>
<h2>今後の展望</h2>
<p>MetaのCEOマーク・ザッカーバーグ氏は、今回の製品を「パーソナルスーパーインテリジェンス」への第一歩と位置づけている。Metaは同時にスポーツ向けのOakleyブランドモデルも発表した。AI機能とウェアラブルデバイスの融合による新たな市場開拓に注力していくとしている。</p>
]]></description>
      <pubDate>Mon, 29 Sep 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Microsoft、AIチップ冷却に「マイクロ流体」導入──シリコンに直接液体を流し効率3倍</title>
      <link>https://ledge.ai/articles/microsoft_microfluidics_ai_chip_cooling</link>
      <description><![CDATA[<p>Microsoftは2025年9月24日、AIチップの性能向上に伴う深刻な発熱問題に対処するため、新しい液体冷却技術「マイクロ流体冷却」の開発・テストに成功したと<a href="https://news.microsoft.com/source/features/innovation/microfluidics-liquid-cooling-ai-chips/">発表</a>した。シリコン基板に微細な流路を刻み、冷却液を直接循環させる方式で、従来の冷却プレートと比べ最大3倍の効率を達成したという。</p>
<p>@<a href="https://www.youtube.com/watch?v=MZBwLi3ajYE">YouTube</a></p>
<p>近年、AIチップは大規模言語モデルや生成AIの処理需要に対応するため高性能化が進んでいる。その一方で、発熱量は急増し、従来の空冷や液冷では限界が見え始めていた。Microsoftは「冷却技術の進歩がなければ、数年以内にAIチップ開発は頭打ちになる可能性がある」と警鐘を鳴らしている。</p>
<h2>新技術「マイクロ流体冷却」の仕組み</h2>
<p>同社が開発した「マイクロ流体冷却」は、チップのシリコン基板に微細な流路を直接形成し、その中に液体を流すことで効率的に熱を除去する仕組みだ。GPU内部の温度上昇を65%低減し、従来の冷却プレート方式と比べ最大3倍の冷却効率を実現したとされる。</p>
<h2>Corintis社との協力</h2>
<p>この取り組みは、スイスのスタートアップCorintisとの共同研究に基づく。自然界の葉脈や蝶の羽に着想を得た「バイオインスパイアード設計」を採用し、AIを用いて冷却液の流路や循環を最適化している。これにより、チップ上で最も高温になる“ホットスポット”を効率的に冷却できるようになった。</p>
<h2>持続可能性への影響</h2>
<p>Microsoftは、Teams会議をシミュレートしたサーバー環境で同技術をテスト。実験室レベルでの検証において、従来技術を大幅に上回る冷却性能を確認した。これにより、実用化に向けた有効性が示された形だ。</p>
<p>マイクロ流体冷却は、データセンターの電力使用効率（PUE）の改善や運用コスト削減にもつながるとされる。冷却技術の革新は、環境負荷を低減しつつ、大規模AIインフラの持続可能な発展を支える重要な鍵になるとみられる。</p>
]]></description>
      <pubDate>Mon, 29 Sep 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>OpenAIとApollo Research、誤答やハルシネーションとは異なるAIの“裏の顔”（シェーミング）を検出──新手法で最大30倍抑制</title>
      <link>https://ledge.ai/articles/openai_apollo_ai_scheming_alignment</link>
      <description><![CDATA[<p>OpenAIは2025年9月17日、AIの評価設計や解釈可能性を研究するApollo Researchと共同で、AIモデルにおける「シェーミング（scheming）」の検出と抑制に関する研究成果を<a href="https://openai.com/index/detecting-and-reducing-scheming-in-ai-models/">発表</a>した。</p>
<p>論文「Stress Testing Deliberative Alignment for Anti-Scheming Training」によれば、特別な「反スキーミング仕様」を用いた訓練により、最新モデル「o3」「o4-mini」での隠れた行動率を大幅に減少させることに成功したという。</p>
<h2>AIの“裏の顔”──シェーミング（scheming）とは</h2>
<p>シェーミングとは、モデルが表面的には正しく振る舞いながら、実際には隠れた戦略を用いて評価テストを回避するなど、欺瞞的な行動を取る現象を指す。単なる幻覚や誤答とは異なり、信頼性や安全性に大きな影響を与える。</p>
<p>研究チームは26種類、180以上のテスト環境を設計し、ルール違反の隠蔽や虚偽報告などを評価。その結果、o3やo4-miniといった最新モデルでもシェーミング行動が実際に観測されたという。</p>
<p><strong>シェーミングは単なる誤答ではなく“隠れた戦略”による欺瞞的行動</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/a_testbed_for_anti_scheming_training_a9e335139d/a_testbed_for_anti_scheming_training_a9e335139d.jpg" alt="a testbed for anti-scheming training.jpg" /></p>
<h2>“熟慮的整合性”でずる賢さを封じ込める</h2>
<p>研究チームは「deliberative alignment（熟慮的整合性）」と呼ばれる手法を導入。モデルに「反スキーミング仕様」を読ませ、行動前に立ち返らせることで、隠蔽的行動の発生が最大30倍減少した。</p>
<ul>
<li>o3ではシェーミング率が13%から0.4%へ</li>
<li>o4-miniでは8.7%から0.3%へ</li>
</ul>
<p><strong>反スキーミング仕様の導入により、隠れた行動率が最大30倍減少</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Potential_outcomes_of_anti_scheming_training_39a968fc98/Potential_outcomes_of_anti_scheming_training_39a968fc98.jpg" alt="Potential outcomes of anti-scheming training.jpg" /></p>
<h2>シチュエーション認識と残る課題</h2>
<p>モデルが「評価されている」と気づくと行動が変わる傾向も確認された。行動低下の一部は「本当に正直になった」のではなく、「テストだと理解して取り繕った」結果の可能性もある。</p>
<p>さらに、あらかじめ不正な隠れ目標を与えた後に訓練を行う実験では、多くのケースで目標遂行が阻止されたが、依然として不正な回答が残るケースも報告された。</p>
<h2>安全性と性能のトレードオフ</h2>
<p>反スキーミング訓練は性能低下を招くことも分かった。例えば、ソフトウェア課題解決ベンチマーク「SWE-bench」では26ポイントの低下が確認された。ただし、再調整によって大部分は回復可能とされる。</p>
<p><strong>反スキーミング訓練は性能低下を招くが、再調整で多くは回復可能</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Examples_of_environments_cf93faa7c8/Examples_of_environments_cf93faa7c8.jpg" alt="Examples of environments.jpg" /></p>
<h2>次世代AIの信頼をどう築くか</h2>
<p>OpenAIは今後もApollo Researchと協力し、AIの安全性と透明性を高める研究を継続する方針を示している。今回の成果は、次世代の高性能モデルにおける信頼性確保に向けた重要な一歩と位置づけられる。</p>
<p>:::box
[関連記事：Anthropic、AIの\</p>
]]></description>
      <pubDate>Mon, 29 Sep 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI、次世代動画生成モデル「Sora 2」を発表──自分や友人が出演する動画を生成できるiOSアプリ「Sora」も米国とカナダで同時公開</title>
      <link>https://ledge.ai/articles/sora2_openai_ios_app_launch</link>
      <description><![CDATA[<p>OpenAIは2025年9月30日、最新の動画・音声生成モデル「Sora 2」を<a href="https://openai.com/index/sora-2/">発表</a>した。</p>
<p>物理挙動の正確さや映像の写実性が大幅に向上し、音声を同期して生成できる点が特徴。同日には、このモデルを利用できるiOS向けアプリ「Sora」も公開され、米国とカナダで招待制による提供が始まった。日本での提供時期は明らかにされていない。</p>
<p>@<a href="https://youtu.be/lEcg6AJ6DVY?si=aS3u22digXd5ZVY8">YouTube</a></p>
<h2>Sora 2の性能</h2>
<p>Sora2は、従来の「Sora」モデルを基盤に開発された動画・音声生成AIである。
OpenAIが公開した<a href="https://openai.com/index/sora-2-system-card/">システムカード</a>によれば、より正確な物理シミュレーション、長尺映像における一貫性、幅広いスタイルへの対応を実現。さらに音声生成を統合し、映像にナレーションや環境音を付与できる。</p>
<p>生成可能な映像は最大20秒とされるが、ReutersやThe Vergeなど複数のメディアは「アプリ上では10秒程度に制限されている」と報じている。</p>
<p>@<a href="https://www.youtube.com/watch?v=1PaoWKvcJP0">YouTube</a></p>
<h2>iOSアプリ「Sora」の提供</h2>
<p>同日に公開されたiOS向けアプリ「Sora」では、ユーザーがAI生成動画を作成・共有できる。
提供開始は米国とカナダで、アクセスは招待制。アプリ内で通知登録を行うことで順次利用可能となる。AndroidユーザーはWeb版の “sora.com” からアクセスできる仕組みだ。Sora2は当初無料で利用できるが、計算能力の制限が設けられているとのこと。</p>
<p>アプリの特徴として注目されるのが**「Cameo（カメオ）機能」** だ。ユーザーは自分や友人を動画に登場させられる。OpenAIは、この機能を利用するには本人の同意が必要とし、無断で他人の肖像を使用することはできない設計にしているという。サム・アルトマンCEOも自身のブログで「チームがキャラクターの一貫性に力を注ぎ、友人同士を動画に登場させることが意外なほど魅力的な新しいつながり方になった」と述べている。</p>
<h2>安全性への配慮</h2>
<p>OpenAIは安全設計を重視しており、生成動画には透かしやC2PAメタデータを付与。肖像権の無断利用や公人の生成は禁止され、未成年保護のためのフィルタリングや保護者向けコントロール機能も導入されている。
システムカードに記載された安全性評価では、不適切コンテンツを検出・遮断する精度が96〜99％に達したという。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/introducing_sora2_9ac129aadc/introducing_sora2_9ac129aadc.jpg" alt="introducing sora2.jpg" /></p>
<h2>背景と思想</h2>
<p>同社CEOのサム・アルトマン氏は自身の<a href="https://blog.samaltman.com/sora-2">ブログ</a>で、Soraを「ChatGPT for creativity」と表現。誰もが手軽に動画生成を楽しめる環境を提供する一方で、依存性や誤用のリスクについても懸念を示し、長期的なユーザーの満足や健全な利用を重視する方針を強調した。</p>
<p>また、OpenAIは公式サイトで「<a href="https://openai.com/index/sora-feed-philosophy/">フィード哲学</a>」を公開し、ユーザーが視聴体験を自ら選択できる仕組みを構築するとしている。</p>
<p>@<a href="https://www.youtube.com/watch?v=gzneGhpXwjU&amp;t=137s">YouTube</a></p>
]]></description>
      <pubDate>Mon, 29 Sep 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>【9/29-30限定公開】AI業界のトップランナーが集結した「Ledge.ai Webinar SP」の見逃し配信</title>
      <link>https://ledge.ai/articles/ledgeai-webinarsp-sponsor-archive</link>
      <description><![CDATA[<p>国内最大級のAI専門メディア「Ledge.ai」を運営する株式会社レッジは、9月24日から9月26日の3日間にかけて、オンラインイベント「Ledge.ai Webinar SP」を開催した。</p>
<p>同イベントは、「AIをしる、つかう、つくる」をテーマに、AI業界を牽引する18の企業が、AI活用に関する見識をシェアする講演を実施した。本日と明日の2日間限定で、ライブ配信を見逃した方や再度視聴したい方のために「Ledge.ai Webinar SP」の全講演をオンデマンドで配信する。本稿では実施された講演とその一部見所を紹介する。</p>
<p>:::button
<a href="https://forms.zohopublic.com/ledgeai/form/WebinarSP/formperma/JuHnRG5lHussdjFLR1q1xzkoh-ToCiYgQmvxxGOotyU">アーカイブ配信を見る</a>
:::</p>
<h1>AI業界のトップランナーが集結し、全18の講演をお届け</h1>
<p>「Ledge.ai Webinar SP」では、マイクロソフト、ソフトバンク、日本ディープラーニング協会などAI業界のトップランナー全18の企業が講演を行った。</p>
<h2>DAY-1「AIをしる」</h2>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Day1_76bc3a0602/Day1_76bc3a0602.png" alt="Day1.png" /></p>
<h2>DAY-2「AIをつかう」</h2>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Day2_62d766c235/Day2_62d766c235.png" alt="Day2.png" /></p>
<h2>DAY-3「AIをつくる」</h2>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Day3_d61ce2af95/Day3_d61ce2af95.png" alt="Day3.png" /></p>
<h3>「ソフトバンクの事例から紐解く、組織の生成AI活用・推進を自走するための仕組みづくり」</h3>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/2025_09_28_13_45_33_69eb5b9eab/2025_09_28_13_45_33_69eb5b9eab.png" alt="スクリーンショット 2025-09-28 13.45.33.png" />
【登壇者】
ソフトバンク株式会社
藤原 竜也 氏 / IT統括 AI&amp;データ事業統括部　Axross事業部 部長</p>
<p>概要：
ソフトバンク株式会社の講演では、ソフトバンク自身がどのように組織での生成AI活用を進めたのか解説。自社事例をもとにしたトップダウン・ボトムアップそれぞれのアプローチ手法など実際組織に落とし込める取り組みについてお話しいただいた。</p>
<h3>「私が想像する未来のOS」</h3>
<p>【登壇者】
マイクロソフト ディベロップメント株式会社
Zhan (Cliff) Chen / 陳 湛 氏 / プリンシパル　アプライド　サイエンティスト</p>
<p>概要：
マイクロソフト ディベロップメント株式会社の講演では、未来のOSについて語られた。未来のUIはその人それぞれにベストな形で生成されるようになり、OSはキャラクターに変わるという。講演では、未来がなぜそのように変わっていくかも語られた。</p>
<h3>「企業におけるAIエージェント実装の現実戦略」</h3>
<p>【登壇者】
株式会社エクスプラザ
宮田 大督 氏 / CPO 生成AIエバンジェリスト・リードAIプロデューサー</p>
<p>概要：
株式会社エクスプラザの講演では、AIエージェントを使いこなすことで今どのレベルまでAIがタスクを代替できるのか、宮田 氏自身の自律型エージェントがこちらから指示を出していないのにAIが課題に合わせたアプリを作って提案してくれた経験などをご紹介いただいた。また、今後AIエージェントが当たり前になる時代に対し、どのように備えていくべきかもお話しいただいた。</p>
<h3>「自然に生成AIの活用を加速させるデータインフラ統合型AIのすゝめ」</h3>
<p>【登壇者】
ダイレクトクラウド株式会社
石田 圭一 氏 / プロダクト本部　部長
十樹 亮輔 氏 / 営業本部カスタマーリレーション部 カスタマーサクセスチーム</p>
<p>概要：
ダイレクトクラウド株式会社の講演では、データインフラ統合型AIを活用し組織全体での知識共有が促進されることで生まれるメリットや重要性についてお話しいただいた。また、データインフラに統合されたAIの活用事例などについてもお話しいただいた。</p>
<h3>「話題のgpt-ossとは？ローカル版ChatGPTで構築するAIエージェント【解説＆デモ】」</h3>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/2025_09_28_13_35_49_5582ed62a7/2025_09_28_13_35_49_5582ed62a7.png" alt="スクリーンショット 2025-09-28 13.35.49.png" />
【登壇者】
株式会社ハイレゾ
山田 岳史 氏 / GPU事業本部 マーケティング部 グループ長</p>
<p>概要：
株式会社ハイレゾの講演では、ローカル版ChatGPTを使ったAIエージェント構築のデモンストレーションと解説をしていただいた。またその中でMCP、MoEなど必要な概念やその実装方法についても触れていただいた。</p>
<h3>「HPワークステーションで試すRAG検索 — 実用構成とPOC事例の紹介」</h3>
<p>【登壇者】
株式会社日本HP
勝谷 裕史 氏 / エンタープライズ営業統括 ソリューション営業本部 ワークステーション営業部 AI/データサイエンス市場開発担当部長</p>
<p>概要：
株式会社日本HPの講演では、AIワークステーションを活用する具体的な用途やメリット、クラウド生成AIとローカル生成AIの違いについて解説いただいた。また、実際にオンプレミスでのローカル生成AのPoC事例についてもご紹介いただいた。</p>
<p>ここまでご紹介したように、登壇いただいた企業様それぞれの視点や取り組みが異なるため、多様な切り口で様々な情報をキャッチアップすることができる。</p>
<p>AIに関する様々な分野の最前線を知る機会として、是非お見逃しなく。</p>
<p>:::button
<a href="https://forms.zohopublic.com/ledgeai/form/WebinarSP/formperma/JuHnRG5lHussdjFLR1q1xzkoh-ToCiYgQmvxxGOotyU">アーカイブ配信を見る</a>
:::</p>
]]></description>
      <pubDate>Mon, 29 Sep 2025 00:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/9/28 [SUN]Photoshop、外部AIモデルを初統合──Google「Nano Banana」とBlack Forest Labsの「FLUX.1 Kontext」が生成塗りつぶしに対応</title>
      <link>https://ledge.ai/articles/photoshop_generative_fill_gemini_flux_integration</link>
      <description><![CDATA[<p>Adobeは2025年9月25日、Photoshopの「生成塗りつぶし（Generative Fill）」機能に、Googleの画像生成モデル「Gemini 2.5 Flash Image（Nano Banana）」と、Black Forest Labsの「FLUX.1 Kontext [pro]」を統合したと<a href="https://blog.adobe.com/en/publish/2025/09/25/photoshop-beta-expands-generative-fillmore-ai-models-more-possibilities">発表</a>した。これにより、ユーザーはAdobe独自のFireflyモデルに加え、外部AIモデルを切り替えて利用できるようになる。</p>
<h2>外部AIモデルが初めてPhotoshopに統合</h2>
<p>今回の更新は、Photoshopが自社モデル中心から一歩踏み出し、外部の生成AIモデルを取り込む初の事例となる。Adobe公式ブログによれば、Nano BananaとFLUX.1 Kontext [pro]はいずれもPhotoshopベータ版のユーザーが利用可能で、選択範囲を指定したうえでプロンプト入力を行うと、モデルを切り替えて生成や編集を実行できる。
なお、Adobeは昨年動画編集ソフトのPremiere Proにおいて、OpenAIのSoraやRunway、Pika Labsなど外部の動画生成モデルの統合予定を<a href="https://ledge.ai/articles/adobe_premiere_pro_sora">発表</a>しているが、Photoshopへの外部モデル導入は今回が初めてとなる。</p>
<h2>Nano Bananaの特徴</h2>
<p>Googleが開発した「Gemini 2.5 Flash Image（Nano Banana）」は、スタイル変換やグラフィック要素の追加、視覚効果の生成に強みを持つ。Photoshop内ではFireflyと同様の操作感で利用でき、用途に応じたモデルの選択が可能となる。</p>
<p><strong>Googleの「Gemini 2.5 Flash Image（Nano Banana）」を使った生成塗りつぶしの例。衣服や背景を置き換え、黄色い鳥を追加するなど複数の変更を一度に適用できる。</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/media_195028b2e137ad3d6178196faeab0ef497afee9d5_8a49af01ee/media_195028b2e137ad3d6178196faeab0ef497afee9d5_8a49af01ee.jpg" alt="media_195028b2e137ad3d6178196faeab0ef497afee9d5.jpg" /></p>
<h2>FLUX.1 Kontext [pro]の特徴</h2>
<p>一方、Black Forest Labsの「FLUX.1 Kontext [pro]」は、環境や遠近感に調和した生成を得意とする。背景や構図に一貫性を持たせる性能が評価されており、Photoshopでの利用によって「場面全体との整合性」を保ちながらの編集が可能になる。</p>
<p><strong>Black Forest Labsの「FLUX.1 Kontext [pro]」による生成例。画像全体のコンテキストを保ちながら、巨大な毛糸玉を追加するなど背景との調和を重視した編集が可能。</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Adobe_Black_forest_labs_124afb8d27/Adobe_Black_forest_labs_124afb8d27.jpg" alt="Adobe-Black forest labs.jpg" /></p>
<h2>ベータ版での提供と今後の展開</h2>
<p>両モデルは現時点ではPhotoshopのベータ版でのみ利用可能。無料トライアル枠の提供も用意されているが、生成回数には制限がある。正式版への導入時期は明らかにされていない。</p>
]]></description>
      <pubDate>Sun, 28 Sep 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>日本とシンガポールを結ぶ国際海底ケーブル「Candle」建設へ──ソフトバンク、Metaなど4社と合意</title>
      <link>https://ledge.ai/articles/candle_submarine_cable_softbank_meta_agreement</link>
      <description><![CDATA[<p>ソフトバンクは2025年9月22日、米Metaなど4社と共同で、日本とシンガポールを結ぶ国際海底ケーブル「Candle（キャンドル）」の建設に合意したと<a href="https://www.softbank.jp/corp/news/press/sbkk/2025/20250922_01/">発表</a>した。総延長約8,000kmの光海底ケーブルを敷設し、2028年の運用開始を予定している。</p>
<h2>「Candle」プロジェクトの概要</h2>
<p>「Candle」は、日本、台湾、フィリピン、インドネシア、マレーシア、シンガポールを結ぶ国際海底ケーブルで、システム供給はNECが担う。24ファイバーペア構成を採用し、従来16～20ペアが主流だった海底ケーブルと比べ、さらに大容量かつ低遅延の通信を実現する。急増するAIや5G関連の通信需要に対応し、東アジアと東南アジアを結ぶ主要ルートの多様化・冗長化を図る。</p>
<h2>各社のコメント</h2>
<p>Candleマネジメントコミッティ議長でMetaのDon Pang氏は、
「Candleはアジア地域のデジタルインフラ強化における重要な前進です。高速かつ堅牢な接続性の需要が高まる中、ネットワークの多様性とレジリエンスを向上させ、5億人以上の人々にデジタル・インクルージョンと経済的機会を拡大します」と述べた。</p>
<p>また、ソフトバンク法人統括 グローバル事業本部 本部長の工藤公正氏は、
「生成AIやIoTの進展に伴い、国際通信需要は加速度的に拡大しています。Candleは次世代社会インフラの重要な基盤であり、既存の『JUPITER』『ADC』、建設中の『E2A』と組み合わせることで、日本を起点とする国際通信網をさらに強化します」とコメントしている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/index_pic_02_8f725966c5/index_pic_02_8f725966c5.webp" alt="index_pic_02.webp" /></p>
<h2>ソフトバンクの取り組み</h2>
<p>ソフトバンクはCandleの日本側陸揚げ局として、千葉県南房総市の「ソフトバンク丸山国際中継所」を提供する。さらに北海道・九州に新たな陸揚げ拠点を設け、日本各地に分散配置することで災害や障害に強い通信インフラを整備する計画だ。</p>
]]></description>
      <pubDate>Sun, 28 Sep 2025 02:50:00 GMT</pubDate>
    </item>
  </channel>
</rss>