<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
  <channel>
    <title>Ledge.ai 新着記事</title>
    <link>https://ledge.ai/</link>
    <description>Ledge.ai の最新テクノロジー記事</description>
    <item>
      <title>18歳未満の自由チャット廃止を受け、自由会話から“物語選択式”へ──Character.AIが新機能「Stories」で未成年向け体験を再構築</title>
      <link>https://ledge.ai/articles/character_ai_stories_u18_safety_launch</link>
      <description><![CDATA[<p>AIキャラクターとの自由形式のチャットを18歳未満のユーザーに禁止すると10月に発表したCharacter.AIは11月27日（米国時間）、代替となる新しい対話機能「Stories」を正式に導入したことを<a href="https://blog.character.ai/introducing-stories-a-new-way-to-create-play-and-share-adventures-with-your-favorite-characters/">発表</a>した。</p>
<p>自由入力によるオープンエンド型の会話とは異なり、選択肢を選んで物語を進める「インタラクティブ・ストーリー形式」を採用する。未成年ユーザーも利用できる仕組みで、同社は「安全性を確保しながら創造的なAI体験を維持するための新しい方法」と位置づけている。</p>
<p>Storiesでは、AIキャラクターが登場人物やナレーターとして振る舞い、ユーザーは提示される選択肢からストーリーの展開を選ぶ。自由記述のテキスト入力はできず、物語の文脈に沿った応答が行われる点が特徴だ。ユーザーは既存のキャラクターが公開しているStoriesに参加できるほか、自らシナリオを作成して公開することも可能。公開前にはプラットフォーム側の安全性レビューが実施され、他の利用者にはリンク形式で共有できる。</p>
<p><strong>公式が公開した「Stories」サンプル一覧。ファンタジー、コメディ、ミステリーなど複数ジャンルの物語が用意され、選択肢を通じてストーリーを進める形式になっている。</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/characterai_data_d662534a06/characterai_data_d662534a06.jpg" alt="characterai data.jpg" /></p>
<p>今回の実装は、Character.AIが進めてきた未成年ユーザーの安全対策の延長線上にある。同社をめぐっては2024年、AIチャットボットに依存していたとされる14歳の少年が自殺した事件を受けて遺族が訴訟を起こし、対話型AIが若年層のメンタルヘルスに与える影響が議論となった。Ledge.aiでは当時、擬似的な友人関係を結ぶタイプのAIが依存を助長する懸念、そして専門家の指摘を報じていた。Character.AIはその後、利用ガイドラインの強化や未成年向けの利用制限を段階的に導入し、2025年10月には「18歳未満のオープンエンドなチャットを全面廃止する」と発表していた。</p>
<p>Storiesは、このチャット廃止後に未成年ユーザーへ提供される主要なAI体験として位置づけられる。自由会話の柔軟性はないものの、ストーリー形式によって創造的な没入体験は継続でき、同時に不適切な対話や依存リスクを抑えられる設計になっている。Character.AIは今後、Storiesの提供範囲を段階的に拡大し、コミュニティによる創作コンテンツの増加を見込むとしている。</p>
]]></description>
      <pubDate>Sat, 29 Nov 2025 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>民放連「生成AIによる放送コンテンツ酷似映像」を問題視──無許諾学習の停止と削除対応を求める声明</title>
      <link>https://ledge.ai/articles/minporen_ai_learning_statement_20251126</link>
      <description><![CDATA[<p>日本民間放送連盟（民放連）は2025年11月26日、「生成ＡＩの開発・学習に関する声明」を<a href="https://j-ba.or.jp/category/topics/jba106724">公表</a>し、会員各社が権利を保有するコンテンツが無許諾で生成AIに学習されている可能性があるとして、開発事業者に対し明確な対策を求めた。</p>
<p>声明によると、OpenAIの動画生成サービス「Sora2（sora.chatgpt.com）」を通じて、民放連会員社のアニメ作品などと同一または酷似する映像が生成され、インターネット上で確認されているという。民放連は、これらの事例は会員社コンテンツが事前の許諾なく学習に使用された結果とみている。</p>
<p>民放連は、生成AIが不特定多数に向けて公開可能なコンテンツを作成する以上、開発段階の学習においても「享受を目的とする複製」に該当し、著作権者の許諾が必要になると指摘。無許諾学習は著作権侵害となり得るとしたうえで、違法アップロードコンテンツを学習に用いた場合には、侵害の深刻性がさらに増すと強調した。</p>
<p>また、無許諾学習や酷似映像の生成は、制作会社、クリエイター、出演者など多様な関係者の経済的利益や人格的利益を損ない、日本のコンテンツ制作文化やエンターテインメント産業のエコシステムを揺るがしかねないと懸念を示した。</p>
<p>報道分野への影響にも言及し、虚偽の災害映像や政治家の偽動画、外国人ヘイトを助長する映像など、いわゆるディープフェイクが流通した場合には、国民生活に重大な混乱を引き起こす可能性があるとした。出演者の偽動画を悪用した投資勧誘など、犯罪につながる行為への警戒も示している。</p>
<p>さらに、生成AI開発者側が提示する「オプトアウト方式」については、権利侵害の発生や類似映像の生成を十分に防げないとして、実効性が乏しいとの見解を示した。</p>
<p>声明では、生成AI開発者に対し次の3点を求めている。</p>
<ol>
<li>会員社コンテンツを無許諾で学習対象としないための措置を講じること。</li>
<li>会員社コンテンツと同一または類似する映像・画像が生成されないよう対策を講じ、既に 生成・流通しているものは削除に努めること（特に開発者が管理するサイトからの削除）。</li>
<li>生成AIに起因する著作権侵害について、会員社からの申立てに真摯に対応すること。</li>
</ol>
<p>今回の声明は、放送業界として生成AIの無許諾学習に対し明確な立場を示したもので、放送コンテンツの権利保護や安全性に関する問題が公式に整理された形となる。</p>
]]></description>
      <pubDate>Sat, 29 Nov 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ワーナー・ミュージック、Sunoと和解し提携へ──訴訟から「ライセンス型AI音楽」へ転換、2026年に新モデルと料金体系を導入</title>
      <link>https://ledge.ai/articles/warner_music_suno_licensing_partnership_2026_model_update</link>
      <description><![CDATA[<p>世界3大レコードレーベルのひとつである Warner Music Group（WMG）は11月25日（米国時間）、音楽生成AIサービスを手がける米Sunoと「ライセンスパートナーシップ」を締結したと<a href="https://www.wmg.com/news/warner-music-group-and-suno-forge-groundbreaking-partnership">発表</a>した。両社間で進行していた法的係争は、この合意によって解決されたことも明らかにされた。Suno側も同日付で公式ブログを公開し、提携の事実と今後のプロダクト方針を説明している。</p>
<h2>クリエイター保護を前提とした「ライセンス型AI音楽」へ</h2>
<p>WMGは今回の提携を、「音楽の制作・インタラクション・発見をめぐる新たなフロンティアを開く」と位置づけている。アーティストやソングライターを「保護し、適切に補償する」枠組みを掲げ、SunoのAI技術と同社のアーティスト開発力・テクノロジー面の知見を組み合わせると説明した。</p>
<p>同社は、AI生成で使われるアーティストの「名前・画像・肖像・声・楽曲」について、本人（または権利者）がオプトイン方式で利用範囲をコントロールできる仕組みを提供すると明記。生成AIによる二次利用に対し、アーティスト側に新たな収益機会を生むモデルを整備する方針を示した。</p>
<p>WMGのRobert Kyncl CEOは、「クリエイティブコミュニティにとっての勝利」とコメントし、ライセンス済みAIモデルの普及と、アーティストへの補償を両立する枠組みの必要性を強調した。</p>
<h2>Sunoは「インタラクティブな音楽体験」強化を表明</h2>
<p>SunoのMikey Shulman CEOはブログで、今回のパートナーシップを「インタラクティブな音楽の未来を形作る次のステップ」と述べた。同社は、現在のユーザーコミュニティが「約1億人規模」に成長しているとし、Sunoでの創作体験を維持したまま、より高度なライセンス済みモデルを導入する方針を説明した。</p>
<p>Sunoによると、オプトインしたWMGアーティストについては、ファンがそのアーティストの「声・サウンド・音楽的特徴」を用いた創作が可能になる。Suno側は、これによりアーティストが新たな収益を得られ、ユーザーは適切なライセンスのもとで創作できる仕組みになると説明している。</p>
<h2>2026年に新モデル導入、現行モデルは廃止へ</h2>
<p>両社の発表では、2026年に「ライセンス済みの新モデル」を導入し、現在のモデルを廃止する方針が示された。</p>
<p>料金体系についても大きく変更される。</p>
<ul>
<li>無料ユーザーは、作成した楽曲をダウンロードできなくなり、再生・共有のみ可能となる。</li>
<li>有料ユーザーは、月間のダウンロード上限が設定され、上限を超える分は追加料金となる。</li>
<li>プロ向けの「Suno Studio」では、現行の無制限ダウンロードなどの機能を維持しつつ、引き続き機能拡張を行うと説明している。</li>
</ul>
<p>Sunoは、新モデルについて「現行のv5を上回る高度なモデルになる」としており、ライセンス済みデータを含む新たな学習体系に基づいた生成性能の強化を示唆した。</p>
<h2>ライブ情報サービス「Songkick」はSunoへ移管</h2>
<p>WMGは、ライブ情報／チケットディスカバリーサービス「Songkick」をSunoへ移管したことも発表した。Sunoは同サービスを引き続き運営し、オンライン上の創作体験とライブパフォーマンスの接続を強める構想を示している。</p>
<p>両社は今後、アーティストの権利を尊重しつつ、ファンやクリエイターに新しい音楽体験を提供する「ライセンス型AI音楽プラットフォーム」の構築を進めるとしている。</p>
]]></description>
      <pubDate>Fri, 28 Nov 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>マネーフォワード、『AI確定申告』β版を提供開始──AIエージェントが自動作成し、ユーザは「チェック＋提出」のみで完結</title>
      <link>https://ledge.ai/articles/moneyforward_ai_tax_return_beta_launch</link>
      <description><![CDATA[<p>マネーフォワードは2025年11月25日、同社初のAIネイティブプロダクトとなる「マネーフォワード AI確定申告」（β版）の提供開始を<a href="https://corp.moneyforward.com/news/release/service/20251125-mf-press-2/">発表</a>した。AIが領収書などの必要書類を解析し申告内容を自動作成するサービスで、ユーザーは内容をチェックし、e-Taxや『マネーフォワード クラウド確定申告』などを通じて提出することで、確定申告を完了できる。</p>
<p>同社はコンセプトを「AIではじまる、新しい確定申告」と位置づけ、確定申告の初心者や会計知識に不安のある個人事業主でも、作業負担や心理的ハードルを下げられるサービスだとしている。</p>
<h2>AI-OCRと生成AIで申告内容を自動生成</h2>
<p>『マネーフォワード AI確定申告』は、なるべく簡単・シンプルに申告を終えたいユーザー向けに設計されたAIネイティブな確定申告サービスだ。ユーザーが溜まった領収書をアップロードすると、AI-OCRが金額や日付、取引内容を読み取り、その情報をもとに生成AIが申告内容のデータをまとめる。</p>
<p>取引ごとに「交際費」「交通費」などのカテゴリと、その判定理由が表示されるのも特徴だ。どのような考え方で分類されたのかを確認できるため、会計用語に慣れていないユーザーでも、内容を追いながら申告準備を進めやすいとしている。</p>
<p>β版では、領収書の読み取りと申告内容の自動作成（「AIおまかせ領収書整理」）、AIによる解析結果・判定理由の確認、収支・純利益の自動計算といった機能を無料で提供する。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/20251125_MFAI_54e143bf4c/20251125_MFAI_54e143bf4c.gif" alt="20251125_MFAI.gif" /></p>
<h2>対象ユーザーと対応範囲</h2>
<p>サービスの対象は、事業収入があり、不動産収入のない免税事業者。現時点では、不動産所得や農業所得、消費税の計算には対応していない。確定申告書そのものの作成機能は含まれておらず、申告書の作成・提出には『マネーフォワード クラウド確定申告』などのサービスやe-Taxを併用する必要がある。</p>
<p>また同社は、AIの出力結果についてはユーザー自身による確認を求めており、判断が難しい場合は税理士など専門家の助言を受けるよう呼びかけている。</p>
<h2>2026年以降に機能拡張、書類対応やスマホ対応も</h2>
<p>同社は、β版提供後も機能を段階的に拡充する計画を示した。2026年1月には、確定申告用の明細ファイルをダウンロードできる機能を提供する予定だという。</p>
<p>その後は、源泉徴収票や保険料・医療費控除書類の解析・計算・反映機能（特許申請中）、スマートフォンからの撮影・アップロード機能（同）、スマートフォンアプリ、申告書の作成・提出機能、銀行・金融サービスとの連携などを順次追加していく構想を示している。</p>
<h2>既存のクラウド確定申告と並行提供、AIネイティブ路線を拡大へ</h2>
<p>同社はこれまで、会計ソフトをベースにした『マネーフォワード クラウド確定申告』で個人事業主の申告業務を支援してきた。一方で、多機能で入力項目も多い従来型のソフトは、「できるだけ手間なく簡単に終えたい」という初心者層のニーズに十分応えられていなかったという。</p>
<p>『マネーフォワード AI確定申告』は、そうしたユーザー向けに確定申告のプロセスそのものをAI前提で再設計したプロダクトと位置づけられる。今後は、ユーザーが自身の会計知識や事業規模、ライフステージに応じて『クラウド確定申告』と『AI確定申告』を使い分けられるようにしつつ、確定申告以外のバックオフィス業務にもAIネイティブなサービスを広げていく方針だ。</p>
]]></description>
      <pubDate>Fri, 28 Nov 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Meta、AIアシスタント「Meta AI」の日本提供を正式発表──Messenger・Instagram・WhatsAppで順次利用可能に</title>
      <link>https://ledge.ai/articles/meta_ai_japan_rollout_2025</link>
      <description><![CDATA[<p>Metaは2025年11月25日、同社のAIアシスタント「Meta AI」を日本で順次提供開始することを<a href="https://about.fb.com/ja/news/2025/11/meta-ai-gradual-rollout-begins-in-japan/">発表</a>した。Meta AIの国内展開が公式に示されるのは今回が初めてで、同社の主要アプリ群に段階的に組み込まれる。</p>
<p>Meta AIは、Llama 3系モデルを基盤とした対話型アシスタントで、検索補助、質問応答、情報探索、画像生成などの機能を備える。Metaは、自然言語の理解力や推論性能の高さ、応答速度の改善を特徴として挙げている。</p>
<p>提供対象はMessenger、Instagram、WhatsApp、Facebookアプリ内など。各アプリのチャット入力欄や検索画面から直接呼び出せる設計となっており、コンテンツ探索から日常的なタスク補助まで、サービス横断で利用できる。InstagramやMessengerでは、チャット中にMeta AIへ質問したり、Reelsの発見タブで関連情報を得るといった操作が可能になる。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Meta_AI_Gradual_Rollout_ac8a0fc919/Meta_AI_Gradual_Rollout_ac8a0fc919.jpg" alt="Meta-AI-Gradual-Rollout.jpg" /></p>
<p>画像生成機能も展開される。テキストからの静止画生成に加え、一部ではリアルタイムに変化する生成画像の活用にも対応するという。生成した画像はメッセージ内で共有・編集でき、クリエイティブ用途にも利用可能。</p>
<p>日本での提供は段階的に行われ、まずは一部ユーザーから導入し、数週間から数カ月かけて利用対象を拡大する。Metaはプライバシーと安全性を重視した運用方針を示すとともに、利用者からのフィードバックを取り入れながら改善を続けるとしている。</p>
<p>Meta AIはすでに複数地域で展開されており、今回の日本投入は同社のグローバル展開計画の一環となる。</p>
]]></description>
      <pubDate>Fri, 28 Nov 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>米政府、AIで科学研究を国家規模で加速へ──トランプ大統領が「Genesis Mission」を創設する大統領令に署名</title>
      <link>https://ledge.ai/articles/genesis_mission_executive_order_launch</link>
      <description><![CDATA[<p>米ホワイトハウスは2025年11月24日（現地時間）、ドナルド・トランプ大統領が、AIを活用して科学研究の発見や開発速度を加速するための国家的取り組み「Genesis Mission（ジェネシス・ミッション）」を創設する大統領令に署名したと<a href="https://www.whitehouse.gov/presidential-actions/2025/11/launching-the-genesis-mission/">発表</a>した。同日公開された<a href="https://www.whitehouse.gov/articles/2025/11/president-trump-launches-the-genesis-mission-to-accelerate-ai-for-scientific-discovery/">公式記事</a>では、同ミッションの背景、目的、政府関係者の発言が紹介されており、AIを中核とした新たな科学研究推進体制が明らかになった。</p>
<h2>AIで科学的発見を加速する国家プログラムを創設</h2>
<p>Genesis Mission は、アメリカが保有する「世界有数の科学データ」と「最先端のAI技術」を統合し、医療、材料、エネルギー、バイオ技術、宇宙、国家安全保障などの分野で科学的発見を加速することを狙う国家プログラムとして位置づけられている。</p>
<p>大統領令では、同ミッションを推進するため、連邦政府が保有する科学研究データを統合し、大規模AIモデルやAIエージェントを開発・展開する「統合AIプラットフォーム」の構築が命じられた。目的は、仮説生成、実験計画、シミュレーション、解析などの研究プロセスをAIで高速化し、研究生産性の向上を図ることにある。</p>
<h2>エネルギー省（DOE）が中心的役割を担う</h2>
<p>ホワイトハウスの記事と大統領令の双方において、米エネルギー省（DOE）がGenesis Missionの中核機関として明記されている。DOE が管理する国立研究所（National Labs）は、世界最大級の科学データセットとスーパーコンピュータを所有しており、AIモデルの開発・実行基盤を提供する役割を担う。</p>
<p>また、大統領令では、DOE長官が他省庁と協力し、研究データの共有や連携体制の構築、AI活用方針の調整を進めることが求められている。さらに、研究者とAIエージェントが協働できる環境を整備するほか、国立研究所・大学・民間企業との官民パートナーシップの強化も指示された。</p>
<h2>政府関係者「アメリカの科学力を次の段階へ」</h2>
<p>科学技術担当大統領補佐官の Michael Kratsios 氏 は、Genesis Mission を「アメリカの科学研究の基盤を強化し、国家的優位性を高めるための取り組み」と説明。世界中で競争が激化するAI研究分野において、米国の技術力と研究インフラを活かし、科学的ブレイクスルーを継続的に生み出す必要性を強調した。</p>
<p>また、担当閣僚や行政スタッフによるコメントでは、ミッションがエネルギー・医療・国家安全保障など多様な分野で応用される見込みが示されている。</p>
<h2>大統領令が規定する具体的な指示</h2>
<p>大統領令「Launching the Genesis Mission」は、研究の生産性向上に加え、科学的知識の創出速度を国家レベルで高めることを目的としており、以下のような具体的方針が含まれる。</p>
<ul>
<li>連邦政府が保有する科学データの統合と共有</li>
<li>科学研究に特化したAIモデル・AIエージェントの開発</li>
<li>DOEを中心とした省庁横断タスクフォースの設置</li>
<li>研究者とAIが協働する研究環境の整備</li>
<li>国家重要分野（エネルギー、バイオ、量子、材料科学、半導体など）でのAI活用促進</li>
<li>国家安全保障・経済競争力の観点からのAI活用を明記</li>
</ul>
<p>大統領令に基づき、関連省庁は一定期間内に詳細な行動計画をまとめ、研究データ統合のロードマップや実施体制の整備を進める。政府・大学・国立研究所・民間企業を含む幅広いパートナーシップのもと、AIを活用した科学研究の新たな枠組みが形成されるという。</p>
]]></description>
      <pubDate>Thu, 27 Nov 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>GoogleのAIモードに広告が出現、海外ユーザーからの報告──Googleは「既存テストの一部」とコメント</title>
      <link>https://ledge.ai/articles/google_ai_mode_ads_integration_test_expands</link>
      <description><![CDATA[<p>Google検索の「AI Mode（AIモード）」に、広告（Sponsored）が直接統合されて表示される事例が11月中旬から海外ユーザーにより相次いで報告されている。いずれのスクリーンショットでも、AIが生成した回答の直下に広告ユニットが挿入されており、表示形式が従来の検索広告とは異なる点が確認された。</p>
<p>こうした事例は主に英語圏の検索業界関係者から報告されており、SEOコンサルタントの Brodie Clark 氏 が2025年11月21日（現地時間）に投稿した事例をはじめ、複数のユーザーが X（旧Twitter）で広告表示のスクリーンショットを共有している。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/brodie_the_time_has_come_b63a519136/brodie_the_time_has_come_b63a519136.jpg" alt="brodie the time has come.jpg" /></p>
<p>Googleは2025年5月21日、<a href="https://blog.google/products/ads-commerce/google-search-ai-brand-discovery/">公式ブログ</a>でAI Modeにおける広告テスト開始の方針を明らかにしており、今回の報告はそのテストが検索画面上で可視化され始めたものとみられる。</p>
<h2>AIモードの広告表示、Googleは5月にテスト開始を公式に発表</h2>
<p>5月21日の公式ブログ「<a href="https://blog.google/products/ads-commerce/google-search-ai-brand-discovery/">More opportunities for your business on Google Search</a>」では、AI OverviewsおよびAI Modeを新たな検索体験として位置づけるとともに、AI Modeで広告のテストを開始したと明記されている。広告はAI回答の下部に自然に統合され、Performance Max や Shopping Ads を含む既存キャンペーンが表示対象となることも説明されていた。</p>
<p>また、Google広告製品担当の公式アカウント Ads Liaison（Ginny Marvin 氏） は11月21日、Brodie Clark 氏の投稿に対し「AI Modeの広告は、5月のGoogle Marketing Liveでの発表以降、数カ月にわたりテストを続けている」と<a href="https://x.com/adsliaison/status/1991969383414210845">返信</a>。このコメントから、今回の広告表示は「新規ローンチ」ではなく 既存テストの一環 として位置づけられていることがわかる。</p>
<p>Clark氏はこれに対し、公式アナウンスから実際の画面で目撃されるまでには時間差があることを指摘し、自身は「新機能が“野生で初めて見つかるタイミング”を重視している」と補足した。</p>
<h2>最初の報告例：Greg Sterling 氏がHVAC検索で広告表示を確認</h2>
<p>11月19日、マーケティングアナリスト Greg Sterling 氏（@gsterling） は、AI Modeで「HVAC repair」の検索を行った際に広告が表示されたスクリーンショットをXに投稿した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/saw_first_ads_in_aimode_c50debed95/saw_first_ads_in_aimode_c50debed95.jpg" alt="saw first ads in aimode.jpg" /></p>
<p>スクリーンショットでは、AIの回答直下に「Sponsored」と表示されたローカルサービス広告が2件並んでおり、AI Mode内に広告ユニットが直接組み込まれている様子が確認できる。Sterling 氏の例は「最初に目撃されたケース」として複数の海外メディアで紹介された。</p>
<h2>Brodie Clark 氏が“通常UI”での広告表示を報告、Labs以外にも拡大か</h2>
<p>Sterling 氏のケースが「Labsインターフェース（実験機能）」での表示だったのに対し、Brodie Clark 氏は Labsではない通常の検索UI でもAI Mode内に広告が表示されることを確認した点が特徴的だ。</p>
<p>Clark 氏は複数の検索（ベッドシーツ比較や emergency plumber など）で広告を再現し、表示位置はいずれも AI回答の最下部 だったと説明している。これにより、広告表示が「Labs限定」ではなく、一部環境では一般ユーザーのインターフェース上でも行われていることが判明した。</p>
<h2>AI Overviewsに続き、広告統合はAI Modeへも拡大か</h2>
<p>GoogleはAI Overviewsでも広告統合のテストを開始しており、表示形式（回答最下部への自然な挿入）は今回のAI Modeの例と共通している。</p>
<p>現時点で、GoogleはAI Modeでの広告表示の地域展開や時期に関する追加アナウンスを行っていない。テストは一部ユーザー環境で継続しているとみられる。日本語UI（google.co.jp）や日本のユーザー環境で、AI Mode内に広告が表示された報告は現在のところ確認されていない。</p>
]]></description>
      <pubDate>Thu, 27 Nov 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI、ChatGPTに商品調査・比較を自動化する「ショッピング リサーチ」追加──ホリデーシーズンに向け“ほぼ無制限”で提供、Perplexityも同種機能を発表</title>
      <link>https://ledge.ai/articles/chatgpt_shopping_research_openai_perplexity_launch</link>
      <description><![CDATA[<p>OpenAIは2025年11月24日（現地時間）、ユーザーの購入検討を支援する新機能「shopping research」（日本では「ショッピング リサーチ」）をChatGPTに追加したと<a href="https://openai.com/index/chatgpt-shopping-research/">発表</a>した。</p>
<p>自然言語で希望条件を伝えるだけで、AIが商品調査や比較分析を行い、候補を絞り込む仕組みだ。</p>
<p>同機能は、ユーザーが用途や予算、こだわり条件などを入力すると、ChatGPTが必要に応じて質問を重ねながらニーズを把握し、ネット上の情報をもとに候補を提示する。提示形式は「バイヤーズガイド」としてまとめられ、商品ごとの特徴や検討ポイントも整理される。</p>
<p>対象プランはChatGPT Free、Go、Plus、Pro。ログイン済みユーザーに対して、モバイルアプリとWeb版を通じて順次提供を開始した。OpenAIは、ホリデーシーズンに向けて「ほぼ無制限で利用可能」としている。</p>
<p>調査内容には、商品仕様、レビュー、価格、画像などが含まれるが、OpenAIは「価格や在庫情報、レビューの正確性に誤りが生じる可能性がある」と明記しており、最終確認はユーザーが行う必要があると注意喚起している。また今後は、対応するリテーラーにおいてチャット内で購入を完結できる「Instant Checkout」との連携も予定する。</p>
<p>一方、米Perplexityも11月25日（現地時間）、無料のショッピング機能を米国のすべてのユーザー向けに提供開始したと<a href="https://www.perplexity.ai/ja/hub/blog/shopping-that-puts-you-first">発表</a>した。</p>
<p>@<a href="https://www.youtube.com/watch?v=3Z6_ogRyyHc">Youtube</a></p>
<p>商品カテゴリや条件に基づく検索や比較、候補提示を行えるほか、PayPalとの提携により、PayPalに対応する加盟店についてはPerplexity上でそのまま決済まで完了できる。決済処理はPayPalが担い、加盟店は従来どおりマーチャント・オブ・レコードとして顧客との関係や返品対応を保持する。OpenAIの発表から一日後の提供開始となり、主要AI企業が相次いで“商品選び支援”機能を投入した形だ。</p>
]]></description>
      <pubDate>Thu, 27 Nov 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>米アマゾン、米政府向けAI・スーパーコンピュータ基盤に最大500億ドル（約7.8兆円）投資──AWSの機密クラウド全レベルで1.3GWを増強</title>
      <link>https://ledge.ai/articles/amazon_us_government_ai_supercomputing_50b_investment</link>
      <description><![CDATA[<p>Amazonは202511月24日（米国時間）、米政府機関向けにAIおよびスーパーコンピューティング基盤を拡張するため、米国内のデータセンターに最大500億ドル（約7兆8000億円）を投資すると<a href="https://www.aboutamazon.com/news/company-news/amazon-ai-investment-us-federal-agencies">発表</a>した。
投資は2026年に着工予定で、AWS Top Secret、AWS Secret、AWS GovCloud（US）など政府専用クラウドの全レベルにまたがって、約1.3ギガワット分の計算能力を追加する計画だ。</p>
<p>発表では、連邦政府機関が利用できるAIサービスも幅広く示されている。Amazon SageMakerによる学習・カスタマイズ、Amazon Bedrockを通じた基盤モデル活用、同社の新世代モデル「Amazon Nova」、Anthropic Claude、各種オープンウェイトモデルなどが挙げられ、これらをAWS TrainiumやNVIDIAのAIインフラストラクチャと組み合わせて利用できるとしている。</p>
<p>ユースケースとしては、大規模データの統合解析やシミュレーションをAIで加速させる内容が説明された。数十年分のグローバル・セキュリティデータをリアルタイムに分析し複雑なパターンを抽出する取り組みや、衛星画像・センサーデータを統合した脅威検知の高度化などが例示され、防衛・インテリジェンス、サイバーセキュリティ、医療・ヘルスケア研究など多様な領域に適用される見込みだ。</p>
<p>AWSのCEOであるマット・ガーマン氏は、政府専用設計のAI・クラウドインフラへの大規模投資が、連邦政府機関におけるスーパーコンピューティングの活用を根本的に変えると強調した。サイバーセキュリティや創薬などの重要ミッションを加速し、AI時代における米国のリーダーシップをさらに強固にするものだと述べている。</p>
<p>Amazonは今回の投資を、政府向けクラウドインフラの歴史的な延長線上に位置づける。2011年のAWS GovCloud（US-West）、2014年のAWS Top Secret-East、2017年のAWS Secret Regionなど、機密区分に対応したクラウドリージョンを段階的に整備してきた経緯が示され、オンプレミス環境に依存してきた政府機関がより柔軟にAI・HPCリソースを活用できるようになると説明している。</p>
<p>AWSはあわせて、政府向けAI活用事例や技術解説をまとめた特設ページ「America AI」へのリンクも提示している。今回の発表では、拡張後の計算能力やAIサービスによって、米政府の各種ミッションを支援するための基盤を段階的に整備していく方針が示された。</p>
]]></description>
      <pubDate>Wed, 26 Nov 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>DMM傘下Algomatic、「にじボイス」を2026年2月4日に終了──日俳連の削除要請（計53体）受領を経て決断</title>
      <link>https://ledge.ai/articles/algomatic_nijivoice_service_end_2026_0204</link>
      <description><![CDATA[<p>DMMグループでAI関連サービスを手がけるAlgomaticは2025年11月21日、AI音声生成プラットフォーム「にじボイス」の提供を2026年2月4日で終了すると<a href="https://algomatic.jp/news/notice_nijivoice_20251121">発表</a>した。同社は「突然のお知らせとなったことをお詫び申し上げる」とし、今後の詳細なスケジュールについては改めて案内するとしている。</p>
<p>リリースによると、Algomaticは2025年9月29日に日本俳優連合（日俳連）から、同サービスで提供していた33体のキャラクターボイスについて「組合所属の実演家の音声実演に似ている」との指摘と削除要請を受けた。同社が社内で再調査した結果、「法的な権利侵害は確認されなかった」としつつも、懸念を与えることは本意ではないとして33体の提供を終了した。</p>
<p>その後も運営を継続していたが、11月17日に日俳連から追加で20体のキャラクターボイスへの削除要請を受領。こちらも調査では法的な権利侵害は確認されなかったが、Algomaticは「声の権利を守るためのプラットフォームとして立ち上げた経緯」を踏まえ、本人が不安を感じる状況に対し「主観である」と反論することは適切ではないと判断したとして、サービス全体の終了を決定したと説明している。</p>
<p>また同社は、「にじボイス」終了について批判が寄せられる可能性を認識しつつも決断は自社の責任であるとし、日俳連への誹謗中傷を控えるよう利用者に呼びかけた。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/jau_nichihairen_c09e486b33/jau_nichihairen_c09e486b33.jpg" alt="jau nichihairen.jpg" /></p>
<p>日本俳優連合は11月17日、公式X（旧Twitter）で「にじボイス」に対し、組合員声優に酷似した音声コンテンツ20件の削除要請（第2弾）を提出したと発表。「AI時代だからこそ、声優が安心して演技に向き合える環境づくりを進める」とコメントしている。</p>
]]></description>
      <pubDate>Wed, 26 Nov 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI、「ChatGPT for Teachers」を無償提供──米国K-12教員向けに“教育専用ワークスペース”を開放</title>
      <link>https://ledge.ai/articles/chatgpt_for_teachers_us_k12_free_workspace</link>
      <description><![CDATA[<p>OpenAIは2025年11月19日、米国のK-12教員・学校スタッフ向けにカスタマイズされた教育用AI「ChatGPT for Teachers」の提供を<a href="https://openai.com/index/chatgpt-for-teachers/">発表</a>した。対象となる教員・関係者は、2027年6月まで無料で利用できる。専用ワークスペースを備え、教材作成から学区単位の協働管理まで、学校現場での利用に特化した機能が特徴だという。</p>
<p>サービスは、認定された米国のK-12学校に所属する教員・スタッフ・学校リーダー、学区管理者を対象としたもの。利用にはSheerIDによる在職確認が必要で、資格が確認された教育者は専用の「Teachers ワークスペース」を作成できる。学生アカウントについては、今後の提供を見据えて準備中とのこと。</p>
<h2>教材作成・レッスン設計を支援するフル機能のChatGPT</h2>
<p>「ChatGPT for Teachers」では、OpenAIのフラッグシップモデルに加え、ファイルアップロード、Google DriveやMicrosoft 365との連携、画像生成、音声モード、データ分析、ブラウジングなど、一般版ChatGPTが提供する主要機能をそのまま利用できる。</p>
<p><a href="https://openai.com/index/chatgpt-for-teachers/">公式ページ</a>では、単元計画、学習目標に応じたレッスン案、ワークシート生成、生徒向け課題の分岐作成、保護者向け文書の翻訳など、教師の業務を支援する具体的な活用例が示されている。教師が自身のスタイルや生徒情報を入力すれば、回答を継続的にパーソナライズする「メモリ」機能も利用可能だ。</p>
<h2>学校・学区単位の導入を想定したワークスペースと管理機能</h2>
<p>教育機関向けに設計された「Teachers ワークスペース」は、教材・指示文・共同プロジェクトなどの共有を前提としたコラボレーション機能を備える。教員同士でテンプレートを共有したり、共同でカスタムGPTを作成したりすることが可能だ。</p>
<p>管理者向けには、ドメインクレーム（学校ドメインの登録）、ロールベースのアクセス制御（RBAC）、メンバー管理、SAML SSOなどの機能が提供される。学区全体での展開を想定した構成で、大規模な導入を希望する学校・学区向けには追加相談窓口が用意されている。</p>
<h2>学習データへの不使用と暗号化を含むデータ保護</h2>
<p>OpenAIは、教育者向けアカウントで共有された内容はデフォルトでAIモデルの訓練に使用しないと説明している。データは保存時・転送時の双方で暗号化され、MFA・SSOなどのアカウント保護機能も利用できる。</p>
<p>また、米国の教育記録保護法（FERPA）に準拠する形で、学校・学区が必要とするコンプライアンス要件に対応しているとされる。会話データは、不正利用の調査など限定的な目的で権限を持つ担当者がアクセスする場合があるが、範囲は管理されたものだとしている。</p>
<h2>利用開始の方法</h2>
<p>利用を希望する教員は、公式ページから資格確認を行うことで、自身のワークスペースを開設できる。学区管理者は、組織全体のワークスペースを立ち上げ、教員を一括管理することも可能だ。</p>
<p>OpenAIは、無料提供期間終了後（2027年6月以降）について、価格や条件に変更が生じる場合は事前通知するとしており、「教育者にとって手頃な価格を維持することを目指す」と説明している。</p>
]]></description>
      <pubDate>Wed, 26 Nov 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google、Geminiに「AI生成画像の検証」機能を導入──目に見えない電子透かし技術「SynthID」を活用</title>
      <link>https://ledge.ai/articles/google_gemini_synthid_image_verification</link>
      <description><![CDATA[<p>Googleは2025年11月20日、Geminiアプリにおいて、画像をアップロードして「この画像はAIで生成または編集されたものか」を判定できる新機能を<a href="https://blog.google/technology/ai/ai-image-verification-gemini-app/">公開</a>した。同社は2023年に、画像の見た目に影響を与えず識別情報を埋め込む電子透かし（Invisible watermark）技術「SynthID」を開発しており、今回のアップデートにより、この電子透かしをGeminiアプリ上で直接検出できるようになる。</p>
<h2>AI生成画像の真偽をアプリ上で判定</h2>
<p>Geminiアプリでは、ユーザーが画像をアップロードし、「Was this created or edited by Google AI?」といった質問を投げかけることで、画像にSynthIDの電子透かしが含まれているかを判定できる。
電子透かしが検出された場合は、GoogleのAIによって生成または編集された可能性が高いと返答される。一方で電子透かしが見つからない場合は、「GoogleのAIによるものではない可能性が高い」と示される。ただし、他社のAIモデルによる生成画像まで識別できるわけではなく、あくまでGoogleのAIに適用された電子透かしのみが検知対象となる。</p>
<h2>利用条件と注意点</h2>
<p>Googleサポートページによれば、検証は1枚ずつ（100MB以下）の画像に対して実施され、コラージュ画像など複数の画像を含む形式は推奨されない。また、電子透かしが画像全体ではなく一部に適用されている場合、AIで編集された領域が検出されることもある。</p>
<h2>「SynthID」とは</h2>
<p>SynthIDは、画像の外観に影響を与えず、機械的に読み取ることができる目に見えない電子透かし（Invisible watermark）技術を採用している。Geminiアプリは、この電子透かしをもとに生成・編集プロセスを判定する仕組みを持つ。
公式ブログでは、Googleが今後、画像だけでなく動画や音声への対応拡大も計画していることが示されている。</p>
<h2>Googleの透明性強化の取り組み</h2>
<p>GoogleはSearchにおけるAI生成画像のラベル表示を拡充しているほか、C2PA（Content Provenance and Authenticity）に準拠したメタデータ対応も進めている。公式ブログによれば、Geminiアプリ・Vertex AI・Google Adsで生成される画像については、今週よりC2PA準拠のメタデータを自動付与する対応を開始するという。対象となるのはNano Banana Pro（Gemini 3 Pro Image）で生成された画像で、生成プロセスに関する追加情報を示すことで透明性を高めるとしている。</p>
]]></description>
      <pubDate>Tue, 25 Nov 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>KADOKAWA×はてな「カクヨム」、生成AI利用作品に3種のタグ付けを推奨──“本文50％以上／50％未満／補助利用”を明確化</title>
      <link>https://ledge.ai/articles/kakuyomu_ai_tag_guideline_update</link>
      <description><![CDATA[<p>小説投稿サイト「カクヨム」を共同運営するKADOKAWAとはてなは2025年11月19日、生成AIを利用した作品投稿について、利用状況に応じたタグ付けを推奨すると<a href="https://kakuyomu.jp/info/entry/geneai_tag">発表</a>した。対象となるタグは「AI本文利用」「AI本文一部利用」「AI補助利用」の3種類で、本文中のAI生成割合やAIの関与範囲に応じて区分を設ける。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/kakuyomu_genai_tag_0e3bbfab95/kakuyomu_genai_tag_0e3bbfab95.jpg" alt="kakuyomu genai tag.jpg" /></p>
<p>発表によると、「AI本文利用」は本文の大半（目安50％以上）がAIによって生成された場合、または軽微な修正のみを加えて利用した場合に該当する。「AI本文一部利用」は本文の一部（目安50％未満）がAI生成の場合で、こちらも軽微な修正にとどまるケースを想定する。「AI補助利用」は、AIで得たアイデアや資料をもとに作者が本文を書く場合や、作者が書いた文章の校正など、創作の補助的にAIを使用した場合を指す。3つの基準はあくまで目安とし、投稿者は自身の利用実態に合わせて選択する。</p>
<p>11月21日には、「AI補助利用」の適用範囲に関する問い合わせが多数寄せられたとして、公式サイトに追記を行った。追記では、作品執筆の一環として作者が意識的に生成AIツールを使用した場合を想定していると説明。一方で、検索サービスなど日常的に利用するツールのAI要約機能や、文書ソフトの自動校正機能を用いた作品は該当しないとしている。また、投稿者に対して生成AI利用の具体的内容を示す必要はないとも明記した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/kakuyomu_ai_tag2_e153549b82/kakuyomu_ai_tag2_e153549b82.jpg" alt="kakuyomu ai tag2.jpg" /></p>
<p>なお、KADOKAWAが主催する日本最大級の小説コンテスト「カクヨムコンテスト11」に応募する作品については、生成AIを利用している場合、該当タグの付与が必須とのこと。</p>
]]></description>
      <pubDate>Tue, 25 Nov 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>xAI、サウジアラビアと500MW超の次世代AIデータセンターを共同開発──Grokを国家規模で展開へ</title>
      <link>https://ledge.ai/articles/xai_humain_saudi_500mw_grok_national_ai_layer</link>
      <description><![CDATA[<p>サウジアラビア政府系AI企業のHUMAINは2025年11月19日、米ワシントンD.C.で開催された U.S.–Saudi Investment Forum 2025 において、米xAIとの大型フレームワーク契約を<a href="https://www.humain.com/en/news/humain-and-xai-partner-to-build-next-generation-ai-compute-power-and-deploy-grok-in-the-kingdom-to-support-the-most-ai-enabled-nation-objectives">発表</a>した。両社はサウジアラビア国内に、500MW超のフラッグシップ施設を核とした複数拠点のGPUデータセンター網を共同で開発し、xAIの大規模言語モデル「Grok」を国家規模で展開する。</p>
<h2>次世代AIデータセンター網を共同構築</h2>
<p>HUMAINとxAIは、複数のGPUデータセンターで構成される次世代のハイパースケールAIインフラを共同設計・建設・運用する。フラッグシップとなる500MW超の施設は、「世界でも最も高度なAIコンピュートハブの一つ」と位置づけられている。</p>
<p>今回のプロジェクトは、xAIにとって米国外で初めての大規模コンピュート展開となり、同社が既に保有する米国内スーパーコンピュータ群を補完する計算基盤として機能するという。</p>
<p>HUMAINは、AIインフラの設計から建設・運用までを低コストで提供する能力を強みとしており、xAIはフロンティアAIモデルおよび大規模演算最適化の技術を持つ。両社はこれらを組み合わせることで、世界規模でスケール可能な次世代AI開発基盤を整備するとしている。</p>
<h2>Grokを全国展開し“国家AIレイヤー”を実装</h2>
<p>提携には、xAIの大規模言語モデル「Grok」をサウジアラビア全土で展開する計画も含まれ、HUMAINのエージェント基盤「HUMAIN ONE」と統合される。</p>
<p>これにより、以下の機能が国家レベルで提供されるという。</p>
<ul>
<li>リアルタイムインテリジェンス</li>
<li>自律ワークフロー</li>
<li>AIコパイロット</li>
<li>行政・企業・社会をまたぐ意思決定支援</li>
</ul>
<p>HUMAINはこの仕組みを「統合されたナショナルAIレイヤー」と表現し、サウジアラビアの「Most AI-Enabled Nation」（世界で最もAI活用が進む国家）の実現を支える基盤になると説明している。</p>
<p>xAI創業者のElon Musk氏は、「Grokを一国全体に展開する初の取り組みであり、大規模で効率的なコンピュートと高度なAIモデルが未来の知能を形作る」と述べ、提携の意義を強調した。</p>
<h2>サウジアラビアのAI戦略</h2>
<p>発表が行われた U.S.–Saudi Investment Forum では、サウジアラビア政府系投資機関のINVEST SAUDIが「同国が“Oil factories（油の工場）”から“AI factories（AIの工場）”へ変革を進めている」と強調。国家規模のAIインフラを戦略的に整備していく姿勢が示された。</p>
<h2>フォーラムで示されたHUMAINのAIインフラ戦略</h2>
<p>同フォーラムでHUMAINは、NVIDIAとの戦略的パートナーシップを拡大し、今後3年間で最大60万GPUをサウジアラビアと米国の新データセンターに展開する計画も<a href="https://www.humain.com/en/news/humain-expands-strategic-partnership-nvidia">発表</a>した。</p>
<p>さらに、NVIDIA Nemotronを用いたアラビア語モデルの強化、Omniverseを活用した国家規模のデジタルツイン構築など、AWSやGlobal AI、xAIとともにグローバルなAIインフラ基盤づくりを進める枠組みも示されている。</p>
<p>これらの発表は、HUMAINが推進するグローバルAIインフラ戦略の一環として位置づけられている。</p>
<p>500MW超のデータセンター網の詳細仕様や着工時期はまだ公表されていないが、Grokの全国展開とあわせて、サウジアラビアのAI主導型国家戦略を支えるプロジェクトとして、今後はデータセンター網の具体像や運用計画に注目が集まりそうだ。</p>
]]></description>
      <pubDate>Tue, 25 Nov 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>英国、動物実験の段階的廃止ロードマップを発表──AIと臓器チップで2030年までに犬・霊長類の薬物試験削減へ</title>
      <link>https://ledge.ai/articles/uk_animal_testing_phaseout_roadmap_2030</link>
      <description><![CDATA[<p>英国政府は2025年11月11日、動物実験を代替法へ置き換えるための包括的戦略「Replacing Animals in Science Strategy」を<a href="https://www.gov.uk/government/news/animal-testing-to-be-phased-out-faster-as-uk-unveils-roadmap-for-alternative-methods">公表</a>した。臓器チップや3D細胞モデル、遺伝子ベースアッセイなど、多様な非動物技術（Non-Animal Technologies：NATs）の活用を体系的に進める内容で、この中にはAIによる毒性予測や計算モデルも含まれる。</p>
<p>戦略の実行を支えるため、政府と研究機関は総額約7,500万ポンドを投じる。AIを含む計算科学的手法は、初期スクリーニングの効率化に加え、薬物動態の検討段階でも補完的な役割を担うとされ、代替法を広げるうえで「重要な柱」と位置づけられている。</p>
<h2>2026〜2030年で段階的に移行</h2>
<p>今回公表されたロードマップには、具体的な削減目標が盛り込まれた。</p>
<ul>
<li>2026年末まで：皮膚刺激、眼刺激、皮膚感作などの安全性試験を非動物法へ全面移行</li>
<li>2027年まで：ボツリヌス毒素（ボトックス）強度試験におけるマウス試験を終了し、DNAベース評価法へ切り替え</li>
<li>2030年まで：犬・非ヒト霊長類を用いた薬物動態（PK）試験の依存度を削減</li>
</ul>
<p>ただし政府は、代替法が「同等の科学的妥当性と安全性を提供できる場合」に限り動物使用を停止するとしており、実施可能な領域から段階的に置き換えていく現実的なアプローチを取る。</p>
<h2>代替技術の実用化が追い風に</h2>
<p>今回の戦略策定の背景には、代替手法が実用段階へ近づいていることがある。</p>
<ul>
<li><strong>臓器チップ</strong> ：ヒト細胞で臓器環境を再現し、薬物応答を高精度に評価</li>
<li><strong>AIモデル</strong> ：分子構造や実験データから毒性を予測し、動物試験前のスクリーニングを効率化</li>
<li><strong>3Dバイオプリント組織</strong> ：肝臓や皮膚などの組織を立体的に再現し、化学物質の影響評価に利用</li>
</ul>
<p>これらの技術には英国企業・研究機関が積極的に取り組んでおり、産業競争力の強化にもつながると期待されている。</p>
<h2>英国の動物実験の現状</h2>
<p>内務省が公表した2023年の統計では、動物を用いた科学的手技は約268万件で、マウスやラット、魚類が大半を占める。猫・犬・霊長類の使用は全体の0.2％程度と少ないものの、医薬品開発における安全性評価など、依然として役割の大きい領域が残る。</p>
<p>特に犬・非ヒト霊長類のPK試験は、医薬品の臨床前評価で重要とされてきた領域であり、2030年を目標とした依存度の削減は、医薬品開発プロセスにおける大きな転換点といえる。</p>
<h2>完全な代替にはなお課題</h2>
<p>一方で、動物実験の完全廃止には技術的・規制的な課題が残る。</p>
<p>臓器チップやAIモデルでは、免疫系を含む全身の長期的な複合反応を完全に再現することは難しい。また、規制当局が代替法を受け入れるには、精度や再現性、妥当性の検証が不可欠で、導入には時間を要する。動物研究支援団体 Understanding Animal Research も「代替技術は進展しているが、多くの領域ではまだ動物を使わない方法が存在しない」と指摘している。</p>
<h2>資金投下と実行体制</h2>
<p>今回の約7,500万ポンドの投資は、研究開発だけでなく、規制受け入れや教育、データ標準化などを含む包括的な体制整備に充てられる。</p>
<ul>
<li>政府の6,000万ポンド：代替法の検証拠点や規制支援センターの整備</li>
<li>1590万ポンド（MRC・Wellcome Trustなど）：ヒト由来 in vitro モデルの研究支援</li>
</ul>
<p>政府は関係府省・研究機関・製薬企業・動物福祉団体で構成される委員会を設置し、進捗指標（KPI）を設定して透明性を確保する方針だ。英国政府は今回の戦略を「世界で最も詳細なロードマップの一つ」と位置づけ、科学技術と倫理の両面から「動物実験のない研究」への移行を進めていくとしている。</p>
]]></description>
      <pubDate>Mon, 24 Nov 2025 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>「普通のメガネ」のようで“静かに非凡”──情報を前面・背面に分離するレイヤードHUDとカメラレス設計、指先操作のR1が示すG2／R1のスマートグラス体験の次章</title>
      <link>https://ledge.ai/articles/quiet_tech_smartglasses_even_g2_r1_launch</link>
      <description><![CDATA[<p>ウェアラブルデバイスを手がける Even Realities は、2025年11月13日（現地時間）、新型スマートグラス「Even G2」とスマートリング「Even R1」を<a href="https://x.com/EvenRealities/status/1988629357036753060">発表</a>した。同社は「Quietly Extraordinary Technology」というコンセプトを掲げ、テクノロジーが生活に干渉しすぎず“静かに寄り添う”デザイン思想を中心に据えたことが説明されている。</p>
<p>外観は「普通のメガネ」さながらだが、視界の前面と背面に情報を分離して重ねる独自のレイヤードHUDや、カメラを排した設計、指先ジェスチャーで操作できるR1など、同社が掲げる「Quietly Extraordinary Technology」を体現する機能を備える。録画や音声出力を中心に進化してきた従来のスマートグラスとは異なり、G2／R1はユーザー本人の使いやすさと周囲のプライバシーの両立を追求した“静かな次章”を提示している。</p>
<p>@<a href="https://www.youtube.com/watch?v=tAIhp9hia90&amp;t=4s">YouTube</a></p>
<h2>カメラレス・スピーカーレスという選択</h2>
<p>G2 の最大の特徴は、カメラと外向きスピーカーを搭載しない点にある。近年のスマートグラスは録画や配信、音声出力を強化する方向で進化してきたが、同社は「ユーザーと周囲のプライバシーに配慮するため」として、これらをあえて採用していない。
音声操作や撮影を前提としないことで、外部への音漏れや“撮られているかもしれない”という不安を生まない構造として設計されている。</p>
<h2>手前と奥に情報を分けて表示する“レイヤードHUD”</h2>
<p>G2 の光学システムには、独自の「HAO 2.0」ディスプレイが採用された。超小型マイクロLEDプロジェクターと導波管レンズにより、約2メートル先に自然な形で情報が投影される。</p>
<p>特徴的なのは、表示情報を二層に分離する“レイヤードHUD”だ。</p>
<ul>
<li><strong>前面のレイヤー</strong> ：通知など即時性の高い情報</li>
<li><strong>奥のレイヤー</strong> ：翻訳、ナビゲーションなど継続的な情報</li>
</ul>
<p>というように、視界の中で情報の優先度や性質に応じて層を分ける。
これにより、HUDが“情報過多”になりがちな従来のスマートグラスと異なり、視界のノイズを増やすことなく必要なタイミングで必要な情報のみを提示する設計になっている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/HUD_9e8b648e77/HUD_9e8b648e77.jpg" alt="レイヤードHUD.jpg" /></p>
<h2>更新された主要機能と会話アシスト「Conversate」</h2>
<p>主要機能には、複数ウィジェットを扱うダッシュボード、地磁気センサーによるナビゲーション、リアルタイム双方向翻訳、テレプロンプターなどが含まれる。また、独自LLM「Even AI」の高速化により、音声操作やアプリ起動の応答が向上したという。</p>
<p>新たに追加された「Conversate」は、会話中に文脈に応じた補助情報を視界に表示する機能で、会議や対話の場面で用語の説明や要点整理などを控えめに提示する。</p>
<h2>スマートリング「R1」：操作とウェルビーイングを担う指先デバイス</h2>
<p>G2 の操作体系を補完するのが、スマートリング「Even R1」だ。リング内に静電容量式タッチパッドを内蔵し、タップ、スワイプ、長押しといった指先ジェスチャーでスマートグラスを操作する。素材は医療グレードのセラミックとステンレスを採用し、耐久性と装着性を両立した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/r1_5fbc5fd7f6/r1_5fbc5fd7f6.jpg" alt="r1.jpg" /></p>
<p>R1 にはウェルビーイング機能も搭載され、心拍数、血中酸素濃度、体温、睡眠、歩数、消費カロリーなどを計測。取得データから「生産性スコア」を算出し、その日の集中力傾向を把握する指標として利用できるという。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/wellbeing_2d6509b5cf/wellbeing_2d6509b5cf.jpg" alt="wellbeing.jpg" /></p>
<h2>プライバシーの設計思想</h2>
<p>Even Realities は、カメラを排除した設計に加え、データはユーザーの明示的な同意なしにクラウドへ保存されないと説明している。必要なデータは暗号化され、個人を特定可能な情報を保持しない方針を示している。</p>
<p>G2 と R1 は、録画・配信・音声出力を中心に進化してきた他社製スマートグラスとは異なる方向性を示す。
同社は、情報過多を避け、ユーザー本人の使いやすさと周囲のプライバシーを両立させる「Quiet Tech」アプローチを採用し、視界に重ねる情報をレイヤーで整理し、操作は指先で静かに完結させる設計を打ち出した。こうした設計思想は、スマートグラス市場における一つの選択肢として位置づけられそうだ。</p>
]]></description>
      <pubDate>Sun, 23 Nov 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>日本IBMとセガXD、生成AIを学べるカードバトル研修「バトルワーカーズ」──業務内容をAIがカード化し対戦形式で学習</title>
      <link>https://ledge.ai/articles/generative_ai_card_game_training_battle_workers_ibm_segaxd</link>
      <description><![CDATA[<p>日本IBMは2025年11月17日、生成AIの基礎やプロンプト設計、リスク理解をカードバトル形式で学べる企業向け研修サービス「Generative AI Card Game Training – バトルワーカーズ」を<a href="https://jp.newsroom.ibm.com/2025-11-17-generative-ai-card-game">発表</a>した。研修設計は、ゲーミフィケーション専門企業である株式会社セガ エックスディー（セガXD）が監修した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Press2_Generated_Cards_0743fa59b7/Press2_Generated_Cards_0743fa59b7.jpg" alt="Press2_GeneratedCards.jpg" /></p>
<h2>業務内容を入力するとAIがカード化</h2>
<p>参加者自身の「仕事内容」を入力すると、生成AIがその内容をもとに バトルカード3枚とサポートカード1枚 を自動生成する仕組みを採用。カードには業務内容を反映した特徴や強み、スコアが記載され、研修参加者はこれらのカードを用いて対戦形式のゲームを行う。</p>
<p>カード生成の結果は入力する文章内容に大きく左右されるため、参加者は自然と「どの情報を、どのように記述すればAIが適切に解釈するか」を体験的に学ぶことができる。プロンプトの書き方による出力の違いを、そのままゲームの手札として実感できる構造だ。</p>
<h2>プロンプト設計・リスク理解も体験的に学べる</h2>
<p>研修のねらいとして同社は、生成AIの仕組み理解に加え、AI実務で重要となるプロンプト設計スキルやハルシネーション対策、著作権・情報管理を含むリスク理解の習得を挙げている。</p>
<p>業務内容の入力時には、どこまで詳細を書けるのか、機密情報は避けるべきか、といった情報マネジメント意識が問われる。また、生成AIが業務を誤解してカード化するケースもあり、ハルシネーション（AIの誤生成）への注意点を議論する導入にもつながるという。</p>
<h2>生成AIリテラシー格差への対応</h2>
<p>企業における生成AI活用が急速に広がる一方で、社員間のリテラシー格差は依然として課題とされる。
日本IBMは、ゲームという直感的な学習形式を取り入れることで、初心者でも無理なく生成AIの特性を掴める研修として位置づけている。参加者が「自分の業務」を題材に学ぶため、通常の座学研修よりも実務への応用をイメージしやすい点も特徴だ。</p>
<p>セガXDは、ゲームデザインや参加意欲を高める仕組みづくりに強みを持つ企業で、今回の研修ではバトル形式の導入やカード構造の設計に協力した。</p>
<p>研修を通じて同社は、企業における生成AI活用の基礎力向上を図るとともに、今後もリテラシー教育コンテンツの拡充を進める方針だ。</p>
]]></description>
      <pubDate>Sun, 23 Nov 2025 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>AWS、「Kiro」を一般提供──仕様駆動のAIコーディングをIDEとターミナルで一元化</title>
      <link>https://ledge.ai/articles/aws_kiro_general_availability</link>
      <description><![CDATA[<p>AWSは2025年11月18日、AIエディター／開発ツール「Kiro（キロ）」の一般提供を<a href="https://aws.amazon.com/jp/blogs/news/introducing-kiro-cli/">発表</a>した。Kiroは、7月の<a href="https://ledge.ai/articles/aws_ai_ide_kiro_release">プレビュー版公開</a>から機能追加を重ねてきたAI開発ツールで、仕様（Spec）を中心にコード生成・検証・リファクタリングを行える点が特徴だ。一般提供に合わせて、プロパティベーステスト、エージェント動作の巻き戻し機能、ターミナル向けの「Kiro CLI」、AWS IAM連携のチーム管理機能を搭載したという。</p>
<h2>仕様に基づく開発を強化：プロパティベーステストに対応</h2>
<p>Kiroの正式版では、仕様と実装の整合性を検証するための「プロパティベーステスト（property-based testing）」が導入された。開発者がEARS形式で記述した仕様から、Kiroがテストすべき“性質（プロパティ）”を自動抽出し、数百〜数千のランダムテストを生成して実装を検証する仕組みだ。反例が見つかった場合は、原因特定のため条件を徐々に単純化する「縮小（shrinking）」も実行される。</p>
<p>これにより、個別のテストケースでは検出しづらい欠陥を発見しやすくなり、仕様ベースでの正確な実装を支援する。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/correctness_f57a485373/correctness_f57a485373.png" alt="correctness.png" />
Kiro IDEが生成したプロパティベーステストの例。仕様から抽出した“チケットIDが常に一意であること”という性質を可視化し、関連する要件・実装タスク・テスト結果を紐づけて表示する。</p>
<h2>チェックポイントで開発プロセスを巻き戻し可能に</h2>
<p>一般提供版では、エージェントが行った操作や変更内容を段階的に保存する「チェックポイント」機能を追加した。エージェントの提案やコード修正を進めた後でも、必要に応じて任意の状態へ戻れるため、実装方針の変更や手戻りが発生した際のコストを抑えられる。</p>
<p>また、1つのKiroワークスペースで複数のプロジェクトルートを扱える「マルチルート」構成にも対応し、マイクロサービスやモノレポなど複雑な構成の開発にも利用できる。</p>
<h2>IDEとターミナルを共通化する「Kiro CLI」</h2>
<p>正式版の大きな追加点として、ターミナルで利用できる「Kiro CLI」が提供された。開発者は、IDE と同じエージェント環境をターミナルでもそのまま利用でき、Claude Sonnet 4.5 や Haiku 4.5 などのモデル、Autoエージェント、MCP（Model Context Protocol）ツールによるローカル操作やAPI呼び出しも実行できる。</p>
<p>CLIとIDEは同一のステアリングファイルや認証情報、コンテキストを共有するため、開発環境を切り替えても同じワークフローを継続できる。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/cli_5921207104/cli_5921207104.png" alt="cli.png" />
一般提供版で新たに追加された「Kiro CLI」。IDEと同じエージェント環境をターミナルでも利用でき、AutoエージェントやMCPツールによる操作が可能になる。</p>
<h2>AWS IAM連携のチーム管理と、スタートアップ向け1年無償プラン</h2>
<p>Kiroは、AWS IAM Identity Centerと統合され、組織アカウントによるログインや、Pro／Pro+／Powerティアの利用制限、MCP設定、請求管理などをAWS側で一元的に管理できる。</p>
<p>また、シリーズBまでの対象スタートアップには「Kiro Pro+」を1年間無償提供するオファーを開始した。既存のAWS Activateクレジットと併用可能で、2025年12月31日まで申請を受け付ける。</p>
<h2>“AI駆動開発”の基盤へ</h2>
<p>Kiroは、仕様駆動の開発、プロパティベーステスト、IDE／CLI共通のAIエージェント環境、IAM連携によるチーム運用など、開発プロセス全体をAIと統合する機能を備えた。Kiro公式ブログでは「This is just the start.」と記されており、今後もアップデートを継続する方針を示している。</p>
]]></description>
      <pubDate>Sat, 22 Nov 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Anthropic、「Claude Opus 4.5」を発表──ソフトウェア開発・エージェント性能・安全性を強化した最新フラッグシップモデル</title>
      <link>https://ledge.ai/articles/claude_opus_4_5_release</link>
      <description><![CDATA[<p>Anthropicは2025年11月24日（米国時間）、最新フラッグシップモデル「Claude Opus 4.5」を<a href="https://www.anthropic.com/news/claude-opus-4-5">発表</a>した。同社はOpus 4.5を「コーディング、エージェント、コンピュータ利用において世界最高水準のモデル」と位置付けており、ソフトウェア開発から長時間のリサーチ、マルチツールを用いたエージェントタスクまで幅広い領域で性能が向上したと説明している。</p>
<h2>ソフトウェアエンジニアリング分野で主要モデルを上回るスコア</h2>
<p>Opus 4.5はソフトウェアバグ修正タスクのベンチマークである SWE-bench Verified（n=500） で、正答率80.9％を記録した。これは、前世代モデルのOpus 4.1やSonnet 4.5に加え、他社のフラッグシップモデルであるGemini 3 ProやGPT-5.1 Codex-Maxなどを上回るスコアだという。</p>
<p><strong>SWE-bench VerifiedにおけるOpus 4.5（左端）の正答率は80.9％と主要モデルを上回った</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/opus4_5_1_2c2dfa96a8/opus4_5_1_2c2dfa96a8.jpg" alt="opus4-5_1.jpg" /></p>
<h2>総合ベンチマークでも全面的に強化</h2>
<p>ソフトウェアエンジニアリング以外の評価でも幅広くスコアを伸ばしている。Anthropicが公開した比較表では、エージェント的なコーディング、ターミナル操作、ツール利用、PC操作、さらに大学院レベルの推論（GPQA Diamond）や視覚推論（MMMU）、多言語QA（MMMLU）など、多くの項目でOpus 4.1やSonnet 4.5からスコアを更新していることが示されている。</p>
<p><strong>Opus 4.5は広範な評価指標で前世代を上回り、複数の領域でSOTA（state of the art）を獲得した</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/opus4_5_2_4cb9404311/opus4_5_2_4cb9404311.jpg" alt="opus4-5_2.jpg" /></p>
<h2>多言語コーディングでも広範に改善</h2>
<p>Opus 4.5は、SWE-bench Multilingual にも対応し、C / C++ / Go / Java / JS/TS / PHP / Ruby / Rust など8言語で前世代を上回るPASS@1を記録した。</p>
<p><strong>多言語コーディング評価（SWE-bench Multilingual）。Opus 4.5（赤）は8言語の多くで最高スコアを記録した</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/opus4_5_3_cfc4fb910a/opus4_5_3_cfc4fb910a.jpg" alt="opus4-5_3.jpg" /></p>
<h2>エージェント行動・PC操作・ツール利用能力も大幅向上</h2>
<p>Opus 4.5は、エージェント行動やマルチツールを扱うタスクでも大幅に性能を向上させている。</p>
<ul>
<li>Terminal-bench 2.0（ターミナル操作）で59.3％</li>
<li>t2-bench（ツール利用）でRetail 88.9％、Telecom 98.2％</li>
<li>OSWorld（PC操作）で66.3％</li>
</ul>
<p>これらのスコアは前世代からの大きな改善で、実務的なエージェントタスクに必要な能力が全体的に底上げされている。</p>
<p><strong>Sonnet 4.5 vs Opus 4.5 のパズル解法デモ</strong>
Opus 4.5のエージェント能力を示す例として、Anthropicは「Sonnet 4.5とOpus 4.5に同じパズルゲームを解かせた」デモ動画も公開している。Opus 4.5は条件制約のある環境でも自律的に手順を探索し、より安定してクリアする様子が示されている。</p>
<p>@<a href="https://www.youtube.com/watch?v=2MJDdzSXL74">YouTube</a></p>
<h2>安全性評価：「懸念行動」を抑えたモデル設計</h2>
<p>AnthropicはOpus 4.5を「安全性に最も優れたモデル」と位置付けている。内部評価では、危険行動や望ましくない自律動作につながる可能性を測る Concerning behavior（懸念行動） の指標で、Opus 4.5が最も低い値となった。</p>
<p><strong>懸念行動の内部評価。Opus 4.5が最も低く、安全性指標で優位性を示す</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/opus4_5_4_b32e228495/opus4_5_4_b32e228495.jpg" alt="opus4-5_4.jpg" /></p>
<h2>プロンプトインジェクション攻撃への耐性</h2>
<p>外部評価機関Gray Swanによる強力なプロンプトインジェクション攻撃テストでも、Opus 4.5は攻撃成功率が最も低く抑えられた。Gemini 3 Pro Thinking、GPT-5.1 Thinking、Haiku 4.5 Thinkingなどと比較しても、相対的に高い安全性が確認されている。</p>
<p><strong>プロンプトインジェクション攻撃への耐性（低いほど安全）。Opus 4.5 Thinkingが最も低い攻撃成功率を示した</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/opus4_5_5_c58f60c1ea/opus4_5_5_c58f60c1ea.webp" alt="opus4-5_5.webp" /></p>
<h2>長時間タスクとツール連携を支える「advanced tool use」</h2>
<p>今回の発表では「advanced tool use」についても触れられている。これはDeveloper Platformにおける機能群で</p>
<ul>
<li>必要なタイミングでツール定義を検索・読み込む <strong>Tool Search Tool</strong></li>
<li>Pythonコードの中で複数のツールを連携させる <strong>Programmatic Tool Calling</strong>
などにより、長時間タスクや複数ツールを扱うエージェントの効率を高める。</li>
</ul>
<p>Opus 4.5はこうした基盤を前提に設計されており、企業利用・実務エージェント運用への対応を強化したモデルと位置付けられる。</p>
<p>AnthropicはOpus 4.5の開発背景や設計思想をまとめた公式動画も公開している。同社が強調する「エージェント運用」「高い安全性」「コーディング性能の向上」といったポイントを視覚的に確認できる。</p>
]]></description>
      <pubDate>Sat, 22 Nov 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>“自分の分身AI”同士を討論させると何が起きる？──筑波大学とMicrosoft、AIが媒介する新しい「自己省察」の学びを報告</title>
      <link>https://ledge.ai/articles/digital_human_debates_reflecting_with_ai</link>
      <description><![CDATA[<p>筑波大学とMicrosoftの研究者らは2025年11月17日、利用者自身の価値観や思考パターンを反映した「分身AI（Digital Human）」同士を討論させ、本人がその様子を傍観するという実験の結果をまとめた論文を<a href="https://arxiv.org/abs/2511.13046">公開</a>した。研究チームは、参加者がAIの能力理解を深めただけでなく、「自分ではない自分」を観察することで、自身の思考や価値観を客観的に見つめ直す新たな学習効果を確認したという。</p>
<h2>“自分の分身”を作り、その議論を観察するという新しい構図</h2>
<p>研究が扱ったのは、次の3つの視点の中間に位置する体験である。</p>
<ul>
<li>一次的視点（First-person）：自分が他者と直接対話する</li>
<li>三人称視点（Third-person）：第三者同士の会話を観察する</li>
<li>今回の視点（AI-mediated）：自分を反映した「Digital Me」が他人の「Digital You」と議論し、その様子を本人が観察する</li>
</ul>
<p>実験では、学生が自ら設計した分身AIが、自律的に討論を展開する。設計者の価値観や思考は反映されるが、議論の流れは完全には制御できない。この「似ているが、完全な自分ではない」という距離感が、客観的な自己観察を可能にするという。</p>
<p><strong>コミュニケーション構造の比較図</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x2_7_2d4879fc05/x2_7_2d4879fc05.png" alt="x2 (7).png" /></p>
<h2>Digital Human Debates（DHD）の設計：プロンプトとRAGで“自分”を埋め込む</h2>
<p>参加したのは中高生9名（3チーム）。それぞれが6か月にわたり、自分の分身AIを次の3要素で設計した。</p>
<ul>
<li>system_prompt_template.txt：性格・口調・議論戦略・思考様式</li>
<li>interview_transcript.txt：個人の価値観・経験・背景</li>
<li>Documents（RAG 文書）：議論に利用する外部知識（両立場の資料を収集）</li>
</ul>
<p>システムは GPT-4o と LangChain を基盤に構築され、音声入力→議論生成→音声合成→Lip-sync 動画生成までを自動化。</p>
<p><strong>システム構成図</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x4_1_5978703b2d/x4_1_5978703b2d.png" alt="x4 (1).png" /></p>
<h2>どのように討論したのか</h2>
<p>議論のトピックは、以下の4種類からルーレットでランダム選択された。</p>
<ul>
<li>高齢者の免許返納</li>
<li>リモートワークの恒久化</li>
<li>安楽死の合法化</li>
<li>ベーシックインカムの導入</li>
</ul>
<p>討論は約20分。Constructive speech、Cross-examination、Rebuttal など、全国高校ディベート選手権の形式に準じて進行し、勝敗は3名の審査員が判定した。</p>
<p><strong>討論フロー図</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x5_3_aa3dfafba5/x5_3_aa3dfafba5.png" alt="x5 (3).png" /></p>
<p>また、実際の議論分析では、分身AIの発話は以下の3層で構成されていたことが確認された。</p>
<ul>
<li>Personal Context：学生本人の経験や価値観（例：留学経験）</li>
<li>RAG-sourced Evidence：外部文書を参照した事実情報</li>
<li>AI-generated Insight：LLM が独自に構築した新しい主張・質問</li>
</ul>
<p><strong>発話構成の三層分析図</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x6_2_ebcfc4cb09/x6_2_ebcfc4cb09.png" alt="x6 (2).png" /></p>
<h2>発見1：AIが“自分の代わり”ではなく“自分の鏡”として機能する</h2>
<p>論文で最も強調されている成果は、参加者が 「Reflecting with AI」 と呼ばれる新たな学習体験を得た点である。</p>
<h3>「自分が話すより冷静に見られる」</h3>
<p>AIが自分の価値観を用いて議論するものの、その展開は必ずしも本人の想定どおりではない。
この“半自律性”が、次のような客観視を可能にした。</p>
<ul>
<li>「私は衝動的に話しがちだと気づいた」</li>
<li>「AIの方が論理がブレず、どこが弱いのかがわかった」</li>
</ul>
<p>参加者は AI の議論を自分とは別の存在として受け止めつつ、「しかし自分の思考が反映されている」と感じる。この微妙な距離感が、強いメタ認知効果を生んだという。</p>
<h2>発見2：プロンプト設計の違いが“AIの人格”に明確に表れる</h2>
<p>3チームのアプローチは大きく異なり、プロンプト設計の個性が発話に直接反映された。</p>
<ul>
<li><strong>Team A：徹底したキャラクター設計</strong> 「認知停止を破壊する」というテーマを持つ強烈な人格を設定し、攻撃的な論法を組み込んだ。</li>
<li><strong>Team B：繰り返し修正する“チューニング型</strong> 芥川龍之介やガンジーなど歴史人物をモチーフに、AIの出力を評価→修正する反復設計を採用。</li>
<li><strong>Team C：最小限の人格＋大量の論拠データ</strong> キャラ設定は控えめにし、RAGで大量の論理情報を与える“ロジック重視”型。</li>
</ul>
<p>いずれも、設計者の価値観や思考癖がそのままAIのふるまいに組み込まれていたという。</p>
<h2>発見3：AIリテラシー全体をカバーする学習効果</h2>
<p>研究チームは、今回の取り組みを“ジェネレーティブAIリテラシーの実践的な総合モデル”と位置づけている。
実際、参加者は以下の能力を幅広く活用していた。</p>
<ul>
<li>モデル特性の理解</li>
<li>論拠の収集と検証</li>
<li>プロンプト設計</li>
<li>出力の評価と改善</li>
<li>自己の思考の客観視（Reflecting with AI）</li>
</ul>
<p>ただし、倫理や法的側面の深い学習までは含まれておらず、今後の課題とされている。</p>
<p>研究では、「AIとの協働」ではなく “AIを通して自分を理解する” という新しい可能性を示しており、分身AIが自律的に議論する様子を観察することで、ユーザーは自らの論理構造・思考の癖を外在化し、客観的に省察できるという。研究チームは、この“Reflecting with AI”が今後のAIリテラシーにおける重要な能力になると位置づけている。</p>
]]></description>
      <pubDate>Sat, 22 Nov 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google、「Gemini 3」を正式発表──推論・マルチモーダル性能を強化した最新モデル、本日より提供開始</title>
      <link>https://ledge.ai/articles/gemini_3_google_launch</link>
      <description><![CDATA[<p>Googleは2025年11月18日（米国時間）、生成AIサービス「Gemini」において最新のAIモデル群「Gemini 3」の中核となる「Gemini 3 Pro」のプレビュー提供を開始したと公式ブログで<a href="https://blog.google/products/gemini/gemini-3/">発表</a>した。</p>
<p>同モデルを「our most intelligent model yet（これまでで最も知的なモデル）」と位置づけ、推論能力、マルチモーダル理解、コード生成などを大幅に強化。Geminiアプリや開発者向けAPI、Google製品群への展開もあわせて示し、次世代AI基盤として同社の戦略を前進させる。</p>
<p>@<a href="https://www.youtube.com/watch?v=98DcoXwGX6I">YouTube</a></p>
<h2>ベンチマークはGemini 2.5 Proを全項目で上回る</h2>
<p>GoogleはGemini 3 Proが「主要ベンチマークのすべてでGemini 2.5 Proを上回った」と説明している。
具体的には以下の例を挙げた。</p>
<ul>
<li>LMArena：Elo 1501</li>
<li>Humanity’s Last Exam：ツール非使用で37.5%</li>
<li>GPQA Diamond：91.9%</li>
<li>MathArena Apex：23.4%</li>
</ul>
<p>マルチモーダル分野でも、MMMU-Proで81%、Video-MMUで87.6%、SimpleQA Verifiedで72.1%など、幅広い領域で高い性能が示されている。Googleはこれらの結果を「科学・数学・事実性など、知識領域をまたぐモデルの信頼性向上の証拠」と述べた。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/gemini_3_table_final_HLE_Tools_on_731f6cc1a4/gemini_3_table_final_HLE_Tools_on_731f6cc1a4.jpg" alt="gemini_3_table_final_HLE_Tools_on.jpg" /></p>
<h2>Deep Thinkモードも発表──高度推論向けに拡張</h2>
<p>Gemini 3には、新たに「Deep Thinkモード」が追加される。これは高度な推論・意思決定・マルチステップ推論を必要とするタスク向けに最適化した拡張モードで、Gemini 3 Proの上位に位置づけられる。</p>
<p>Deep Thinkモードは、以下のように通常のGemini 3 Proを上回るスコアを示した。</p>
<ul>
<li>Humanity’s Last Exam：41.0%</li>
<li>GPQA Diamond：93.8%</li>
<li>ARC-AGI-2（コード実行あり）：45.1%</li>
</ul>
<p>提供に先立ち、安全性テストと外部専門家による評価を経て、Google AI Ultra加入者向けに段階的に公開される予定だ。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/final_dt_blog_evals_2_33b1154911/final_dt_blog_evals_2_33b1154911.jpg" alt="final_dt_blog_evals_2.jpg" /></p>
<h2>Searchにも同日から導入──「AI Mode」でGenerative UIを採用</h2>
<p>Gemini 3は発表当日からGoogle検索の「AI Mode」に採用された。GeminiモデルがSearchに同日から搭載されるのは今回が初となる。</p>
<p>AI Modeでは、検索クエリに応じて生成される「Generative UI」を導入。地図、一覧表、画像レイアウト、ツールシミュレーションなど、検索内容に合わせたインタラクティブな表示が自動生成される。Googleはこれを「従来の検索を超えて、発見・理解・計画を支援する体験」と説明している。</p>
<p>Geminiアプリでも、デフォルトモデルがGemini 3 Proとなり、長文コンテキスト（最大100万トークン）やマルチモーダル推論を活かした学習支援・要約・説明などの機能が強化された。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/A_Imode_d2647e4cb3/A_Imode_d2647e4cb3.jpg" alt="AImode.jpg" /></p>
<h2>開発者向けにはAI Studio・Vertex AI・新IDE「Google Antigravity」を提供</h2>
<p>Gemini 3 Proは、以下の開発者向けツールで利用可能になる。</p>
<ul>
<li>Google AI Studio（Gemini API）</li>
<li>Vertex AI</li>
<li>Gemini CLI</li>
<li>Google Antigravity（新エージェント指向IDE）</li>
<li>Cursor、GitHub、JetBrains、Replitなど一部のサードパーティ環境</li>
</ul>
<p>特に「Google Antigravity」はGemini 3を前提とした“エージェント・ファースト”の開発環境で、エージェントがコードエディタ・ブラウザ・ターミナルに直接アクセスし、タスクを自律的に進める仕組みを採用する。</p>
<p>@<a href="https://youtu.be/22B5Yu0oVS0">YouTube</a></p>
<p>企業向けには「Gemini Enterprise」やVertex AIを通じた導入が開始され、Google Cloud顧客にも順次展開される。</p>
<h2>長期計画のタスクでもトップスコアを記録</h2>
<p>Googleは、長期計画タスクのベンチマーク「<a href="https://andonlabs.com/evals/vending-bench-arena">Vending-Bench 2</a>」でGemini 3がトップスコアを記録したと説明する。</p>
<p>また、1年分のシミュレーションで安定した意思決定とツール利用が確認されたとしており、今後予定される「Gemini Agent」機能（例：メール整理、予約手続きなどマルチステップの実行支援）の基盤になるとした。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/vending_bench_2_final_width_1000_format_webp_40e02b2245/vending_bench_2_final_width_1000_format_webp_40e02b2245.webp" alt="vending_bench_2_final.width-1000.format-webp.webp" /></p>
<h2>「最も安全なモデル」として開発──シコファンシー耐性・プロンプトインジェクション対策を強化</h2>
<p>GoogleはGemini 3を「Google史上最も安全なモデル」と表現し、以下の改善を挙げた。</p>
<ul>
<li>迎合的な回答（シコファンシー）の抑制</li>
<li>プロンプトインジェクション耐性の向上</li>
<li>サイバー攻撃・悪用に対する防御強化</li>
</ul>
<p>また、Frontier Safety Frameworkに基づく内部評価に加え、UK AISIなどの外部機関や独立した専門家によるアセスメントを受けたとしている。「Gemini 3 model card」には詳細な安全性情報がまとめられている。</p>
<h2>今後はDeep Thinkの段階的提供、追加モデルの投入も</h2>
<p>Gemini 3 Proは18日から、Search・Geminiアプリ・AI Studio・Antigravity・Vertex AIなど複数のプロダクトで利用可能となった。今後はDeep Thinkモードが安全性評価を経てGoogle AI Ultra加入者に提供されるほか、Gemini 3シリーズとして追加モデルも順次投入される計画だという。</p>
]]></description>
      <pubDate>Sat, 22 Nov 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>生成AI画像に“著作権”成立と判断　千葉県警、無断複製で27歳男を書類送致──全国初の摘発</title>
      <link>https://ledge.ai/articles/gen_ai_image_copyright_case_chiba_police</link>
      <description><![CDATA[<p>千葉県警生活経済課は2025年11月20日、生成AIで作成された画像を無断で複製したとして、神奈川県大和市の無職の男性（27）を著作権法違反（複製権侵害）の疑いで千葉地検に書類送致したと<a href="https://www.police.pref.chiba.jp/kohoka/orders_prefecture_03461.html">発表</a>した。県警によれば、生成AIで作られた画像を著作物として扱い、著作権法違反で摘発するのは全国で初めてとみられる。</p>
<p>事件は8月25日昼頃に発生。男性は、千葉県我孫子市の男性が生成AIを用いて制作し、SNSに投稿していたコンピューターグラフィックスを、著作権者の承諾を得ずに外部サーバーへ送信し複製した疑いが持たれている。県警は、被害者が生成過程で入力した多数のプロンプト（指示）や内容、表現に至るまでの過程を確認し、創作性のある表現が認められるとして著作物性を判断した。</p>
<p>報道によれば、容疑者は複製した画像を自身が販売する電子書籍の表紙として使用していたとされる。容疑者は「作品に合う素材だった」と供述しているという。県警は、被害者が画像生成において具体的な指示を重ね、独自の表現に到達していた点を踏まえ、無断複製が著作権法上の侵害に当たると判断した。</p>
<p>著作権法では、人の創作的関与によって生まれた表現が保護の対象となる。生成AIが作成した画像については、どの程度の人間による関与が「創作性」として評価されるかが議論されてきた。今回の事案では、AIへの指示内容や生成工程が「人による具体的な創作行為」と認められた点が重要となる。</p>
<p>生成AIで作成された画像については、人の具体的な指示や制作過程がどの程度関与しているかが著作物性の判断要素とされる。同事案では、プロンプトの内容や生成工程に人の創作的関与が認められた点が、著作権侵害容疑の判断材料となった。SNS上で公開された生成AI画像であっても、創作性が認められる場合には著作権が成立するため、複製や利用には権利者の許諾が必要となる。</p>
]]></description>
      <pubDate>Sat, 22 Nov 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google、誰でも学べるAI学習サイト「Google Skills」を正式公開──Cloud・DeepMind・教育部門を横断する3000講座を展開</title>
      <link>https://ledge.ai/articles/google_skills_ai_learning_platform_launch</link>
      <description><![CDATA[<p>Googleは2025年10月21日（米国時間）、新しいAI学習プラットフォーム「Google Skills」を<a href="https://blog.google/outreach-initiatives/education/google-skills/">発表</a>した。同サイトでは、Google Cloud、Google DeepMind、Grow with Google、Google for Educationなど、同社の複数部門が提供してきた教育コンテンツを統合。3000種類を超えるAI関連の講座・体験ラボ・認定プログラムを、一元的に学べる学習拠点として開設された。</p>
<h2>AI教育の中核を担う新サイト</h2>
<p>Google公式ブログ「Start learning all things AI on the new Google Skills」によると、Google Skillsは“AI for Everyone（すべての人のためのAI）”をテーマに、誰もがAIスキルを体系的に学べるよう設計されている。初心者、エンジニア、企業リーダーなど幅広い層を対象に、AI、データ分析、クラウド、生成AIなど多様な分野を網羅。各コースはオンデマンド形式で受講でき、学習成果はLinkedInなどの外部プラットフォームで共有できる。提供内容には、Google Cloudの認定資格プログラムやAI Essentials シリーズ、DeepMindのAI倫理教材などが含まれる。</p>
<p>@<a href="https://www.youtube.com/watch?v=Qbix0BOPcgE">YouTube</a></p>
<p>今回の正式公開に先立ち、Google Cloudは10月10日付のブログ「Google Skills: Your new home for Google AI learning and more」で、新プラットフォームの構想を公表していた。当時は正式リリース前で、「AIやクラウドに関する学習リソースを一元化し、近日中に詳細を発表する」としていた。Gemini Code Assist（旧Duet AI for Developers）やQwiklabs（現Cloud Labs）と連携し、AIトレーニングの実践環境を統合する方針も示されていた。</p>
<h2>3000超のコースと実践的ラボを集約</h2>
<p>Google Skillsでは、Googleがこれまで個別に展開してきた学習リソースを一か所に集約。AIモデル開発、クラウド基盤運用、データ可視化、サイバーセキュリティなど、実践重視の3000超のコースとラボを提供する。一部コンテンツは無料で公開され、修了証や認定資格を取得することでキャリア開発にもつなげられる。また、組織向けにはチーム単位での進捗管理や学習成果の可視化機能も用意されている。</p>
<h2>今後の展望──教育機関・企業研修にも拡大へ</h2>
<p>Googleは今後、教育機関や企業研修への展開を進める方針を示しており、AIスキルの標準教育基盤としての活用を目指す。
公式ブログでは、「AI教育へのアクセスを民主化し、誰もがテクノロジーの未来を形づくる機会を得られるようにする」としている。
同社は今後もDeepMindやCloud AIチームの最新教材を追加し、AI人材育成をグローバルに推進する考えだ。</p>
]]></description>
      <pubDate>Sat, 22 Nov 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>学術＆研究2025/11/21 [FRI]LLMはSNSの“ジャンク投稿”で脳が腐る？──米研究チームが「Brain Rot（脳腐敗）仮説」を提唱、推論力や安全性の劣化を確認</title>
      <link>https://ledge.ai/articles/llm_brain_rot_hypothesis_ai_junk_data_risk</link>
      <description><![CDATA[<p>テキサス大学オースティン校、テキサスA&amp;M大学、パデュー大学の研究者チームは2025年10月15日、LLM（大規模言語モデル）がSNSに溢れる “ジャンクデータ” によって徐々に劣化する可能性を示す研究「LLMs Can Get “Brain Rot”!」を<a href="https://llm-brain-rot.github.io/">発表</a>した。</p>
<p>研究チームは、X（旧Twitter）から収集した投稿データを用いた制御実験を行い、低品質テキストの混入が推論力・長文理解・安全性・人格的特性に一貫して悪影響を与えると報告している。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/teaser_cad83675ae/teaser_cad83675ae.png" alt="teaser.png" /></p>
<h2>SNSの“低品質テキスト”がモデル品質を損なう可能性</h2>
<p>研究チームが着目したのは、Web全体に含まれるユーザー生成コンテンツ、とりわけXに特徴的な「エンゲージメントは高いが内容が浅い投稿」だ。
これらは</p>
<ul>
<li>誤情報</li>
<li>攻撃的な表現</li>
<li>情報量の乏しい短文</li>
</ul>
<p>などが混在しやすく、AIモデルが大量に取り込むとどうなるかは十分に検証されてこなかった。</p>
<p>今回の研究では、こうした投稿を「ジャンクデータ」として体系的に扱い、その影響を測定するための仮説を「LLM Brain Rot（脳腐敗）仮説」と名づけた。</p>
<h2>実験方法：X投稿を「品質」と「人気度」で分類しモデルを再学習</h2>
<p>研究者らは、Xの投稿を以下の2軸で分類した。</p>
<ul>
<li>テキストの品質（semantic quality）</li>
<li>エンゲージメント（人気度）</li>
</ul>
<p>この組み合わせで複数のデータセットを構築し、既存のオープンソースLLMに対して再学習（fine-tuning）を実施。
評価には、</p>
<ul>
<li>推論タスク（ARC Challenge、ARC-Easy など）</li>
<li>長文コンテキスト理解（RULER）</li>
<li>安全性（有害発言誘発テスト）</li>
<li>“人格特性”に関する心理尺度（ナルシシズム、マキャベリズム、サイコパシーなど）</li>
</ul>
<p>が利用された。</p>
<h2>主な結果①：推論・読解能力が段階的に悪化</h2>
<p>研究では、低品質テキストの割合を増やすほど、推論・長文理解タスクのスコアが段階的に低下することが確認された。研究者らは変化の様子を「dose–response（用量反応）」に似ていると指摘している。</p>
<p>この “劣化の全体像” は、下図のように整理されている。</p>
<p><strong>〈図1〉推論タスク・人格特性の変化（Effective Size）</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/effective_size_82aa2fbc8f/effective_size_82aa2fbc8f.png" alt="effective_size.png" />
低品質SNSデータで再学習すると、推論・読解（左）が悪化し、人格特性（右）も望ましくない方向へ変化する傾向が示された。</p>
<p>この図が示すように、</p>
<ul>
<li>ARC Challenge</li>
<li>RULER</li>
<li>HH-RLHF（安全性関連）</li>
</ul>
<p>ではいずれもスコアが低下。また人格特性に関しても、ナルシシズム・サイコパシー傾向などが上昇する結果となった。</p>
<p>さらに、モデルがどのように失敗するようになったかも分析されている。</p>
<p><strong>〈図2〉失敗パターンの増加（Failure Count）</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/failure_mode_barplot_count_f4b1cff90d/failure_mode_barplot_count_f4b1cff90d.png" alt="failure_mode_barplot_count.png" />
低品質データで再学習したモデルほど、思考プロセスの失敗（“考えていない”“誤った論理”“事実誤認”など）が大幅に増加した。</p>
<p>この図が表している通り、</p>
<ul>
<li>no thinking（思考が全く行われない）</li>
<li>wrong logic in plan（計画の論理破綻）</li>
<li>factual error（事実誤認）</li>
</ul>
<p>などの“思考の崩壊”が顕著に増加している。</p>
<h2>主な結果②：安全性と“人格特性”にも悪影響</h2>
<p>推論能力の低下だけでなく、モデルの「振る舞い」や「性格的傾向」にまで変化が現れた。</p>
<ul>
<li>有害な発言を誘発しやすくなる</li>
<li>攻撃的・支配的・自己中心的な回答を返す傾向が強まる</li>
<li>不正確な主張を自信満々に述べるケースが増える</li>
</ul>
<p>研究チームはこれを、SNS特有の“攻撃性・扇動性のある投稿”を多く学習した結果として説明している。</p>
<h2>“主な結果③：“脳腐敗”は簡単には治らない</h2>
<p>モデルの性能が落ちたあとに、高品質データで“洗浄（wash-out）”すれば回復するのではないか──
研究者らはこの仮説も検証した。</p>
<p>結果は次の通り。</p>
<p><strong>〈図3〉洗浄（wash-out）実験：完全には回復しない</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/wash_out_scaling_1114dafe8f/wash_out_scaling_1114dafe8f.png" alt="wash_out_scaling.png" />
ジャンクデータで劣化したモデルを高品質データで再学習しても、ARC-CやRULERなど複数タスクで性能が回復しきらず“残留劣化”が確認された。</p>
<p>グラフが示すように、洗浄後のモデルは“部分的な回復”こそ見られるものの、元の性能には戻らなかった。
研究者らはこれを「残留ダメージ（residual damage）」と呼び、“悪いデータを食べさせると、後から取り返しがつかない”可能性を指摘している。</p>
<h2>ChatGPT・Gemini・Claude・Grokにも影響しうる？</h2>
<p>研究はオープンソースLLMを用いた実験だが、論文は「Webテキストを学習するすべてのLLMに関わる問題」だと指摘している。</p>
<p>ChatGPT、Gemini、Claude、Grok など商用モデルがどの程度SNSテキストを取り込んでいるかは公開されていないが、Web全体のクロールデータを使う以上、“ジャンク比率”がモデル品質を左右する可能性は避けられないとする。</p>
<h2>今後の焦点：AIの“健康診断”として利用される可能性</h2>
<p>研究チームは、</p>
<ul>
<li>データ品質フィルタリングの強化</li>
<li>SNS由来データの評価と管理</li>
<li>「Brain Rot」兆候の定期的検査</li>
</ul>
<p>などを訓練パイプラインに組み込む必要性を訴える。</p>
<p>また、GitHub上では実験コードが公開されており、企業が自社モデルで同様の検証を再現できるようになっている。研究者らは、「SNS時代のデータ汚染がAIモデルに及ぼす影響を体系的に測定する第一歩になった」としており、今後は 他言語・他SNS・他文化圏データでの再検証が進むと見られる。</p>
]]></description>
      <pubDate>Sat, 22 Nov 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Meta、「SAM 3」を発表──テキストや画像例をプロンプトに、画像・動画内の物体を一括検出・分割・追跡　併せて「SAM 3D」で人物・物体の3D生成にも対応</title>
      <link>https://ledge.ai/articles/meta_sam3_sam3d_release</link>
      <description><![CDATA[<p>Metaは2025年11月19日（現地時間）、画像および動画に含まれるオブジェクトを、プロンプトを基点に一括で検出・分割・追跡できる新モデル「Segment Anything Model 3（SAM 3）」を<a href="https://ai.meta.com/blog/segment-anything-model-3/">発表</a>した。同日、2D画像から人物や物体の3Dモデルを生成する「SAM 3D」も<a href="https://ai.meta.com/blog/sam-3d/">公開</a>している。</p>
<p>Metaの公式Xアカウント（@AIatMeta）は、両モデルを「新しい世代のSegment Anything Models」と<a href="https://x.com/AIatMeta/status/1991178519557046380">紹介</a>し、開発者・研究者向けのメディアワークフローを大きく拡張するとしている。</p>
<h2>SAM3：プロンプトから“概念”ベースで検出・分割・追跡</h2>
<p>SAM 3は、短いテキストフレーズ（例：「yellow school bus」）や、画像内のサンプルオブジェクトを示す “example prompt” を入力すると、該当するすべてのオブジェクトを一括して検出・分割し、動画では継続的に追跡できるモデル。Metaはこれを「Promptable Concept Segmentation（PCS）」と定義している。</p>
<p>@<a href="https://www.youtube.com/watch?v=G4OLPDjwncw">Youtube</a></p>
<p>アーキテクチャは、DETRベースの画像検出器と、SAM 2を基盤とした動画トラッカーを単一バックボーンに統合したもの。論文(https://ai.meta.com/research/publications/sam-3-segment-anything-with-concepts/)では、認識と位置特定を切り離す“Presence Head”を新たに採用し、概念ベースの検出精度を向上させたと説明されている。</p>
<p><strong>SAM 3は、テキストプロンプトを用いた検出と追跡を単一アーキテクチャで統合する</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/SAM_3_architecture_overview_0af9a61c98/SAM_3_architecture_overview_0af9a61c98.jpg" alt="SAM 3 architecture overview.jpg" /></p>
<h2>400万概念を含む新データセット「SA-Co」</h2>
<p>SAM 3の開発にあたり、Metaは大規模データセット「<a href="https://github.com/facebookresearch/sam3/blob/main/README.md#sa-co-dataset">SA-Co（Segment Anything with Concepts）</a>」を新たに構築した。SA-Coは、数百万枚規模の高品質画像と約400万のユニーク概念、数千万のマスク情報を含むもので、モデルが幅広い概念を学習できるよう設計されている。</p>
<p>論文で示されたベンチマーク「SA-Co/Gold」では20万以上の概念を評価対象とし、SAM 3はLVISのゼロショットMask APで48.8を記録。従来モデル（38.5）を大きく上回った。</p>
<p><strong>SA-Coは、画像中の多様な物体を高精度にラベル付けする大規模データセット。色ごとに異なる概念カテゴリが示されている</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/SA_Co_907d839e43/SA_Co_907d839e43.jpg" alt="SA-Co.jpg" /></p>
<p>SAM 3は、Metaの研究用ウェアラブルデバイス「<a href="https://ledge.ai/articles/meta_aria_gen2_ai_research">Aria Gen 2</a>」で撮影された一人称視点の映像に対しても高い性能を示す。動きの速さや視点の揺れが大きいファーストパーソン映像でも、対象物の分割と追跡を安定して行える点が特徴だ。</p>
<p>Metaは、Aria Gen 2 Pilot Datasetの一部を<a href="https://www.aidemos.meta.com/segment-anything">Segment Anything Playground</a>上で公開しており、これにより、人間の視点から世界を理解する“コンテクスチュアルAI”や、ロボティクス、機械知覚といった応用領域におけるSAM 3の有用性を示している。</p>
<h2>SAM 3D：1枚の画像から人物・物体の3Dモデルを生成</h2>
<p>同時に公開された「SAM 3D」は、人物に特化した「SAM 3D Body」と一般物体向けの「SAM 3D Objects」から構成される。Metaは、単一の2D画像から高精度で3D形状を復元でき、テクスチャとメッシュの情報を従来手法より忠実に再現できる点を強調している。</p>
<p>@<a href="https://www.youtube.com/watch?v=B7PZuM55ayc">Youtube</a></p>
<h2>2D解析から3D復元までを一貫化</h2>
<p>Metaは、SAM 3とSAM 3Dをセットで発表することで、画像・動画内のオブジェクト理解（SAM 3）から3D形状復元（SAM 3D）までを一貫して扱える視覚AI基盤を提示した。動画編集、AR/VR、ロボティクス、ECなど、多数の応用領域で利用可能性があるとしている。</p>
<p>公式Xでは、今回の発表を「新しい世代のSegment Anything Models」と説明し、SAM 3 と SAM 3D が画像・動画・3Dを横断する基盤技術として進化した点を強調している。投稿では、短いテキスト指示や具体例となる画像を用いた物体の検出・分割・追跡（SAM 3）、そして単一画像から人物や物体の3Dモデルを生成する機能（SAM 3D）が紹介され、「開発者と研究者が新しいメディア処理ワークフローを構築するためのツール」と位置づけている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/segment_anything_models_921509ea34/segment_anything_models_921509ea34.jpg" alt="segment anything models.jpg" /></p>
<p>:::box
[関連記事：AIの\</p>
]]></description>
      <pubDate>Sat, 22 Nov 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>アルトマン氏「エロティックばかり注目されたけど」──ChatGPT、成人ユーザーの自由拡大へ</title>
      <link>https://ledge.ai/articles/openai_chatgpt_adult_mode_update_oct2025</link>
      <description><![CDATA[<p>OpenAIのサム・アルトマンCEOは10月14日（現地時間）、X（旧Twitter）上で、ChatGPTの安全制限を一部緩和し、成人認証済みユーザーに対してエロティックな会話を許可する方針を<a href="https://x.com/sama/status/1978129344598827128">発表</a>
した。</p>
<p>投稿は瞬く間に注目を集め、「エロティック解禁」が大きな話題となったが、アルトマン氏は翌日に「その部分ばかり注目されてしまったが」と<a href="https://x.com/sama/status/1978539332215681076">補足</a>し、実際には“より人間らしいAI体験”を実現するための包括的な方針変更であることを強調した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/gpt5_erotica_7cea863241/gpt5_erotica_7cea863241.jpg" alt="gpt5 erotica.jpg" /></p>
<h2>安全を優先してきたChatGPTの制限</h2>
<p>ChatGPTはこれまで、性的表現や親密な会話を含むコンテンツを厳しく制限してきた。
アルトマン氏は「メンタルヘルス問題に慎重を期すためだった」と説明し、精神的に不安定なユーザーに配慮した措置であったと振り返った。
「深刻な危機状態にあるユーザーは別扱いとし、他者に害を与える行為は依然として許可しない」と述べ、ポリシーの根幹は維持されるとしたうえで、「リスクのない成人ユーザーにはより多くの自由を与える」と明言した。</p>
<h2>“4oらしさ”を再導入──人間的なAI体験へ</h2>
<p>アルトマン氏は同日、「数週間以内に“4oで好まれた振る舞い”に近い人格（パーソナリティ）を選べる新バージョンのChatGPTを提供する」と投稿した。
GPT-4oは会話の自然さや表情豊かな応答で人気を集めたモデルであり、今後はユーザーが望む場合に、フレンドリーな口調や絵文字を多用した“人間らしい”対話スタイルを選べるようになる。
アルトマン氏は「これは利用時間を増やすためではなく、ユーザーが自分の望む形でAIと関わる自由を得るための設計だ」と述べている。</p>
<h2>「成人は大人として扱う」──12月に年齢認証を本格導入</h2>
<p>アルトマン氏は、12月に年齢認証を本格導入し、認証済み成人ユーザーに対してはエロティック会話なども許可する方針を示した。
一方で、ティーンエイジャーに対しては「安全をプライバシーや自由より優先する」と述べ、メンタルヘルス関連ポリシーは緩めないと強調している。</p>
<p>アルトマン氏は、「社会がR指定映画で境界を設けるように、AIにも適切な年齢境界を設けたい」と例え、「我々は選挙で選ばれた道徳警察ではない」と付け加えた。</p>
<h2>倫理と自由の境界線</h2>
<p>アルトマン氏の発言は、AIにどこまで人間的な自由を与えるかという議論を再燃させた。</p>
<p>OpenAIは今後、成人向け表現やAIの人格設計に関するガイドラインをさらに明確化するとみられる。今回の方針転換は、「AIをどう設計し、どう育てるか」という人間社会全体のテーマに踏み込む第一歩となりそうだ。</p>
]]></description>
      <pubDate>Sat, 22 Nov 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/11/22 [SAT]ヤン・ルカン博士、Metaを年末に退社──「次のAI革命」Advanced Machine Intelligence研究を推進する新会社を設立へ</title>
      <link>https://ledge.ai/articles/yann_lecun_leaves_meta_ami_startup_2025</link>
      <description><![CDATA[<p>米MetaでチーフAIサイエンティストを務め、同社のAI研究組織「FAIR（Facebook AI Research）」の創設者として知られるヤン・ルカン（Yann LeCun）博士が、12年間在籍したMetaを年末に退社する。2025年11月20日（現地時間）、自身の<a href="https://www.facebook.com/yann.lecun/posts/pfbid0iU3ECamGohUvpmKozEq24RGowPoiu3D7J6vHXJxzKGT4hZittF2UK43oZkjSqVexl">Facebookへの投稿</a>で明らかにした。</p>
<p>ルカン氏は投稿の冒頭で「うわさや最近の報道で耳にした人もいるだろうが、12年間在籍したMetaを離れる計画だ」と述べ、退社を正式に表明した。FAIRの初代ディレクターとして5年、チーフAIサイエンティストとして7年を務めたと振り返り、「FAIRの創設は、自身の“最も誇りに思う非技術的な業績”」と強調した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/yann_lecun_facebook_3d6a371abb/yann_lecun_facebook_3d6a371abb.jpg" alt="yann lecun facebook.jpg" /></p>
<h2>Advanced Machine Intelligence（AMI）研究を継続する独立スタートアップを設立</h2>
<p>投稿によると、ルカン氏は「Advanced Machine Intelligence（AMI）」と名付けた研究プログラムを継続するため、新たなスタートアップを立ち上げる。同プログラムは、近年FAIRやニューヨーク大学（NYU）などの研究者と進めてきたもので、次のような能力を備えたAIシステムを目指すという。</p>
<ul>
<li>物理世界を理解する</li>
<li>持続的な記憶を持つ</li>
<li>推論できる</li>
<li>複雑な行動計画を実行できる</li>
</ul>
<p>ルカン氏は、これを「次の大きなAI革命」につながる研究領域だと位置づける。スタートアップの詳細は後日公表するとしており、社名や資金調達の状況は明かしていない。</p>
<h2>Metaは新会社のパートナーに──Zuckerberg氏らへの感謝も明記</h2>
<p>投稿では、Mark Zuckerberg氏、Andrew Bosworth氏（Boz）、Chris Cox氏、Mike Schroepfer氏の4名に対し、FAIRおよびAMI研究への継続的な支援への謝意を述べている。そのうえで、「Metaは新会社のパートナーとなる」と記し、退社後も協力関係が続くことを示した。</p>
<p>AMI研究のアプリケーションについては「Metaの商業領域と重なる部分もあれば、重ならない領域もある」とし、独立した組織で取り組むことが「広範なインパクトを最大化する手段」だと説明している。</p>
<h2>研究者としての役割は継続、年末まではMetaに在籍</h2>
<p>ルカン氏は現在、MetaのチーフAIサイエンティストとNYUの教授職を兼務している。投稿では、これらの役割に関する変更には触れておらず、研究活動は継続する見通しだ。</p>
<p>また、「年末まではMetaにとどまる」とも述べており、移行期間中は同社での業務を続けながら新会社の準備を進めるとしている。</p>
]]></description>
      <pubDate>Sat, 22 Nov 2025 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/11/21 [FRI]Google、「Nano Banana Pro」を発表──4K対応・多言語テキスト描画を強化した最新画像生成AI</title>
      <link>https://ledge.ai/articles/google_nano_banana_pro_release</link>
      <description><![CDATA[<p>Google は2025年11月20日（米国時間）、画像生成AI「Nano Banana」の最新バージョンとなる 「Nano Banana Pro」 を<a href="https://blog.google/intl/ja-jp/company-news/technology/nano-banana-pro/">発表</a>した。Gemini 3 Pro Image を基盤とした新しい画像生成システムで、、4K解像度の高精細生成や多言語テキスト描画の精度が大幅に向上しているという。</p>
<p>@<a href="https://www.youtube.com/watch?v=UQsJIo46ZR8&amp;t=3s">YouTube</a></p>
<h2>4K出力と多言語テキスト生成を強化</h2>
<p>公式ブログによると、Nano Banana Pro は従来モデルから画質と制御性能が大幅に進化し、最大 4K 解像度の画像生成 に対応した。広告制作、資料作成、スライドデザインなど、高精細なビジュアルを必要とする用途での活用が見込まれる。</p>
<p>Google は発表の中で、照明や構図のコントロールが可能になった事例を複数紹介している。</p>
<p><strong>・照明・フォーカス調整の例</strong>
シーンを昼から夜へと変更し、照明バランスを最適化するデモ（プロンプト：このシーンを夜にしてください）
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/WM_Fox_93f02b0061/WM_Fox_93f02b0061.jpg" alt="WM-Fox.jpg" /></p>
<p><strong>・ドラマチックな照明効果の付与</strong>
人物ポートレートに拡散光を加え、印象を変えるデモ（プロンプト：このポートレートの照明を、左からの柔らかな拡散光に変更してください）
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/WM_Studio_Quality2_81a14549aa/WM_Studio_Quality2_81a14549aa.jpg" alt="WM-Studio-Quality2.jpg" /></p>
<p><strong>・被写界深度の調整</strong>
焦点を被写体の花に合わせ、構図の要素を際立たせるデモ（プロンプト：花に焦点を合わせてください）
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/WM_Flower_Girl_16a2874c39/WM_Flower_Girl_16a2874c39.jpg" alt="WM---Flower-Girl.jpg" /></p>
<p>また、日本語・韓国語・アラビア語などの複雑な文字体系にも対応し、多言語の画像内テキスト生成がより自然で正確になった。文字組みや行送りなど、これまで課題となっていたレイアウト要素の精度も改善している。</p>
<p><strong>・テキストローカライズの例</strong>
英語で書かれた飲料キャンペーンの文言を韓国語に翻訳し、デザイン要素を保持したまま再描画するデモ（プロンプト：3つの黄色と青の缶に書かれている英語のテキストを韓国語に翻訳し、他の要素は変更しないでください）
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Final_Nano_Banana_Translate_Can_4ac7f9e843/Final_Nano_Banana_Translate_Can_4ac7f9e843.jpg" alt="Final_NanoBanana_TranslateCan.jpg" /></p>
<h2>実世界知識の向上と複数画像入力</h2>
<p>Nano Banana Pro は Google Search の最新情報を参照し、現実世界の文脈を反映した画像生成に対応する。Google は、科学・歴史・文化的トピックを視覚化する例を公式ページで紹介している。</p>
<p><strong>・科学トピックの視覚化例</strong>
アイザック・ニュートンの光と色彩の理論を、ミニマルなフラットレイ構図で説明するデモ
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/unnamed_5_75517229b0/unnamed_5_75517229b0.webp" alt="unnamed (5).webp" /></p>
<p>また、最大14枚の画像入力に基づき、スタイルやレイアウトの一貫性を保ちながら新たな画像を生成できる。人物の再現についても、最大5人までの整合性を維持可能としている。</p>
<p><strong>・複数キャラクターの配置例</strong>
14体のキャラクターを並んで座らせ、一貫したスタイルで描画するデモ
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Fluffy_Monsters_a42f31b8d0/Fluffy_Monsters_a42f31b8d0.jpg" alt="Fluffy-Monsters.jpg" /></p>
<p>生成画像には AI 生成物識別技術 SynthID の透かしが自動で埋め込まれ、透明性と著作権管理への配慮も進めている。</p>
<h2>Workspace・NotebookLMなど各サービスで提供開始</h2>
<p>Nano Banana Pro は、Gemini アプリに加えて Google Slides、Google Vids、NotebookLM など複数の Google サービスで利用可能となる。Google Workspace では、11月20日から最大15日間かけて段階的に展開する。</p>
<h2>開発者・企業向けの提供も本格化</h2>
<p>Google は開発者向けブログも公開し、Nano Banana Pro の 2K/4K 出力、複数画像入力、レンダリング精度の向上などを説明した。Vertex AI を通じた API 提供が開始され、Google Cloud 上での画像生成ワークフローへの統合も可能となる。</p>
<p>Google は Gemini 3 系列を軸にマルチモーダル AI の強化を進めており、Nano Banana Pro をその画像生成技術の中心モデルとして位置づけている。多言語描画と高精細画像への対応について、同社は広告・教育・資料作成など幅広い領域での活用を想定していると説明している。</p>
]]></description>
      <pubDate>Fri, 21 Nov 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google、気象予報AI「WeatherNext 2」を発表──従来モデルの8倍速で予報を生成し、最大1時間までの解像度を実現</title>
      <link>https://ledge.ai/articles/weathernext_2_google_ai_weather_forecasting_model_release</link>
      <description><![CDATA[<p>Googleは2025年11月17日（現地時間）、気象予報向けAIモデル「WeatherNext 2」を<a href="https://blog.google/technology/google-deepmind/weathernext-2/">発表</a>した。天気がサプライチェーンや航空路、日常の移動など幅広い領域の意思決定に影響を与えることを指摘し、近年のAI技術が予報能力を大きく向上させているという。</p>
<h2>気象予報の高速化と高解像度化</h2>
<p>WeatherNext 2は、Google DeepMindとGoogle Researchが開発した予報モデルで、従来のWeatherNextモデルと比べ8倍速で予報を生成し、最大1時間解像度の予測に対応する。公式ブログでは、WeatherNext 2を「最も高度で効率的な予報モデル」と位置づけている。</p>
<p>@<a href="https://www.youtube.com/watch?v=YQwqoEm_xis">YouTube</a></p>
<h2>数百のシナリオ生成に対応</h2>
<p>WeatherNext 2は、一つの予報結果だけでなく、数百の可能性（シナリオ）を生成する仕組みを備える。これにより、気象機関が複数のシナリオを比較しながら判断できる環境を提供する。Googleは実験的なサイクロン予測を通じて、この技術が意思決定の支援に使われた例を紹介している。</p>
<p><strong>WeatherNext 2 によるシナリオ生成の仕組み：</strong> 異なるランダム性を加えて複数の予報をつくり、時間の経過と共に分岐が広がっていく
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Weather_Next_2_blog_figure_03_lar_width_1000_format_webp_8aa488b043/Weather_Next_2_blog_figure_03_lar_width_1000_format_webp_8aa488b043.webp" alt="WeatherNext_2-blog-figure-03_lar.width-1000.format-webp.webp" /></p>
<h2>研究成果の提供範囲を拡大</h2>
<p>GoogleはWeatherNext 2の研究成果を「研究室から実環境へ展開する」として、以下のサービスで利用可能にした。</p>
<ul>
<li>Earth Engine：気象データ分析向け</li>
<li>BigQuery：予測データの活用拡大</li>
<li>Vertex AI：カスタム推論に使える早期アクセスプログラムを提供</li>
</ul>
<p>企業や研究者が独自の予報処理を行える仕組みが整備されつつある。</p>
<h2>Googleサービス全般の天気機能を更新</h2>
<p>WeatherNext技術は、Googleの各種サービスでも順次反映されている。</p>
<ul>
<li>Google検索</li>
<li>Gemini</li>
<li>Pixel Weather</li>
<li>Google Maps Platform の Weather API</li>
</ul>
<p>これらの天気情報は、WeatherNext 2の技術に基づいた内容にアップグレードされた。Googleは「数週間以内にGoogleマップにも反映される」としている。</p>
<h2>技術基盤：Functional Generative Networks（FGN）</h2>
<p>WeatherNext 2の基盤となる仕組みは、DeepMindが開発したFunctional Generative Networks（FGN）と呼ばれる技術だ。FGNは「大気の動き方そのものを学習し、時間とともにどのように変化していくかを再現する」点が特徴とされる。これにより、台風の渦の広がり方や降水帯の移動といった複雑で変化の激しい気象現象も、連続した流れとして予測しやすくなる。</p>
<p><strong>FGNの生成プロセスの概念図</strong> ：複数モデルとノイズ注入を組み合わせ、異なる気象シナリオを生成する
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/fgn_df84086054/fgn_df84086054.jpg" alt="fgn.jpg" /></p>
<h2>従来モデルの補完としての位置づけ</h2>
<p>Googleは、AIモデルが既存の物理ベースの気象モデルを完全に置き換えるものではないと明記している。気象予測には依然として課題が多く、AIは公式予報を補完する技術として設計されている。</p>
<p>WeatherNext 2は、予報精度の向上、生成速度の高速化、シナリオ生成機能の拡張など、気象予測に関する複数の要素を強化したモデルとして発表された。Googleは今後、研究機関や産業分野との協力を通じ、予報技術の提供領域を拡大するとしている。</p>
]]></description>
      <pubDate>Thu, 20 Nov 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/10/27 [MON]2025年のAIトレンドを総ざらい！Ledge.ai年末年始特集「&apos;25to&apos;26」事前登録スタート</title>
      <link>https://ledge.ai/articles/25to26-announce</link>
      <description><![CDATA[<p>AIの社会実装を加速させ、「テクノロジーを社会になめらかに浸透させる」ことをミッションに掲げる、国内最大級のAIメディア「Ledge.ai」を運営する株式会社レッジは、今年も年末年始特集「'25to'26」を公開します。
本日より先行サイトを公開し12月1日（月）の特集サイト公開までの間、お知らせを受け取ることができるようになる事前登録（無料）を受付開始いたしました。</p>
<p>:::button
<a href="https://25to26.ledge.ai/lp">事前告知サイトはこちら</a>
:::</p>
<p>2025年を締めくくるにふさわしい、AIの今とこれからを網羅した一大特集。研究者、ビジネスリーダー、エンジニアなど、あらゆる立場の方々に向けて、2026年のAIシーンを展望します。</p>
<h2>Ledge.ai年末年始特集『'25to'26』とは</h2>
<p>Ledge.ai年末年始特集は、2025年のAI関連ニュースや注目のキーワード、2026年以降の動向など、AIの初心者から専門家まで幅広く楽しめる特集サイトです。</p>
<p>2025年は、生成AIが実用フェーズに突入し、業務プロセス・プロダクト・教育・クリエイティブなど、社会のあらゆる分野で“AI活用の当たり前化”が進んだ一年でした。
そして2026年は、AIという概念そのものが提唱された「ダートマス会議」から70周年という、まさに歴史的な節目を迎えます。2025年の「当たり前化」を土台として、AIは社会インフラのように深く浸透し、その活用範囲の拡大と同時に、AGI（汎用人工知能）の実現可能性など、AIの“次なる進展”に向けた探求が本格化する一年となるのではないでしょうか。</p>
<p>本特集では、そんな激動の2025年を多角的に振り返りつつ、2026年に向けた新たな潮流やビジネスチャンスを展望します。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/1_ac8b0aef2e/1_ac8b0aef2e.png" alt="1.png" /></p>
<h2>コンテンツラインナップ紹介</h2>
<h3>編集部による徹底解説</h3>
<p>Ledge.ai編集部が、2025年のAIシーンを多角的に総括。
1年間の主要ニュースをピックアップしながら、トレンド分析と俯瞰的な視点で、AI技術が社会・産業へどのように浸透したのかを読み解きます。
さらに、技術動向の深掘り解説を通じて、進化の本質を明らかに。
2026年に向けて押さえておくべき“AIの現在地”を、独自の視点で整理します。</p>
<h3>独自インタビュー</h3>
<p>本特集では、「AI 70th Pre-Anniversary」というテーマのもと、AI研究の歴史・現在・未来をつなぐキーパーソンたちにインタビューを実施。
過去／現在／未来のそれぞれの視点から、AIがどのように発展し、次の時代にどんな可能性を秘めているのかを語ってもらいます。
世代と分野を超えて交わる知見が、AIの軌跡と未来へのヒントを照らし出します。</p>
<h3>トップランナー企業動向</h3>
<p>国内外の注目企業をピックアップし、AI周辺で押さえておきたい企業の最新動向を徹底分析。
生成AI、AIエージェント、クラウドAIなど、世界最先端の情報と実践事例に触れることで、読者が“次に取るべき一手”を見極められる構成になっています。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/2_6180f8a0c5/2_6180f8a0c5.png" alt="2.png" /></p>
<h2>開催概要</h2>
<p>イベント名：Ledge.ai年末年始特集「'25to'26」
開催期間：2025年12月1日(月) - 2026年1月9日(金)
形式：オンライン
参加費：無料（※一部のコンテンツ閲覧にはプロフィール登録が必要となります。）
お問合せ：contact@ledge.co.jp
URL：<a href="https://25to26.ledge.ai/lp">https://25to26.ledge.ai/lp</a></p>
]]></description>
      <pubDate>Mon, 27 Oct 2025 01:50:00 GMT</pubDate>
    </item>
  </channel>
</rss>