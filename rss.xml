<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
  <channel>
    <title>Ledge.ai 新着記事</title>
    <link>https://ledge.ai/</link>
    <description>Ledge.ai の最新テクノロジー記事</description>
    <item>
      <title>Spotify、Sony・Universal・Warnerの3大レーベルと提携──“アーティスト優先”のAI音楽製品を共同開発へ</title>
      <link>https://ledge.ai/articles/spotify_artist_first_ai_music_collaboration</link>
      <description><![CDATA[<p>Spotifyは2025年10月16日（現地時間）、Sony Music Group、Universal Music Group、Warner Music Groupの3大レーベルに加え、ライセンス大手Merlinおよびデジタル音楽企業Believeと提携し、「アーティスト優先（Artist-First）」のAI音楽製品を共同開発すると<a href="https://newsroom.spotify.com/2025-10-16/artist-first-ai-music-spotify-collaboration/">発表</a>した。AI技術の拡大を受け、アーティストやソングライターの権利を尊重しながら、透明性と倫理性を備えた“責任あるAI”の活用を進める。</p>
<h2>主要レーベルと連携、AI時代の「共創モデル」を構築</h2>
<p>提携では、Sony、Universal、Warnerの3大レーベルに加え、音楽ライセンス団体Merlinとデジタル音楽企業Believeが参画。Spotifyは「AIを音楽の創造性を広げるためのツールとして活用し、アーティストが主体的に選べる仕組みをつくる」と説明している。</p>
<p>開発される「Artist-First AI Music Products」は、AIを用いた音楽体験を創出しつつ、クリエイターの利益と選択権を中心に据えた設計となる予定だ。</p>
<h2>“責任あるAI”を支える4つの原則</h2>
<p>Spotifyは発表の中で、AI製品開発の基盤となる4つの原則を掲げている。</p>
<ol>
<li><strong>ライセンスに基づく正当な利用</strong>  — 合法的かつ倫理的なAI活用を保証。</li>
<li><strong>アーティストとソングライターの選択権</strong>  — 生成AIの利用可否を、創作者自身が選べる仕組みを導入。</li>
<li><strong>公正な報酬の確保</strong>  — AI生成を含むコンテンツ利用に対して、正当な収益分配を保証する。</li>
<li><strong>アーティストとファンの関係強化</strong>  — AIを通じて創作の多様性と新しい表現の機会を広げる。</li>
</ol>
<p>Spotifyは、「AIが創作の妨げではなく、アーティストの創造力を支援する存在になるべきだ」としている。</p>
<h2>1か月前に「AI保護強化」策を導入</h2>
<p>この発表の前段として、Spotifyは9月25日に「AI保護強化（Spotify Strengthens AI Protections）」を<a href="https://newsroom.spotify.com/2025-09-25/spotify-strengthens-ai-protections/">公表</a>{target=\</p>
]]></description>
      <pubDate>Sun, 19 Oct 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google DeepMind、AIに“造語”を教えて振る舞いを制御──Geminiが自ら意味を説明する能力も確認</title>
      <link>https://ledge.ai/articles/deepmind_neologism_learning_for_ai_controllability</link>
      <description><![CDATA[<p>Google DeepMindの研究チームは2025年10月9日、AIに新しい架空の言葉（造語）を学習させることで、その振る舞いを精密に制御できる手法を<a href="https://arxiv.org/abs/2510.08506">発表</a>した。論文「Neologism Learning for Controllability and Self-Verbalization」は、AIが学習した造語の意味を自然言語で説明できる“自己言語化（self-verbalization）”という現象も初めて報告している。</p>
<p>この研究はarXiv上で公開されたプレプリント（査読前論文）で、AIの内部表現を「言葉」で理解・制御する新しいアプローチとして注目を集めている。</p>
<h2>造語でAIをコントロール</h2>
<p>従来、AIの出力傾向を操作するには、プロンプト設計や外部ツール（例：steering vector、autoencoderなど）による内部操作が必要だった。今回の手法では、モデル本体のパラメータを一切変更せず、造語に対応する新しい単語埋め込み（embedding）だけを学習する。</p>
<p>たとえば「Give me a lack answer.」と指示すると、AIは短い回答を返すようになり、別の造語では「誤った回答」「お世辞」「拒否」など異なる挙動を誘発できる。</p>
<p>研究チームはこの方法を「ネオロジズム学習（Neologism Learning）」と呼び、言語による“行動パラメータ”の追加と位置づけている。</p>
<p><strong>ネオロジズム学習のプロセス。左から「造語による概念の学習」「AIによる自己言語化（Verbalization）」「説明文を使った再検証（Plug-In Evaluation）」の流れを示す</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x1_2e7a14d15f/x1_2e7a14d15f.png" alt="x1.png" /></p>
<h2>AIが自ら意味を説明</h2>
<p>研究チームは、AIが学習した造語の意味を英語で説明できることを確認した。
たとえば、短文回答を誘発する造語を学習したモデルに「What does lack mean?（lackとは何を意味しますか？）」と尋ねると、
「It means to give a shorter response.（短い回答をすることを意味します）」と答える。</p>
<p>このようにAI自身が学習した内部概念を自然言語で記述する能力を、研究チームは“self-verbalization（自己言語化）”と定義。
さらに、造語をその説明文に置き換えても同様の挙動が再現されるかを検証する「plug-in evaluation」を導入し、自己説明の信頼性を評価した。</p>
<h2>モデル間で“機械語”が通じる</h2>
<p>DeepMindの実験では、Gemma-3-4B-ITが学んだ造語を別のモデル――Gemini 2.5 Flash――に入力したところ、意味が通じ、ほぼ同じ制御効果を示した。
たとえば“lack”という語を用いた場合、Gemmaでは回答の平均文数が42.9から15.8に減少し、Geminiでも中央値が37から4に減少した。</p>
<p>研究チームはこの現象を「machine-only synonym（機械専用類義語）」と呼び、
人間には直感的に理解できないが、AI同士では通じ合う“共通語彙”が形成される可能性を指摘している。</p>
<h2>複合的な概念も制御可能</h2>
<p>ネオロジズム学習は、単純な行動特性だけでなく、複数の概念を組み合わせた複合的制御にも対応する。
たとえば「短く・数値を含む・高確率」といった3つの条件を、それぞれに対応する造語を同時に指定することで達成できるという。</p>
<p>研究では、「短文」「誤答」「お世辞」「拒否」など7種類の単純概念や、言語的特徴を扱うベンチマークAxBenchにおいても、高い制御性能が確認された。</p>
<h2>AIの「内なる言葉」への道</h2>
<p>著者のひとりであるジョン・ヒューイット（John Hewitt）氏は、
「私たちは既存の語彙だけではAIの内部概念を十分に理解できない」と述べ、造語学習をその橋渡しと位置づけている。</p>
<p>研究は、AIの制御可能性（controllability）と説明可能性（explainability）を同時に高める新しい方向性を示すものであり、
将来的にはAI間通信や人間との協調学習に応用できる可能性があるとみられる。</p>
]]></description>
      <pubDate>Sat, 18 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>「見て、聞いて、考えるCopilot」──Windows 11が“AIネイティブOS”へ進化</title>
      <link>https://ledge.ai/articles/windows11_copilot_vision_ai_pc_strategy</link>
      <description><![CDATA[<p>マイクロソフトは2025年10月16日（米国時間）、Windows 11における次世代AI機能の拡充方針を発表した。公式ブログ「<a href="https://blogs.windows.com/windowsexperience/2025/10/16/making-every-windows-11-pc-an-ai-pc/">Making every Windows 11 PC an AI PC</a>」で、すべてのWindows 11搭載端末を「AI PC」として再定義し、Copilotを中核に据えたAI体験をOS全体に統合していく構想を明らかにした。</p>
<h2>新しいウェイクワード「Hey Copilot」で話しかけるだけ</h2>
<p>@<a href="https://www.youtube.com/watch?v=7Nbf1fqxcCM">YouTube</a></p>
<p>同社は「AIはWindows体験の中心になる」として、音声・視覚・行動理解を備えた次世代のCopilot機能を順次展開する。音声による起動「Hey Copilot」、カメラや画像から状況を読み取る「Copilot Vision」、アプリ操作や設定変更などを文脈的に実行する「Copilot Actions」などが含まれるという。</p>
<p>たとえば、Copilot Visionは開いているグラフを要約したり、エラー画面を読み取って修正案を提示したりすることが可能。さらに音声起動「Hey Copilot」と組み合わせれば、「この表をPowerPointにまとめて」「この画像を説明して」といった自然な音声操作にも対応する。AIが“見て・聞いて・動く”ことで、ユーザーとのインタラクションがより直感的なものに変わる。</p>
<p>技術面では、NPU（Neural Processing Unit）を搭載した「AI PC」でオンデバイスAI処理を実行。これにより応答速度の向上とプライバシー保護を両立する。マイクロソフトは、AIをクラウド依存ではなくOSレベルに組み込む方針を明確にし、「AIネイティブOS」への移行を本格化させた。</p>
<p>同日公開された別の公式ブログ「<a href="https://blogs.windows.com/windowsexperience/2025/10/16/new-experiences-currently-rolling-out-for-windows-11">New experiences currently rolling out for Windows 11</a>」では、新しいCopilot体験の段階的な提供が開始されていることを明らかにした。ユーザーは自然言語でシステム設定やファイル操作、スクリーンショット分析、スケジュール調整などを行えるようになる。</p>
<p>さらに、開発者向けには外部サービスと連携できる「Copilot Extensions」を提供し、Microsoft Storeを通じてAI対応アプリの配信を拡充する方針も示された。</p>
<p>Windows 10のサポート終了（2025年10月）を目前に控え、マイクロソフトはWindows 11をAIネイティブOSとして再設計する姿勢を強調している。同社は「AIがPC体験そのものを再定義する」として、SurfaceシリーズやOEMメーカーと連携し、AI PC時代の普及を進めていく考えだ。</p>
]]></description>
      <pubDate>Sat, 18 Oct 2025 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>アルトマン氏「エロティックばかり注目されたけど」──ChatGPT、成人ユーザーの自由拡大へ</title>
      <link>https://ledge.ai/articles/openai_chatgpt_adult_mode_update_oct2025</link>
      <description><![CDATA[<p>OpenAIのサム・アルトマンCEOは10月14日（現地時間）、X（旧Twitter）上で、ChatGPTの安全制限を一部緩和し、成人認証済みユーザーに対してエロティックな会話を許可する方針を<a href="https://x.com/sama/status/1978129344598827128">発表</a>
した。</p>
<p>投稿は瞬く間に注目を集め、「エロティック解禁」が大きな話題となったが、アルトマン氏は翌日に「その部分ばかり注目されてしまったが」と<a href="https://x.com/sama/status/1978539332215681076">補足</a>し、実際には“より人間らしいAI体験”を実現するための包括的な方針変更であることを強調した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/gpt5_erotica_7cea863241/gpt5_erotica_7cea863241.jpg" alt="gpt5 erotica.jpg" /></p>
<h2>安全を優先してきたChatGPTの制限</h2>
<p>ChatGPTはこれまで、性的表現や親密な会話を含むコンテンツを厳しく制限してきた。
アルトマン氏は「メンタルヘルス問題に慎重を期すためだった」と説明し、精神的に不安定なユーザーに配慮した措置であったと振り返った。
「深刻な危機状態にあるユーザーは別扱いとし、他者に害を与える行為は依然として許可しない」と述べ、ポリシーの根幹は維持されるとしたうえで、「リスクのない成人ユーザーにはより多くの自由を与える」と明言した。</p>
<h2>“4oらしさ”を再導入──人間的なAI体験へ</h2>
<p>アルトマン氏は同日、「数週間以内に“4oで好まれた振る舞い”に近い人格（パーソナリティ）を選べる新バージョンのChatGPTを提供する」と投稿した。
GPT-4oは会話の自然さや表情豊かな応答で人気を集めたモデルであり、今後はユーザーが望む場合に、フレンドリーな口調や絵文字を多用した“人間らしい”対話スタイルを選べるようになる。
アルトマン氏は「これは利用時間を増やすためではなく、ユーザーが自分の望む形でAIと関わる自由を得るための設計だ」と述べている。</p>
<h2>「成人は大人として扱う」──12月に年齢認証を本格導入</h2>
<p>アルトマン氏は、12月に年齢認証を本格導入し、認証済み成人ユーザーに対してはエロティック会話なども許可する方針を示した。
一方で、ティーンエイジャーに対しては「安全をプライバシーや自由より優先する」と述べ、メンタルヘルス関連ポリシーは緩めないと強調している。</p>
<p>アルトマン氏は、「社会がR指定映画で境界を設けるように、AIにも適切な年齢境界を設けたい」と例え、「我々は選挙で選ばれた道徳警察ではない」と付け加えた。</p>
<h2>倫理と自由の境界線</h2>
<p>アルトマン氏の発言は、AIにどこまで人間的な自由を与えるかという議論を再燃させた。</p>
<p>OpenAIは今後、成人向け表現やAIの人格設計に関するガイドラインをさらに明確化するとみられる。今回の方針転換は、「AIをどう設計し、どう育てるか」という人間社会全体のテーマに踏み込む第一歩となりそうだ。</p>
]]></description>
      <pubDate>Sat, 18 Oct 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ロサンゼルス「パリセーズ火災」容疑者を逮捕──司法当局、ChatGPTで生成した“燃える都市”画像を押収と発表</title>
      <link>https://ledge.ai/articles/palisades_fire_chatgpt_image_doj</link>
      <description><![CDATA[<p>アメリカ司法省（U.S. Department of Justice）は2025年10月7日、ロサンゼルスのパシフィック・パリセーズ地区で1月に発生し、12人が死亡・6,000棟以上が焼失した大規模火災を「故意に発生させた」として、フロリダ州在住の29歳の男、ジョナサン・リンダークネヒト（Jonathan Rinderknecht）容疑者を逮捕・起訴したと<a href="https://www.justice.gov/usao-cdca/pr/florida-man-arrested-federal-criminal-complaint-alleging-he-maliciously-started-what">発表</a>した。</p>
<p>司法省の臨時連邦検事ビル・エッサイリ（Bill Essayli）氏は同日、X（旧Twitter）上で、押収されたデジタル機器の中に「容疑者がChatGPTで生成した“燃える都市”の画像」が含まれていたと<a href="https://x.com/USAttyEssayli/status/1975954598201536880">投稿</a>。同氏は「この悲劇による被害を元に戻すことはできないが、逮捕と起訴が犠牲者に一部でも正義をもたらすことを願う」と述べた。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/attorney_la_bill_f7db2f0510/attorney_la_bill_f7db2f0510.jpg" alt="attorney la bill.jpg" /></p>
<p>司法省カリフォルニア中部地区連邦検事局によると、リンダークネヒト容疑者は「悪意をもって火を放った（maliciously set a fire）」として連邦法に基づき起訴された。捜査にはATF（火器・爆発物取締局）、ロサンゼルス市消防局（LAFD）、ロサンゼルス郡保安局（LASD）が協力した。容疑者はフロリダ州オーランドで拘束され、同日午後1時30分（米東部時間）に連邦地方裁判所で初公判が予定されている。司法省は、容疑者が有罪と確定するまで無罪と推定されると明記している。</p>
<p>司法省の説明によれば、火災は2025年1月1日に発生し、乾燥と強風の影響で急速に拡大した。地下でくすぶった火が再び燃え上がり、最終的に6,000棟を超える住宅や建物が焼失した。ロサンゼルス市消防局はこの火災を「近年で最も被害の大きい火災のひとつ」としている。</p>
]]></description>
      <pubDate>Fri, 17 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google、動画生成AI「Veo 3.1」を発表──1分超のシーン拡張「Extend」搭載、Flowと統合強化</title>
      <link>https://ledge.ai/articles/google_veo3_1_flow_integration</link>
      <description><![CDATA[<p>Googleは現地時間2025年10月15日、動画生成AI「Veo」の最新版となるVeo 3.1を<a href="https://blog.google/technology/ai/veo-updates-flow/">発表</a>した。</p>
<p>新バージョンでは、AI映像制作ツール「Flow」に新機能を追加し、その中核としてVeo 3.1を統合。照明・構図・音声をAIが自動的に制御できるようになり、リッチなオーディオ、物語制御（narrative control）の強化、質感のリアリズム向上を図ったアップデートとなっている。</p>
<p>@<a href="https://www.youtube.com/watch?v=I06Ef8alr2Y">YouTube</a></p>
<h2>Flowとの統合で生成から編集まで一体化</h2>
<p>Googleは今回、AI映像制作ツール「Flow」へのアップデートを発表した。Flowは5か月前の導入以降、すでに2億7,500万本以上の動画を生成しており、Veo 3.1の搭載によって生成から編集までのプロセスがさらに統合された。</p>
<p>Flowは、テキスト・画像・音声・映像素材といった“ingredients”を組み合わせて動画を構築できるツールである。
既存の「Ingredients to Video」「Frames to Video」「Extend」機能に加え、今回は音声統合を拡張。ユーザーは複数の素材をもとに、AIがシーン構成やカメラワーク、トーンを自動的に最適化した一貫性のある映像を生成できる。</p>
<p>新しいExtendでは、直前のクリップの終端1秒を手がかりに、1分以上の連続ショットとして自然に拡張することも可能。Googleはこれにより、「映像制作をより直感的で対話的な体験へと変える」としている。</p>
<p>@<a href="https://www.youtube.com/watch?v=B78BJuPxmBU">YouTube</a></p>
<h2>照明や構図、音声もAIが自動編集</h2>
<p>Veo 3.1では、照明・陰影・カメラ構図をAIが自動的に制御し、シーン全体のトーンや一貫性を高める。
また、AIによる音声生成と映像への同期統合にも対応し、環境音や効果音を含む“音響的なリアリティ”を再現できるようになった。</p>
<p>Flow内には、シーンに要素を追加する「Insert」と、不要な物体を背景ごと削除する「Remove（近日提供）」の編集機能も加わった。影や照明の整合を自動で処理することで、合成感を抑えた自然な編集を可能にしている。
Googleは、こうした機能群を通じて「richer audio」「more narrative control」「enhanced realism」の実現を掲げている。</p>
<h2>Gemini APIで提供、Standard／Fastモデルを展開</h2>
<p>Veo 3.1は、「Standard」モデルと「Fast」モデルの2種類を用意し、Gemini API経由で開発者向けに提供が開始された。生成速度を優先するワークフローにはFastモデル、品質を重視する制作用途にはStandardモデルが推奨される。</p>
<p>さらに、Veo 3.1はVertex AIおよびGeminiアプリからも利用可能。新機能はGemini API／Vertex AIの双方で順次展開される予定で、API向けの「Scene extension」機能も今後提供される見込みだ。
Googleは「AIによる創造的表現の民主化をさらに進める」とし、プロフェッショナルから一般ユーザーまで、誰もが高品質な映像制作にアクセスできる環境の構築を目指している。</p>
<h2>Veoシリーズの進化</h2>
<p>Veoシリーズは、2024年12月の「Veo 2」、2025年春の「Veo 3」に続く最新バージョン。
今回のVeo 3.1では、「AI任せの自動生成」から「人とAIが協働して映像を作る」方向へと進化した。
Flowとの連携により、テキストによる指示だけでなく、素材・音声・カメラ指示などを含めた多層的なプロンプト設計が可能になり、AI映像生成の精度と自由度が大幅に向上している。</p>
<p>Googleは今後、VeoをGeminiエコシステムの中核技術として位置づけ、AIを活用したクリエイティブツールの拡充を進める方針を示している。</p>
]]></description>
      <pubDate>Fri, 17 Oct 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>日本プロ野球選手会、SNS上の誹謗中傷を検出するAIを導入──クライマックスシリーズと日本シリーズで運用開始</title>
      <link>https://ledge.ai/articles/npb_ai_antislander_2025</link>
      <description><![CDATA[<p>日本プロ野球選手会は2025年10月10日、クライマックスシリーズおよび日本シリーズに出場する選手を対象に、SNS上での誹謗中傷を自動検出するAIシステムを導入すると<a href="https://jpbpa.net/2025/10/10/12723/">発表</a>した。AIがSNS上の投稿を常時モニタリングし、不適切な内容を検出・通報する仕組みを構築する。選手会は「選手が安心して競技に集中できる環境をつくる」ことを目的としている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/JPBPA_Press1_eeab9f12c0/JPBPA_Press1_eeab9f12c0.jpg" alt="JPBPA_Press1.jpg" /></p>
<h2>クライマックスシリーズと日本シリーズで試験導入</h2>
<p>対象となるのは、「2025 JERA クライマックスシリーズ セ」「2025 パーソル クライマックスシリーズ パ」および「SMBC日本シリーズ2025」に出場登録された全選手。</p>
<p>AIがSNS上の投稿を監視し、誹謗中傷に該当する事案を自動的に検出。検出後は選手ごとにリスト化し、SNS運営元への通報や削除要請、NPB球団との情報共有、ダイレクトメッセージ（DM）対応、発信者情報開示請求や証拠保全などを通じて迅速な対応を行う。</p>
<h2>英Signify GroupのAIを活用</h2>
<p>導入されるのは、英国に本社を置くSignify Group社の誹謗中傷検出・通報支援サービス「Threat Matrix」。日本語を含む42言語と絵文字に対応し、主要SNS上の投稿をAIが自動的に分析して不適切な内容を検出する。
同システムは「FIFAワールドカップカタール2022」や「ラグビーワールドカップフランス2023」、女子テニス協会（WTA）および国際テニス連盟（ITF）などでも採用されており、英プレミアリーグ・アーセナルFCでは導入後、誹謗中傷の検出件数が<a href="https://www.arsenal.com/news/24-supporters-banned-abusive-behaviour">約90％減少</a>したという。</p>
<h2>選手と家族を守る取り組み</h2>
<p>選手会は「誹謗中傷が選手の家族に及んだ場合には家族へのサポートも行う」としており、対象を家族にも拡大。
SNS上の誹謗中傷が社会問題化する中、選手とその家族の精神的安全を守るための包括的な取り組みとして位置づけている。
同会は今後も、日本プロフェッショナル野球組織（NPB）および12球団と連携しながら、選手が安心して競技に集中できる環境づくりを進めていく。</p>
]]></description>
      <pubDate>Fri, 17 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>NTTとNTTドコモビジネス、自動運転レベル4対応の「通信安定化ソリューション」を提供開始──IOWN技術で遠隔監視の通信を高信頼化</title>
      <link>https://ledge.ai/articles/ntt_docomo_business_autonomous_driving_stable_communication_solution</link>
      <description><![CDATA[<p>NTTドコモビジネス株式会社とNTT株式会社は2025年10月8日、自動運転車両など移動体向けに通信の安定性を高める「通信安定化ソリューション」の提供開始を<a href="https://www.ntt.com/about-us/press-releases/news/article/2025/1008.html">発表</a>した。</p>
<p>両社が開発したIOWN（Innovative Optical and Wireless Network）技術を活用し、無線品質の予測に基づく複数回線のマルチパス通信制御と、データ連携システムを組み合わせることで、自動運転レベル4の遠隔監視を支える高信頼・低遅延な通信を実現する。</p>
<h2>自動運転レベル4の社会実装を支える通信基盤</h2>
<p>全国で進む自動運転レベル4の社会実装では、基地局の切り替えや通信干渉による一時的な映像途切れが課題となっている。NTTドコモビジネスとNTTは、これまで各地の自動運転実証実験で得た知見をもとに、通信の安定化に必要な複数技術をパッケージ化。導入までのリードタイムを短縮し、自治体や企業が容易に利用できる形で提供する。</p>
<h2>3つの技術を統合したパッケージ構成</h2>
<p>ソリューションは、次の3つの技術で構成される。</p>
<ol>
<li><strong>無線品質予測</strong> ：公衆ネットワークやローカル5G、Wi-Fiなどの無線品質を機械学習により予測（IOWN技術「Cradio」を活用）</li>
<li><strong>マルチパス通信制御</strong> ：通信状況に応じて複数回線を制御し、高い接続性を実現（IOWN技術「協調型インフラ基盤」を活用）</li>
<li><strong>リアルタイムデータ伝送</strong> ：車載カメラ映像やセンサーデータなどをリアルタイムに遠隔監視システムへ伝送（アプトポッド社「intdash」を活用）</li>
</ol>
<p>これにより、車両と遠隔監視システム間の通信を複数経路で同時接続し、無線品質の変化を先読みして制御することで、映像の途切れを抑えた安定した通信環境を構築できるという。</p>
<p><strong>通信安定化ソリューションの構成イメージ</strong>：車載クライアント、クラウドサーバー（ドコモMEC）、監視センタを連携し、IOWN技術「Cradio」による無線品質予測とマルチパス通信制御、intdashによるデータ伝送を統合している。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/1008b_423a2f1ca4/1008b_423a2f1ca4.jpg" alt="1008b.jpg" /></p>
<h2>実証で通信遅延を大幅に改善</h2>
<p>両社は公道で自動運転車両を用いた実証実験を実施。クライアントとクラウド間の通信遅延400ミリ秒以下を維持できた割合は、非適用時の92％（1回線）および53％（2回線）に対し、本ソリューション適用時は99％に達したとしている。</p>
<h2>各社の役割と今後の展開</h2>
<p>このソリューションはNTTドコモビジネスが提供を担当し、intdashを用いた環境構築や活用提案を行う。NTTは「Cradio」および「協調型インフラ基盤」に関する研究開発を担う。</p>
<p>今後は、自動運転における遠隔監視の安定化だけでなく、建設現場や工場、倉庫などでの遠隔操作・自動化分野への展開も見据える。両社は、通信安定化技術を通じて人手不足や安全確保などの社会課題の解決に貢献するとしている。</p>
]]></description>
      <pubDate>Fri, 17 Oct 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Boston Dynamics、トヨタ・リサーチ・インスティテュートと共同でヒューマノイドロボット「Atlas」の新型グリッパーを公開──“器用さ”を極める7DoFハンド開発の舞台裏</title>
      <link>https://ledge.ai/articles/bostondynamics_atlas_gripper_tri_202510</link>
      <description><![CDATA[<p>ボストン・ダイナミクス（Boston Dynamics）は2025年10月8日、トヨタ・リサーチ・インスティテュート（TRI）と共同研究を進めるヒューマノイドロボット「Atlas」の最新映像を<a href="https://www.youtube.com/watch?v=gS4rOqNDTBk">公開</a>した。</p>
<p>動画は同社公式シリーズ「Inside the Lab」の新作で、「Perception and Adaptability（知覚と適応）」をテーマに、ヒューマノイドの“手”にあたるグリッパー（ハンド）の進化を紹介している。</p>
<h2>油圧式から電動式へ──操作能力に焦点</h2>
<p>Atlasはこれまで、油圧駆動によるジャンプや宙返りなどのモビリティ性能で知られてきた。
今回の研究では、「移動」から「操作」へと重心を移し、より高い器用さ（dexterity）を持つ電動式グリッパーの開発に注力している。
Boston Dynamicsは「より人間に近い作業能力を備えたヒューマノイド」を目指し、把持（grasping）と認識（perception）の融合を進めているという。</p>
<h2>7DoF＋触覚＋掌カメラ──“感じてつかむ手”</h2>
<p><a href="https://bostondynamics.com/blog/ask-a-roboticist-meet-karl/">公式ブログ</a>によれば、現行のグリッパは7つの自由度（DoF）を備え、指先の触覚センサーと掌部カメラを搭載。滑りや力加減を検知して把持を調整でき、3本指＋対向親指（opposable thumb）の構成により、大型・重量物から繊細な小物まで幅広く扱える。</p>
<h2>左右ミラーハンドと指の可動</h2>
<p>グリッパーには左右のミラーバージョンがあり、指は内側に90°／後方に90°曲げられる設計。Atlasは人のような“利き手”を持たず、立ち位置や対象物に応じて最適な手を選んで作業する。</p>
<p>@<a href="https://www.youtube.com/watch?v=gS4rOqNDTBk">YouTube</a></p>
<h2>製造現場の実用タスクを見据える</h2>
<p>目標は自動車製造ラインなどで役立つ“実用タスク”の達成だ。外装部品のような重い部材を傷つけずに扱う作業と、ネジや工具の扱いといった繊細な作業の両立が求められる——今回のグリッパー設計は、まさにその要件に合わせて磨かれている。</p>
]]></description>
      <pubDate>Thu, 16 Oct 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ライオン、研究開発データで追加学習した独自LLM「LION LLM」開発に着手──AWS協力のもと、ものづくりDXを加速</title>
      <link>https://ledge.ai/articles/lion_llm_development_aws_dx_2025</link>
      <description><![CDATA[<p>ライオン株式会社は2025年10月8日、自社の長年にわたる研究開発データを用いて追加学習を行った独自の大規模言語モデル（LLM）「LION LLM」の開発に着手したと<a href="https://prtimes.jp/main/html/rd/p/000000218.000039983.html">発表</a>した。アマゾン ウェブ サービス ジャパン合同会社（AWSジャパン）の生成AI実用化推進プログラムを通じた協力を受け、ものづくり分野のデジタルトランスフォーメーション（DX）を加速する狙いだ。</p>
<h2>暗黙知の継承と「ものづくりDX」の推進</h2>
<p>ライオンは、「次世代ヘルスケアのリーディングカンパニーへ」を掲げる中期経営計画「Vision 2030 2nd STAGE」を2025年から始動。デジタル分野では「ものづくりDX」を重点テーマに位置づけ、収益力とクリエイティビティを両立させた競争力のある製品・サービスを迅速に市場へ投入できる体制を目指している。</p>
<p>同社では、製造業において重要な「暗黙知」の継承が課題となっていた。熟練技術者の退職により貴重なノウハウが失われつつあることを背景に、2023年12月には生成AIと検索システムを組み合わせた研究ナレッジ検索ツールを導入。情報検索時間を従来の5分の1以下に短縮するなどの成果を上げたが、専門知識を要する複雑な質問には対応が難しい状況が続いていた。</p>
<p>この課題を解決するため、ライオンはAWSジャパンの協力のもと、自社の知見を生かした独自LLMの内製開発を進めることを決定した。</p>
<h2>AWS支援のもとで社内に分散学習基盤を構築</h2>
<p>ライオンは2025年4月から、AWSジャパンが提供する「生成AI実用化推進プログラム」に参加。クレジット付与によるコスト支援や科学的助言などの技術協力を受け、内製開発体制を整備した。</p>
<p>社内には分散学習環境を構築。AWS ParallelClusterとNVIDIAのMegatron-LMを組み合わせ、複数GPUサーバーを効率的に連携させる仕組みを採用した。ベースモデルには「Qwen 2.5-7B」を使用し、研究報告書、製品組成情報、品質評価データなど、数十年にわたる社内知見を学習データとして投入している。</p>
<p>初期フェーズの評価では、過去の知見に基づく具体的なアドバイスや、複数の事例を統合した回答が可能であることを確認。従来ツールと比較して、回答の網羅性が大幅に向上したと社内で報告されている。</p>
<h2>今後の展開</h2>
<p>今後は、学習データの拡充と品質向上を目的に、プレゼン資料など構造化が難しいデータのクリーニングを進める。さらに、経済産業省およびNEDOが主導する「Generative AI Accelerator Challenge（GENIAC）」で開発された国産モデルの活用など、多角的なアプローチで精度向上を図る計画だ。</p>
<p>これらを既存のナレッジ検索ツールと統合し、高度な質問やタスクにも対応できるシステムを目指す。同社は、知識資産の最大活用を通じて「ものづくりDX」を加速し、競争優位性の強化につなげるとしている。</p>
]]></description>
      <pubDate>Wed, 15 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>東京大学・ソフトバンク・LINEヤフー、「Beyond AI技術研究組合」を設立──AI研究成果の事業化を加速</title>
      <link>https://ledge.ai/articles/beyond_ai_technology_consortium_tokyo_univ_softbank_line_yahoo</link>
      <description><![CDATA[<p>東京大学とソフトバンク、LINEヤフーは2025年10月10日、AI（人工知能）研究の成果を迅速に社会実装・事業化するための新たな産学連携組織「Beyond AI技術研究組合」を9月19日に設立したと<a href="https://beyondai.jp/contents/2025/10/10/cip3/">発表</a>した。</p>
<p>同組合は、経済産業省の「CIP（コーポレート・イノベーション・プラットフォーム）」制度を活用し、AI研究から事業化までを一貫して推進する体制を構築するという。</p>
<h2>研究と事業化の橋渡しを強化</h2>
<p>東京大学とソフトバンク、LINEヤフーの3者は、2020年設立の「Beyond AI研究推進機構」を通じてAI分野の基礎研究を進めてきた。今回の「Beyond AI技術研究組合」は、その研究成果を社会や産業へ迅速に還元するための組織で、知的財産や人材育成、資金調達などの面から事業化を支援する「プラットフォーム型CIP」として位置づけられる。</p>
<p>また、2024年6月に経済産業省がCIP設立・運営ガイドラインを改正し、1つのCIPから複数の事業会社を設立できる枠組みを導入したことを受け、同組合もその新制度を活用して設立された。</p>
<h2>今後の展開</h2>
<p>同組合は、パーソナルAIエージェント時代を見据えたAI技術の高度化や基盤技術開発を進めるとともに、Beyond AI連携事業で取り組んできた医療ヘルスケア領域などへのAI応用研究を推進する。
また、ソフトバンクグループと連携し、産業領域横断のデータ活用や社会実装に向けた実証実験（PoC）を進めていく。</p>
]]></description>
      <pubDate>Wed, 15 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Microsoft、自社設計の画像生成モデル「MAI-Image-1」を発表──フォトリアルな表現と高速生成を両立</title>
      <link>https://ledge.ai/articles/microsoft_mai_image_1_announcement</link>
      <description><![CDATA[<p>MicrosoftのAI部門であるMicrosoft AIは2025年10月14日（米国時間）、同社が初めて自社で設計・開発した画像生成モデル「MAI-Image-1」を<a href="https://microsoft.ai/news/introducing-mai-image-1-debuting-in-the-top-10-on-lmarena/">発表</a>した。</p>
<p>フォトリアルな質感や構図の整った出力に加え、既存モデルと比べて生成速度を大幅に向上させた点が特徴で、今後はCopilotやBing Image CreatorなどMicrosoft製AIサービスへの順次統合を予定している。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/sand_1_07e63f2f03/sand_1_07e63f2f03.jpg" alt="sand-1.jpg" /></p>
<h2>Microsoft初の完全社内開発モデル</h2>
<p>MAI-Image-1は、Microsoft AIが完全に社内で設計・訓練した初の画像生成モデルであり、既存の外部モデルを基盤とせず独自開発された。研究チームは、効率的な学習パイプラインとデータ選別技術を構築し、独自の高品質データセットを用いて訓練を行ったという。</p>
<p>モデルの評価は、画像生成AIの国際ベンチマーク「<a href="https://lmarena.ai/leaderboard/text-to-image">LMArena</a>」で実施され、初登場にしてトップ10入りを果たした。</p>
<h2>フォトリアルな描写と生成速度の両立</h2>
<p>公式ブログによると、MAI-Image-1は「現実に近い光表現」「安定した構図」「被写体の一貫性」に優れており、特に人間や風景のレンダリング品質が高いとされる。また、内部最適化により生成処理を高速化し、既存の大規模モデルよりも短時間で高解像度の画像を生成可能にした。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/crosswalk_7e79e8b8d5/crosswalk_7e79e8b8d5.jpg" alt="crosswalk.jpg" /></p>
<p>Microsoft AIは、テキストから画像を生成するプロンプト理解力の向上にも重点を置き、指示の意図や文脈をより正確に反映するアルゴリズムを採用したという。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/roadrunner_HQ_f5373a8b23/roadrunner_HQ_f5373a8b23.jpg" alt="roadrunner-HQ.jpg" /></p>
<h2>今後の展開：CopilotやDesignerに統合へ</h2>
<p>MAI-Image-1は今後、Copilot、Designer、Bing Image Creatorなど、同社の生成AIプロダクト群に順次統合される予定。Microsoftはこれにより、「安全で信頼性の高い画像生成を誰もが利用できる環境を提供する」としている。</p>
<p>発表ではまた、MAI-Image-1を同社の新しい社内モデル群「MAI-」シリーズの第1弾と位置づけており、今後は動画生成や3D生成分野への展開も視野に入れている。</p>
]]></description>
      <pubDate>Wed, 15 Oct 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI、Broadcomと戦略的提携──自社設計AIアクセラレーターを10GW規模で展開へ</title>
      <link>https://ledge.ai/articles/openai_broadcom_ai_accelerator_10gw_partnership</link>
      <description><![CDATA[<p>OpenAIは2025年10月13日（米国時間）、米半導体大手Broadcomとの戦略的提携を<a href="https://openai.com/index/openai-and-broadcom-announce-strategic-collaboration/">発表</a>した。両社は、OpenAIが設計したAIアクセラレーターとBroadcomのネットワークソリューションを組み合わせ、総計10ギガワット（GW）規模のAIクラスターを共同で構築する。展開は2026年後半に開始し、2029年末までの完了を予定している。（<a href="https://jp.broadcom.com/company/news/product-releases/63631">Broadcomのリリース</a>）</p>
<h2>10GW規模のAIアクセラレーターを共同開発・展開</h2>
<p>発表によるとOpenAIは自社でAIアクセラレーターとシステムを設計し、Broadcomが開発・展開を担う。ラックにはBroadcomのEthernet、PCIe、光接続などのソリューションを採用し、完全にEthernetベースで構築される。これにより、スケールアップ（拡張）とスケールアウト（分散拡張）の両面で柔軟なAIインフラを実現するという。</p>
<p>両社は、AIアクセラレーターおよびBroadcomのネットワーク技術を組み込んだラックを展開するための契約書（term sheet）を締結済み。すでに長期にわたる共同開発・供給の枠組みを構築しており、今後はOpenAIの自社施設およびパートナーデータセンターを中心に順次導入を進める。</p>
<h2>アルトマン氏「AIの可能性を引き出すための重要な一歩」</h2>
<p>OpenAIの共同創業者兼CEOであるサム・アルトマン氏は次のように述べた。
「Broadcomとの提携は、AIの潜在能力を解き放ち、人々や企業に実際の恩恵をもたらすための重要なステップです。自社アクセラレーターの開発は、AIの最前線を押し広げ、人類全体に利益を届けるための取り組みでもあります。」</p>
<p>また、共同創業者兼プレジデントのグレッグ・ブロックマン氏も、「自社設計チップによって、これまでのモデル開発で得た知見をハードウェアに直接組み込み、新たなレベルの知性と能力を引き出す」とコメントしている。</p>
<h2>Broadcom「AGIへの道を拓く協業」と強調</h2>
<p>Broadcomの社長兼CEOホック・タン氏は、「OpenAIとの協業は汎用人工知能（AGI）実現に向けた転換点だ」と述べ、10GW規模の次世代アクセラレーターとネットワークシステムの共同開発が「AIの未来への道を切り開く」と強調した。</p>
<p>同社の半導体ソリューション事業プレジデントであるチャーリー・カワス氏（Ph.D.）も、「オープンでスケーラブル、かつ電力効率の高いAIクラスター設計において新たな業界基準を打ち立てる」と述べ、Ethernetベースのアプローチがコストと性能の両立を可能にすると説明した。</p>
<h2>OpenAI、週8億人の利用基盤を背景に次世代インフラを整備</h2>
<p>OpenAIは現在、ChatGPTをはじめとする製品群を通じて、週あたり8億人を超えるアクティブユーザーを抱える。今回のBroadcomとの連携により、AIインフラの自社開発と最適化を進め、同社が掲げる「AGIの恩恵をすべての人に届ける」という使命の実現を加速させる狙いだ。</p>
]]></description>
      <pubDate>Tue, 14 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>AIがソフトウェアの脆弱性を自動修正──Google DeepMind、新エージェント「CodeMender」を発表</title>
      <link>https://ledge.ai/articles/deepmind_codemender_ai_vulnerability_fix</link>
      <description><![CDATA[<p>Google DeepMindは2025年10月10日、ソフトウェアの脆弱性を自動的に検出・修正するAIエージェント「CodeMender」を<a href="https://deepmind.google/discover/blog/introducing-codemender-an-ai-agent-for-code-security/">発表</a>した。</p>
<p>同エージェントは大規模言語モデル「Gemini Deep Think」を基盤とし、深層推論を活用して安全で安定したコード改変を行う。DeepMindは「AIが自ら脆弱性を理解し、修正案を提案・検証する」という新しいセキュリティサイクルを構築したと説明している。</p>
<h2>72件の脆弱性を自動修正</h2>
<p>CodeMenderはすでにオープンソースソフトウェアの実環境でテストされており、これまでに72件のセキュリティ修正を開発元のリポジトリへ統合した。対象コードには約450万行規模の大規模プロジェクトも含まれる。
DeepMindによれば、修正対象の一例として画像処理ライブラリ「libwebp」でのメモリ管理エラーがあり、AIによる自動改修によってバッファオーバーフロー耐性が向上したという。</p>
<h2>「反応型」と「先制型」AIによる修正</h2>
<p>CodeMenderは、既知の脆弱性に即応する反応型（reactive）と、将来的に問題を起こす可能性のある構造を事前に書き換える先制型（proactive）の2モードを備える。</p>
<p>内部では、静的解析・動的解析・ファジングなどを組み合わせた検査手法を使用し、修正案の妥当性をGemini Deep Thinkが多段階で検証。AIが生成したパッチは必ず人間エンジニアのレビューを経る仕組みで、誤修正を防ぐ安全策も講じられている。</p>
<p><strong>図：CodeMenderの自動修正フロー。LLMエージェントが脆弱性を検出し、Validatorが修正案を検証した後、人間によるパッチレビューを経てコードリポジトリへ統合する。</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/fixing_vulnerabilities_4975259e2f/fixing_vulnerabilities_4975259e2f.jpg" alt="fixing vulnerabilities.jpg" /></p>
<h2>セキュリティ運用の自動化へ</h2>
<p>DeepMindは、「CodeMenderはAIによるソフトウェアセキュリティの自動化を推進する重要な一歩」と位置づけている。今後はGoogleのSecure AI Framework（SAIF）やAI Vulnerability Reward Program（AI VRP）など、既存のセキュリティ施策と連携して展開を進める予定だ。
また、オープンソース開発者コミュニティとの協力を通じて、より多くのプロジェクトでのAI自動修正を実用化していくとしている。</p>
]]></description>
      <pubDate>Tue, 14 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>リコー、推論性能を強化した日本語LLMを発表──GPT-5相当の性能で金融業務に特化</title>
      <link>https://ledge.ai/articles/ricoh_reasoning_llm_gpt5_equivalent_20251010</link>
      <description><![CDATA[<p>リコーは2025年10月10日、推論（）性能を追加搭載した日本語大規模言語モデル（LLM）を<a href="https://jp.ricoh.com/release/2025/1010_1">発表</a>した。</p>
<p>このモデルは700億パラメータ規模のオンプレミス対応型LLMで、金融業務に特化して開発されたもの。同社によれば、多段推論能力（Chain-of-Thoughts: CoT）の導入により、融資稟議など専門的な業務遂行力を強化し、代表的な日本語ベンチマークでOpenAIの「GPT-5」と同等レベルの性能を確認したという。</p>
<h2>金融分野に特化した推論能力</h2>
<p>リコーは、有価証券報告書などの公開データを活用し、金融分野に特有の専門知識を追加学習したうえで、多段推論能力（Chain-of-Thoughts: CoT）を導入。複数の情報を論理的ステップに分解し、融資稟議業務などの複雑な判断を支援する能力を強化した。</p>
<p>このモデルはオンプレミス環境で運用できる「プライベートLLM」として提供され、企業が自社データを安全に学習させることが可能だとしている。</p>
<h2>ベンチマーク評価でGPT-5と同等スコア</h2>
<p>発表資料によると、代表的な日本語ベンチマークで次の結果を得た。
評価には「ELYZA-tasks-100」「Japanese MT-Bench」「japanese-lm-fin-harness」およびリコー独自の融資稟議ベンチマークが用いられた。</p>
<ul>
<li><strong>ELYZA-tasks-100</strong> ：複雑な指示タスク（要約、対話、意図の理解など）</li>
<li><strong>Japanese MT-Bench</strong> ：マルチターン対話能力</li>
<li><strong>japanese-lm-fin-harness</strong> ：金融分野における感情分析・証券知識・試験問題など</li>
<li><strong>融資稟議ベンチマーク</strong> ：企業・財務・信用の総合評価</li>
</ul>
<p>評価の結果、リコーの「Llama-3.3-Ricoh-70B-20251001」は日本語タスク全般でGPT-5に匹敵するスコアを示し、金融ベンチマークでは同規模以上のオープンソースモデルを上回ったとされる。</p>
<p><strong>ベンチマークツールにおける他モデルとの比較結果</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/_d289815140/_d289815140.jpg" alt="ベンチマークツールにおける他モデルとの比較結果.jpg" /></p>
<h2>高精度化を支える独自技術</h2>
<p>リコーのモデルは、Meta社の「Llama-3.3-70B-Instruct」を基に、日本語性能を強化した「Llama-3.3-Swallow-70B-v0.4」をベースモデルとして構築。独自のインストラクション・チューニングや、Chat Vectorを用いたモデルマージ技術、独自カリキュラムを組み合わせることで高精度化を実現した。大規模ながら省コストで動作し、GPUリソースの少ない環境でも運用できる点が特徴とされる。</p>
<h2>今後の展開</h2>
<p>リコーは今後、金融以外にも製造業・医療などの業種特化型モデルを順次開発するとしている。
同社は「使える・使いこなせるAI」の提供を掲げ、企業のデジタルトランスフォーメーション（DX）を支援していく方針だ。</p>
]]></description>
      <pubDate>Tue, 14 Oct 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>公共2025/10/13 [MON]AIとの出会い、早すぎる？──Pew調査で明らかになった子どものチャットボット利用の低年齢化</title>
      <link>https://ledge.ai/articles/pew_children_ai_chatbot_early_use</link>
      <description><![CDATA[<p>米国の調査機関Pew Research Centerは2025年10月8日、保護者3,251人を対象にした全国調査レポート「How parents manage screen time for kids」を<a href="https://www.pewresearch.org/internet/2025/10/08/how-parents-manage-screen-time-for-kids/">発表</a>した。報告によると、5〜7歳の子どもの3％、8〜10歳の7％、11〜12歳の15％が、ChatGPTやGeminiなどのAIチャットボットを利用した経験があると回答。生成AIとの接触が、幼児期を含む低年齢層にまで広がりつつあることが明らかになった。</p>
<h2>スクリーンタイム調査で浮かび上がる「AIの早期接触」</h2>
<p>この調査は、子どものスクリーンタイム管理やデジタル機器利用をテーマとするもので、ゲームやSNSと並び、AIチャットボットの利用状況も尋ねている。</p>
<p>Pew Researchは「幼児では少数だが、年齢が上がるにつれてAI利用が明確に増加している」と指摘。AIが“スクリーン利用”の一形態として、家庭生活の中に入り込み始めている様子が浮かび上がった。</p>
<p><strong>12歳以下の子どものデバイス利用（12歳以下、米国）</strong>：TV 90％、タブレット 68％、スマートフォン 61％など。AIチャットボットは全体で8％。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/PI_2025_10_08_parents_kids_screens_0_01_540d7def3a/PI_2025_10_08_parents_kids_screens_0_01_540d7def3a.webp" alt="PI_2025.10.08_parents-kids-screens_0-01.webp" /></p>
<p>Pew Researchは「幼児では少数だが、年齢が上がるにつれてAI利用が明確に増加している」と指摘。AIが“スクリーン利用”の新たな形態として、家庭生活の中に入り込み始めている様子が浮かび上がった。</p>
<h2>ChatGPTを“友達”と語る子どもも──親世代の戸惑い</h2>
<p>レポートには、10歳の子どもを持つ親のコメントとして、
\u003E「My child talks about ChatGPT like it’s a friend. I’m not sure how much I should worry about that.」
（うちの子はChatGPTのことを友達のように話す。どの程度心配すべきかわからない）</p>
<p>という声が掲載されている。AIを単なるツールではなく、対話相手として受け入れる子どもが現れ始めており、親世代との認識差も表れている。</p>
<h2>理解が追いつかない保護者、監督と教育のはざまで</h2>
<p>Pew Researchは、AIツールをめぐって「多くの親がまだ理解を深めている段階にある」と報告。一部の保護者は、子どものAI利用実態の把握に課題を感じているとの記述もある。</p>
<p>調査では、AIチャットボットを「学習支援」や「創作のきっかけ」として肯定的に見る一方で、内容の信頼性や年齢に応じた利用ルールの整備を求める声も寄せられた。</p>
<p><strong>年齢別にみたデバイス利用状況（米国）</strong>：5–7歳 3％／8–10歳 7％／11–12歳 15％。年齢上昇とともに利用が増える傾向。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/PI_2025_10_08_parents_kids_screens_0_02_1058158cf7/PI_2025_10_08_parents_kids_screens_0_02_1058158cf7.webp" alt="PI_2025.10.08_parents-kids-screens_0-02.webp" /></p>
<h2>家庭に入り込むAI、問われる「年齢相応の付き合い方」</h2>
<p>報告書では、AIチャットボットを
\u003E“conversational tools such as ChatGPT, Gemini, or similar products that allow users to generate text responses through typed prompts（ChatGPT、Gemini、またはユーザーが入力されたプロンプトを通してテキスト応答を生成できる類似製品のような対話ツール）”</p>
<p>と定義し、会話型の生成AI全般を対象にしている。スクリーンタイムという枠組みの中で、AIチャットボットが独立の利用カテゴリーとして扱われている。Pew Researchは、今後の課題として「家庭でのAI利用に関する年齢相応のガイドラインづくり」の必要性を指摘した。</p>
<h2>調査概要</h2>
<ul>
<li>発表日：2025年10月8日</li>
<li>調査名：How parents manage screen time for kids</li>
<li>実施機関：Pew Research Center（米国ワシントンD.C.）</li>
<li>対象：米国の保護者 3,251人（オンライン調査）</li>
<li>実施期間：2025年5月〜6月</li>
<li>調査対象年齢：5〜12歳の子ども</li>
<li>調査目的：家庭におけるスクリーンタイム管理とテクノロジー利用の実態把握</li>
</ul>
]]></description>
      <pubDate>Mon, 13 Oct 2025 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>LLMが「心の中でイメージ」を描く？──人間の想像課題を超える精度で解答、GPT-5が人間平均を12％上回る</title>
      <link>https://ledge.ai/articles/artificial_phantasia_llm_visual_reasoning</link>
      <description><![CDATA[<p>米ノースイースタン大学の研究チームは2025年9月27日、言語モデル（LLM）が視覚情報なしに、頭の中でイメージを描くような課題を解けることを示した論文「Artificial Phantasia: Evidence for Propositional Reasoning-Based Mental Imagery in Large Language Models」を<a href="https://arxiv.org/abs/2509.23108">発表</a>した。</p>
<p>人間の「心的イメージ（mental imagery）」を模した課題を、テキスト入力だけで解答させた結果、OpenAIのGPT-5とo3モデル群が平均67%の正答率を示し、人間（54.7%）を上回ったという。</p>
<h2>言葉だけで「形」を思い描くタスク</h2>
<p>研究は、認知心理学で半世紀以上議論されてきた「心的イメージが言語的か、それとも視覚的か」という論争をAIで再検証したもの。
参加者には、頭の中で文字や図形を組み合わせて新しい形を作り、それが何に見えるか答える課題が与えられた。</p>
<p>たとえば――</p>
<p><strong>図：心的イメージ課題の一例</strong> ：「大文字のD」を左に90度回転し、「J」を下に組み合わせると傘の形になる。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x1_7_26c25bc70d/x1_7_26c25bc70d.png" alt="x1 (7).png" /></p>
<p>こうした「視覚イメージなしでは解けない」とされてきたタスクを、研究チームは60題（うち48題を新規作成）用意し、LLMに文章だけで解答させた。</p>
<h2>GPT-5とo3が人間を上回る精度</h2>
<p>結果、GPT-5は67.0%、OpenAI o3 Proは66.6%、標準o3は64.1% の正答率を記録。人間の平均54.7%を9〜12%上回り、統計的に有意な差が確認された（p \u003C .00001）。
一方、Claude Sonnet 4やGemini 2.5 Proは40〜46%と低迷し、画像生成を併用した場合はむしろ精度が下がった。研究者は「画像を使わせると推論が乱れ、言語ベースの方が安定する」と分析している。</p>
<h2>「見ずに考える」命題的推論</h2>
<p>論文では、LLMが絵を思い浮かべているのではなく、言語構造に基づいて空間関係を再構築する「命題的推論（propositional reasoning）」を行っていると結論づけている。この結果は、「心的イメージは視覚的でなければならない」とする通説を覆す可能性があり、人間の想像力に関する認知科学の議論にも新たな示唆を与える。</p>
<h2>人間とAIの“アファンタジア”の比較へ</h2>
<p>研究チームは、視覚イメージを持たない「アファンタジア（aphantasia）」の人々も同様の課題をこなせる点に着目。「視覚表象を持たずとも、言語的・構造的な推論でイメージ依存課題を解ける」ことを、AIと人間の両方で確認した形だ。
今後は、アファンタジアの被験者とLLMの思考過程を比較し、「人工的想像力（Artificial Imagination）」の本質を探るとしている。</p>
<p>研究チームはGitHubで実験コードとデータを公開し（subjectivitylab/artificial_phantasia）、今後はマルチモーダルAIや新しい推論ベンチマークへの応用を予定している。</p>
]]></description>
      <pubDate>Mon, 13 Oct 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>デロイトの報告書に生成AIのハルシネーションで存在しない文献を引用・参照、豪政府に代金を一部返金──脚注誤りを訂正し再公開、コンサル業界に波紋</title>
      <link>https://ledge.ai/articles/deloitte_ai_refund_australia_report</link>
      <description><![CDATA[<p>コンサルティング大手のデロイト・オーストラリアが、AIを利用して作成した政府向け報告書に誤りが見つかり、オーストラリア連邦政府（雇用・職場関係省＝DEWR）に代金の一部を返金したことが分かった。報告書には存在しない論文や不正確な引用が含まれており、AI生成文書の品質管理をめぐる議論が広がっている。</p>
<p>DEWRは2025年9月26日付で、問題となった報告書「Targeted Compliance Framework（TCF） Assurance Review」と、その概要をまとめた「Statement of Assurance」を更新し、訂正版を公式サイトで公開した。
同省は<a href="https://www.dewr.gov.au/assuring-integrity-targeted-compliance-framework/resources/targeted-compliance-framework-assurance-review-final-report">公式ページ</a>で「この文書は9月26日に更新され、参照と脚注の誤りを訂正した。修正は結論や提言に影響を与えない」と明記している。</p>
<h2>存在しない文献をAIが生成</h2>
<p>調査対象となったのは、雇用支援制度「Targeted Compliance Framework（TCF）」に関する外部監査報告書で、総額約43万9,000豪ドル（約4,200万円）でデロイトに発注されていた。</p>
<p><a href="https://apnews.com/article/australia-ai-errors-deloitte-ab54858680ffc4ae6555b31c8fb987f3">AP通信</a>は、報告書に「実在しない学術論文への参照や、誤った引用が含まれていた」と報じている。また、Financial Times（FT）によると、デロイトは報告書の一部作成でMicrosoftの「Azure OpenAI」ツールを使用していたことを認め、改訂版にはその利用事実が追記されたという。</p>
<p>複数の誤りがAIによるハルシネーション（幻覚）に起因していると<a href="https://www.theguardian.com/australia-news/2025/oct/06/deloitte-to-pay-money-back-to-albanese-government-after-using-ai-in-440000-report">The Guardian</a>が指摘している。</p>
<h2>デロイト、返金を実施</h2>
<p>デロイトが最終支払い分を返金することでオーストラリア政府と合意した。デロイトは声明で「参照と脚注に関する誤りを認識し、修正を完了した」と説明している。DEWR側は、報告書の主要な所見や提言自体に変更はないとしており、「修正は文献参照に限定され、内容の妥当性には影響していない」としている。</p>
<h2>政府の声明と再発防止</h2>
<p>DEWRの事務次官は10月3日付の声明で、「Targeted Compliance Frameworkの透明性と信頼性を高めるため、独立レビューを踏まえて制度改良を進めている」と述べた。
声明では、Deloitteによる報告書も改善プロセスの一部として参照していることが明らかにされ、政府側の対応は継続中とみられる。</p>
<h2>コンサル業界で問われる「AIの信頼性」</h2>
<p>デロイトはAI導入を強化しており、同時期にAnthropicとの提携拡大を発表したと<a href="https://techcrunch.com/2025/10/06/deloitte-goes-all-in-on-ai-despite-having-to-issue-a-hefty-refund-for-use-of-ai/">TechCrunch</a>が報道。一方で、AI生成文書の誤りによって公共契約の信頼性が揺らいでいると指摘した。デロイトは世界的にAIツールを業務へ統合しているが、今回の件は「AIを利用した文書の監査体制」が整備途上であることを浮き彫りにした。</p>
]]></description>
      <pubDate>Mon, 13 Oct 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google、AIがPCを操作する「Gemini 2.5 Computer Use model」を開発者向けに公開──ClaudeやOpenAIモデルを上回る性能を実証</title>
      <link>https://ledge.ai/articles/google_gemini_2_5_computer_use_release</link>
      <description><![CDATA[<p>Google DeepMind は2025年10月7日（米国時間）、AI が実際のコンピューター画面を理解し、クリックや入力などの操作を実行できる新モデル「Gemini 2.5 Computer Use model」を開発者向けにプレビュー提供したと<a href="https://blog.google/technology/google-deepmind/gemini-computer-use-model/">発表</a>した。</p>
<p>Gemini API を通じて利用でき、AI が人間と同様にブラウザやアプリのUI（ユーザーインターフェース）を操作することを可能にする。</p>
<h2>Gemini API に“computer_use”ツールを追加</h2>
<p>今回発表された新モデルは、Gemini 2.5 の機能拡張として API に追加された「computer_use」ツールを用いて動作する。</p>
<p>AI はユーザーからの指示に加え、スクリーンショットと直近の操作履歴を入力として受け取り、次に取るべきアクション（クリック・入力・スクロールなど）を出力。実行結果を再び画面キャプチャとして取得し、目標達成までループ処理を行う。これにより、設定変更やフォーム入力、情報検索など、複数ステップを自律的に完了できる。</p>
<p>Google は公式ブログで、「このモデルはユーザー許可を前提に、安全性と透明性を重視して設計されている」と強調している。</p>
<p><strong>Computer Use model の処理ループ。AI がスクリーンショットと操作履歴をもとに次の行動を生成し、クライアント環境で実行 → 状況を再取得して次の判断へとつなげる</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/CTU_Diagram_RD_4_V01_width_1000_format_webp_b3415f41ee/CTU_Diagram_RD_4_V01_width_1000_format_webp_b3415f41ee.webp" alt="CTU-Diagram-RD4-V01.width-1000.format-webp.webp" /></p>
<h2>プレビュー提供と利用方法</h2>
<p>開発者は Google AI Studio および Vertex AI を通じて Computer Use model にアクセスできる。プレビュー版の段階では主にブラウザ操作に最適化されており、今後はより広範なアプリやデスクトップ環境への対応も検討されているという。</p>
<p>Google は、操作範囲やデータアクセスを制御する仕組みを組み込み、「責任ある自動化（Responsible Automation）」の実現を掲げている。</p>
<h2>ベンチマーク性能：Claude Sonnet 4.5 を上回る</h2>
<p>Google DeepMind は、Gemini 2.5 Computer Use model の性能を複数の標準ベンチマークで検証した。
Browserbase による Online-Mind2Web テストでは 65.7 % の精度を記録し、Claude Sonnet 4.5 や OpenAI Computer-Using Model を上回った。
さらに WebVoyager や AndroidWorld でも高スコアを達成し、実行速度（レイテンシ）でも優位性を示している。</p>
<p><strong>Gemini 2.5 Computer Use model は、Claude Sonnet 4.5 や OpenAI Computer-Using Model に比べ、低レイテンシかつ高精度を示した。</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/CTU_Scatterplot_RD_7_width_1000_format_webp_7e4e545c1c/CTU_Scatterplot_RD_7_width_1000_format_webp_7e4e545c1c.webp" alt="CTU-Scatterplot-RD7.width-1000.format-webp.webp" /></p>
<p><strong>複数ベンチマークで高い精度を記録。特に WebVoyager と AndroidWorld で際立ったスコアを達成した。</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/CTU_Benchmark_Chart_RD_5_V01_width_1000_format_webp_e0982edd33/CTU_Benchmark_Chart_RD_5_V01_width_1000_format_webp_e0982edd33.webp" alt="CTU-Benchmark_Chart-RD5_V01.width-1000.format-webp.webp" /></p>
<h2>動作デモ：AI がブラウザを自律操作</h2>
<p>公式ブログでは、実際の操作デモ動画も公開されている。
動画では AI が画面を認識し、ブラウザ上でリンクをクリックしたり、テキストを入力してタスクを完了する様子が確認できる。</p>
<p>@<a href="https://www.youtube.com/watch?v=_lu-FcPUIfM">YouTube</a></p>
<h2>AI による“手の届く自動化”へ</h2>
<p>今回の発表は、AI が人間の指示をもとに実際のUI を操作できる「エージェント時代」の幕開けを示す。
Google は Computer Use を “次世代の AI アシスタント” 開発の基盤と位置づけており、将来的には業務支援やウェブ操作、アプリ間連携など、より幅広い自動化領域への展開が期待される。</p>
]]></description>
      <pubDate>Mon, 13 Oct 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google、企業向けAIの“入口”「Gemini Enterprise」発表──従業員全員にGoogle AIを届ける統合プラットフォーム</title>
      <link>https://ledge.ai/articles/google_gemini_enterprise_ai_platform_launch</link>
      <description><![CDATA[<p>Googleは米国時間2025年10月9日、企業向けの新しいAIプラットフォーム「Gemini Enterprise（ジェミニ・エンタープライズ）」を<a href="https://blog.google/products/google-cloud/gemini-enterprise-sundar-pichai/">発表</a>した。
職場におけるAI活用の“入口（front door）”として位置づけられ、従業員全員が直感的なチャットインターフェースを通じて、GoogleのAI機能を安全かつ統合的に利用できる環境を提供する。</p>
<h2>部門単位のAIから、全社横断のAIへ</h2>
<p>Googleは発表の冒頭で、「AIは働き方、ビジネス運営、顧客体験のすべてを変革する歴史的機会をもたらす」としながらも、従来のAI活用が部門ごとに孤立していた課題を指摘した。Gemini Enterpriseは、この分断を解消し、ワークフローと従業員をつなぐ包括的なAIプラットフォームとして開発された。</p>
<p>Google Cloudによると、同社の強みは「信頼性の高いAIインフラ」「Google DeepMindによる先駆的研究」「多用途なGeminiモデルファミリー」の3層構造にある。このフルスタックAI戦略が、企業におけるAI変革を支える中核基盤になるという。</p>
<h2>6つの中核コンポーネントを統合</h2>
<p>Gemini Enterpriseは、以下の6つの要素を単一のインターフェースで統合する：</p>
<ul>
<li><strong>最新のGeminiモデル</strong> ：あらゆる業務タスクの“頭脳”として、高度な推論と生成を実現。</li>
<li><strong>ノーコード・ワークベンチ</strong> ：非エンジニアでもデータ分析やエージェント連携が可能。</li>
<li><strong>事前構築エージェント</strong> ：リサーチやデータインサイトなどの専門業務に即対応。</li>
<li><strong>企業データとの安全な接続</strong> ：Google WorkspaceやMicrosoft 365、Salesforce、SAPなどにシームレス接続。</li>
<li><strong>統合ガバナンス</strong> ：すべてのエージェントを一元的に可視化・保護・監査。</li>
<li><strong>オープンなエコシステム</strong> ：10万社を超えるパートナーによる拡張性を確保。</li>
</ul>
<p>Googleはこれにより、「単一タスクの効率化を超え、ワークフロー全体を自動化する」としている。</p>
<h2>業務アプリとの統合と新機能</h2>
<p>Gemini EnterpriseはGoogle Workspaceとも密接に連携する。
テキスト・画像・動画・音声を理解・生成できる初のマルチモーダルエージェントが導入され、文書作成や会議運営などの作業を支援。
「Google Vids」によるAI生成動画の作成機能や、「Google Meet」でのリアルタイム音声翻訳機能も提供される。
後者は発話者のトーンや表現を反映し、自然な多言語コミュニケーションを可能にするという。</p>
<p>さらに、「データサイエンスエージェント（プレビュー）」が発表された。
データ取り込みから探索、モデルトレーニングの自動化までを担い、VodafoneやWalmartなどの企業が既に活用している。</p>
<h2>導入事例の拡大と実用成果</h2>
<p>Googleは、Banco BV、Klarna、Mercedes-Benz、Swarovskiなどの導入事例を紹介した。
Banco BVでは、分析作業の自動化により、マネージャーが新規ビジネス開拓に注力できるようになった。Mercedes-BenzはGeminiを用いてドライバーと自然な会話ができる自動車内AIアシスタントを構築している。</p>
<p>@<a href="https://www.youtube.com/watch?v=ijqTReRzG8M&amp;t=26s">YouTube</a></p>
<p>日本企業では、メルカリがGoogle AIをコールセンターに導入。
AI主導のカスタマーサービス体験を実現し、業務量を20％削減、ROI（投資収益率）を500％向上させる見込みとしている。</p>
<h2>開発者×エージェント経済──A2AとAP2で拡張</h2>
<p>Gemini Enterpriseは企業だけでなく、開発者にも開かれたプラットフォームとして進化する。
すでに100万人以上が利用する「Gemini CLI」に加え、AIをコマンドラインから拡張できる「Gemini CLI Extensions」を導入。
さらに、開発者やISV（独立系ソフトウェアベンダー）がエージェントを構築・販売・収益化できる「エージェントエコノミー」の構想を発表した。</p>
<p>この仕組みを支えるのが、</p>
<ul>
<li><strong>Agent2Agent Protocol（A2A）</strong> ：エージェント間通信を標準化する新プロトコル</li>
<li><strong>Agent Payments Protocol（AP2）</strong> ：エージェントによる安全な金融取引を実現する決済標準
の2つである。AP2は、American Express、Mastercard、PayPalなど100社超のパートナーと共同で策定された。</li>
</ul>
<h2>学習・導入支援プログラム</h2>
<p>Googleは、AI人材育成と現場導入支援の両面から変革を後押しする。
新たに全従業員が無料でAIスキルを学べる「Google Skills」を開設し、開発者向けの教育プログラム「Gemini Enterprise Agent Ready（GEAR）」を開始した。
さらに、顧客企業に伴走して導入支援を行うAIエンジニアチーム「Team Delta」も新設している。</p>
<p>@<a href="https://www.youtube.com/watch?v=Qbix0BOPcgE">YouTube</a></p>
<h2>AIの“入口”としての意義</h2>
<p>Googleは、AIの未来を「オープンで協力的なエコシステム」にあるとし、Box、Workday、ServiceNowなど主要企業との連携を拡大している。クロスプラットフォームでのワークフロー連携や導入支援、検証済みエージェントの発見、収益化の仕組みなど、10万社超のパートナー基盤を活用し、AI導入を全層で支援する方針だ。</p>
<p>\u003E「Gemini Enterpriseは、Google AIの最高の機能をすべての従業員とワークフローに届ける“職場AIの新たな入口”です」（Google Cloud公式ブログより）</p>
<p>Googleは、企業にとってのAI導入を「一部の実験」から「全員の変革」へと引き上げる転換点として、この新プラットフォームを位置づけている。</p>
]]></description>
      <pubDate>Mon, 13 Oct 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>学術＆研究2025/10/11 [SAT]Huawei、LLMの精度を保持したまま最大70%メモリ削減できる新手法を発表──コンシューマーGPUでの高精度生成AI実行も視野に</title>
      <link>https://ledge.ai/articles/huawei_sinq_quantization_llm</link>
      <description><![CDATA[<p>中国の大手テクノロジー企業Huawei（華為技術）は2025年9月26日、大規模言語モデル（LLM）を一般的なGPU環境でも高品質に動作させるための新しい量子化技術「Sinkhorn-Normalized Quantization（SINQ）」を<a href="https://www.arxiv.org/abs/2509.22944">発表</a>した。</p>
<h2>Sinkhorn正規化で“再調整なし”を実現</h2>
<p>従来のLLM量子化では、精度を維持するために一部データを用いて再調整（キャリブレーション）を行う必要があった。SINQはその工程を省略し、「再調整なし」で精度を保つ新しい方式だ。</p>
<p>仕組みの中核となるのが、「Sinkhorn-Knoppアルゴリズム」を応用した正規化手法である。モデルの重み行列に対して、行方向と列方向の2つのスケーリングベクトルを設定（dual-scaling）し、両軸の分散を均一化することで、外れ値（outlier）が特定の行や列に偏る問題を防ぐ。この工程により、量子化後の誤差を最小限に抑えられるという。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x1_818acd143f/x1_818acd143f.png" alt="x1.png" /></p>
<h2>精度を保ちながら70％のメモリ削減</h2>
<p>Huaweiの研究チームは、同社のQwen3モデル（1.7B〜235B）やDeepSeek-V2.5（236B）などで実験を行い、SINQが既存手法（RTN、HQQ、Hadamard変換など）を上回る精度を示したと報告している。</p>
<p>具体的には、4bit量子化時にパープレキシティ（文章予測精度）を最大40％改善し、メモリ使用量を最大70％削減。さらに、8GB程度の一般的なGPU上でQwen3-7Bモデルを実行できたとしている。処理時間も高速で、量子化プロセスは従来のRTN法に比べてわずか1.1倍。再調整を伴う手法（AWQやGPTQなど）よりも最大30倍速いという。</p>
<h2>幅広いモデルで動作、非一様量子化とも互換</h2>
<p>SINQは、Qwenシリーズだけでなく、Llama 2・Llama 3・DeepSeek-V3などの異なるモデルにも適用可能。
また、非一様量子化フォーマット（NF4）との併用でも精度を維持しており、調整を行うAWQと組み合わせた「A-SINQ」ではさらに高い性能を達成した。論文では、Mixture-of-Experts（MoE）構造の大型モデルでも安定して動作することが示されている。</p>
<h2>コンシューマーGPUでのLLM実行を視野に</h2>
<p>Huaweiは、SINQを「キャリブレーション不要の汎用量子化手法」と位置づけており、高性能GPUに依存しないLLM運用を可能にする技術として注目されている。論文著者らは、SINQの目的を「メモリ効率と速度を両立し、エッジデバイスでも高品質な生成AIを実行可能にすること」と説明している。</p>
<p>コードはGitHub上で<a href="https://github.com/huawei-csl/SINQ">公開</a>されており、研究者や開発者が自由に評価・応用できる環境が整っている。</p>
]]></description>
      <pubDate>Mon, 13 Oct 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ジョニー・アイブとサム・アルトマンが語る「AIと人間の新しい関係」──次世代デバイス構想をDevDay 2025で明かす</title>
      <link>https://ledge.ai/articles/openai_jony_ive_ai_device_philosophy</link>
      <description><![CDATA[<p>OpenAIのCEOであるサム・アルトマン氏と、Apple製品のデザインを手がけたジョニー・アイブ氏（LoveFrom代表）は、2025年10月に米サンフランシスコで開催された開発者会議「DevDay 2025」で<a href="https://www.youtube.com/watch?v=7cKbPLzNYws">対談</a>を行い、人間中心のAIデバイス構想を明らかにした。両氏は、テクノロジーがもたらす「過剰な情報と不安」を乗り越え、AIを通じて「穏やかで幸福な体験をもたらす新しいデバイス」を目指すと語った。</p>
<h2>パートナーシップの起点はChatGPT</h2>
<p>アイブ氏は、約30年間在籍したAppleを退社後、デザイナーや建築家などの専門家で構成するチーム「LoveFrom」を設立した。当初は明確な目標を持たずに活動していたが、ChatGPTの登場が転機になったという。
「ChatGPTを見たとき、私たちの目的が結晶化した」とアイブ氏は語る。この出会いが、サム・アルトマン氏との協業を決定づけた。両者は約3年前から新しいAIデバイスの構想を練り始めたという。</p>
<h2>「クラフト」と「ケア」に支えられた創造哲学</h2>
<p>アイブ氏は創造の出発点を「人類への愛」と表現し、ものづくりの根幹に「クラフト（職人技）」と「ケア（思いやり）」を置く。
彼は「人に見えない部分へのこだわりこそが、作り手の誠実さを示す」と述べ、細部まで丁寧に仕上げる姿勢を強調した。さらに「人々は、ケアが込められた製品を直感的に感じ取る。逆に、無頓着さ（ケアレスネス）は容易に見抜かれる」とも語った。</p>
<p>この“ケアの精神”をAIデバイスにも反映させ、「技術のための技術」ではなく、「人の幸福のためのテクノロジー」を設計することを目指すという。</p>
<p>@<a href="https://www.youtube.com/watch?v=7cKbPLzNYws">YouTube</a></p>
<h2>AIがもたらす「人間らしい関係」</h2>
<p>両氏が共通して強調したのは、AIを用いて人とテクノロジーの関係を再構築するというビジョンだ。
アイブ氏は、スマートフォンが2007年に登場して以降、人々がテクノロジーとの関係で「圧倒的な情報量と絶望感」に直面していると指摘。「AIはこの問題を悪化させるものではなく、正面から向き合うチャンスだ」と述べた。</p>
<p>理想とする体験として、彼は「必然性と自明性」「ユーモアと喜び」「穏やかさと幸福感」を挙げる。アルトマン氏も「AIは、人間とテクノロジーの関係を再設計する力を持つ」と応じ、「このプロジェクトは、人がテクノロジーを“感じる”新しい方法を探る実験だ」と語った。</p>
<h2>「誰もが初心者」──AI時代の創造者の課題</h2>
<p>アイブ氏は、急速に進化するAI分野では「誰もが初心者」であり、過去の経験が時に足枷になることもあると述べた。
「AIの勢いがあまりに速く、焦点をどこに定めるかが難しい。しかし、動機が“人類への愛”である限り、進むべき方向は見失わない」と語った。</p>
<h2>理念と現実、その間にある課題</h2>
<p>両氏が掲げた人間中心のビジョンは、AI時代のデバイス設計に新たな指針を与えるものだ。
一方で、<a href="https://www.ft.com/content/58b078be-e0ab-492f-9dbf-c2fe67298dd3">Financial Times</a>などによると、同プロジェクトはハードウェア面の設計やプライバシー制御などで難航しており、思想と実装のギャップにも注目が集まっている。
理念と現実の交差点に立つこの挑戦が、どのような形で結実するのかが問われている。</p>
<h2>「今を変えるチャンスがある」</h2>
<p>両氏は、AIデバイス開発の目的を「人を幸せで穏やかにし、不安を和らげること」に置いている。
アイブ氏はセッションの最後にこう締めくくった。</p>
<p>同氏はAIの可能性を「現状の延長ではなく、根本的な変化をもたらすもの」として位置づけた。</p>
]]></description>
      <pubDate>Mon, 13 Oct 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>政府、OpenAIに著作権侵害防止を要請──「Sora 2」問題で平デジタル相は“オプトイン方式”を提言</title>
      <link>https://ledge.ai/articles/openai_sora2_government_copyright_request_oct2025</link>
      <description><![CDATA[<p>OpenAIの動画生成AI「Sora 2」による日本のアニメ作品に酷似した映像がSNS上で拡散している問題を受け、政府が対応に乗り出した。城内実内閣府特命担当大臣（知的財産戦略・クールジャパン戦略担当）は10月10日の<a href="https://www.gov-online.go.jp/press_conferences/minister_of_state/202510/video-303104.html">記者会見</a>で、OpenAIに対し著作権侵害となる行為を行わないよう要請したと明らかにした。</p>
<p>城内大臣は「アニメや漫画は世界の人々を魅了し続ける、我が国が世界に誇る宝」と述べ、知的財産権の保護を重視する姿勢を強調。要請は内閣府の知的財産戦略推進事務局からオンラインで直接行われたという。記者質問の内容から、実施時期は10月上旬で、Sora 2による“酷似動画”が相次いだ直後とみられる。</p>
<p>一方、平将明デジタル大臣は10月12日、TBS番組でAIの学習段階における権利処理の在り方について言及した。「OpenAIには、きちんと権利処理をしていただくようお願いしている」と述べたうえで、「AIの学習データについても、事前の同意を得るオプトイン方式が望ましい」と発言。AI事業者に対し、無断利用ではなく同意制に基づくデータ利用の仕組みを導入するよう求めた。</p>
<p>これに先立つ10月3日、OpenAIのCEOであるサム・アルトマン氏は、動画生成AI「Sora 2」に関連する著作権保護と収益分配制度に関する方針をブログで明らかにしていた。同氏は「試行錯誤を重ねながら早期に開始する」と述べ、経済的な利益と新しい関係構築の双方を実現したい考えを示している。</p>
<p>政府は今後もAI事業者に対し、著作権および文化的資産の保護を重視した対応を求める方針を示している。AIによる創作支援が拡大するなかで、学習データの扱いと権利保護の両立が国際的な課題となりつつある。</p>
]]></description>
      <pubDate>Mon, 13 Oct 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>世界初のAI女優「ティリー・ノーウッド」にハリウッドが揺れる──SAG-AFTRAや著名俳優が「創造性の危機」と警告</title>
      <link>https://ledge.ai/articles/tilly_norwood_ai_actress_controversy_sagaftra</link>
      <description><![CDATA[<p>世界初の「AI女優」と称される Tilly Norwood（ティリー・ノーウッド）が、ハリウッドの俳優や映画俳優組合SAG-AFTRA（全米映画俳優組合・テレビ・ラジオ芸術家連盟）から強い批判を受けている。</p>
<p>SAG-AFTRAは2025年9月30日に「創造性は人間中心であるべき」と声明を<a href="https://www.sagaftra.org/sag-aftra-statement-synthetic-performer">発表</a>した。女優のエミリー・ブラント氏やウーピー・ゴールドバーグ氏も相次いで懸念を表明し、映画・テレビ業界全体に議論が広がっている。</p>
<h2>「AI Commissioner」──ティリー誕生の舞台</h2>
<p>Tilly Norwoodは、ロンドン拠点の制作会社Particle6が2025年9月にYouTube上で公開したコメディスケッチ『AI Commissioner | Comedy Sketch | Particle6』で初登場した。</p>
<p>この作品は、テレビ業界におけるAIの急速な普及を風刺的に描いたもので、AIが脚本作成からキャスティング、予算編成まですべてを自動化する世界を描く。劇中では、AIが生み出したインタラクティブスリラー『I Know What You Streamed Last Summer』に登場する100％AI生成の俳優としてTillyが紹介される。</p>
<p>@<a href="https://www.youtube.com/watch?v=3sVO_j4czYs">YouTube</a></p>
<p>登場人物の1人はTillyについて「僕の言うことを何でも聞いてくれる。恋をしてしまったかもしれない（She’ll do anything I say. I think I’m in love）」と評し、AIによる従順で“最適化された”俳優像を象徴的に表現。
さらに、「“She’ll cry on Graham Norton and be monetized on TikTok by lunchtime.”（彼女はグレアム・ノートンの番組で泣いて、その日の昼までにTikTokで収益化されるだろう）」という台詞が、人間の感情までもがAIによって即座に商業化される未来への皮肉として話題を呼んだ。</p>
<h2>SAG-AFTRAが声明「Tillyは俳優ではない」</h2>
<p>SAG-AFTRAは9月30日に「Statement on Synthetic Performer（合成パフォーマーに関する声明）」を<a href="https://www.sagaftra.org/sag-aftra-statement-synthetic-performer">発表</a>し、次のように明言した。</p>
<p>\u003E“Tilly Norwood is not an actor. Creativity must remain human-centered.”
（ティリー・ノーウッドは俳優ではない。創造性は人間中心であるべきだ。）</p>
<p>声明では、AIによって作られた “合成俳優” が芸術表現を侵食する可能性を指摘し、「経験や感情を持たない存在を“俳優”と呼ぶことは、芸術の根幹を損なう」と警鐘を鳴らした。SAG-AFTRA会長のショーン・アスティン氏もVarietyの取材に対し、「AI倫理と補償問題を正式な交渉テーマとして扱う」と述べている。</p>
<h2>著名俳優からの反発</h2>
<p>女優のエミリー・ブラント氏は、Varietyのポッドキャスト番組でTillyの画像を見せられ、驚きを隠さずこう語った。</p>
<p>\u003E“Good Lord, we’re screwed. That is really, really scary. Come on, agencies, don’t do that. Please stop taking away our human connection.”
「なんてこと、私たちは終わりね。本当に恐ろしいわ。お願い、エージェントはそんなことをやめて。人間のつながりを奪わないで。」</p>
<p>また、取材でTillyを「次のスカーレット・ヨハンソンに」と問われると、「“But we have Scarlett Johansson.”（でも私たちにはスカーレット・ヨハンソンがいる」 と返し、人間俳優の価値を強調した。</p>
<p>俳優・司会者のウーピー・ゴールドバーグ氏も、ABCのトーク番組『The View』（9月30日放送）で次のように発言した。</p>
<p>\u003E“You’re looking at 5,000 actors rolled into one synthetic person. That’s not fair. We all move differently.”
「5,000人の俳優の特性をひとつの合成存在にまとめるなんてフェアじゃない。私たちは皆、動きも表情も違う。」</p>
<h2>開発側の見解：「人間の代替ではなく、芸術表現」</h2>
<p>Tillyを制作したエライン・ファン・デル・フェルデン氏（Eline Van der Velden）は9月28日、ティリーのInstagram（<a href="https://www.instagram.com/tillynorwood/">@tillynorwood</a>）に投稿し、AI俳優の創作意図について次のように述べている。</p>
<p>\u003E “She is not a replacement for a human being, but a creative work — a piece of art.”
「ティリーは人間の代替ではなく、創造的な作品＝ひとつのアートです。」</p>
<p>同氏は、AIを「人間の代わり」ではなく「新しい絵筆のようなツール」と位置づけ、「アニメーションや人形劇、CGIがライブ演技を奪うことなく新しい可能性を開いたように、AIも物語を構築する新たな手段を提供する」と説明した。</p>
<p>\u003E “I’m an actor myself, and nothing — certainly not an AI character — can take away the craft or joy of human performance.”
「私は俳優でもあり、AIキャラクターであっても、人間の演技の技や喜びを奪うことはできません。」</p>
<p>投稿ではさらに、AIを“人間と競わせる存在”ではなく“芸術の新しいジャンルの一部”として評価すべきだと訴えている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/instagram_eline_at_tillynorwood_b80dee2451/instagram_eline_at_tillynorwood_b80dee2451.jpg" alt="instagram eline at tillynorwood.jpg" /></p>
<p>同氏が率いるParticle6は「AI俳優を活用すれば制作コストを最大90％削減できる」と説明し、複数のAIタレントを育てる「デジタル・タレント・ユニバース」構想を掲げている。</p>
<h2>今後の焦点</h2>
<p>各報道や関係者の声明からは、主に次の3点が論点として浮上している。</p>
<ul>
<li>著作権と肖像権：AI俳優の訓練データやモデル構築に使用された素材の扱い</li>
<li>契約・補償問題：AI使用を前提とした俳優契約の新たな枠組み</li>
<li>文化的受容：AIが「演技」を行うことを人々がどう受け入れるか</li>
</ul>
<p>SAG-AFTRAをはじめとする業界団体や俳優たちは、AIが創作活動や雇用に及ぼす影響を注視しており、今後は法制度や契約の整備を含めた議論が進む見通しだ。</p>
]]></description>
      <pubDate>Mon, 13 Oct 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>清華大学、AIエージェントが仮想都市で経済活動を行う「SimCity」開発──フィリップス曲線やエンゲルの法則を自律再現</title>
      <link>https://ledge.ai/articles/tsinghua_simcity_ai_urban_economy</link>
      <description><![CDATA[<p>清華大学の研究チームは、複数のAIエージェントが仮想都市内で経済活動を行うシミュレーションシステム「SimCity」を開発した。研究成果は2025年10月1日付で論文「SimCity: Multi-Agent Urban Development Simulation with Rich Interactions」（arXiv:2510.01297）として<a href="https://arxiv.org/abs/2510.01297">公開</a>した。</p>
<p>SimCityは、家庭・企業・政府・中央銀行という4種類のエージェントを大規模言語モデル（LLM）で駆動し、都市の発展やマクロ経済の動きを自律的に再現する。エージェントは自然言語による推論を行い、労働市場・財市場・金融市場で相互に作用。人間のように意思決定を行いながら、都市経済の変動を模倣することができるという。
なお、論文中にはその記載はないが、同名の商用ゲームシリーズとは無関係と見られる。</p>
<h2>4種類のエージェントがつくる「AI経済」</h2>
<p>SimCityには、以下の4つの主要な役割を担うエージェントが登場する。</p>
<ul>
<li><strong>家計（Households）</strong> ：消費、就業、住宅選択、貯蓄・投資を決定する。</li>
<li><strong>企業（Firms）</strong> ：生産や雇用、価格設定、設備投資を行う。</li>
<li><strong>政府（Government）</strong> ：所得税や付加価値税（VAT）の徴収、公共支出や福祉政策を担当する。</li>
<li><strong>中央銀行（Central Bank）</strong> ：インフレ率やGDPの動向に応じて、修正版テイラー・ルールに基づき政策金利を設定する。</li>
</ul>
<p>また、視覚・言語モデル（VLM）が企業の立地や都市構造を決定。住宅地や工業地帯が自然に分かれるなど、都市の空間構成も自律的に形成される。</p>
<h2>経済法則を自然に再現</h2>
<p>研究チームは、44種類の財と最大200世帯から成る仮想経済を構築し、180ステップ（約15年）にわたるシミュレーションを実施した。前半36カ月を「移住フェーズ」、後半144カ月を「発展フェーズ」として進行した。</p>
<p>その結果、SimCityは以下のような実経済の特徴（stylized facts）を再現できたという。</p>
<ul>
<li><strong>フィリップス曲線</strong> ：インフレ率と失業率の逆相関</li>
<li><strong>オークンの法則</strong> ：失業率の変化とGDP成長率の負の関係</li>
<li><strong>ベバリッジ曲線</strong> ：求人率と失業率の逆相関</li>
<li><strong>エンゲルの法則</strong> ：所得の上昇に伴い食費の割合が低下</li>
<li><strong>需要の価格弾力性</strong> ：必需品は非弾力的、贅沢品は弾力的</li>
<li><strong>投資の高ボラティリティ</strong> ：消費よりも投資が景気に敏感</li>
</ul>
<p>これらは実際の経済データ（FRED, 1970Q1–）との比較でも一致傾向を示し、従来のエージェント・ベース・モデル（ABM）では再現が難しかった現象を多く含むと報告されている。</p>
<h2>都市発展と価格ショックへの応答</h2>
<p>SimCityは「移住フェーズ（36カ月）」と「発展フェーズ（144カ月）」の2段階で進行する。
初期段階では移住者の流入によりGDPが上昇し、住宅と生産拠点の配置が形成される。VLMの判断により、住宅は都市中心部に、工場は周辺部に集中するなど、現実的なゾーニング構造が自律的に現れた。</p>
<p>さらに研究チームは、一部の財の価格を50％変動させる「価格インパルス実験」を実施。価格は短期的に大きく変動したが、数年のうちに均衡へ回帰。実際の経済で観察される「価格の粘着性（menu cost）」と同様の挙動が確認された。</p>
<h2>「AIがつくる社会をAIで観察」</h2>
<p>研究では、OpenAIの「GPT-4o-mini」を中心に、Azure OpenAI APIを通じてLLMを運用。各エージェントは独立して推論し、シミュレーション1ステップあたり約0.25ドルのコストで実行された。
総トークン数は約80万で、全体の実行コストは約180ドル。</p>
<p>研究チームは、SimCityを「AIがつくる社会をAIで観察するための基盤」と位置づけており、都市計画や経済政策の設計、AI社会の倫理実験などへの応用を見込んでいる。今後は、金融市場や株式取引などの要素を導入し、より現実的な経済表現へ拡張する予定だ。</p>
]]></description>
      <pubDate>Mon, 13 Oct 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/10/11 [SAT]ソフトバンクとオラクル、AI活用のためのソブリンクラウドを共同構築──国内運用基盤「Cloud PF Type A」でデータ主権を確保</title>
      <link>https://ledge.ai/articles/softbank_oracle_sovereign_cloud_pf_type_a</link>
      <description><![CDATA[<p>ソフトバンク株式会社と米オラクルは2025年10月8日、データ主権（ソブリン性）を確保しつつ、AI活用を前提としたクラウド基盤「Cloud PF Type A」を日本国内で運用するための協業を開始したと<a href="https://www.softbank.jp/corp/news/press/sbkk/2025/20251008_01/">発表</a>した。オラクルのクラウド技術「Oracle Alloy」を採用し、ソフトバンクが国内データセンターで独自運用を担う。政府機関や企業が自国の管理下で安全にAIやクラウドを活用できるソブリンクラウドの実現を目指す。</p>
<h2>データ主権を守る「ソブリンクラウド」</h2>
<p>両社が開発する「Cloud PF Type A」は、すべてのデータ処理・保存・管理を日本国内で完結させる構成をとる。国外へのデータ移転を伴わずにクラウドやAIを活用できるようにすることで、経済安全保障やガバナンス上のリスクを最小化する狙いがある。</p>
<p>ソフトバンクは通信事業で培った国内ネットワークと運用ノウハウを活用し、オラクルは独自のクラウド基盤「Oracle Alloy」を通じて技術支援を提供。両社は「日本の社会インフラの一部として信頼されるクラウドを構築する」としている。</p>
<h2>「Cloud PF Type A」を2026年に提供開始</h2>
<p>新たに構築される「Cloud PF Type A」は、2026年4月に東日本拠点、同年10月に西日本拠点での提供を予定している。
この基盤では、Oracle Cloud Infrastructure（OCI）の約200種類のクラウドおよびAIサービスを国内運用で利用可能になる。</p>
<p>主な特徴は次の通り。</p>
<ul>
<li><strong>鍵管理（KMS）</strong> ：Oracle Vaultとソフトバンク独自システムを併用し、暗号鍵を完全に国内で管理。</li>
<li><strong>通信構成</strong> ：「OnePort」や「SmartVPN」などの閉域網接続に対応し、安全なデータ通信を実現。</li>
<li><strong>災害対策</strong> ：東西データセンターの冗長構成を採用し、事業継続（BCP）に対応。</li>
<li><strong>運用支援</strong> ：ソフトバンクがMSP（運用管理代行）を提供し、導入から保守まで一括サポート。</li>
</ul>
<p>この構成により、政府・自治体・企業などがAIモデルの学習や推論を行っても、データが国外へ移動することはない。</p>
<h2>両社のコメント</h2>
<p>ソフトバンクは、自社DCの高いセキュリティ水準に適合したクラウドを提供し、生成AIやGPUを統合して多様な顧客ニーズに応える方針と述べた。オラクルは、Oracle Alloyにより日本国内のデータ主権要件に対応し、OCIの幅広いAI/クラウドを国内DCで利用可能にする取り組みだと説明した。</p>
<h2>国内で広がる“主権クラウド”構想</h2>
<p>欧州を中心に広がる「ソブリンクラウド（主権クラウド）」の潮流は、日本国内でも加速している。
NTT、富士通、日立などが相次いで国産クラウド基盤の整備を進めており、今回のソフトバンクとオラクルの取り組みもその一環と位置づけられる。</p>
<p>両社は今後、生成AIや自然言語処理、画像解析などの高負荷AIワークロードにも対応する予定で、AI時代のデータ主権を支える中核的なインフラを目指す。</p>
]]></description>
      <pubDate>Sat, 11 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/10/10 [FRI]イーロン・マスク率いるxAI、動画生成AI「Imagine v0.9」を公開──静止画に声と動きを与える“ネイティブ映像生成”モデル</title>
      <link>https://ledge.ai/articles/xai_imagine_v09_release</link>
      <description><![CDATA[<p>イーロン・マスク氏が率いるAI開発企業xAIは2025年10月8日（現地時間）、新たな動画生成モデル「Imagine v0.9」を<a href="https://x.com/xai/status/1975607901571199086">発表</a>した。視覚品質、動き、音声生成などを全面的に改良し、すべてのxAI製品で無料利用が可能となっている。</p>
<p>xAIは公式X（旧Twitter）で「Imagine v0.9は、映像品質・モーション・音声生成などにおいてv0.1から大幅に進化した」と投稿。<a href="https://grok.com/imagine">grok.com/imagine</a> では、Grokプラットフォーム上で同モデルを試せるようになっている。</p>
<h2>音声と映像を同時生成──“編集不要”の体験を掲げる</h2>
<p>xAIは今回の発表で、「Imagine v0.9は音声と映像を同時に生成する“ネイティブ・オーディオ＋ビデオ生成”を実現した」と説明。</p>
<p>\u003E“Imagine v0.9 pushes the boundaries of native audio + video generation, creating cinematic experiences straight out of the box—no editing required.”</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x_AI_Imagine_v0_9_1_dragon_6a3a28422f/x_AI_Imagine_v0_9_1_dragon_6a3a28422f.jpg" alt="xAI Imagine v0-9-1 dragon.jpg" /></p>
<p>投稿では、音声と映像が同期したドラゴンのデモ映像を公開。ユーザーは、生成後の編集作業なしで“完成された動画”を得られるとしている。</p>
<h2>モーション精度とカメラ効果を強化</h2>
<p>Imagine v0.9では、被写体の滑らかな動きとリアリズムを高精度で再現するモーション制御を導入した。
さらに「インテリジェント・フォーカスシフト」と呼ばれる自動焦点移動など、ストーリーテリングに適したダイナミックなカメラ効果も追加された。</p>
<p>\u003E“And lets you add dynamic camera effects like intelligent focus shifts for better storytelling.”</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x_AI_Imagine_v0_9_1_camera_effects_5febfab05e/x_AI_Imagine_v0_9_1_camera_effects_5febfab05e.jpg" alt="xAI Imagine v0-9-1 camera effects.jpg" /></p>
<h2>自然な対話・歌唱・リズムまで再現</h2>
<p>Imagine v0.9では、音声表現の幅も拡大。自然な対話や歌唱を含む“声の演出”が可能になった。</p>
<p>\u003E“v0.9 also brings videos to life with natural dialogue and strong audio-visual harmony.”</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x_AI_Imagine_v0_9_1_natural_dialogue_f30bf3d090/x_AI_Imagine_v0_9_1_natural_dialogue_f30bf3d090.jpg" alt="xAI Imagine v0-9-1 natural dialogue.jpg" /></p>
<p>\u003E“It also brings expressive singing to life with clear vocals and synchronized emotion.”</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x_AI_Imagine_v0_9_1_singing_05b4553dab/x_AI_Imagine_v0_9_1_singing_05b4553dab.jpg" alt="xAI Imagine v0-9-1 singing.jpg" /></p>
<p>xAIはさらに、音と動きを合わせたダンス生成の例として、キャラクター「Ani」の動画を紹介している。</p>
<p>\u003E“Plus good rhythm: here's Ani with smooth dance moves.”</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x_AI_Imagine_v0_9_1_ani_dancing_c19660cb25/x_AI_Imagine_v0_9_1_ani_dancing_c19660cb25.jpg" alt="xAI Imagine v0-9-1 ani dancing.jpg" /></p>
<p>xAIはユーザーからのフィードバックを積極的に求め、モデル改良に反映させる方針を示した。</p>
]]></description>
      <pubDate>Fri, 10 Oct 2025 23:50:00 GMT</pubDate>
    </item>
  </channel>
</rss>