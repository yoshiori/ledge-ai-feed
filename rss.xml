<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
  <channel>
    <title>Ledge.ai 新着記事</title>
    <link>https://ledge.ai/</link>
    <description>Ledge.ai の最新テクノロジー記事</description>
    <item>
      <title>ソフトバンクグループ、スイスABBのロボティクス事業を約8187億円で買収──AIと自律ロボットの統合を加速</title>
      <link>https://ledge.ai/articles/softbank_acquires_abb_robotics_for_ai_integration</link>
      <description><![CDATA[<p>ソフトバンクグループ株式会社は2025年10月8日、スイスのABB Ltd.（以下ABB）から同社のロボティクス事業を買収する最終契約を締結したと<a href="https://group.softbank/news/press/20251008">発表</a>した。買収総額は53億7500万ドル（約8187億円）で、取引の完了は2026年半ばから後半を予定している。AIとロボティクスを融合させるグローバル戦略の一環として、産業用・協働ロボットの分野で新たな展開を図る。</p>
<h2>AIとロボットを結ぶ「フィジカルAI」構想</h2>
<p>ソフトバンクグループ代表取締役会長兼社長の孫正義氏は、同社の次のフロンティアは「フィジカルAI」にあると述べ、AIの知能とロボティクスの身体を結びつけることで、人類の未来を切り拓く画期的な進化を実現していくと強調した。
同グループはこれまで、SoftBank Robotics GroupやBerkshire Grey、AutoStore、Agile Robots、Skild AIなどに投資しており、今回の買収を通じてAIと実機ロボットの統合をさらに加速させる考えだ。</p>
<h2>ABBの発表：「AI時代のロボティクスを担う最適な拠点」</h2>
<p>ABBは同日発表した<a href="https://new.abb.com/news/detail/129685/abb-to-divest-robotics-division-to-softbank-group">リリース</a>で、ロボティクス事業をソフトバンクに譲渡すると公表。</p>
<p>ABBのCEOであるMorten Wierod氏は、ソフトバンクグループはABBロボティクス事業と従業員にとって最適な新しい拠点となり、AIを基盤とするロボティクスの新時代において事業の成長をさらに後押しするだろうと述べた。</p>
<p>新会社の代表は、現ABB ロボティクス部門社長Marc Segura氏が務める予定で、従業員は約7,000人。2024年の売上は22億7,900万ドルで、ABB全体の売上の約7%を占めている。名称や資本金などの詳細は現時点で未定とされている。</p>
<h2>産業オートメーションから自律ロボットへ</h2>
<p>ABBロボティクスは、AIを活用した協働ロボット（コボット）や産業オートメーションソリューションを展開し、グローバル市場で高いシェアを持つ。
ソフトバンクはこれらの技術をAIプラットフォームと連携させ、製造、物流、医療、サービスなど多様な領域で自律ロボットの社会実装を進める構想を描く。
今回の買収は、AIを「知能」から「行動」へ拡張し、デジタルとフィジカルの融合を現実世界で具現化する取り組みの一環といえる。</p>
<h2>取引の枠組みと今後の見通し</h2>
<p>本取引は、ABBがロボティクス事業を分社化し、新設する持株会社の全株式をソフトバンクグループが取得する形で実施される。両社の取締役会で承認済みであり、各国の規制当局による承認を経て、2026年半ばから後半に完了する見通し。
取引後も両社は協力関係を維持し、ロボティクス分野での技術開発や市場展開を継続していく方針を示している。</p>
]]></description>
      <pubDate>Fri, 10 Oct 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>LLMが「心の中でイメージ」を描く？──人間の想像課題を超える精度で解答、GPT-5が人間平均を12％上回る</title>
      <link>https://ledge.ai/articles/artificial_phantasia_llm_visual_reasoning</link>
      <description><![CDATA[<p>米ノースイースタン大学の研究チームは2025年9月27日、言語モデル（LLM）が視覚情報なしに、頭の中でイメージを描くような課題を解けることを示した論文「Artificial Phantasia: Evidence for Propositional Reasoning-Based Mental Imagery in Large Language Models」を<a href="https://arxiv.org/abs/2509.23108">発表</a>した。</p>
<p>人間の「心的イメージ（mental imagery）」を模した課題を、テキスト入力だけで解答させた結果、OpenAIのGPT-5とo3モデル群が平均67%の正答率を示し、人間（54.7%）を上回ったという。</p>
<h2>言葉だけで「形」を思い描くタスク</h2>
<p>研究は、認知心理学で半世紀以上議論されてきた「心的イメージが言語的か、それとも視覚的か」という論争をAIで再検証したもの。
参加者には、頭の中で文字や図形を組み合わせて新しい形を作り、それが何に見えるか答える課題が与えられた。</p>
<p>たとえば――</p>
<p><strong>図：心的イメージ課題の一例</strong> ：「大文字のD」を左に90度回転し、「J」を下に組み合わせると傘の形になる。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x1_7_26c25bc70d/x1_7_26c25bc70d.png" alt="x1 (7).png" /></p>
<p>こうした「視覚イメージなしでは解けない」とされてきたタスクを、研究チームは60題（うち48題を新規作成）用意し、LLMに文章だけで解答させた。</p>
<h2>GPT-5とo3が人間を上回る精度</h2>
<p>結果、GPT-5は67.0%、OpenAI o3 Proは66.6%、標準o3は64.1% の正答率を記録。人間の平均54.7%を9〜12%上回り、統計的に有意な差が確認された（p \u003C .00001）。
一方、Claude Sonnet 4やGemini 2.5 Proは40〜46%と低迷し、画像生成を併用した場合はむしろ精度が下がった。研究者は「画像を使わせると推論が乱れ、言語ベースの方が安定する」と分析している。</p>
<h2>「見ずに考える」命題的推論</h2>
<p>論文では、LLMが絵を思い浮かべているのではなく、言語構造に基づいて空間関係を再構築する「命題的推論（propositional reasoning）」を行っていると結論づけている。この結果は、「心的イメージは視覚的でなければならない」とする通説を覆す可能性があり、人間の想像力に関する認知科学の議論にも新たな示唆を与える。</p>
<h2>人間とAIの“アファンタジア”の比較へ</h2>
<p>研究チームは、視覚イメージを持たない「アファンタジア（aphantasia）」の人々も同様の課題をこなせる点に着目。「視覚表象を持たずとも、言語的・構造的な推論でイメージ依存課題を解ける」ことを、AIと人間の両方で確認した形だ。
今後は、アファンタジアの被験者とLLMの思考過程を比較し、「人工的想像力（Artificial Imagination）」の本質を探るとしている。</p>
<p>研究チームはGitHubで実験コードとデータを公開し（subjectivitylab/artificial_phantasia）、今後はマルチモーダルAIや新しい推論ベンチマークへの応用を予定している。</p>
]]></description>
      <pubDate>Fri, 10 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>デロイトの報告書に生成AIのハルシネーションで存在しない文献を引用・参照、豪政府に代金を一部返金──脚注誤りを訂正し再公開、コンサル業界に波紋</title>
      <link>https://ledge.ai/articles/deloitte_ai_refund_australia_report</link>
      <description><![CDATA[<p>コンサルティング大手のデロイト・オーストラリアが、AIを利用して作成した政府向け報告書に誤りが見つかり、オーストラリア連邦政府（雇用・職場関係省＝DEWR）に代金の一部を返金したことが分かった。報告書には存在しない論文や不正確な引用が含まれており、AI生成文書の品質管理をめぐる議論が広がっている。</p>
<p>DEWRは2025年9月26日付で、問題となった報告書「Targeted Compliance Framework（TCF） Assurance Review」と、その概要をまとめた「Statement of Assurance」を更新し、訂正版を公式サイトで公開した。
同省は<a href="https://www.dewr.gov.au/assuring-integrity-targeted-compliance-framework/resources/targeted-compliance-framework-assurance-review-final-report">公式ページ</a>で「この文書は9月26日に更新され、参照と脚注の誤りを訂正した。修正は結論や提言に影響を与えない」と明記している。</p>
<h2>存在しない文献をAIが生成</h2>
<p>調査対象となったのは、雇用支援制度「Targeted Compliance Framework（TCF）」に関する外部監査報告書で、総額約43万9,000豪ドル（約4,200万円）でデロイトに発注されていた。</p>
<p><a href="https://apnews.com/article/australia-ai-errors-deloitte-ab54858680ffc4ae6555b31c8fb987f3">AP通信</a>は、報告書に「実在しない学術論文への参照や、誤った引用が含まれていた」と報じている。また、Financial Times（FT）によると、デロイトは報告書の一部作成でMicrosoftの「Azure OpenAI」ツールを使用していたことを認め、改訂版にはその利用事実が追記されたという。</p>
<p>複数の誤りがAIによるハルシネーション（幻覚）に起因していると<a href="https://www.theguardian.com/australia-news/2025/oct/06/deloitte-to-pay-money-back-to-albanese-government-after-using-ai-in-440000-report">The Guardian</a>が指摘している。</p>
<h2>デロイト、返金を実施</h2>
<p>デロイトが最終支払い分を返金することでオーストラリア政府と合意した。デロイトは声明で「参照と脚注に関する誤りを認識し、修正を完了した」と説明している。DEWR側は、報告書の主要な所見や提言自体に変更はないとしており、「修正は文献参照に限定され、内容の妥当性には影響していない」としている。</p>
<h2>政府の声明と再発防止</h2>
<p>DEWRの事務次官は10月3日付の声明で、「Targeted Compliance Frameworkの透明性と信頼性を高めるため、独立レビューを踏まえて制度改良を進めている」と述べた。
声明では、Deloitteによる報告書も改善プロセスの一部として参照していることが明らかにされ、政府側の対応は継続中とみられる。</p>
<h2>コンサル業界で問われる「AIの信頼性」</h2>
<p>デロイトはAI導入を強化しており、同時期にAnthropicとの提携拡大を発表したと<a href="https://techcrunch.com/2025/10/06/deloitte-goes-all-in-on-ai-despite-having-to-issue-a-hefty-refund-for-use-of-ai/">TechCrunch</a>が報道。一方で、AI生成文書の誤りによって公共契約の信頼性が揺らいでいると指摘した。デロイトは世界的にAIツールを業務へ統合しているが、今回の件は「AIを利用した文書の監査体制」が整備途上であることを浮き彫りにした。</p>
]]></description>
      <pubDate>Fri, 10 Oct 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>GoogleのノーコードAIミニアプリ「Opal」日本を含む15カ国に拡大──ワークフロー可視化と高速化を提供</title>
      <link>https://ledge.ai/articles/google_opal_global_expansion</link>
      <description><![CDATA[<p>Googleは2025年10月7日（現地時間）、自然言語からウェブアプリを作成できるノーコードAIツール「Opal」の提供地域を、日本を含む15カ国へ拡大したと<a href="https://blog.google/technology/google-labs/opal-expansion/">発表</a>した。OpalはGoogle Labsの実験プロジェクトで、コードを書くことなくAIミニアプリを構築できるのが特徴だ。</p>
<h2>米国先行公開からグローバル展開へ</h2>
<p>Opalは7月24日（現地時間）に米国で<a href="https://ledge.ai/articles/google_opal_no_code_ai_tool">初公開</a>。ユーザーが「タスク管理アプリを作成」「画像から色を抽出」といった指示を与えると、AIがHTML/CSS/JavaScriptで構成されたアプリを自動生成する。初期ユーザーの利用が想定以上に高度化したことを受け、今回、日本、韓国、インド、カナダ、ブラジル、アルゼンチン、ベトナム、インドネシア、シンガポール、コロンビア、エルサルバドル、コスタリカ、パナマ、ホンジュラス、パキスタンの15カ国での提供を開始した。</p>
<h2>ワークフローの可視化と高速化</h2>
<p>同時に、Opalの実用性を高める改良が行われた。</p>
<ul>
<li><strong>高度デバッグ</strong> ：ノーコードのまま、ビジュアルエディタ上でワークフローをステップごとに実行・検証。エラーは発生ステップにリアルタイム表示され、原因特定を容易にする。</li>
<li><strong>パフォーマンス改善</strong> ：新規Opal作成の起動時間を短縮。**並列実行（parallel runs）**により複数ステップを同時に処理でき、待機時間を抑える。</li>
</ul>
<h2>クリエイティブから業務効率化まで</h2>
<p>Opalは、個人の創作活動やマーケティング支援、業務プロセスの自動化など、多様な用途に対応する。Googleは「ユーザーが複雑なプロセスを自動化したり、アイデアを素早く形にしたりできるよう支援する」としており、開発者コミュニティはDiscord上でも展開されている。</p>
<p>@<a href="https://youtu.be/g9RBGnz-vqk">YouTube</a></p>
]]></description>
      <pubDate>Thu, 09 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>三井住友FG、社内AI「SMBC-GAI」にRAG機能を搭載──約130万件の社内文書を横断検索可能に</title>
      <link>https://ledge.ai/articles/smbc_gai_rag_internal_search</link>
      <description><![CDATA[<p>三井住友フィナンシャルグループ（SMBCグループ）は2025年10月6日、社内向けAIアシスタントツール「SMBC-GAI」に、RAG（Retrieval-Augmented Generation）技術を活用した社内情報検索機能を新たに搭載したと<a href="https://www.smfg.co.jp/news/pdf/j20251006_01.pdf">発表</a>した。社内規程や業務マニュアルなど約130万件に及ぶファイルをインデックス化し、従業員が一つのインターフェース上で横断的に検索・参照できるようにしたという。</p>
<h2>RAG技術で社内情報を高精度に検索</h2>
<p>RAG（Retrieval-Augmented Generation）とは、AIが関連性の高い情報を検索・参照し、それをもとに自然言語で回答を生成する技術だ。従来の生成AIと比べ、文脈理解と回答の正確性を高めることができる。「SMBC-GAI」ではこの仕組みを活用し、膨大な社内文書を効率的に検索できる環境を実現したという。</p>
<h2>約130万件のファイルをインデックス化</h2>
<p>対象は社内規程、通達、業務マニュアルなど約130万件にのぼるファイル群。RAGによって情報を体系的に整理・索引化することで、従業員は検索から回答生成までを一気通貫で行えるようになった。リリースによると、国内企業のRAG活用事例としては、学習ファイル数および利用人数の両面で「最大級の規模」としている。</p>
<h2>三井住友銀行から導入、グループ展開を視野に</h2>
<p>新機能はまず三井住友銀行の従業員向けに導入され、今後はSMBCグループ各社への展開が検討される。回答生成時には参照元を明示する仕組みも導入され、回答の根拠を追跡できるようになっている。</p>
<h2>金融業務に特化したAI基盤へ</h2>
<p>「SMBC-GAI」は2023年7月のリリース以降、社内の声を取り入れながら順次機能を強化してきた。SMBCグループは今後も「SMBC-GAI」を金融分野における汎用AI基盤へと進化させる方針で、従業員の業務効率化や顧客サービスの高度化につなげていくとしている。</p>
]]></description>
      <pubDate>Thu, 09 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>OpenAIとAMD、6GW規模のAIインフラ契約を締結──次世代GPU「Instinct」複数世代を供給へ</title>
      <link>https://ledge.ai/articles/openai_amd_6gw_ai_infrastructure</link>
      <description><![CDATA[<p>OpenAIと米半導体大手AMDは2025年10月6日（現地時間）、OpenAIの次世代AIインフラ構築に向け、6GW規模の計算能力を備えたシステムを展開する包括的パートナーシップを締結したと<a href="https://openai.com/index/openai-amd-strategic-partnership/">発表</a>した。
複数世代にわたるAMD「Instinct」GPUを活用し、2026年後半に最初の1GW分を導入する計画だ。</p>
<h2>6GW規模の包括的パートナーシップ</h2>
<p>AMDの<a href="https://ir.amd.com/news-events/press-releases/detail/1260/amd-and-openai-announce-strategic-partnership-to-deploy-6-gigawatts-of-amd-gpus">発表</a>によると、今回の契約では、AIトレーニングや推論に最適化されたデータセンター向けGPU「Instinct」シリーズ（MI450および将来世代）をOpenAIに継続的に供給する。これにより、OpenAIは複数世代にわたるAMD GPUを活用し、スケーラブルで高効率なAIインフラを構築する方針だ。</p>
<p>初期段階では、2026年後半に1GW相当の計算能力を展開。数年をかけて最大6GW規模まで拡張する。AMDによれば、この契約は同社のデータセンターGPU事業として過去最大級の供給量にあたるという。</p>
<h2>株式ワラントによる資本提携</h2>
<p>AMDは契約の一環として、OpenAIに対して約1億6千万株分の株式ワラントを発行。これにより、OpenAIは今後、AMDの発行済み株式の最大10%を取得する権利を持つことになる。
このスキームは、長期的な供給関係の安定化と、両社の戦略的連携を強化する目的があると説明されている。</p>
<h2>双方のコメント</h2>
<p>OpenAIは、AMDとの協力が次世代の生成AIシステムをより効率的かつスケーラブルに運用するための重要な一歩であると説明し、今後、AMDのGPUを活用して自社モデルのトレーニングおよび推論を拡張していく考えを示した。AMDのCEOであるリサ・スー氏も、この提携がAI計算基盤の進化を推進し、Instinct GPUの性能と電力効率を最大限に引き出す機会になると強調している。</p>
<h2>AIインフラ競争の新局面</h2>
<p>同提携は、拡大を続ける生成AI需要に対応するための計算資源の長期確保と、複数世代GPUにわたる供給体制を明示した点で、AIインフラ整備の大きな節目となる。両社は今後も、研究開発と運用の両面で協力を継続する方針だ。</p>
]]></description>
      <pubDate>Thu, 09 Oct 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI、ChatGPTの週間利用者が8億人を突破──DevDay 2025でApps SDKやGPT-5 Proなどを発表</title>
      <link>https://ledge.ai/articles/openai_devday2025_chatgpt_800m_wau</link>
      <description><![CDATA[<p>OpenAIのCEOであるサム・アルトマン氏は2025年10月6日（現地時間）、米サンフランシスコで開催された年次開発者会議「DevDay 2025」の基調講演で、対話型AI「ChatGPT」の週間アクティブユーザー数（WAU）が8億人を超えたと<a href="https://www.youtube.com/live/hS1YqcewH0c?si=1psz2A76y_mpcmvn">発表</a>した。2023年のDevDayでは1億人規模とされており、2年間で大幅に拡大したことになる。</p>
<p>アルトマン氏は会場で「ChatGPTの利用は世界中で急速に広がっている」と述べた。OpenAIのDevDay公式ページによれば、同社のプロダクトで開発した開発者は累計400万人、APIプラットフォームの処理量は毎分60億トークンと示されている。</p>
<p>イベントでは、ChatGPTを“アプリプラットフォーム”として拡張する新機能も発表された。ChatGPT内にネイティブアプリを構築できる「<a href="https://openai.com/index/introducing-apps-in-chatgpt/">Apps SDK</a>」プレビューが公開され、Booking.com、Canva、Coursera、Expedia、Figma、Spotify、Zillowによるデモ（対応市場で順次提供）が紹介された。</p>
<p>さらに、コード補助AI「<a href="https://openai.com/index/codex-now-generally-available/">Codex</a>」の一般提供（GA）が始まり、新しいSDKとともに開発者向け機能を強化。モデル面では、高精度な推論能力を備える「GPT-5 Pro」や、動画生成AI「Sora 2」、リアルタイム音声対応の「GPT-Realtime Mini」など、複数の最新モデルが紹介された。</p>
<p>今回の発表は、ChatGPTを単なるチャットツールから開発基盤へと位置づけ直す動きの一環とされる。OpenAIは、Apps SDKを通じて開発者がChatGPT内でアプリを提供できる環境を整えると案内しており、アプリ審査・ディレクトリ公開・マネタイズの詳細も順次共有する方針を示した。</p>
<p>@<a href="https://www.youtube.com/watch?v=hS1YqcewH0c&amp;t=133s">YouTube</a></p>
]]></description>
      <pubDate>Wed, 08 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Amazonの配送ドローン2機が墜落　米当局が調査開始──アリゾナ州トールソンで事故発生</title>
      <link>https://ledge.ai/articles/amazon_drone_crash_tolleson</link>
      <description><![CDATA[<p>アメリカ連邦航空局（FAA）は2025年10月1日（現地時間）、アリゾナ州トールソンで発生したAmazonの配送ドローン2機の墜落事故について、調査を開始したと<a href="https://www.faa.gov/newsroom/statements/accident_incidents">発表</a>した。FAAによると、事故による負傷者は報告されていない。</p>
<p>FAAの公式声明では、現地時間10月1日午前10時ごろ、Amazon Prime Airが運用する新型ドローン「MK-30」2機がトールソン市内で墜落したことを確認。国家運輸安全委員会（NTSB）と協力し、操縦手順、飛行経路、気象条件など複数の要因を調査する方針を示している。</p>
<p><a href="https://www.reuters.com/business/retail-consumer/ntsb-faa-probe-crashes-two-amazon-delivery-drones-2025-10-02/">Reuters</a>によると、墜落した2機はいずれも商用配送用のPrime Air機で、現場のクレーンのブーム部分に衝突した後、地上に墜落したという。Amazonはこの事故を受けてトールソン地区でのドローン配送を一時停止し、「安全性を最優先にし、関係当局と協力して原因を調べている」とコメントしている。</p>
<p>AmazonのPrime Airは、2022年にカリフォルニア州ロックフォードとテキサス州カレッジステーションでサービスを開始。2024年からアリゾナ州トールソンにも拡大し、最新型ドローン「MK-30」による商用配送を行っていた。Reutersは、今回の事故がMK-30の初期運用段階で発生したと報じている。</p>
<p>FAAは「商用ドローン運用に関する安全基準の適用を確認し、今後の事故防止に向けた対応を進める」としており、調査結果は後日公表される見込みだ。</p>
]]></description>
      <pubDate>Wed, 08 Oct 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google ResearchとDeepMind、「StreetViewAI」を発表──視覚障害者がAIと対話しながらストリートビューを“歩く”</title>
      <link>https://ledge.ai/articles/streetviewai_accessible_navigation</link>
      <description><![CDATA[<p>Google ResearchとGoogle DeepMindの研究チームは2025年9月28日（米国時間）、視覚障害者がAIと対話しながらGoogleストリートビュー上を探索できるツール「StreetViewAI」をACM国際会議UIST 2025で<a href="https://dl.acm.org/doi/10.1145/3746059.3747756">発表</a>した。視覚障害者がAIと対話しながらGoogleストリートビュー上を探索できるツールを開発し、マルチモーダルAIを活用した新しいアクセシビリティ体験を提示している。</p>
<h2>AIが「見る」街を、言葉で伝える</h2>
<p>StreetViewAIは、ストリートビュー画像・地図情報・地理メタデータなどを統合して、AIが環境を自然言語で説明するシステムだ。
会話の文脈を理解し、ユーザーが「右側には何がありますか？」「この角を曲がるとどうなりますか？」と尋ねると、AIが視覚と位置の情報をもとに答える。</p>
<p>AIが“街の目”となり、利用者は言葉のガイドによって世界中の街を“歩く”ことができる。</p>
<h2>背景：地理情報アクセシビリティの壁を超えて</h2>
<p>地図やストリートビューは視覚情報中心の体験であり、視覚障害者には利用のハードルが高い。
これまでの音声ガイドはランドマークの名称や距離といった部分情報にとどまり、空間全体の理解を支える仕組みは十分ではなかった。</p>
<p>StreetViewAIは、AIが空間の意味と文脈を理解し、自然な対話で伝えることを目指す。Googleが蓄積してきた地理空間理解技術に、DeepMindのマルチモーダルモデル技術を組み合わせた成果だという。</p>
<h2>システム構成：マルチモーダルAIによる空間理解</h2>
<p>StreetViewAIは、Googleのマルチモーダル大規模言語モデル（MLLM）を中核に、Street View画像、地図メタデータ、ナビゲーション履歴、周辺施設情報を統合し、文脈に沿った説明を生成する。</p>
<p>たとえば、ユーザーが「前に進んで」と音声指示を出すと、AIが実際に次のパノラマへ移動し、新たな環境を分析してリアルタイムに説明を更新する。この動作を支えるのが「StreetViewAI Control System」であり、AIが空間を“歩きながら”理解する構造を実現している。</p>
<p><strong>StreetViewAIの全体構成。ユーザーが音声で質問すると、マルチモーダルAIが地図情報と画像を統合して回答する</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/uist25_162_fig2_6f6636db4f/uist25_162_fig2_6f6636db4f.jpg" alt="uist25-162-fig2.jpg" /></p>
<h2>AI Describer：街の「語り手」としてのAI</h2>
<p>StreetViewAIの中核モジュールの一つが「AI Describer」だ。これは視覚障害者支援向けの“説明者AI”で、ストリートビュー画像から状況を自然言語で描写する。さらに、観光案内モード（Tour Guide Prompt）に切り替えると、文化的背景や観光情報を加味した解説も行える。</p>
<p>例として、渋谷のスクランブル交差点の場面では、標準モードでは「前方に大きな横断歩道と点字ブロックがあります」と伝えるが、ツアーガイドモードでは「ここは渋谷スクランブル交差点。世界で最も人通りが多い交差点の一つです」と説明する。</p>
<p><strong>対話インターフェースの例。ユーザーの質問（信号・横断歩道・看板の文字など）に対し、画像と地理メタデータに基づいて応答する</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/uist25_162_fig4_1c434bb912/uist25_162_fig4_1c434bb912.jpg" alt="uist25-162-fig4.jpg" /></p>
<h2>多様な都市での適用と検証</h2>
<p>研究チームは、シアトル、サンフランシスコ、ニューヨークなど複数都市のストリートビューで実験を実施。StreetViewAIは、都市構造や文化的文脈が異なる環境でも一貫した説明を生成でき、地域固有のランドマークや道路構造にも柔軟に対応した。
また、視覚障害当事者11名との評価を通じて、POI調査や遠隔での経路検討を支援する有用性も確認された。
こうした結果から、地域横断的なスケーラビリティと実利用の可能性が示されたと論文は述べている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/uist25_162_fig6_ffec020f07/uist25_162_fig6_ffec020f07.jpg" alt="uist25-162-fig6.jpg" /></p>
<h2>世界中の街を「声で歩く」未来へ</h2>
<p>StreetViewAIは、100以上の国・2200億枚超のストリートビュー画像のカバレッジを背景に、地域や言語を問わず動作する設計だ。現在、シアトル、ニューヨーク、東京、サンフランシスコ、ダブリンなど複数の都市でテストされており、地理的スケーラビリティを確認中だ。</p>
<p>今後は、Google MapsやAndroidナビゲーション、教育用アプリへの応用が期待される。Google Researchは論文で、「StreetViewAIは、アクセシビリティAIと地理空間AIの融合の一歩」と位置づけている。</p>
]]></description>
      <pubDate>Tue, 07 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>国立国会図書館×NII、官庁出版物30万点のテキストデータ提供で合意　日本発LLM開発を後押し</title>
      <link>https://ledge.ai/articles/ndl_nii_textdata_llm_collaboration</link>
      <description><![CDATA[<p>国立国会図書館（NDL）は2025年10月1日、国立情報学研究所（NII）に対して、同館が保有する官庁出版物などのテキストデータを提供することで合意したと<a href="https://www.ndl.go.jp/jp/news/fy2025/251001_01.html">発表</a>した。NIIが進める大規模言語モデル（LLM）の構築・研究を支援することを目的とし、日本語モデルの精度向上と公共データの再活用を促す取り組みとなる。</p>
<h2>約30万点の官庁出版物を対象</h2>
<p>提供されるのは、国立国会図書館が所蔵する官庁出版物を中心とした約30万点の資料のテキストデータ。主に1995年以前に刊行された図書・雑誌・官報などが対象で、OCR（光学式文字認識）により全文をデジタル化したものとなる。NIIはこれらのデータを研究用として活用し、日本語大規模言語モデルの構築や性能評価に役立てる。</p>
<h2>公共アーカイブとAI研究の連携</h2>
<p>今回の合意は、公共機関が保有する文献データを学術研究やAI開発に活用する新たな枠組みを示すもの。NDLはこれまでも、デジタルアーカイブ化やメタデータ公開などを通じて情報の利活用を推進してきたが、AI研究への提供は初の本格的な事例となる。</p>
<h2>日本語モデル開発を後押し</h2>
<p>NIIでは、研究機関や大学向けの日本語LLM開発を進めており、今回の提供データが学習素材として活用されることで、公共情報に基づく透明性の高いAIモデルの開発が期待される。
NDLは今後も、デジタル化資料の活用促進を通じて、学術研究や社会的知の発展に寄与していく方針を示している。</p>
]]></description>
      <pubDate>Tue, 07 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>KyoHA、国産ヒューマノイド試作プロジェクトを始動──早稲田大学・テムザック・村田製作所らが連携</title>
      <link>https://ledge.ai/articles/kyoha_humanoid_prototype_project_2025</link>
      <description><![CDATA[<p>一般社団法人「京都ヒューマノイドアソシエーション（KyoHA）」は2025年10月3日、国産ヒューマノイドの開発を目的とした「国産ヒューマノイド試作プロジェクト」を正式に始動したことを<a href="https://www.tmsuk.co.jp/topics/7608/">発表</a>した。
理事長は早稲田大学理工学術院の高西淳夫教授。中心メンバーとして株式会社テムザック、株式会社村田製作所、SREホールディングス株式会社が参画し、2026年3月を目標に全高約170cmの初期プロトタイプを公開する計画だ。</p>
<h2>京都発の産学連携プロジェクト</h2>
<p>KyoHAは、ヒューマノイド研究の第一人者である高西教授を中心に、産学官が協働する新しい枠組みとして発足。テムザックは全体構想および試作をリードし、村田製作所はセンシング・通信技術を、SREホールディングスはAI領域のディレクションを担う。</p>
<p>プロジェクトにはこのほか、マブチモーター、KYB、ヒーハイスト精工、OIST（沖縄科学技術大学院大学）など、国内の主要技術企業・研究機関が名を連ねている。</p>
<h2>技術分担と参画企業の役割</h2>
<p><strong>各社の技術分担：テムザックが全体設計を統括し、村田製作所はセンシング、マブチモーターとKYBは駆動系を担当する</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Slide1_1024x432_e6f41719e7/Slide1_1024x432_e6f41719e7.jpg" alt="Slide1-1024x432.jpg" /></p>
<p>KyoHAは開発領域を「技術全体」「ハード機体」「センシング」「AI」「アクチュエータ」「活動推進」に分類。
たとえば、テムザックは全体構想と機体設計、村田製作所はセンシング部品の開発と供給、マブチモーターとKYBは駆動系（モーター・油圧ユニット）の開発を担う。AI領域では、SREホールディングスとOISTが共同でディレクションを行い、AIプラットフォームや学習環境との連携を進めるという。</p>
<h2>初期プロトタイプは「ベースモデル」から2系統へ</h2>
<p>プロジェクトの第一段階では、汎用部品を活用した「ベースモデル」と呼ばれる初期型を試作し、ヒューマノイドモデルの基礎構築と技術課題の抽出を進める。</p>
<p><strong>ベースモデルを基礎に、2つの方向性（パワー重視／俊敏性重視）でモデル展開を進める</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Slide2_1024x460_9e41c8d6f9/Slide2_1024x460_9e41c8d6f9.jpg" alt="Slide2-1024x460.jpg" /></p>
<h2>京都・けいはんな学研都市で実証</h2>
<p>試作および検証の拠点は、京都市とけいはんな学研都市に設けられる予定だ。
産業界と研究機関、自治体が一体となって開発と実証を進め、地域発のイノベーション拠点として機能させる。
プロジェクトでは、災害対応や介護支援、研究開発など多様な分野での社会実装を視野に入れており、京都から全国への技術波及を目指している。</p>
<h2>国内産業の連携と再構築へ</h2>
<p>KyoHAは、ロボット産業の基盤を国内で完結できる形に再構築することを掲げている。
部品調達からソフトウェア開発、組立・実証までを国内企業が連携して担う体制を整え、安定したサプライチェーンと技術継承の仕組みを築く考えだ。
今後も参画企業や研究機関を広く募り、オープンな協創ネットワークを通じて“国産ヒューマノイド”の開発体制を強化していくとしている。</p>
]]></description>
      <pubDate>Tue, 07 Oct 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>元OpenAI・DeepMind研究者が設立「Periodic Labs」、3億ドルを調達しAIで科学研究を自動化へ</title>
      <link>https://ledge.ai/articles/periodic_labs_ai_scientist_seed</link>
      <description><![CDATA[<p>元OpenAIやDeepMindの研究者が中心となって設立したスタートアップ Periodic Labs が、シード資金として約3億ドルを調達し、非公開での活動（ステルスモード）を経て発足した。共同創業者のWilliam Fedus氏は2025年9月30日（現地時間）、X（旧Twitter）で「AI科学者を構築し、自律的な実験室を運営することを目指す」と<a href="https://x.com/LiamFedus/status/1973055380193431965">発表</a>した。</p>
<h2>AI科学者と自律実験室</h2>
<p>Fedus氏は、「科学は仮説を立て、実験を行い、結果から学ぶことで進展する」と述べ、AIがそのサイクルを担う未来を描いた。Periodic Labsは自律的な実験室を構築し、高品質な科学データや従来発表されにくい“失敗の実験結果”を含めて活用することで、新しい知見の創出を加速させる狙いだ。まずは物理科学分野から着手し、自然界を「強化学習の環境」として扱うことでAIによる発見を推進する。</p>
<h2>研究テーマと応用例</h2>
<p>研究テーマの一つは高温超伝導体の探索で、実現すれば次世代輸送技術や高効率の送電網に大きな影響を与えるとされる。また、同社はすでに半導体メーカーと協力し、チップの放熱問題に対応するため、実験データを解析するカスタムAIエージェントを開発している。</p>
<h2>創業メンバー</h2>
<p>創業チームには、ChatGPTの共同開発、DeepMindのGNoME、OpenAIのOperator（現Agent）、ニューラルネットのアテンション機構、MatterGenなどに関わった研究者が名を連ねる。過去10年で重要な材料発見を主導してきた人材も加わり、幅広い経験を持つ研究陣が揃っている。</p>
<h2>資金調達と投資家</h2>
<p>今回のシード資金はa16z（Andreessen Horowitz）が主導。Felicis、DST Global、NVentures（NVIDIAのVC部門）、Accelなどが参加し、個人投資家としてジェフ・ベゾス氏、エリック・シュミット氏、ジェフ・ディーン氏らも名を連ねる。
a16zは<a href="https://a16z.com/announcement/investing-in-periodic-labs/">公式ブログ</a>で「Periodic Labsの取り組みは科学の進め方を根本的に変える可能性がある」と述べ、支援の背景を明らかにしている。</p>
]]></description>
      <pubDate>Sun, 05 Oct 2025 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>地球観測をAIエージェントで刷新──「Earth-Agent」と新ベンチマーク「Earth-Bench」登場</title>
      <link>https://ledge.ai/articles/earth_agent_earth_bench_launch</link>
      <description><![CDATA[<p>中国の研究チームは2025年9月27日、地球観測データを高度に解析できるAIフレームワーク「Earth-Agent」を<a href="https://arxiv.org/abs/2509.23141">発表</a>した。RGB画像や分光データを統合し、従来のAIが苦手としてきた複雑な推論や専門ツールの利用を可能にする。あわせて、248タスク・13,729枚の画像を収録した新ベンチマーク「Earth-Bench」も公開した。</p>
<h2>Earth-Agentとは</h2>
<p>Earth-Agentは、地球観測（Earth Observation, EO）に特化した初のエージェント型AIだ。RGB画像や分光観測データを活用し、地球物理パラメータの推定、気候解析、都市管理、災害監視などに応用できる。人間のように「考える→ツールを呼び出す→結果を更新」というステップを繰り返し、複雑な課題を解決する構造を持つ。</p>
<p><strong>Earth-Agentの処理フレームワーク。思考・ツール利用・更新を繰り返す構造が特徴</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Earth_Agent_Framework_2a4b1e8bf6/Earth_Agent_Framework_2a4b1e8bf6.jpg" alt="Earth-Agent Framework.jpg" /></p>
<h2>従来モデルの限界を突破</h2>
<p>一般的なマルチモーダルLLM（GPT-4やGeminiなど）は、地球観測の現場で必要とされるマルチステップ推論に弱いとされてきた。Earth-Agentは専門ツールを動的に呼び出すことでこの課題を克服。例えば、干ばつ指数の解析、夜間光データの回帰分析、港湾面積の比較といった応用を実証した。</p>
<p><strong>Earth-Agentによる解析例。干ばつ監視、夜間光解析、港湾面積の差分計算を実行している</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Earth_Agent_solving_tasks_across_Spectrum_Products_and_RGB_data_155fe5c084/Earth_Agent_solving_tasks_across_Spectrum_Products_and_RGB_data_155fe5c084.jpg" alt="Earth-Agent solving tasks across Spectrum Products and RGB data.jpg" /></p>
<h2>新ベンチマーク「Earth-Bench」</h2>
<p>研究チームは同時に、新しい評価データセット「Earth-Bench」を構築した。248タスク、13,729画像を収録し、スペクトル解析・プロダクト解析・RGB解析の3カテゴリに分類。既存のEarthVQAやGeo-Benchに比べ、クロスモダリティや複雑な推論に対応しているのが特徴だ。</p>
<p><strong>Earth-Benchの全体像。既存ベンチマークとの比較や各カテゴリの内訳</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Dataset_Comparison_and_Overview_3fc2821c52/Dataset_Comparison_and_Overview_3fc2821c52.jpg" alt="Dataset Comparison and Overview.jpg" /></p>
<h2>データ構築の仕組み</h2>
<p>Earth-Benchは、Google Earth EngineやNASA Earth Dataから取得した衛星画像をもとに研究者が課題を設計している。論文では「2021年にシカゴ大都市圏で、地域の25%以上が300K（約27℃）を超えた日は何日あったか」といった問題が例示されている。これは都市ヒートアイランド現象や極端高温日数のモニタリングに直結する。</p>
<p>こうした課題はPythonコードで解法プロセスを記録し、JSON形式でアノテーション化。データには、どの衛星画像を使い、どのツールをどの順序で呼び出し、最終的に何日と算出されたかまでが明示されている。</p>
<p><strong>Earth-Benchの構築プロセス。衛星データから課題を生成し、ツール利用手順を記録</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Construction_and_Annotation_of_Earth_Bench_53ceef7fb0/Construction_and_Annotation_of_Earth_Bench_53ceef7fb0.jpg" alt="Construction and Annotation of Earth-Bench.jpg" /></p>
<h2>今後の展望</h2>
<p>論文によると、Earth-Agentのコードとデータは公開される予定とのこと。研究チームは、地球科学研究の高度化だけでなく、防災計画や気候変動対策、都市政策など幅広い応用に貢献できると見込んでいる。</p>
]]></description>
      <pubDate>Sun, 05 Oct 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>AIは「音を想像して考える」ことができるか──韓国の研究チームがAuditoryBench++と新手法AIR-CoTを発表</title>
      <link>https://ledge.ai/articles/ai_auditory_imagination_aircot</link>
      <description><![CDATA[<p>韓国の研究チームは2025年9月22日、言語モデル（LLM）が実際に音を聞かずとも聴覚的知識を想起できるかを検証する新しいベンチマーク「AuditoryBench++」を開発したと<a href="https://arxiv.org/abs/2509.17641">発表</a>した。さらに、音を頭の中で「想像」させる推論手法 AIR-CoT（Auditory Imagination Reasoning with Chain-of-Thought） を導入し、従来のモデルよりも高い精度で聴覚推論を実現したと報告している。</p>
<h2>人間は音を「想像」できるがLLMは苦手</h2>
<p>研究チームは、カント哲学の視点を引きながら「想像力は知覚に不可欠な要素である」と指摘する。例えば「大雨と稲妻の夜を描写した文章」を読むと、人は雨が太鼓を打つように降り注ぐ音や雷鳴を思い浮かべることができる。しかし、既存のLLMにはこのような聴覚的想像力が欠けているという。</p>
<h2>AuditoryBench++の概要</h2>
<p>この課題を検証するために開発された「AuditoryBench++」は、テキストのみを用いて音に関する推論力を評価するベンチマークで、以下の5種類のタスクを含む。</p>
<ul>
<li><strong>ピッチ比較</strong> （高い／低い音）</li>
<li><strong>持続時間比較</strong> （長い／短い音）</li>
<li><strong>大きさ比較</strong> （大きい／小さい音）</li>
<li><strong>動物の鳴き声認識</strong> （例：「meow」＝猫）</li>
<li><strong>文脈的聴覚推論</strong> （状況文から音を推定）</li>
</ul>
<p>合計で6,732問の問題が用意され、既存のベンチマークよりも大規模かつ精緻な評価が可能になっている。</p>
<h2>新手法AIR-CoTの仕組み</h2>
<p>研究チームはさらに、モデルに「音を想像させる」新しい推論手法 AIR-CoT を導入した。AIR-CoTでは、モデルが推論中に音の知識を必要とする場面に遭遇すると、特殊トークン [imagine] を生成する。するとシステムが想像プロセスを発動し、CLAPなどの音声モデルから得られた音の埋め込みを挿入、推論を継続する。これにより、まるで音を「頭の中で鳴らして」考えるかのような動作が可能になる。</p>
<p><strong>AIR-CoTの仕組み：モデルは [imagine] トークンで音を必要とする箇所を検出し、CLAPによる音埋め込みを注入して推論を続ける</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x1_cd93e9a630/x1_cd93e9a630.jpg" alt="x1.jpg" /></p>
<h2>実験結果</h2>
<p>AuditoryBench++を用いた評価では、既存のLLM（LLaMA3、Qwen2、Phi-4など）やマルチモーダルLLMはいずれもランダムに近い精度しか出せなかった。一方、AIR-CoTは以下のような大幅な改善を示した。</p>
<ul>
<li><strong>ピッチ比較</strong> ：83.9%（従来比 +8.3pt）</li>
<li><strong>動物の鳴き声認識</strong> ：71.6%（+9.3pt）</li>
<li><strong>文脈的聴覚推論</strong> ：82.7%（+11.9pt）</li>
</ul>
<p>ただし「持続時間」や「大きさ」の推論は依然として難しく、改善の余地があるとされている。</p>
<h2>今後の展望</h2>
<p>研究チームは、AuditoryBench++とAIR-CoTを提案し、従来のモデルを上回る成果を確認したと結論づけている。同研究は、直接オーディオ入力がなくても聴覚情報を想像できる言語モデルの基盤を提供し、最終的にはより自然で人間らしいマルチモーダル推論を可能にするとしている。</p>
]]></description>
      <pubDate>Sat, 04 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Metaが、AI チャットでの対話内容をFacebook／Instagramの広告最適化に利用すると発表— 12月導入、ユーザーはオプトアウト不可</title>
      <link>https://ledge.ai/articles/meta_ai_chat_ads_december</link>
      <description><![CDATA[<p>Metaは2025年10月1日（現地時間）、利用者とAIチャットボットのやり取りを、FacebookやInstagram上で表示されるコンテンツや広告のパーソナライズに活用する方針を公式ブログで<a href="https://about.fb.com/news/2025/10/improving-your-recommendations-apps-ai-meta/">発表</a>した。通知は10月7日から開始され、正式導入は12月16日としている。</p>
<h2>新方針の概要</h2>
<p>Metaは、利用者がAIチャット機能を通じて行った対話内容を、これまでの「いいね」や「フォロー」と同様に、レコメンデーションの新たな信号として扱う。これにより、ユーザーの関心により沿った投稿や広告が表示される仕組みを強化する。</p>
<p>新方針は世界的に展開されるが、当面は欧州連合（EU）、英国、韓国の利用者は対象外とされる。Metaによれば、健康や宗教、政治的立場、人種、性的指向など「センシティブな情報」は広告ターゲティングに利用しない方針だ。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/01_Notification_Carousel_01_1_767b81e54c/01_Notification_Carousel_01_1_767b81e54c.jpg" alt="01_Notification_Carousel-01-1.jpg" /></p>
<h2>オプトアウト不可の仕組み</h2>
<p>利用者が一度AIチャット機能を使い始めると、その対話データが広告最適化に活用されることを拒否することはできない。Metaは従来の「広告設定」や「なぜこの広告か」といった機能は引き続き提供するものの、AIチャット自体の利用データを除外する選択肢は設けていない。</p>
<h2>広告・表示への影響</h2>
<p>たとえば、ユーザーがAIと「ハイキング」の話題を交わすと、アウトドア用品の広告や関連投稿がフィードに増える可能性がある。Metaはこの仕組みにより、個人の関心と広告主の提供価値をより一致させることができるとしている。</p>
<h2>プライバシーと今後の焦点</h2>
<p>Metaの新方針は、無料で提供されるAIサービスから得られるデータを収益化モデルに組み込む動きの一環と位置づけられる。一方で、オプトアウトが認められない点は利用者のプライバシー権との関係で議論を呼んでいる。
透明性や説明責任をどう確保するか、また各国の規制対応と整合性を取れるかが、今後の注目点となる。</p>
]]></description>
      <pubDate>Sat, 04 Oct 2025 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>公共2025/10/6 [MON]デジタル庁、OpenAIと連携　政府共用AI「源内」に同社モデルを導入し行政活用を検討</title>
      <link>https://ledge.ai/articles/digital_agency_openai_genai_collaboration</link>
      <description><![CDATA[<p>デジタル庁は2025年10月2日、米OpenAIと生成AI（人工知能）の活用で連携すると<a href="https://www.digital.go.jp/news/e950673b-73eb-4f65-bf6a-339e4f0e7ef1">発表</a>した。政府職員が安全に生成AIを利用できる共用環境「源内（げんない）」に、OpenAIの大規模言語モデル（LLM）を新たにラインアップとして追加し、職員が業務で直接利用できるようにする方針だ。</p>
<h2>OpenAI選定の背景</h2>
<p>平将明デジタル大臣は翌3日の記者会見で、OpenAIを選定した理由について、最先端のAI研究と展開を進める企業として評価している旨を述べた。
源内では従来から複数のモデルを比較・検証しつつ活用しており、OpenAIのモデル追加により選択肢を拡充し、行政の生産性向上につなげる考えだ。</p>
<p>この協力方針は、2024年9月にOpenAIの最高戦略責任者（CSO）ジェイソン・クォン（Jason Kwon）氏がデジタル庁を訪問した際の会談で確認されたという。平大臣は「生成AIの利活用を推進するうえで重要な一歩」と述べた。</p>
<h2>セキュリティ体制の整備</h2>
<p>行政での生成AI活用にあたっては安全性と信頼性の確保が前提となる。会見の質疑で平大臣は、政府情報システムのためのセキュリティ評価制度「ISMAP（イスマップ）」に言及し、認証の有無により扱える情報の範囲が変わることを説明した。</p>
<p>@<a href="https://www.youtube.com/watch?v=hWf2w9br940">YouTube</a></p>
<p>OpenAI側も同日、自社の公式<a href="https://openai.com/ja-JP/global-affairs/strategic-collaboration-with-japan-digital-agency/">ブログ</a>で日本政府との戦略的協力を発表した。公共分野での活用モデルの共同検討や、ISMAP認証の取得をはじめ安全・安心に資する取り組みを前向きに検討する方針を示している。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/digital_openai_7ada319b72/digital_openai_7ada319b72.jpg" alt="digital openai.jpg" /></p>
]]></description>
      <pubDate>Fri, 03 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>学術＆研究2025/10/6 [MON]オープンLLMの日本語性能でトップ──FLUX、「Flux Japanese LLM」公開　独自手法でQwen2.5を進化</title>
      <link>https://ledge.ai/articles/flux_japanese_llm_release</link>
      <description><![CDATA[<p>国内スタートアップのFLUX株式会社は2025年9月29日、日本語特化の大規模言語モデル「Flux Japanese LLM」を<a href="https://flux.jp/news/1093/">発表</a>した。</p>
<p>同モデルはAlibaba Cloudの大規模言語モデル「Qwen2.5-32B」を基盤に、日本語理解・生成性能を独自の新手法で強化したもので、Open Japanese LLM Leaderboard（通称：LLM勉強会ランキング）で総合スコア第1位（0.7417）を記録したという。</p>
<h2>日本語能力を高める新手法「Precise-tuning」とは</h2>
<p>FLUXは今回のモデル開発にあたり、従来のファインチューニングとは異なる「Precise-tuning（プリサイズチューニング）」手法を導入した。日本語データセット全体でパラメーターを再学習するのではなく、日本語能力強化に必要なネットワーク回路のみを特定して再調整することで、効率的かつ精度の高い言語理解を実現したとしている。</p>
<p><strong>FLUXが開発した「Precise-tuning」手法の概念図</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/1440_810_4_6b464affdc/1440_810_4_6b464affdc.jpg" alt="1440-810-4.jpg" /></p>
<h2>ベンチマークで国内首位に</h2>
<p>同モデルは、LLM勉強会ランキング<a href="https://ledge.ai/articles/open_japanese_llm_leaderboard">オープン日本語LLMリーダーボード</a>において総合スコア0.7417を記録し、他の日本語モデルを上回る評価を得たという。</p>
<p>このランキングは、日本の有志研究者・エンジニアによるコミュニティ「<a href="https://llm-jp.nii.ac.jp/">LLM-jp</a>（通称：LLM勉強会）」が運営しており、複数の日本語LLMを自然言語推論・要約・コード生成などのタスクで比較評価するオープンベンチマークとして知られる。</p>
<p>LLM-jpは、国立情報学研究所（NII）を事務局とする共同研究プロジェクトで、2024年4月にNII内に設立された大規模言語モデル研究開発センター（LLMC）と連携して、「日本語に強いオープンな大規模言語モデル」を開発・評価する活動を進めている。そのため、このランキングは国内の学術・産業両分野で日本語LLMの性能を客観的に測る基準として広く参照されている。</p>
<p><strong>Open Japanese LLM Leaderboardでの評価結果。Flux Japanese LLMが第1位を記録</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/News_main_240718_3_e93e607d1f/News_main_240718_3_e93e607d1f.jpg" alt="News_main_240718-3.jpg" /></p>
<p>モデルはHugging Face上で公開されており、<a href="https://huggingface.co/flux-inc/Flux-Japanese-Qwen2.5-32B-Instruct-V1.0">モデルカード</a>には、自然言語処理・要約・コード生成タスクでの性能指標や学習設計の概要が掲載されている。</p>
<h2>企業・業界別モデル展開へ</h2>
<p>FLUXは、「Flux Japanese LLM」を自社のノーコードAIプラットフォーム群と連携させる計画を進めており、金融業界向けの特化モデル開発も行っている。同社は「AIをすべての人の手に」をミッションに掲げ、企業・研究機関・行政などが安全にLLMを活用できる基盤づくりを目指している。</p>
]]></description>
      <pubDate>Fri, 03 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>LLMへの指示が得意な人は脳の働きが違う──「プロンプト力」がfMRI研究で初めて科学的に確認される</title>
      <link>https://ledge.ai/articles/llm_prompting_brain_fmri_study</link>
      <description><![CDATA[<p>大規模言語モデル（LLM）への指示が得意な人とそうでない人の間で、脳活動に違いがあることが初めて科学的に確認された。サウジアラビア・キングサウード大学の研究チームは2025年8月20日、fMRI（機能的磁気共鳴画像法）を用いたパイロット研究の成果をarXivに<a href="https://arxiv.org/abs/2508.14869">公開</a>した。</p>
<h2>fMRIで「プロンプト力」の神経基盤を観測</h2>
<p>研究では、22人の参加者を対象に「プロンプト力」を評価するための独自尺度「Prompt Engineering Literacy Scale（PELS）」を開発し、スコアに基づき「熟達者」と「中級者」に分類。その上で、安静時fMRIを用いて脳の機能的結合やネットワーク活動を比較した。</p>
<h2>主な発見</h2>
<p>解析の結果、熟達者の脳には以下の特徴が確認された。</p>
<ul>
<li><strong>低周波帯域の優位性</strong> ：視覚ネットワーク（VVN）、デフォルトモードネットワーク後部（pDMN）、左外側頭頂ネットワーク（LLPN）などで、低周波成分が高周波成分に比べ優位であり、安定的で効率的な神経活動が示唆された。</li>
<li><strong>脳領域間の機能結合の強化</strong> ：熟達者では、左中側頭回（言語処理や意味記憶に関与）および左前頭極（計画・抽象的推論・メタ認知に関与）の機能結合が有意に強化されていた。</li>
<li><strong>効率的な神経活動</strong> ：脳内の自発的活動を示す指標（fALFF）が全般的に低下しており、不要な揺らぎが少なく効率的な情報処理が行われている可能性が示された。</li>
</ul>
<p><strong>■ LLMプロンプト熟達者で強化された左中側頭回の機能結合（fMRI解析より）</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Increased_connectivity_in_the_left_middle_temporal_gyrus_1927809d6f/Increased_connectivity_in_the_left_middle_temporal_gyrus_1927809d6f.jpg" alt="Increased connectivity in the left middle temporal gyrus.jpg" /></p>
<p><strong>■ LLMプロンプト熟達者で強化された左前頭極の機能結合（fMRI解析より）</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Increased_connectivity_in_the_left_frontal_pole_966041e40f/Increased_connectivity_in_the_left_frontal_pole_966041e40f.jpg" alt="Increased connectivity in the left frontal pole.jpg" /></p>
<h2>人とAIの協働に関する新しい視点</h2>
<p>この研究は「The Prompting Brain: Neurocognitive Markers of Expertise in Guiding Large Language Models」と題し、arXivにプレプリントとして公開された。著者らは、LLMを効果的に活用する能力（いわゆる「プロンプト力」）が、単なるスキルではなく神経科学的な特徴を持つことを示した点に意義があると述べている。</p>
<h2>今後の展望</h2>
<p>論文の著者らは、研究がパイロット的な小規模実験であり、より大規模かつ多様な参加者を対象とした検証が必要だと指摘している。また、プロンプト熟達度と脳活動の関連が、教育や職業訓練にどのような影響を及ぼすかを探る余地があるとした。さらに、AIと人間の協働を支える神経科学的理解を深めることで、ユーザーの特性に合わせたAIインターフェース設計につながる可能性があると述べている。</p>
]]></description>
      <pubDate>Fri, 03 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Microsoft 365 Copilotに「Agent Mode」「Office Agent」を導入 ～ “vibe working” でAIによるWord・Excelの自動化を推進</title>
      <link>https://ledge.ai/articles/microsoft365_agent_mode_office_agent_vibe_ai</link>
      <description><![CDATA[<p>Microsoftは2025年9月29日（米国時間）、同社の生成AI搭載ツール「Microsoft 365 Copilot」に、新機能「Agent Mode」および「Office Agent」を導入すると<a href="https://www.microsoft.com/en-us/microsoft-365/blog/2025/09/29/vibe-working-introducing-agent-mode-and-office-agent-in-microsoft-365-copilot/">発表</a>した。これらは「vibe working」と呼ばれる新しい作業体験を掲げ、WordやExcelでの文書作成・データ分析をAIが支援・自動化することを目的としている。</p>
<h2>Agent Mode：Officeアプリ内でのAI自動化</h2>
<p>Agent Modeは、WordやExcelなどのOfficeアプリケーションに組み込まれ、複数ステップにわたる作業をAIと対話しながら進められる機能。</p>
<p>Excelでは「Excel Labs」アドインを通じてプレビュー提供が開始され、数値の分析やグラフ化をAIに任せられる。Wordでは、文書の構成提案や修正作業をAIが継続的に補助する機能が実装され、まずはWeb版から展開される。</p>
<p>@<a href="https://youtu.be/nSqCy-7Qabk">YouTube</a></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Excel_benchmark_FINAL_7b6722975b/Excel_benchmark_FINAL_7b6722975b.webp" alt="Excel-benchmark-FINAL.webp" /></p>
<h2>Office Agent：Copilotチャットから文書やプレゼン生成</h2>
<p>Office Agentは、Copilotのチャット環境で稼働するエージェントで、Anthropicのモデルを搭載している。ユーザーが「レポートをまとめて」「会議資料を作成して」といった意図を伝えると、AIがWord文書やPowerPoint資料を生成・編集する。従来の単発的な応答にとどまらず、業務プロセス全体を遂行する“作業型エージェント”としての役割を担う。</p>
<p>@<a href="https://www.youtube.com/watch?v=NPSnD8-TZjY">YouTube</a></p>
<h2>“vibe working”のコンセプト</h2>
<p>Microsoftはこれらの新機能を総称して「vibe working」と表現している。簡潔な指示を入力するだけでAIが作業を補完し、文書作成やデータ分析の完成度を高めることを狙う。ユーザーはAIを相棒のように扱い、業務をより効率的に進められるという。</p>
<h2>提供条件と展開予定</h2>
<p>新機能は「Microsoft 365 Copilot」ライセンスを持つユーザーに順次展開される。Frontierプログラム参加者向けに先行提供されるケースもあり、初期段階では英語やWeb版が中心。今後は地域やアプリケーションの拡大が予定されている。</p>
]]></description>
      <pubDate>Fri, 03 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>落合陽一氏プロデュース「null²」移設クラファン──大阪・関西万博発、開始わずか23時間で1億円達成・次は2億円へ</title>
      <link>https://ledge.ai/articles/ochiai_null2_expo2025_crowdfunding_2oku</link>
      <description><![CDATA[<p>メディアアーティストの落合陽一氏がプロデュースした大阪・関西万博のシグネチャーパビリオン「null²（ヌルヌル）」の移設に向けたクラウドファンディングが、開始から23時間で第一目標の1億円を<a href="https://readyfor.jp/projects/null2_2025">達成</a>した。READYFORのプロジェクトページによれば、2025年10月1日の公開から急速に支援が集まり、翌2日朝には1億円を突破。ネクストゴールとして2億円が新たに設定されている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/null2_project_0c7453a2c9/null2_project_0c7453a2c9.jpg" alt="null2 project.jpg" /></p>
<h2>null²とは</h2>
<p>「null²」は、大阪・関西万博（2025年4月13日～10月13日、夢洲）におけるシグネチャーパビリオンのひとつで、テーマ「いのちを磨く」を体現する施設。伸縮する鏡素材「ミラー膜」とロボットアームを組み合わせ、外観そのものが動き続ける「動く建築」として注目を集めた。内部では鏡面状のLEDに囲まれた没入型シアター体験が提供され、開幕以降は抽選倍率数％という人気ぶりだった。</p>
<h2>クラウドファンディングの目的</h2>
<p>今回のクラウドファンディングは、万博終了後も「null²」を別の場所に移設・再構築するための資金を募るもの。主催は一般社団法人「計算機と自然」（代表：落合陽一氏）。資金は、新天地での企画設計や管理費用、活動資金、そして展示を記録映像として残す制作費などに充てられる予定だ。方式は「All in」で、目標未達でも集まった金額に応じてプロジェクトは実行される。支援募集は12月19日（金）23時まで行われる。</p>
<p>@<a href="https://www.youtube.com/watch?v=XOKS4w0Ige8">YouTube</a></p>
<h2>達成状況と返礼品</h2>
<p>プロジェクトは開始から23時間で第一目標を突破。READYFOR上では「null²と（また）会いたい」という支援者の声が多く寄せられており、7,000人を超える支援者が参加している。
返礼品には、外装に使われた鏡素材を切り出した「null²のカケラ」、限定パーカー、落合氏のプリント作品や模型など、万博の思い出を継承する品が用意されているという。</p>
<h2>今後の展望</h2>
<p>具体的な移設先や公開時期は未定だが、主催者は「全国から候補地としての声が届いている」としており、引越し計画の進展は今後、READYFORの活動報告や落合氏の公式SNSを通じて発信される見通しだ。</p>
<p>万博閉幕後も多くの人が体験できる形で「null²」を残すべく、クラウドファンディングは続いている。</p>
]]></description>
      <pubDate>Fri, 03 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>iRobotの共同創業者 ロドニー・ブルックス氏、「人型ロボットの器用さはまだ数十年先」──触覚なきAIを批判、“車輪付きポスト人型”の未来を予見</title>
      <link>https://ledge.ai/articles/rodney_brooks_humanoid_dexterity_post_humanoid_future</link>
      <description><![CDATA[<p>ロボット工学の第一人者であり、ルンバを開発したiRobotの共同創業者として知られるロドニー・ブルックス氏が、2025年9月26日付の<a href="https://rodneybrooks.com/why-todays-humanoids-wont-learn-dexterity/">公式ブログ</a>{target=\</p>
]]></description>
      <pubDate>Fri, 03 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI、次世代動画生成モデル「Sora 2」を発表──自分や友人が出演する動画を生成できるiOSアプリ「Sora」も米国とカナダで同時公開</title>
      <link>https://ledge.ai/articles/sora2_openai_ios_app_launch</link>
      <description><![CDATA[<p>OpenAIは2025年9月30日、最新の動画・音声生成モデル「Sora 2」を<a href="https://openai.com/index/sora-2/">発表</a>した。</p>
<p>物理挙動の正確さや映像の写実性が大幅に向上し、音声を同期して生成できる点が特徴。同日には、このモデルを利用できるiOS向けアプリ「Sora」も公開され、米国とカナダで招待制による提供が始まった。日本での提供時期は明らかにされていない。</p>
<p>@<a href="https://youtu.be/lEcg6AJ6DVY?si=aS3u22digXd5ZVY8">YouTube</a></p>
<h2>Sora 2の性能</h2>
<p>Sora2は、従来の「Sora」モデルを基盤に開発された動画・音声生成AIである。
OpenAIが公開した<a href="https://openai.com/index/sora-2-system-card/">システムカード</a>によれば、より正確な物理シミュレーション、長尺映像における一貫性、幅広いスタイルへの対応を実現。さらに音声生成を統合し、映像にナレーションや環境音を付与できる。</p>
<p>生成可能な映像は最大20秒とされるが、ReutersやThe Vergeなど複数のメディアは「アプリ上では10秒程度に制限されている」と報じている。</p>
<p>@<a href="https://www.youtube.com/watch?v=1PaoWKvcJP0">YouTube</a></p>
<h2>iOSアプリ「Sora」の提供</h2>
<p>同日に公開されたiOS向けアプリ「Sora」では、ユーザーがAI生成動画を作成・共有できる。
提供開始は米国とカナダで、アクセスは招待制。アプリ内で通知登録を行うことで順次利用可能となる。AndroidユーザーはWeb版の “sora.com” からアクセスできる仕組みだ。Sora2は当初無料で利用できるが、計算能力の制限が設けられているとのこと。</p>
<p>アプリの特徴として注目されるのが**「Cameo（カメオ）機能」** だ。ユーザーは自分や友人を動画に登場させられる。OpenAIは、この機能を利用するには本人の同意が必要とし、無断で他人の肖像を使用することはできない設計にしているという。サム・アルトマンCEOも自身のブログで「チームがキャラクターの一貫性に力を注ぎ、友人同士を動画に登場させることが意外なほど魅力的な新しいつながり方になった」と述べている。</p>
<h2>安全性への配慮</h2>
<p>OpenAIは安全設計を重視しており、生成動画には透かしやC2PAメタデータを付与。肖像権の無断利用や公人の生成は禁止され、未成年保護のためのフィルタリングや保護者向けコントロール機能も導入されている。
システムカードに記載された安全性評価では、不適切コンテンツを検出・遮断する精度が96〜99％に達したという。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/introducing_sora2_9ac129aadc/introducing_sora2_9ac129aadc.jpg" alt="introducing sora2.jpg" /></p>
<h2>背景と思想</h2>
<p>同社CEOのサム・アルトマン氏は自身の<a href="https://blog.samaltman.com/sora-2">ブログ</a>で、Soraを「ChatGPT for creativity」と表現。誰もが手軽に動画生成を楽しめる環境を提供する一方で、依存性や誤用のリスクについても懸念を示し、長期的なユーザーの満足や健全な利用を重視する方針を強調した。</p>
<p>また、OpenAIは公式サイトで「<a href="https://openai.com/index/sora-feed-philosophy/">フィード哲学</a>」を公開し、ユーザーが視聴体験を自ら選択できる仕組みを構築するとしている。</p>
<p>@<a href="https://www.youtube.com/watch?v=gzneGhpXwjU&amp;t=137s">YouTube</a></p>
]]></description>
      <pubDate>Fri, 03 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/10/5 [SUN]Sora、著作権方針を修正──Altman氏「日本の創作物に敬意」発言も。権利保護と生成AIの共存を模索</title>
      <link>https://ledge.ai/articles/sora_update_copyright_and_revenue_sharing_oct2025</link>
      <description><![CDATA[<p>OpenAIは、動画生成AI「Sora（ソラ）」の著作権対応を見直す。</p>
<p>同社CEOであるサム・アルトマン氏は2025年10月3日（現地時間）、自身の公式<a href="https://blog.samaltman.com/">ブログ</a>権利者がキャラクター生成をより細かく管理できる新機能と、収益分配制度の導入計画を発表した。背景には、Soraリリース後に浮上した知的財産権への懸念と、世界中で広がった批判がある。</p>
<h2>Soraをめぐる著作権論争</h2>
<p>OpenAIは2025年9月末、動画生成AIの最新版「Sora 2」と、米国・カナダ向けのソーシャル型iOSアプリ「Sora」を公開した。
テキストから高精細な映像を生成できる能力が話題を呼ぶ一方で、リリース直後には任天堂の「マリオ」や「ピカチュウ」など既存キャラクターに酷似した動画がSNS上で多数共有され、著作権保護の観点から懸念が相次いだ。一部の海外メディアは「任天堂の訴訟を招く可能性がある」と報じ、AIによる無断再現がどこまで許容されるのかが議論となった。</p>
<p>さらに、リリース直前に報じられた「権利者がオプトアウト（除外申請）しない限り自作品が扱われる」という運用観測も反発を招き、“同意なき取り込み”への不信感が増幅した。</p>
<p>この一連の騒動を受け、OpenAIはSoraの利用方針を再検討。今回の発表で、権利者による生成コントロール機能の導入と収益分配制度の構築を正式に打ち出した。</p>
<h2>権利者が生成内容を制御可能に</h2>
<p>アルトマン氏は、「Sora」の今後の方針として、権利者がキャラクター生成の可否や利用範囲を細かく指定できる新機能を追加する考えを示した。
これは、従来の「opt-in for likeness（本人類似モデル許諾）」を拡張したもので、より柔軟で公平な管理を可能にするという。</p>
<p>同氏は次のように述べている。</p>
<p>\u003E“We will give rightsholders more granular control over generation of characters... but want the ability to specify how their characters can be used (including not at all).”
（キャラクター生成について、権利者がより細かく制御できるようにし、使用方法を自ら指定できるようにします。まったく使用を許可しない選択も可能です。）</p>
<p>同氏によると、権利者の多くはSoraを通じた“インタラクティブなファンフィクション（参加型二次創作）”の可能性に期待を寄せつつも、利用のあり方を自ら決めたいと考えているという。OpenAIは、そうした多様な方針を尊重しつつ、全ての権利者に公平な標準を適用する姿勢を示した。</p>
<h2>収益分配モデルの試験導入へ</h2>
<p>OpenAIは、ユーザーによる動画生成量が予想を上回っていることを踏まえ、生成に使用されたキャラクターの権利者に収益を還元する制度を導入する計画も明らかにした。
アルトマン氏は「試行錯誤を重ねながら早期に開始する」とし、経済的な利益と新しい関係構築の双方を実現したい考えを示している。</p>
<p>\u003E “We are going to try sharing some of this revenue with rightsholders who want their characters generated by users.”
（ユーザーによって生成されるキャラクターを許可した権利者に対し、その一部の収益を共有することを試みます。）</p>
<h2>日本の創作文化に敬意を表明</h2>
<p>アルトマン氏は投稿の中で、「日本の創作物は非常に素晴らしい」と述べ、「ユーザーと日本のコンテンツとの深い結びつきに感銘を受けている」と言及した。
Soraをめぐる議論の中心に日本のコンテンツ産業があったことを踏まえ、文化的背景への理解を示した形だ。
今回の発表は、単なる技術的修正にとどまらず、文化と生成AIの関係を再定義する試みともいえる。</p>
<h2>GPT-5も同日にアップデート、安全性を強化</h2>
<p>同日、OpenAIは<a href="https://help.openai.com/en/articles/9624314-model-release-notes">Model Release Notes</a>を更新し、GPT-5 Instantモデルにメンタルヘルス対応の新機能を追加した。感情的または心理的なストレスをより正確に検知し、必要に応じて現実世界の支援リソースへ誘導できるようになったという。</p>
<p>こうした改良は、アルトマン氏が強調する「高速な改善サイクル」の一環として、Soraを含む同社製品全体に順次展開される見通しだ。</p>
<h2>今後の展望</h2>
<p>アルトマン氏は、「ChatGPT初期のように高頻度の改善を続けていく」と述べ、Soraを中心にプロダクト全体の改善を加速させる考えを示した。生成AIと著作権をめぐる議論が国際的に広がるなか、今回の発表は創作支援と権利保護の両立を図る “新しい共存モデル” として注目される。</p>
]]></description>
      <pubDate>Fri, 03 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>エンタメ＆アート2025/10/6 [MON]世界初のAI女優「ティリー・ノーウッド」にハリウッドが揺れる──SAG-AFTRAや著名俳優が「創造性の危機」と警告</title>
      <link>https://ledge.ai/articles/tilly_norwood_ai_actress_controversy_sagaftra</link>
      <description><![CDATA[<p>世界初の「AI女優」と称される Tilly Norwood（ティリー・ノーウッド）が、ハリウッドの俳優や映画俳優組合SAG-AFTRA（全米映画俳優組合・テレビ・ラジオ芸術家連盟）から強い批判を受けている。</p>
<p>SAG-AFTRAは2025年9月30日に「創造性は人間中心であるべき」と声明を<a href="https://www.sagaftra.org/sag-aftra-statement-synthetic-performer">発表</a>した。女優のエミリー・ブラント氏やウーピー・ゴールドバーグ氏も相次いで懸念を表明し、映画・テレビ業界全体に議論が広がっている。</p>
<h2>「AI Commissioner」──ティリー誕生の舞台</h2>
<p>Tilly Norwoodは、ロンドン拠点の制作会社Particle6が2025年9月にYouTube上で公開したコメディスケッチ『AI Commissioner | Comedy Sketch | Particle6』で初登場した。</p>
<p>この作品は、テレビ業界におけるAIの急速な普及を風刺的に描いたもので、AIが脚本作成からキャスティング、予算編成まですべてを自動化する世界を描く。劇中では、AIが生み出したインタラクティブスリラー『I Know What You Streamed Last Summer』に登場する100％AI生成の俳優としてTillyが紹介される。</p>
<p>@<a href="https://www.youtube.com/watch?v=3sVO_j4czYs">YouTube</a></p>
<p>登場人物の1人はTillyについて「僕の言うことを何でも聞いてくれる。恋をしてしまったかもしれない（She’ll do anything I say. I think I’m in love）」と評し、AIによる従順で“最適化された”俳優像を象徴的に表現。
さらに、「“She’ll cry on Graham Norton and be monetized on TikTok by lunchtime.”（彼女はグレアム・ノートンの番組で泣いて、その日の昼までにTikTokで収益化されるだろう）」という台詞が、人間の感情までもがAIによって即座に商業化される未来への皮肉として話題を呼んだ。</p>
<h2>SAG-AFTRAが声明「Tillyは俳優ではない」</h2>
<p>SAG-AFTRAは9月30日に「Statement on Synthetic Performer（合成パフォーマーに関する声明）」を<a href="https://www.sagaftra.org/sag-aftra-statement-synthetic-performer">発表</a>し、次のように明言した。</p>
<p>\u003E“Tilly Norwood is not an actor. Creativity must remain human-centered.”
（ティリー・ノーウッドは俳優ではない。創造性は人間中心であるべきだ。）</p>
<p>声明では、AIによって作られた “合成俳優” が芸術表現を侵食する可能性を指摘し、「経験や感情を持たない存在を“俳優”と呼ぶことは、芸術の根幹を損なう」と警鐘を鳴らした。SAG-AFTRA会長のショーン・アスティン氏もVarietyの取材に対し、「AI倫理と補償問題を正式な交渉テーマとして扱う」と述べている。</p>
<h2>著名俳優からの反発</h2>
<p>女優のエミリー・ブラント氏は、Varietyのポッドキャスト番組でTillyの画像を見せられ、驚きを隠さずこう語った。</p>
<p>\u003E“Good Lord, we’re screwed. That is really, really scary. Come on, agencies, don’t do that. Please stop taking away our human connection.”
「なんてこと、私たちは終わりね。本当に恐ろしいわ。お願い、エージェントはそんなことをやめて。人間のつながりを奪わないで。」</p>
<p>また、取材でTillyを「次のスカーレット・ヨハンソンに」と問われると、「“But we have Scarlett Johansson.”（でも私たちにはスカーレット・ヨハンソンがいる」 と返し、人間俳優の価値を強調した。</p>
<p>俳優・司会者のウーピー・ゴールドバーグ氏も、ABCのトーク番組『The View』（9月30日放送）で次のように発言した。</p>
<p>\u003E“You’re looking at 5,000 actors rolled into one synthetic person. That’s not fair. We all move differently.”
「5,000人の俳優の特性をひとつの合成存在にまとめるなんてフェアじゃない。私たちは皆、動きも表情も違う。」</p>
<h2>開発側の見解：「人間の代替ではなく、芸術表現」</h2>
<p>Tillyを制作したエライン・ファン・デル・フェルデン氏（Eline Van der Velden）は9月28日、ティリーのInstagram（<a href="https://www.instagram.com/tillynorwood/">@tillynorwood</a>）に投稿し、AI俳優の創作意図について次のように述べている。</p>
<p>\u003E “She is not a replacement for a human being, but a creative work — a piece of art.”
「ティリーは人間の代替ではなく、創造的な作品＝ひとつのアートです。」</p>
<p>同氏は、AIを「人間の代わり」ではなく「新しい絵筆のようなツール」と位置づけ、「アニメーションや人形劇、CGIがライブ演技を奪うことなく新しい可能性を開いたように、AIも物語を構築する新たな手段を提供する」と説明した。</p>
<p>\u003E “I’m an actor myself, and nothing — certainly not an AI character — can take away the craft or joy of human performance.”
「私は俳優でもあり、AIキャラクターであっても、人間の演技の技や喜びを奪うことはできません。」</p>
<p>投稿ではさらに、AIを“人間と競わせる存在”ではなく“芸術の新しいジャンルの一部”として評価すべきだと訴えている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/instagram_eline_at_tillynorwood_b80dee2451/instagram_eline_at_tillynorwood_b80dee2451.jpg" alt="instagram eline at tillynorwood.jpg" /></p>
<p>同氏が率いるParticle6は「AI俳優を活用すれば制作コストを最大90％削減できる」と説明し、複数のAIタレントを育てる「デジタル・タレント・ユニバース」構想を掲げている。</p>
<h2>今後の焦点</h2>
<p>各報道や関係者の声明からは、主に次の3点が論点として浮上している。</p>
<ul>
<li>著作権と肖像権：AI俳優の訓練データやモデル構築に使用された素材の扱い</li>
<li>契約・補償問題：AI使用を前提とした俳優契約の新たな枠組み</li>
<li>文化的受容：AIが「演技」を行うことを人々がどう受け入れるか</li>
</ul>
<p>SAG-AFTRAをはじめとする業界団体や俳優たちは、AIが創作活動や雇用に及ぼす影響を注視しており、今後は法制度や契約の整備を含めた議論が進む見通しだ。</p>
]]></description>
      <pubDate>Fri, 03 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>日本の「なぞなぞ」でAIの思考力をテスト──人間並みの正答率はGPT-5のみ。JAISTの研究チーム</title>
      <link>https://ledge.ai/articles/jaist_nazonazo_gpt5_benchmark</link>
      <description><![CDATA[<p>2025年9月18日、北陸先端科学技術大学院大学（JAIST）の研究チームは、日本の子ども向け「なぞなぞ（Nazonazo）」を活用し、大規模言語モデル（LLM）の洞察的推論能力を評価する新ベンチマークを開発したことを<a href="https://arxiv.org/abs/2509.14704">発表</a>した。実験の結果、GPT-5のみが人間に匹敵する正答率を示し、他のモデルは大きく下回った。</p>
<h2>飽和する既存ベンチマーク</h2>
<p>AIの能力を測る代表的なベンチマーク（MMLU、GSM8K、HumanEvalなど）は、最先端モデルが80〜90％の高スコアを記録するようになり、モデル間の性能差を明確に測りにくくなっている。OpenAI共同創業者のアンドレイ・カルパシー氏も「評価危機（evaluation crisis）」を指摘していた。</p>
<h2>日本の「なぞなぞ」はハイレベル？</h2>
<p>研究チームは、この「評価危機」を打開する手段として、日本の伝統的な言葉遊びである「なぞなぞ」を採用した。
なぞなぞは短文形式で低コストに新規作成が可能なうえ、専門知識を必要とせず、純粋な洞察力を試せる。また日本語特有の「漢字の分解」「語呂合わせ」「外来語表記」などにより、多様で難度の高い問題を作れる。</p>
<p>例として有名な「パンはパンでも食べられないパンは、なーんだ？」（答え：フライパン）が紹介されているほか、論文では「侍から“人偏”を取ると寺になる」という仕掛けのなぞなぞ（添付図参照）が示されている。</p>
<p><strong>漢字分解を利用したなぞなぞの例。「侍」から「人偏」を取ると「寺」となり、答えは「寺」</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Nazo_Nazo_Benchmar_7a284b9f17/Nazo_Nazo_Benchmar_7a284b9f17.jpg" alt="NazoNazo Benchmar.jpg" /></p>
<h2>英語 “リドル（riddle）” との差異</h2>
<p>英語圏では「RiddleSense」「BRAINTEASER」などのリドル系ベンチマークが存在するが、既に学習データに取り込まれており、GPT-4が98％超の精度で人間を上回るケースもある。
これに対し、日本語なぞなぞは人間でも平均正答率が52.9％にとどまり、AIモデルはさらに苦戦した。論文は「英語リドルはAIにとって容易になりすぎたが、日本語なぞなぞは汚染リスクが小さく、モデルの純粋な推論力を測るのに適している」と位置づけている。</p>
<h2>実験結果</h2>
<p>研究チームは38種類のLLM（GPT-4o、Claude、Gemini、Grok、Llama、DeepSeekなど）と成人126人を比較。</p>
<ul>
<li>人間の平均正答率は52.9％</li>
<li>GPT-5のみが人間平均と同等のスコアを記録</li>
<li>他のモデルは20〜30％台にとどまり、人間の半分程度にすぎなかった</li>
</ul>
<h2>AIが苦手な「最後のひと押し」</h2>
<p>多くのモデルは正解候補を途中で生成するものの、最終的に選べず「検証失敗」に陥るケースが頻発した。人間が持つ「Aha!（ひらめき）」や「これは正しい」という確信度がAIには弱く、洞察課題に特有の“最後のひと押し”が欠けていると指摘される。
また、モデルのパラメータ数の大きさと正答率には相関がなく、「推論型モデル」であることが成績の向上につながっていた。</p>
<h2>今後の展望</h2>
<p>論文は「GPT-5が例外的に人間並みの成績を示したが、他の最先端モデルは依然として人間に及ばない」と結論づけている。研究チームは、さらに難易度を高めた「Nazonazoベンチマーク2」の準備を進めており、今後はAIの“メタ認知的感覚”──正しいと感じる力──の強化が研究の焦点になる見通しだ。</p>
]]></description>
      <pubDate>Thu, 02 Oct 2025 05:50:00 GMT</pubDate>
    </item>
  </channel>
</rss>