<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
  <channel>
    <title>Ledge.ai 新着記事</title>
    <link>https://ledge.ai/</link>
    <description>Ledge.ai の最新テクノロジー記事</description>
    <item>
      <title>Amazonの配送ドローン2機が墜落　米当局が調査開始──アリゾナ州トールソンで事故発生</title>
      <link>https://ledge.ai/articles/amazon_drone_crash_tolleson</link>
      <description><![CDATA[<p>アメリカ連邦航空局（FAA）は2025年10月1日（現地時間）、アリゾナ州トールソンで発生したAmazonの配送ドローン2機の墜落事故について、調査を開始したと<a href="https://www.faa.gov/newsroom/statements/accident_incidents">発表</a>した。FAAによると、事故による負傷者は報告されていない。</p>
<p>FAAの公式声明では、現地時間10月1日午前10時ごろ、Amazon Prime Airが運用する新型ドローン「MK-30」2機がトールソン市内で墜落したことを確認。国家運輸安全委員会（NTSB）と協力し、操縦手順、飛行経路、気象条件など複数の要因を調査する方針を示している。</p>
<p><a href="https://www.reuters.com/business/retail-consumer/ntsb-faa-probe-crashes-two-amazon-delivery-drones-2025-10-02/">Reuters</a>によると、墜落した2機はいずれも商用配送用のPrime Air機で、現場のクレーンのブーム部分に衝突した後、地上に墜落したという。Amazonはこの事故を受けてトールソン地区でのドローン配送を一時停止し、「安全性を最優先にし、関係当局と協力して原因を調べている」とコメントしている。</p>
<p>AmazonのPrime Airは、2022年にカリフォルニア州ロックフォードとテキサス州カレッジステーションでサービスを開始。2024年からアリゾナ州トールソンにも拡大し、最新型ドローン「MK-30」による商用配送を行っていた。Reutersは、今回の事故がMK-30の初期運用段階で発生したと報じている。</p>
<p>FAAは「商用ドローン運用に関する安全基準の適用を確認し、今後の事故防止に向けた対応を進める」としており、調査結果は後日公表される見込みだ。</p>
]]></description>
      <pubDate>Wed, 08 Oct 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google ResearchとDeepMind、「StreetViewAI」を発表──視覚障害者がAIと対話しながらストリートビューを“歩く”</title>
      <link>https://ledge.ai/articles/streetviewai_accessible_navigation</link>
      <description><![CDATA[<p>Google ResearchとGoogle DeepMindの研究チームは2025年9月28日（米国時間）、視覚障害者がAIと対話しながらGoogleストリートビュー上を探索できるツール「StreetViewAI」をACM国際会議UIST 2025で<a href="https://dl.acm.org/doi/10.1145/3746059.3747756">発表</a>した。視覚障害者がAIと対話しながらGoogleストリートビュー上を探索できるツールを開発し、マルチモーダルAIを活用した新しいアクセシビリティ体験を提示している。</p>
<h2>AIが「見る」街を、言葉で伝える</h2>
<p>StreetViewAIは、ストリートビュー画像・地図情報・地理メタデータなどを統合して、AIが環境を自然言語で説明するシステムだ。
会話の文脈を理解し、ユーザーが「右側には何がありますか？」「この角を曲がるとどうなりますか？」と尋ねると、AIが視覚と位置の情報をもとに答える。</p>
<p>AIが“街の目”となり、利用者は言葉のガイドによって世界中の街を“歩く”ことができる。</p>
<h2>背景：地理情報アクセシビリティの壁を超えて</h2>
<p>地図やストリートビューは視覚情報中心の体験であり、視覚障害者には利用のハードルが高い。
これまでの音声ガイドはランドマークの名称や距離といった部分情報にとどまり、空間全体の理解を支える仕組みは十分ではなかった。</p>
<p>StreetViewAIは、AIが空間の意味と文脈を理解し、自然な対話で伝えることを目指す。Googleが蓄積してきた地理空間理解技術に、DeepMindのマルチモーダルモデル技術を組み合わせた成果だという。</p>
<h2>システム構成：マルチモーダルAIによる空間理解</h2>
<p>StreetViewAIは、Googleのマルチモーダル大規模言語モデル（MLLM）を中核に、Street View画像、地図メタデータ、ナビゲーション履歴、周辺施設情報を統合し、文脈に沿った説明を生成する。</p>
<p>たとえば、ユーザーが「前に進んで」と音声指示を出すと、AIが実際に次のパノラマへ移動し、新たな環境を分析してリアルタイムに説明を更新する。この動作を支えるのが「StreetViewAI Control System」であり、AIが空間を“歩きながら”理解する構造を実現している。</p>
<p><strong>StreetViewAIの全体構成。ユーザーが音声で質問すると、マルチモーダルAIが地図情報と画像を統合して回答する</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/uist25_162_fig2_6f6636db4f/uist25_162_fig2_6f6636db4f.jpg" alt="uist25-162-fig2.jpg" /></p>
<h2>AI Describer：街の「語り手」としてのAI</h2>
<p>StreetViewAIの中核モジュールの一つが「AI Describer」だ。これは視覚障害者支援向けの“説明者AI”で、ストリートビュー画像から状況を自然言語で描写する。さらに、観光案内モード（Tour Guide Prompt）に切り替えると、文化的背景や観光情報を加味した解説も行える。</p>
<p>例として、渋谷のスクランブル交差点の場面では、標準モードでは「前方に大きな横断歩道と点字ブロックがあります」と伝えるが、ツアーガイドモードでは「ここは渋谷スクランブル交差点。世界で最も人通りが多い交差点の一つです」と説明する。</p>
<p><strong>対話インターフェースの例。ユーザーの質問（信号・横断歩道・看板の文字など）に対し、画像と地理メタデータに基づいて応答する</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/uist25_162_fig4_1c434bb912/uist25_162_fig4_1c434bb912.jpg" alt="uist25-162-fig4.jpg" /></p>
<h2>多様な都市での適用と検証</h2>
<p>研究チームは、シアトル、サンフランシスコ、ニューヨークなど複数都市のストリートビューで実験を実施。StreetViewAIは、都市構造や文化的文脈が異なる環境でも一貫した説明を生成でき、地域固有のランドマークや道路構造にも柔軟に対応した。
また、視覚障害当事者11名との評価を通じて、POI調査や遠隔での経路検討を支援する有用性も確認された。
こうした結果から、地域横断的なスケーラビリティと実利用の可能性が示されたと論文は述べている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/uist25_162_fig6_ffec020f07/uist25_162_fig6_ffec020f07.jpg" alt="uist25-162-fig6.jpg" /></p>
<h2>世界中の街を「声で歩く」未来へ</h2>
<p>StreetViewAIは、100以上の国・2200億枚超のストリートビュー画像のカバレッジを背景に、地域や言語を問わず動作する設計だ。現在、シアトル、ニューヨーク、東京、サンフランシスコ、ダブリンなど複数の都市でテストされており、地理的スケーラビリティを確認中だ。</p>
<p>今後は、Google MapsやAndroidナビゲーション、教育用アプリへの応用が期待される。Google Researchは論文で、「StreetViewAIは、アクセシビリティAIと地理空間AIの融合の一歩」と位置づけている。</p>
]]></description>
      <pubDate>Tue, 07 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>国立国会図書館×NII、官庁出版物30万点のテキストデータ提供で合意　日本発LLM開発を後押し</title>
      <link>https://ledge.ai/articles/ndl_nii_textdata_llm_collaboration</link>
      <description><![CDATA[<p>国立国会図書館（NDL）は2025年10月1日、国立情報学研究所（NII）に対して、同館が保有する官庁出版物などのテキストデータを提供することで合意したと<a href="https://www.ndl.go.jp/jp/news/fy2025/251001_01.html">発表</a>した。NIIが進める大規模言語モデル（LLM）の構築・研究を支援することを目的とし、日本語モデルの精度向上と公共データの再活用を促す取り組みとなる。</p>
<h2>約30万点の官庁出版物を対象</h2>
<p>提供されるのは、国立国会図書館が所蔵する官庁出版物を中心とした約30万点の資料のテキストデータ。主に1995年以前に刊行された図書・雑誌・官報などが対象で、OCR（光学式文字認識）により全文をデジタル化したものとなる。NIIはこれらのデータを研究用として活用し、日本語大規模言語モデルの構築や性能評価に役立てる。</p>
<h2>公共アーカイブとAI研究の連携</h2>
<p>今回の合意は、公共機関が保有する文献データを学術研究やAI開発に活用する新たな枠組みを示すもの。NDLはこれまでも、デジタルアーカイブ化やメタデータ公開などを通じて情報の利活用を推進してきたが、AI研究への提供は初の本格的な事例となる。</p>
<h2>日本語モデル開発を後押し</h2>
<p>NIIでは、研究機関や大学向けの日本語LLM開発を進めており、今回の提供データが学習素材として活用されることで、公共情報に基づく透明性の高いAIモデルの開発が期待される。
NDLは今後も、デジタル化資料の活用促進を通じて、学術研究や社会的知の発展に寄与していく方針を示している。</p>
]]></description>
      <pubDate>Tue, 07 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>KyoHA、国産ヒューマノイド試作プロジェクトを始動──早稲田大学・テムザック・村田製作所らが連携</title>
      <link>https://ledge.ai/articles/kyoha_humanoid_prototype_project_2025</link>
      <description><![CDATA[<p>一般社団法人「京都ヒューマノイドアソシエーション（KyoHA）」は2025年10月3日、国産ヒューマノイドの開発を目的とした「国産ヒューマノイド試作プロジェクト」を正式に始動したことを<a href="https://www.tmsuk.co.jp/topics/7608/">発表</a>した。
理事長は早稲田大学理工学術院の高西淳夫教授。中心メンバーとして株式会社テムザック、株式会社村田製作所、SREホールディングス株式会社が参画し、2026年3月を目標に全高約170cmの初期プロトタイプを公開する計画だ。</p>
<h2>京都発の産学連携プロジェクト</h2>
<p>KyoHAは、ヒューマノイド研究の第一人者である高西教授を中心に、産学官が協働する新しい枠組みとして発足。テムザックは全体構想および試作をリードし、村田製作所はセンシング・通信技術を、SREホールディングスはAI領域のディレクションを担う。</p>
<p>プロジェクトにはこのほか、マブチモーター、KYB、ヒーハイスト精工、OIST（沖縄科学技術大学院大学）など、国内の主要技術企業・研究機関が名を連ねている。</p>
<h2>技術分担と参画企業の役割</h2>
<p><strong>各社の技術分担：テムザックが全体設計を統括し、村田製作所はセンシング、マブチモーターとKYBは駆動系を担当する</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Slide1_1024x432_e6f41719e7/Slide1_1024x432_e6f41719e7.jpg" alt="Slide1-1024x432.jpg" /></p>
<p>KyoHAは開発領域を「技術全体」「ハード機体」「センシング」「AI」「アクチュエータ」「活動推進」に分類。
たとえば、テムザックは全体構想と機体設計、村田製作所はセンシング部品の開発と供給、マブチモーターとKYBは駆動系（モーター・油圧ユニット）の開発を担う。AI領域では、SREホールディングスとOISTが共同でディレクションを行い、AIプラットフォームや学習環境との連携を進めるという。</p>
<h2>初期プロトタイプは「ベースモデル」から2系統へ</h2>
<p>プロジェクトの第一段階では、汎用部品を活用した「ベースモデル」と呼ばれる初期型を試作し、ヒューマノイドモデルの基礎構築と技術課題の抽出を進める。</p>
<p><strong>ベースモデルを基礎に、2つの方向性（パワー重視／俊敏性重視）でモデル展開を進める</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Slide2_1024x460_9e41c8d6f9/Slide2_1024x460_9e41c8d6f9.jpg" alt="Slide2-1024x460.jpg" /></p>
<h2>京都・けいはんな学研都市で実証</h2>
<p>試作および検証の拠点は、京都市とけいはんな学研都市に設けられる予定だ。
産業界と研究機関、自治体が一体となって開発と実証を進め、地域発のイノベーション拠点として機能させる。
プロジェクトでは、災害対応や介護支援、研究開発など多様な分野での社会実装を視野に入れており、京都から全国への技術波及を目指している。</p>
<h2>国内産業の連携と再構築へ</h2>
<p>KyoHAは、ロボット産業の基盤を国内で完結できる形に再構築することを掲げている。
部品調達からソフトウェア開発、組立・実証までを国内企業が連携して担う体制を整え、安定したサプライチェーンと技術継承の仕組みを築く考えだ。
今後も参画企業や研究機関を広く募り、オープンな協創ネットワークを通じて“国産ヒューマノイド”の開発体制を強化していくとしている。</p>
]]></description>
      <pubDate>Tue, 07 Oct 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>日本とシンガポールを結ぶ国際海底ケーブル「Candle」建設へ──ソフトバンク、Metaなど4社と合意</title>
      <link>https://ledge.ai/articles/candle_submarine_cable_softbank_meta_agreement</link>
      <description><![CDATA[<p>ソフトバンクは2025年9月22日、米Metaなど4社と共同で、日本とシンガポールを結ぶ国際海底ケーブル「Candle（キャンドル）」の建設に合意したと<a href="https://www.softbank.jp/corp/news/press/sbkk/2025/20250922_01/">発表</a>した。総延長約8,000kmの光海底ケーブルを敷設し、2028年の運用開始を予定している。</p>
<h2>「Candle」プロジェクトの概要</h2>
<p>「Candle」は、日本、台湾、フィリピン、インドネシア、マレーシア、シンガポールを結ぶ国際海底ケーブルで、システム供給はNECが担う。24ファイバーペア構成を採用し、従来16～20ペアが主流だった海底ケーブルと比べ、さらに大容量かつ低遅延の通信を実現する。急増するAIや5G関連の通信需要に対応し、東アジアと東南アジアを結ぶ主要ルートの多様化・冗長化を図る。</p>
<h2>各社のコメント</h2>
<p>Candleマネジメントコミッティ議長でMetaのDon Pang氏は、
「Candleはアジア地域のデジタルインフラ強化における重要な前進です。高速かつ堅牢な接続性の需要が高まる中、ネットワークの多様性とレジリエンスを向上させ、5億人以上の人々にデジタル・インクルージョンと経済的機会を拡大します」と述べた。</p>
<p>また、ソフトバンク法人統括 グローバル事業本部 本部長の工藤公正氏は、
「生成AIやIoTの進展に伴い、国際通信需要は加速度的に拡大しています。Candleは次世代社会インフラの重要な基盤であり、既存の『JUPITER』『ADC』、建設中の『E2A』と組み合わせることで、日本を起点とする国際通信網をさらに強化します」とコメントしている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/index_pic_02_8f725966c5/index_pic_02_8f725966c5.webp" alt="index_pic_02.webp" /></p>
<h2>ソフトバンクの取り組み</h2>
<p>ソフトバンクはCandleの日本側陸揚げ局として、千葉県南房総市の「ソフトバンク丸山国際中継所」を提供する。さらに北海道・九州に新たな陸揚げ拠点を設け、日本各地に分散配置することで災害や障害に強い通信インフラを整備する計画だ。</p>
]]></description>
      <pubDate>Mon, 06 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/9/29 [MON]ChatGPTに「共有プロジェクト」モード導入──OpenAI、セキュリティ強化や外部アプリ連携も拡充</title>
      <link>https://ledge.ai/articles/chatgpt_shared_projects_security_update</link>
      <description><![CDATA[<p>OpenAIは2025年9月25日（現地時間）、ChatGPTにおいて、チームでの共同作業やセキュリティ機能を強化する新機能を<a href="https://openai.com/index/more-ways-to-work-with-your-team/">発表</a>した。新たに「共有プロジェクト」モードを導入するほか、外部アプリとの接続機能やエンタープライズ向けの管理機能を拡充している。</p>
<h2>共有プロジェクトモードで共同作業を効率化</h2>
<p>今回のアップデートの中心となるのが「共有プロジェクト」モードだ。これにより、同じワークスペース内のメンバーがチャット、ファイル、カスタムインストラクションを一元的に共有できる。権限は「Chat」と「Edit」の2種類があり、後者ではファイル追加やメンバー招待などが可能となる。さらにプロジェクト専用のメモリが搭載され、長期的なタスクでも文脈を保持したまま進行できるという。</p>
<p>同機能はChatGPT Business、Enterprise、Eduプランに即日提供され、Free／Go／Plus／Proプランには近日中に展開予定。EnterpriseとEduでは既定でオフとなっており、管理者が制御できるとのこと。</p>
<h2>サードパーティアプリとの接続拡大</h2>
<p>OpenAIはまた、外部アプリとChatGPTを直接つなぐ「コネクタ」機能を強化した。現在はGmail、Google Calendar、Microsoft Outlook、Microsoft Teams、SharePoint、GitHub、Dropbox、Boxなどに対応し、情報の取り込みや操作をChatGPTから直接実行できる。</p>
<p>コネクタは用途に応じて自動的に選択され、回答の速度と正確性が向上。さらに、GitHubやSharePointなどでは同期型コネクタによる事前取り込みにも対応する。既存のアクセス権限は尊重され、Business以上のプランではデータが学習に利用されない。EnterpriseとEduでは既定でオフとなり、管理者が利用可否を制御できる。</p>
<p><strong>ChatGPTが対応する外部アプリの例（Google Drive、Gmail、GitHub、Notionなど）。今後さらに拡大予定</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Connector_logo_wall_16x9_96f5c1f71e/Connector_logo_wall_16x9_96f5c1f71e.webp" alt="Connector_logo_wall_16x9.webp" /></p>
<h2>セキュリティとコンプライアンスを強化</h2>
<p>エンタープライズ利用を想定し、セキュリティ・コンプライアンス機能も拡充された。新たにISO/IEC 27001、27017、27018、27701の認証を取得したほか、SOC 2の適用範囲をSecurity／Confidentiality／Availability／Privacyに拡大。</p>
<p>また、RBAC（カスタムロール・グループ権限管理）に対応し、アクセス制御を柔軟に設定可能となった。シングルサインオン（SSO）は既存のSAMLに加え、OIDCにも対応。さらにEnterpriseとEduではIP許可リストの設定も可能となり、組織全体での安全性が強化されている。</p>
<h2>今後の展開</h2>
<p>新機能はChatGPTにおけるチーム利用を本格化させる第一歩と位置づけられている。OpenAIはブログで「本リリースは、ChatGPTにおけるチームコラボレーションの初期ステップ」と述べ、今後も職場全体での安全な導入を支えるため、機能拡張を続けていく考えを示した。</p>
]]></description>
      <pubDate>Mon, 06 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Anthropic、次世代 AI コーディングモデル『Claude Sonnet 4.5』を発表 — 30時間集中モードが復活、強化されたエージェント性能</title>
      <link>https://ledge.ai/articles/claude_sonnet_4-5_ai_coding_model</link>
      <description><![CDATA[<p>Anthropicは2025年9月30日、最新の大規模言語モデル「Claude Sonnet 4.5」を<a href="https://www.anthropic.com/news/claude-sonnet-4-5">発表</a>した。同社は「世界最高のコーディングモデル」と位置づけ、複雑なエージェントの構築やコンピュータ操作能力で大幅な性能向上を示したと説明している。内部テストでは30時間を超える自律的な作業継続が確認され、推論や数学のベンチマークでも著しい改善が見られた。</p>
<h2>コーディング性能の飛躍</h2>
<p>Sonnet 4.5は、ソフトウェアエンジニアリングのベンチマーク「SWE-bench Verified」で77.2%の正答率を記録し、並列計算を用いた高負荷環境では82.0%に達した。従来モデルのClaude Sonnet 4（72.7%）や競合のGPT-5（72.8%）を大きく上回っている。</p>
<p><strong>ソフトウェアエンジニアリングにおけるベンチマーク「SWE-bench Verified」での比較。Sonnet 4.5が最も高い精度を示した。</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/6421e7049ff8b2c4591497ec92dc4157b2ac1b30_3840x2160_13a7b3e290/6421e7049ff8b2c4591497ec92dc4157b2ac1b30_3840x2160_13a7b3e290.webp" alt="6421e7049ff8b2c4591497ec92dc4157b2ac1b30-3840x2160.webp" /></p>
<h2>幅広い領域での強化</h2>
<p>Claude Sonnet 4.5は、コーディングだけでなく、コンピュータ操作や数学、言語理解など幅広い分野で性能を向上させた。特にOSWorldベンチマークでは61.4%を達成し、前世代の42.2%から大幅に改善。金融や法務、医療、STEMといった専門分野でも、専門家による評価で大きな知識・推論力の進歩が確認されている。</p>
<p><strong>主要ベンチマークでの各モデル比較。Claude Sonnet 4.5は複数分野でトップクラスの性能を示した。</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/67081be1ea2752e2a554e49a6aab2731b265d11b_2600x2288_30c09323a7/67081be1ea2752e2a554e49a6aab2731b265d11b_2600x2288_30c09323a7.webp" alt="67081be1ea2752e2a554e49a6aab2731b265d11b-2600x2288.webp" /></p>
<h2>エージェント開発と新機能</h2>
<p>さらに同社は今回「Claude Agent SDK」を公開。長期タスクのメモリ管理、ユーザー権限の制御、複数エージェントの協調といった仕組みを開発者に提供する。さらに以下の新機能が追加された：</p>
<ul>
<li>Claude Codeへのチェックポイント機能</li>
<li>VS Code拡張と刷新されたターミナル</li>
<li>Claudeアプリでのコード実行・ファイル生成機能</li>
<li>Chrome拡張の一般公開（Maxユーザー向け）</li>
</ul>
<p>これにより、開発者は独自のエージェントやツールを構築できる環境が整備された。</p>
<p>@<a href="https://youtu.be/OZ-aLrJ0oVg">YouTube</a></p>
<h2>安全性とアラインメントの改善</h2>
<p>Sonnet 4.5はAnthropic史上「最もアラインメントが取れた」モデルとして公開された。危険な挙動（虚偽、迎合、権力志向、妄想助長など）を大幅に低減。AI Safety Level 3（ASL-3）の保護レベルで運用され、特に化学・生物・放射線・核（CBRN）分野に関連するリスク低減が強化されている。</p>
<p><strong>各モデルにおける「ミスアラインメント行動スコア」。Sonnet 4.5は最も低い数値を示し、安全性が向上している。</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/33efc283321feeff94dd80973dbcd38409806cf5_3840x2160_f4be449515/33efc283321feeff94dd80973dbcd38409806cf5_3840x2160_f4be449515.webp" alt="33efc283321feeff94dd80973dbcd38409806cf5-3840x2160.webp" /></p>
<h2>提供条件と研究プレビュー</h2>
<p>Claude Sonnet 4.5は即日利用可能で、価格は従来のSonnet 4と同じく入力100万トークンあたり3ドル、出力100万トークンあたり15ドル。
また研究プレビューとして「Imagine with Claude」も公開され、期間限定でリアルタイムにコードを生成するデモを体験できる。</p>
<p>@<a href="https://youtu.be/dGiqrsv530Y">YouTube</a></p>
<p>Anthropicは「Claude Sonnet 4.5は、より安全で強力なフロンティアモデルであり、開発者やビジネスユーザーにとって即戦力となる」としており、今後のAIエージェント活用の加速が期待される。</p>
]]></description>
      <pubDate>Mon, 06 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>オープンLLMの日本語性能でトップ──FLUX、「Flux Japanese LLM」公開　独自手法でQwen2.5を進化</title>
      <link>https://ledge.ai/articles/flux_japanese_llm_release</link>
      <description><![CDATA[<p>国内スタートアップのFLUX株式会社は2025年9月29日、日本語特化の大規模言語モデル「Flux Japanese LLM」を<a href="https://flux.jp/news/1093/">発表</a>した。</p>
<p>同モデルはAlibaba Cloudの大規模言語モデル「Qwen2.5-32B」を基盤に、日本語理解・生成性能を独自の新手法で強化したもので、Open Japanese LLM Leaderboard（通称：LLM勉強会ランキング）で総合スコア第1位（0.7417）を記録したという。</p>
<h2>日本語能力を高める新手法「Precise-tuning」とは</h2>
<p>FLUXは今回のモデル開発にあたり、従来のファインチューニングとは異なる「Precise-tuning（プリサイズチューニング）」手法を導入した。日本語データセット全体でパラメーターを再学習するのではなく、日本語能力強化に必要なネットワーク回路のみを特定して再調整することで、効率的かつ精度の高い言語理解を実現したとしている。</p>
<p><strong>FLUXが開発した「Precise-tuning」手法の概念図</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/1440_810_4_6b464affdc/1440_810_4_6b464affdc.jpg" alt="1440-810-4.jpg" /></p>
<h2>ベンチマークで国内首位に</h2>
<p>同モデルは、LLM勉強会ランキング<a href="https://ledge.ai/articles/open_japanese_llm_leaderboard">オープン日本語LLMリーダーボード</a>において総合スコア0.7417を記録し、他の日本語モデルを上回る評価を得たという。</p>
<p>このランキングは、日本の有志研究者・エンジニアによるコミュニティ「<a href="https://llm-jp.nii.ac.jp/">LLM-jp</a>（通称：LLM勉強会）」が運営しており、複数の日本語LLMを自然言語推論・要約・コード生成などのタスクで比較評価するオープンベンチマークとして知られる。</p>
<p>LLM-jpは、国立情報学研究所（NII）を事務局とする共同研究プロジェクトで、2024年4月にNII内に設立された大規模言語モデル研究開発センター（LLMC）と連携して、「日本語に強いオープンな大規模言語モデル」を開発・評価する活動を進めている。そのため、このランキングは国内の学術・産業両分野で日本語LLMの性能を客観的に測る基準として広く参照されている。</p>
<p><strong>Open Japanese LLM Leaderboardでの評価結果。Flux Japanese LLMが第1位を記録</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/News_main_240718_3_e93e607d1f/News_main_240718_3_e93e607d1f.jpg" alt="News_main_240718-3.jpg" /></p>
<p>モデルはHugging Face上で公開されており、<a href="https://huggingface.co/flux-inc/Flux-Japanese-Qwen2.5-32B-Instruct-V1.0">モデルカード</a>には、自然言語処理・要約・コード生成タスクでの性能指標や学習設計の概要が掲載されている。</p>
<h2>企業・業界別モデル展開へ</h2>
<p>FLUXは、「Flux Japanese LLM」を自社のノーコードAIプラットフォーム群と連携させる計画を進めており、金融業界向けの特化モデル開発も行っている。同社は「AIをすべての人の手に」をミッションに掲げ、企業・研究機関・行政などが安全にLLMを活用できる基盤づくりを目指している。</p>
]]></description>
      <pubDate>Mon, 06 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>日本の「なぞなぞ」でAIの思考力をテスト──人間並みの正答率はGPT-5のみ。JAISTの研究チーム</title>
      <link>https://ledge.ai/articles/jaist_nazonazo_gpt5_benchmark</link>
      <description><![CDATA[<p>2025年9月18日、北陸先端科学技術大学院大学（JAIST）の研究チームは、日本の子ども向け「なぞなぞ（Nazonazo）」を活用し、大規模言語モデル（LLM）の洞察的推論能力を評価する新ベンチマークを開発したことを<a href="https://arxiv.org/abs/2509.14704">発表</a>した。実験の結果、GPT-5のみが人間に匹敵する正答率を示し、他のモデルは大きく下回った。</p>
<h2>飽和する既存ベンチマーク</h2>
<p>AIの能力を測る代表的なベンチマーク（MMLU、GSM8K、HumanEvalなど）は、最先端モデルが80〜90％の高スコアを記録するようになり、モデル間の性能差を明確に測りにくくなっている。OpenAI共同創業者のアンドレイ・カルパシー氏も「評価危機（evaluation crisis）」を指摘していた。</p>
<h2>日本の「なぞなぞ」はハイレベル？</h2>
<p>研究チームは、この「評価危機」を打開する手段として、日本の伝統的な言葉遊びである「なぞなぞ」を採用した。
なぞなぞは短文形式で低コストに新規作成が可能なうえ、専門知識を必要とせず、純粋な洞察力を試せる。また日本語特有の「漢字の分解」「語呂合わせ」「外来語表記」などにより、多様で難度の高い問題を作れる。</p>
<p>例として有名な「パンはパンでも食べられないパンは、なーんだ？」（答え：フライパン）が紹介されているほか、論文では「侍から“人偏”を取ると寺になる」という仕掛けのなぞなぞ（添付図参照）が示されている。</p>
<p><strong>漢字分解を利用したなぞなぞの例。「侍」から「人偏」を取ると「寺」となり、答えは「寺」</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Nazo_Nazo_Benchmar_7a284b9f17/Nazo_Nazo_Benchmar_7a284b9f17.jpg" alt="NazoNazo Benchmar.jpg" /></p>
<h2>英語 “リドル（riddle）” との差異</h2>
<p>英語圏では「RiddleSense」「BRAINTEASER」などのリドル系ベンチマークが存在するが、既に学習データに取り込まれており、GPT-4が98％超の精度で人間を上回るケースもある。
これに対し、日本語なぞなぞは人間でも平均正答率が52.9％にとどまり、AIモデルはさらに苦戦した。論文は「英語リドルはAIにとって容易になりすぎたが、日本語なぞなぞは汚染リスクが小さく、モデルの純粋な推論力を測るのに適している」と位置づけている。</p>
<h2>実験結果</h2>
<p>研究チームは38種類のLLM（GPT-4o、Claude、Gemini、Grok、Llama、DeepSeekなど）と成人126人を比較。</p>
<ul>
<li>人間の平均正答率は52.9％</li>
<li>GPT-5のみが人間平均と同等のスコアを記録</li>
<li>他のモデルは20〜30％台にとどまり、人間の半分程度にすぎなかった</li>
</ul>
<h2>AIが苦手な「最後のひと押し」</h2>
<p>多くのモデルは正解候補を途中で生成するものの、最終的に選べず「検証失敗」に陥るケースが頻発した。人間が持つ「Aha!（ひらめき）」や「これは正しい」という確信度がAIには弱く、洞察課題に特有の“最後のひと押し”が欠けていると指摘される。
また、モデルのパラメータ数の大きさと正答率には相関がなく、「推論型モデル」であることが成績の向上につながっていた。</p>
<h2>今後の展望</h2>
<p>論文は「GPT-5が例外的に人間並みの成績を示したが、他の最先端モデルは依然として人間に及ばない」と結論づけている。研究チームは、さらに難易度を高めた「Nazonazoベンチマーク2」の準備を進めており、今後はAIの“メタ認知的感覚”──正しいと感じる力──の強化が研究の焦点になる見通しだ。</p>
]]></description>
      <pubDate>Mon, 06 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>LLMへの指示が得意な人は脳の働きが違う──「プロンプト力」がfMRI研究で初めて科学的に確認される</title>
      <link>https://ledge.ai/articles/llm_prompting_brain_fmri_study</link>
      <description><![CDATA[<p>大規模言語モデル（LLM）への指示が得意な人とそうでない人の間で、脳活動に違いがあることが初めて科学的に確認された。サウジアラビア・キングサウード大学の研究チームは2025年8月20日、fMRI（機能的磁気共鳴画像法）を用いたパイロット研究の成果をarXivに<a href="https://arxiv.org/abs/2508.14869">公開</a>した。</p>
<h2>fMRIで「プロンプト力」の神経基盤を観測</h2>
<p>研究では、22人の参加者を対象に「プロンプト力」を評価するための独自尺度「Prompt Engineering Literacy Scale（PELS）」を開発し、スコアに基づき「熟達者」と「中級者」に分類。その上で、安静時fMRIを用いて脳の機能的結合やネットワーク活動を比較した。</p>
<h2>主な発見</h2>
<p>解析の結果、熟達者の脳には以下の特徴が確認された。</p>
<ul>
<li><strong>低周波帯域の優位性</strong> ：視覚ネットワーク（VVN）、デフォルトモードネットワーク後部（pDMN）、左外側頭頂ネットワーク（LLPN）などで、低周波成分が高周波成分に比べ優位であり、安定的で効率的な神経活動が示唆された。</li>
<li><strong>脳領域間の機能結合の強化</strong> ：熟達者では、左中側頭回（言語処理や意味記憶に関与）および左前頭極（計画・抽象的推論・メタ認知に関与）の機能結合が有意に強化されていた。</li>
<li><strong>効率的な神経活動</strong> ：脳内の自発的活動を示す指標（fALFF）が全般的に低下しており、不要な揺らぎが少なく効率的な情報処理が行われている可能性が示された。</li>
</ul>
<p><strong>■ LLMプロンプト熟達者で強化された左中側頭回の機能結合（fMRI解析より）</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Increased_connectivity_in_the_left_middle_temporal_gyrus_1927809d6f/Increased_connectivity_in_the_left_middle_temporal_gyrus_1927809d6f.jpg" alt="Increased connectivity in the left middle temporal gyrus.jpg" /></p>
<p><strong>■ LLMプロンプト熟達者で強化された左前頭極の機能結合（fMRI解析より）</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Increased_connectivity_in_the_left_frontal_pole_966041e40f/Increased_connectivity_in_the_left_frontal_pole_966041e40f.jpg" alt="Increased connectivity in the left frontal pole.jpg" /></p>
<h2>人とAIの協働に関する新しい視点</h2>
<p>この研究は「The Prompting Brain: Neurocognitive Markers of Expertise in Guiding Large Language Models」と題し、arXivにプレプリントとして公開された。著者らは、LLMを効果的に活用する能力（いわゆる「プロンプト力」）が、単なるスキルではなく神経科学的な特徴を持つことを示した点に意義があると述べている。</p>
<h2>今後の展望</h2>
<p>論文の著者らは、研究がパイロット的な小規模実験であり、より大規模かつ多様な参加者を対象とした検証が必要だと指摘している。また、プロンプト熟達度と脳活動の関連が、教育や職業訓練にどのような影響を及ぼすかを探る余地があるとした。さらに、AIと人間の協働を支える神経科学的理解を深めることで、ユーザーの特性に合わせたAIインターフェース設計につながる可能性があると述べている。</p>
]]></description>
      <pubDate>Mon, 06 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Microsoft 365 Copilotに「Agent Mode」「Office Agent」を導入 ～ “vibe working” でAIによるWord・Excelの自動化を推進</title>
      <link>https://ledge.ai/articles/microsoft365_agent_mode_office_agent_vibe_ai</link>
      <description><![CDATA[<p>Microsoftは2025年9月29日（米国時間）、同社の生成AI搭載ツール「Microsoft 365 Copilot」に、新機能「Agent Mode」および「Office Agent」を導入すると<a href="https://www.microsoft.com/en-us/microsoft-365/blog/2025/09/29/vibe-working-introducing-agent-mode-and-office-agent-in-microsoft-365-copilot/">発表</a>した。これらは「vibe working」と呼ばれる新しい作業体験を掲げ、WordやExcelでの文書作成・データ分析をAIが支援・自動化することを目的としている。</p>
<h2>Agent Mode：Officeアプリ内でのAI自動化</h2>
<p>Agent Modeは、WordやExcelなどのOfficeアプリケーションに組み込まれ、複数ステップにわたる作業をAIと対話しながら進められる機能。</p>
<p>Excelでは「Excel Labs」アドインを通じてプレビュー提供が開始され、数値の分析やグラフ化をAIに任せられる。Wordでは、文書の構成提案や修正作業をAIが継続的に補助する機能が実装され、まずはWeb版から展開される。</p>
<p>@<a href="https://youtu.be/nSqCy-7Qabk">YouTube</a></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Excel_benchmark_FINAL_7b6722975b/Excel_benchmark_FINAL_7b6722975b.webp" alt="Excel-benchmark-FINAL.webp" /></p>
<h2>Office Agent：Copilotチャットから文書やプレゼン生成</h2>
<p>Office Agentは、Copilotのチャット環境で稼働するエージェントで、Anthropicのモデルを搭載している。ユーザーが「レポートをまとめて」「会議資料を作成して」といった意図を伝えると、AIがWord文書やPowerPoint資料を生成・編集する。従来の単発的な応答にとどまらず、業務プロセス全体を遂行する“作業型エージェント”としての役割を担う。</p>
<p>@<a href="https://www.youtube.com/watch?v=NPSnD8-TZjY">YouTube</a></p>
<h2>“vibe working”のコンセプト</h2>
<p>Microsoftはこれらの新機能を総称して「vibe working」と表現している。簡潔な指示を入力するだけでAIが作業を補完し、文書作成やデータ分析の完成度を高めることを狙う。ユーザーはAIを相棒のように扱い、業務をより効率的に進められるという。</p>
<h2>提供条件と展開予定</h2>
<p>新機能は「Microsoft 365 Copilot」ライセンスを持つユーザーに順次展開される。Frontierプログラム参加者向けに先行提供されるケースもあり、初期段階では英語やWeb版が中心。今後は地域やアプリケーションの拡大が予定されている。</p>
]]></description>
      <pubDate>Mon, 06 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>学術＆研究2025/9/30 [TUE]AIは専門家にどこまで迫ったか──OpenAI「GDPval」が検証する “現実の仕事力”</title>
      <link>https://ledge.ai/articles/openai_gdpval_ai_job_benchmark</link>
      <description><![CDATA[<p>OpenAIは2025年9月25日、最新の大規模言語モデル（LLM）が「現実の経済価値を持つタスク」でどの程度人間に迫っているかを測定する新しい評価指標「GDPval」を<a href="https://openai.com/index/gdpval/">発表</a>した。実際の業務成果物をもとに44職種・1,320件のタスクで性能を比較した結果、最先端モデルは専門家に近い水準に達していることが示された。</p>
<p>GDPvalは、米国GDPに寄与する9産業・44職種を対象に設計された新しい評価ベンチマークである。法律文書や設計図、動画編集、カスタマーサポートの対応記録など、実際の成果物をタスク化し、AIモデルと業界専門家の成果を比較する。</p>
<p><strong>「GDPval」に含まれる実務タスク例。設計図や看護報告、財務分析からカスタマーサポートまで幅広い領域をカバーしている。</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Example_GD_Pval_tasks_from_full_set_1f598d2776/Example_GD_Pval_tasks_from_full_set_1f598d2776.jpg" alt="Example GDPval tasks from full set.jpg" /></p>
<h2>幅広い産業と職種をカバー</h2>
<p>評価対象は、不動産、製造、政府、金融、医療、情報サービスなど主要な産業を網羅。ソフトウェア開発者や弁護士、看護師、金融アナリストなど、幅広い知識労働が調査された。</p>
<p><strong>「GDPval」で評価対象となった9産業・44職種。米国経済に大きく寄与する分野から抽出されている。</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/GD_Pval_includes_real_world_work_from_44_occupations_fc303d69cd/GD_Pval_includes_real_world_work_from_44_occupations_fc303d69cd.jpg" alt="GDPval includes real-world work from 44 occupations.jpg" /></p>
<h2>厳格なレビューと評価方法</h2>
<p>各タスクは平均5回の専門家レビューを経て整備され、最終的に人間とAIの成果物をブラインドで比較。220件のタスクは「ゴールドサブセット」として公開され、自動採点サービスも提供されている。</p>
<p><strong>タスクは複数段階の専門家レビューを経て現実性と品質を担保している。</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Tasks_undergo_multiple_rounds_of_review_to_ensure_realism_and_quality_7fb88c6b59/Tasks_undergo_multiple_rounds_of_review_to_ensure_realism_and_quality_7fb88c6b59.jpg" alt="Tasks undergo multiple rounds of review to ensure realism and quality.jpg" /></p>
<p><strong>人間専門家によるペアワイズ比較。AIの成果物と人間の成果物を並べ、どちらが優れているかを評価する。</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/GD_Pval_uses_pairwise_expert_comparisons_for_grading_69709621f2/GD_Pval_uses_pairwise_expert_comparisons_for_grading_69709621f2.jpg" alt="GDPval uses pairwise expert comparisons for grading.jpg" /></p>
<h2>成果と課題</h2>
<p>評価の結果、Claude Opus 4.1は文書やプレゼン資料のレイアウトで優れ、GPT-5は指示理解や計算精度に強みを示した。一方で、モデルの失敗要因として最も多かったのは「指示を正しく理解できていない場合」であると報告されている。</p>
<p>また、推論時間を増やす、プロンプトを工夫するなどの対応により性能はさらに改善可能であることも確認された。</p>
<h2>今後の展望</h2>
<p>現時点のGDPvalは知識労働に限定されるが、将来的には対話型や現場対応を含むタスクへ拡張する計画だ。OpenAIは220件のタスクを公開し、研究者コミュニティによる継続的な評価を促進することで、AIと人間の協働のあり方を探る。</p>
]]></description>
      <pubDate>Mon, 06 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI、次世代動画生成モデル「Sora 2」を発表──自分や友人が出演する動画を生成できるiOSアプリ「Sora」も米国とカナダで同時公開</title>
      <link>https://ledge.ai/articles/sora2_openai_ios_app_launch</link>
      <description><![CDATA[<p>OpenAIは2025年9月30日、最新の動画・音声生成モデル「Sora 2」を<a href="https://openai.com/index/sora-2/">発表</a>した。</p>
<p>物理挙動の正確さや映像の写実性が大幅に向上し、音声を同期して生成できる点が特徴。同日には、このモデルを利用できるiOS向けアプリ「Sora」も公開され、米国とカナダで招待制による提供が始まった。日本での提供時期は明らかにされていない。</p>
<p>@<a href="https://youtu.be/lEcg6AJ6DVY?si=aS3u22digXd5ZVY8">YouTube</a></p>
<h2>Sora 2の性能</h2>
<p>Sora2は、従来の「Sora」モデルを基盤に開発された動画・音声生成AIである。
OpenAIが公開した<a href="https://openai.com/index/sora-2-system-card/">システムカード</a>によれば、より正確な物理シミュレーション、長尺映像における一貫性、幅広いスタイルへの対応を実現。さらに音声生成を統合し、映像にナレーションや環境音を付与できる。</p>
<p>生成可能な映像は最大20秒とされるが、ReutersやThe Vergeなど複数のメディアは「アプリ上では10秒程度に制限されている」と報じている。</p>
<p>@<a href="https://www.youtube.com/watch?v=1PaoWKvcJP0">YouTube</a></p>
<h2>iOSアプリ「Sora」の提供</h2>
<p>同日に公開されたiOS向けアプリ「Sora」では、ユーザーがAI生成動画を作成・共有できる。
提供開始は米国とカナダで、アクセスは招待制。アプリ内で通知登録を行うことで順次利用可能となる。AndroidユーザーはWeb版の “sora.com” からアクセスできる仕組みだ。Sora2は当初無料で利用できるが、計算能力の制限が設けられているとのこと。</p>
<p>アプリの特徴として注目されるのが**「Cameo（カメオ）機能」** だ。ユーザーは自分や友人を動画に登場させられる。OpenAIは、この機能を利用するには本人の同意が必要とし、無断で他人の肖像を使用することはできない設計にしているという。サム・アルトマンCEOも自身のブログで「チームがキャラクターの一貫性に力を注ぎ、友人同士を動画に登場させることが意外なほど魅力的な新しいつながり方になった」と述べている。</p>
<h2>安全性への配慮</h2>
<p>OpenAIは安全設計を重視しており、生成動画には透かしやC2PAメタデータを付与。肖像権の無断利用や公人の生成は禁止され、未成年保護のためのフィルタリングや保護者向けコントロール機能も導入されている。
システムカードに記載された安全性評価では、不適切コンテンツを検出・遮断する精度が96〜99％に達したという。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/introducing_sora2_9ac129aadc/introducing_sora2_9ac129aadc.jpg" alt="introducing sora2.jpg" /></p>
<h2>背景と思想</h2>
<p>同社CEOのサム・アルトマン氏は自身の<a href="https://blog.samaltman.com/sora-2">ブログ</a>で、Soraを「ChatGPT for creativity」と表現。誰もが手軽に動画生成を楽しめる環境を提供する一方で、依存性や誤用のリスクについても懸念を示し、長期的なユーザーの満足や健全な利用を重視する方針を強調した。</p>
<p>また、OpenAIは公式サイトで「<a href="https://openai.com/index/sora-feed-philosophy/">フィード哲学</a>」を公開し、ユーザーが視聴体験を自ら選択できる仕組みを構築するとしている。</p>
<p>@<a href="https://www.youtube.com/watch?v=gzneGhpXwjU&amp;t=137s">YouTube</a></p>
]]></description>
      <pubDate>Mon, 06 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>デジタル庁、OpenAIと連携　政府共用AI「源内」に同社モデルを導入し行政活用を検討</title>
      <link>https://ledge.ai/articles/digital_agency_openai_genai_collaboration</link>
      <description><![CDATA[<p>デジタル庁は2025年10月2日、米OpenAIと生成AI（人工知能）の活用で連携すると<a href="https://www.digital.go.jp/news/e950673b-73eb-4f65-bf6a-339e4f0e7ef1">発表</a>した。政府職員が安全に生成AIを利用できる共用環境「源内（げんない）」に、OpenAIの大規模言語モデル（LLM）を新たにラインアップとして追加し、職員が業務で直接利用できるようにする方針だ。</p>
<h2>OpenAI選定の背景</h2>
<p>平将明デジタル大臣は翌3日の記者会見で、OpenAIを選定した理由について、最先端のAI研究と展開を進める企業として評価している旨を述べた。
源内では従来から複数のモデルを比較・検証しつつ活用しており、OpenAIのモデル追加により選択肢を拡充し、行政の生産性向上につなげる考えだ。</p>
<p>この協力方針は、2024年9月にOpenAIの最高戦略責任者（CSO）ジェイソン・クォン（Jason Kwon）氏がデジタル庁を訪問した際の会談で確認されたという。平大臣は「生成AIの利活用を推進するうえで重要な一歩」と述べた。</p>
<h2>セキュリティ体制の整備</h2>
<p>行政での生成AI活用にあたっては安全性と信頼性の確保が前提となる。会見の質疑で平大臣は、政府情報システムのためのセキュリティ評価制度「ISMAP（イスマップ）」に言及し、認証の有無により扱える情報の範囲が変わることを説明した。</p>
<p>@<a href="https://www.youtube.com/watch?v=hWf2w9br940">YouTube</a></p>
<p>OpenAI側も同日、自社の公式<a href="https://openai.com/ja-JP/global-affairs/strategic-collaboration-with-japan-digital-agency/">ブログ</a>で日本政府との戦略的協力を発表した。公共分野での活用モデルの共同検討や、ISMAP認証の取得をはじめ安全・安心に資する取り組みを前向きに検討する方針を示している。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/digital_openai_7ada319b72/digital_openai_7ada319b72.jpg" alt="digital openai.jpg" /></p>
]]></description>
      <pubDate>Mon, 06 Oct 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Sora、著作権方針を修正──Altman氏「日本の創作物に敬意」発言も。権利保護と生成AIの共存を模索</title>
      <link>https://ledge.ai/articles/sora_update_copyright_and_revenue_sharing_oct2025</link>
      <description><![CDATA[<p>OpenAIは、動画生成AI「Sora（ソラ）」の著作権対応を見直す。</p>
<p>同社CEOであるサム・アルトマン氏は2025年10月3日（現地時間）、自身の公式<a href="https://blog.samaltman.com/">ブログ</a>権利者がキャラクター生成をより細かく管理できる新機能と、収益分配制度の導入計画を発表した。背景には、Soraリリース後に浮上した知的財産権への懸念と、世界中で広がった批判がある。</p>
<h2>Soraをめぐる著作権論争</h2>
<p>OpenAIは2025年9月末、動画生成AIの最新版「Sora 2」と、米国・カナダ向けのソーシャル型iOSアプリ「Sora」を公開した。
テキストから高精細な映像を生成できる能力が話題を呼ぶ一方で、リリース直後には任天堂の「マリオ」や「ピカチュウ」など既存キャラクターに酷似した動画がSNS上で多数共有され、著作権保護の観点から懸念が相次いだ。一部の海外メディアは「任天堂の訴訟を招く可能性がある」と報じ、AIによる無断再現がどこまで許容されるのかが議論となった。</p>
<p>さらに、リリース直前に報じられた「権利者がオプトアウト（除外申請）しない限り自作品が扱われる」という運用観測も反発を招き、“同意なき取り込み”への不信感が増幅した。</p>
<p>この一連の騒動を受け、OpenAIはSoraの利用方針を再検討。今回の発表で、権利者による生成コントロール機能の導入と収益分配制度の構築を正式に打ち出した。</p>
<h2>権利者が生成内容を制御可能に</h2>
<p>アルトマン氏は、「Sora」の今後の方針として、権利者がキャラクター生成の可否や利用範囲を細かく指定できる新機能を追加する考えを示した。
これは、従来の「opt-in for likeness（本人類似モデル許諾）」を拡張したもので、より柔軟で公平な管理を可能にするという。</p>
<p>同氏は次のように述べている。</p>
<p>\u003E“We will give rightsholders more granular control over generation of characters... but want the ability to specify how their characters can be used (including not at all).”
（キャラクター生成について、権利者がより細かく制御できるようにし、使用方法を自ら指定できるようにします。まったく使用を許可しない選択も可能です。）</p>
<p>同氏によると、権利者の多くはSoraを通じた“インタラクティブなファンフィクション（参加型二次創作）”の可能性に期待を寄せつつも、利用のあり方を自ら決めたいと考えているという。OpenAIは、そうした多様な方針を尊重しつつ、全ての権利者に公平な標準を適用する姿勢を示した。</p>
<h2>収益分配モデルの試験導入へ</h2>
<p>OpenAIは、ユーザーによる動画生成量が予想を上回っていることを踏まえ、生成に使用されたキャラクターの権利者に収益を還元する制度を導入する計画も明らかにした。
アルトマン氏は「試行錯誤を重ねながら早期に開始する」とし、経済的な利益と新しい関係構築の双方を実現したい考えを示している。</p>
<p>\u003E “We are going to try sharing some of this revenue with rightsholders who want their characters generated by users.”
（ユーザーによって生成されるキャラクターを許可した権利者に対し、その一部の収益を共有することを試みます。）</p>
<h2>日本の創作文化に敬意を表明</h2>
<p>アルトマン氏は投稿の中で、「日本の創作物は非常に素晴らしい」と述べ、「ユーザーと日本のコンテンツとの深い結びつきに感銘を受けている」と言及した。
Soraをめぐる議論の中心に日本のコンテンツ産業があったことを踏まえ、文化的背景への理解を示した形だ。
今回の発表は、単なる技術的修正にとどまらず、文化と生成AIの関係を再定義する試みともいえる。</p>
<h2>GPT-5も同日にアップデート、安全性を強化</h2>
<p>同日、OpenAIは<a href="https://help.openai.com/en/articles/9624314-model-release-notes">Model Release Notes</a>を更新し、GPT-5 Instantモデルにメンタルヘルス対応の新機能を追加した。感情的または心理的なストレスをより正確に検知し、必要に応じて現実世界の支援リソースへ誘導できるようになったという。</p>
<p>こうした改良は、アルトマン氏が強調する「高速な改善サイクル」の一環として、Soraを含む同社製品全体に順次展開される見通しだ。</p>
<h2>今後の展望</h2>
<p>アルトマン氏は、「ChatGPT初期のように高頻度の改善を続けていく」と述べ、Soraを中心にプロダクト全体の改善を加速させる考えを示した。生成AIと著作権をめぐる議論が国際的に広がるなか、今回の発表は創作支援と権利保護の両立を図る “新しい共存モデル” として注目される。</p>
]]></description>
      <pubDate>Sun, 05 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>元OpenAI・DeepMind研究者が設立「Periodic Labs」、3億ドルを調達しAIで科学研究を自動化へ</title>
      <link>https://ledge.ai/articles/periodic_labs_ai_scientist_seed</link>
      <description><![CDATA[<p>元OpenAIやDeepMindの研究者が中心となって設立したスタートアップ Periodic Labs が、シード資金として約3億ドルを調達し、非公開での活動（ステルスモード）を経て発足した。共同創業者のWilliam Fedus氏は2025年9月30日（現地時間）、X（旧Twitter）で「AI科学者を構築し、自律的な実験室を運営することを目指す」と<a href="https://x.com/LiamFedus/status/1973055380193431965">発表</a>した。</p>
<h2>AI科学者と自律実験室</h2>
<p>Fedus氏は、「科学は仮説を立て、実験を行い、結果から学ぶことで進展する」と述べ、AIがそのサイクルを担う未来を描いた。Periodic Labsは自律的な実験室を構築し、高品質な科学データや従来発表されにくい“失敗の実験結果”を含めて活用することで、新しい知見の創出を加速させる狙いだ。まずは物理科学分野から着手し、自然界を「強化学習の環境」として扱うことでAIによる発見を推進する。</p>
<h2>研究テーマと応用例</h2>
<p>研究テーマの一つは高温超伝導体の探索で、実現すれば次世代輸送技術や高効率の送電網に大きな影響を与えるとされる。また、同社はすでに半導体メーカーと協力し、チップの放熱問題に対応するため、実験データを解析するカスタムAIエージェントを開発している。</p>
<h2>創業メンバー</h2>
<p>創業チームには、ChatGPTの共同開発、DeepMindのGNoME、OpenAIのOperator（現Agent）、ニューラルネットのアテンション機構、MatterGenなどに関わった研究者が名を連ねる。過去10年で重要な材料発見を主導してきた人材も加わり、幅広い経験を持つ研究陣が揃っている。</p>
<h2>資金調達と投資家</h2>
<p>今回のシード資金はa16z（Andreessen Horowitz）が主導。Felicis、DST Global、NVentures（NVIDIAのVC部門）、Accelなどが参加し、個人投資家としてジェフ・ベゾス氏、エリック・シュミット氏、ジェフ・ディーン氏らも名を連ねる。
a16zは<a href="https://a16z.com/announcement/investing-in-periodic-labs/">公式ブログ</a>で「Periodic Labsの取り組みは科学の進め方を根本的に変える可能性がある」と述べ、支援の背景を明らかにしている。</p>
]]></description>
      <pubDate>Sun, 05 Oct 2025 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>地球観測をAIエージェントで刷新──「Earth-Agent」と新ベンチマーク「Earth-Bench」登場</title>
      <link>https://ledge.ai/articles/earth_agent_earth_bench_launch</link>
      <description><![CDATA[<p>中国の研究チームは2025年9月27日、地球観測データを高度に解析できるAIフレームワーク「Earth-Agent」を<a href="https://arxiv.org/abs/2509.23141">発表</a>した。RGB画像や分光データを統合し、従来のAIが苦手としてきた複雑な推論や専門ツールの利用を可能にする。あわせて、248タスク・13,729枚の画像を収録した新ベンチマーク「Earth-Bench」も公開した。</p>
<h2>Earth-Agentとは</h2>
<p>Earth-Agentは、地球観測（Earth Observation, EO）に特化した初のエージェント型AIだ。RGB画像や分光観測データを活用し、地球物理パラメータの推定、気候解析、都市管理、災害監視などに応用できる。人間のように「考える→ツールを呼び出す→結果を更新」というステップを繰り返し、複雑な課題を解決する構造を持つ。</p>
<p><strong>Earth-Agentの処理フレームワーク。思考・ツール利用・更新を繰り返す構造が特徴</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Earth_Agent_Framework_2a4b1e8bf6/Earth_Agent_Framework_2a4b1e8bf6.jpg" alt="Earth-Agent Framework.jpg" /></p>
<h2>従来モデルの限界を突破</h2>
<p>一般的なマルチモーダルLLM（GPT-4やGeminiなど）は、地球観測の現場で必要とされるマルチステップ推論に弱いとされてきた。Earth-Agentは専門ツールを動的に呼び出すことでこの課題を克服。例えば、干ばつ指数の解析、夜間光データの回帰分析、港湾面積の比較といった応用を実証した。</p>
<p><strong>Earth-Agentによる解析例。干ばつ監視、夜間光解析、港湾面積の差分計算を実行している</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Earth_Agent_solving_tasks_across_Spectrum_Products_and_RGB_data_155fe5c084/Earth_Agent_solving_tasks_across_Spectrum_Products_and_RGB_data_155fe5c084.jpg" alt="Earth-Agent solving tasks across Spectrum Products and RGB data.jpg" /></p>
<h2>新ベンチマーク「Earth-Bench」</h2>
<p>研究チームは同時に、新しい評価データセット「Earth-Bench」を構築した。248タスク、13,729画像を収録し、スペクトル解析・プロダクト解析・RGB解析の3カテゴリに分類。既存のEarthVQAやGeo-Benchに比べ、クロスモダリティや複雑な推論に対応しているのが特徴だ。</p>
<p><strong>Earth-Benchの全体像。既存ベンチマークとの比較や各カテゴリの内訳</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Dataset_Comparison_and_Overview_3fc2821c52/Dataset_Comparison_and_Overview_3fc2821c52.jpg" alt="Dataset Comparison and Overview.jpg" /></p>
<h2>データ構築の仕組み</h2>
<p>Earth-Benchは、Google Earth EngineやNASA Earth Dataから取得した衛星画像をもとに研究者が課題を設計している。論文では「2021年にシカゴ大都市圏で、地域の25%以上が300K（約27℃）を超えた日は何日あったか」といった問題が例示されている。これは都市ヒートアイランド現象や極端高温日数のモニタリングに直結する。</p>
<p>こうした課題はPythonコードで解法プロセスを記録し、JSON形式でアノテーション化。データには、どの衛星画像を使い、どのツールをどの順序で呼び出し、最終的に何日と算出されたかまでが明示されている。</p>
<p><strong>Earth-Benchの構築プロセス。衛星データから課題を生成し、ツール利用手順を記録</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Construction_and_Annotation_of_Earth_Bench_53ceef7fb0/Construction_and_Annotation_of_Earth_Bench_53ceef7fb0.jpg" alt="Construction and Annotation of Earth-Bench.jpg" /></p>
<h2>今後の展望</h2>
<p>論文によると、Earth-Agentのコードとデータは公開される予定とのこと。研究チームは、地球科学研究の高度化だけでなく、防災計画や気候変動対策、都市政策など幅広い応用に貢献できると見込んでいる。</p>
]]></description>
      <pubDate>Sun, 05 Oct 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>AIは「音を想像して考える」ことができるか──韓国の研究チームがAuditoryBench++と新手法AIR-CoTを発表</title>
      <link>https://ledge.ai/articles/ai_auditory_imagination_aircot</link>
      <description><![CDATA[<p>韓国の研究チームは2025年9月22日、言語モデル（LLM）が実際に音を聞かずとも聴覚的知識を想起できるかを検証する新しいベンチマーク「AuditoryBench++」を開発したと<a href="https://arxiv.org/abs/2509.17641">発表</a>した。さらに、音を頭の中で「想像」させる推論手法 AIR-CoT（Auditory Imagination Reasoning with Chain-of-Thought） を導入し、従来のモデルよりも高い精度で聴覚推論を実現したと報告している。</p>
<h2>人間は音を「想像」できるがLLMは苦手</h2>
<p>研究チームは、カント哲学の視点を引きながら「想像力は知覚に不可欠な要素である」と指摘する。例えば「大雨と稲妻の夜を描写した文章」を読むと、人は雨が太鼓を打つように降り注ぐ音や雷鳴を思い浮かべることができる。しかし、既存のLLMにはこのような聴覚的想像力が欠けているという。</p>
<h2>AuditoryBench++の概要</h2>
<p>この課題を検証するために開発された「AuditoryBench++」は、テキストのみを用いて音に関する推論力を評価するベンチマークで、以下の5種類のタスクを含む。</p>
<ul>
<li><strong>ピッチ比較</strong> （高い／低い音）</li>
<li><strong>持続時間比較</strong> （長い／短い音）</li>
<li><strong>大きさ比較</strong> （大きい／小さい音）</li>
<li><strong>動物の鳴き声認識</strong> （例：「meow」＝猫）</li>
<li><strong>文脈的聴覚推論</strong> （状況文から音を推定）</li>
</ul>
<p>合計で6,732問の問題が用意され、既存のベンチマークよりも大規模かつ精緻な評価が可能になっている。</p>
<h2>新手法AIR-CoTの仕組み</h2>
<p>研究チームはさらに、モデルに「音を想像させる」新しい推論手法 AIR-CoT を導入した。AIR-CoTでは、モデルが推論中に音の知識を必要とする場面に遭遇すると、特殊トークン [imagine] を生成する。するとシステムが想像プロセスを発動し、CLAPなどの音声モデルから得られた音の埋め込みを挿入、推論を継続する。これにより、まるで音を「頭の中で鳴らして」考えるかのような動作が可能になる。</p>
<p><strong>AIR-CoTの仕組み：モデルは [imagine] トークンで音を必要とする箇所を検出し、CLAPによる音埋め込みを注入して推論を続ける</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x1_cd93e9a630/x1_cd93e9a630.jpg" alt="x1.jpg" /></p>
<h2>実験結果</h2>
<p>AuditoryBench++を用いた評価では、既存のLLM（LLaMA3、Qwen2、Phi-4など）やマルチモーダルLLMはいずれもランダムに近い精度しか出せなかった。一方、AIR-CoTは以下のような大幅な改善を示した。</p>
<ul>
<li><strong>ピッチ比較</strong> ：83.9%（従来比 +8.3pt）</li>
<li><strong>動物の鳴き声認識</strong> ：71.6%（+9.3pt）</li>
<li><strong>文脈的聴覚推論</strong> ：82.7%（+11.9pt）</li>
</ul>
<p>ただし「持続時間」や「大きさ」の推論は依然として難しく、改善の余地があるとされている。</p>
<h2>今後の展望</h2>
<p>研究チームは、AuditoryBench++とAIR-CoTを提案し、従来のモデルを上回る成果を確認したと結論づけている。同研究は、直接オーディオ入力がなくても聴覚情報を想像できる言語モデルの基盤を提供し、最終的にはより自然で人間らしいマルチモーダル推論を可能にするとしている。</p>
]]></description>
      <pubDate>Sat, 04 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Metaが、AI チャットでの対話内容をFacebook／Instagramの広告最適化に利用すると発表— 12月導入、ユーザーはオプトアウト不可</title>
      <link>https://ledge.ai/articles/meta_ai_chat_ads_december</link>
      <description><![CDATA[<p>Metaは2025年10月1日（現地時間）、利用者とAIチャットボットのやり取りを、FacebookやInstagram上で表示されるコンテンツや広告のパーソナライズに活用する方針を公式ブログで<a href="https://about.fb.com/news/2025/10/improving-your-recommendations-apps-ai-meta/">発表</a>した。通知は10月7日から開始され、正式導入は12月16日としている。</p>
<h2>新方針の概要</h2>
<p>Metaは、利用者がAIチャット機能を通じて行った対話内容を、これまでの「いいね」や「フォロー」と同様に、レコメンデーションの新たな信号として扱う。これにより、ユーザーの関心により沿った投稿や広告が表示される仕組みを強化する。</p>
<p>新方針は世界的に展開されるが、当面は欧州連合（EU）、英国、韓国の利用者は対象外とされる。Metaによれば、健康や宗教、政治的立場、人種、性的指向など「センシティブな情報」は広告ターゲティングに利用しない方針だ。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/01_Notification_Carousel_01_1_767b81e54c/01_Notification_Carousel_01_1_767b81e54c.jpg" alt="01_Notification_Carousel-01-1.jpg" /></p>
<h2>オプトアウト不可の仕組み</h2>
<p>利用者が一度AIチャット機能を使い始めると、その対話データが広告最適化に活用されることを拒否することはできない。Metaは従来の「広告設定」や「なぜこの広告か」といった機能は引き続き提供するものの、AIチャット自体の利用データを除外する選択肢は設けていない。</p>
<h2>広告・表示への影響</h2>
<p>たとえば、ユーザーがAIと「ハイキング」の話題を交わすと、アウトドア用品の広告や関連投稿がフィードに増える可能性がある。Metaはこの仕組みにより、個人の関心と広告主の提供価値をより一致させることができるとしている。</p>
<h2>プライバシーと今後の焦点</h2>
<p>Metaの新方針は、無料で提供されるAIサービスから得られるデータを収益化モデルに組み込む動きの一環と位置づけられる。一方で、オプトアウトが認められない点は利用者のプライバシー権との関係で議論を呼んでいる。
透明性や説明責任をどう確保するか、また各国の規制対応と整合性を取れるかが、今後の注目点となる。</p>
]]></description>
      <pubDate>Sat, 04 Oct 2025 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>落合陽一氏プロデュース「null²」移設クラファン──大阪・関西万博発、開始わずか23時間で1億円達成・次は2億円へ</title>
      <link>https://ledge.ai/articles/ochiai_null2_expo2025_crowdfunding_2oku</link>
      <description><![CDATA[<p>メディアアーティストの落合陽一氏がプロデュースした大阪・関西万博のシグネチャーパビリオン「null²（ヌルヌル）」の移設に向けたクラウドファンディングが、開始から23時間で第一目標の1億円を<a href="https://readyfor.jp/projects/null2_2025">達成</a>した。READYFORのプロジェクトページによれば、2025年10月1日の公開から急速に支援が集まり、翌2日朝には1億円を突破。ネクストゴールとして2億円が新たに設定されている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/null2_project_0c7453a2c9/null2_project_0c7453a2c9.jpg" alt="null2 project.jpg" /></p>
<h2>null²とは</h2>
<p>「null²」は、大阪・関西万博（2025年4月13日～10月13日、夢洲）におけるシグネチャーパビリオンのひとつで、テーマ「いのちを磨く」を体現する施設。伸縮する鏡素材「ミラー膜」とロボットアームを組み合わせ、外観そのものが動き続ける「動く建築」として注目を集めた。内部では鏡面状のLEDに囲まれた没入型シアター体験が提供され、開幕以降は抽選倍率数％という人気ぶりだった。</p>
<h2>クラウドファンディングの目的</h2>
<p>今回のクラウドファンディングは、万博終了後も「null²」を別の場所に移設・再構築するための資金を募るもの。主催は一般社団法人「計算機と自然」（代表：落合陽一氏）。資金は、新天地での企画設計や管理費用、活動資金、そして展示を記録映像として残す制作費などに充てられる予定だ。方式は「All in」で、目標未達でも集まった金額に応じてプロジェクトは実行される。支援募集は12月19日（金）23時まで行われる。</p>
<p>@<a href="https://www.youtube.com/watch?v=XOKS4w0Ige8">YouTube</a></p>
<h2>達成状況と返礼品</h2>
<p>プロジェクトは開始から23時間で第一目標を突破。READYFOR上では「null²と（また）会いたい」という支援者の声が多く寄せられており、7,000人を超える支援者が参加している。
返礼品には、外装に使われた鏡素材を切り出した「null²のカケラ」、限定パーカー、落合氏のプリント作品や模型など、万博の思い出を継承する品が用意されているという。</p>
<h2>今後の展望</h2>
<p>具体的な移設先や公開時期は未定だが、主催者は「全国から候補地としての声が届いている」としており、引越し計画の進展は今後、READYFORの活動報告や落合氏の公式SNSを通じて発信される見通しだ。</p>
<p>万博閉幕後も多くの人が体験できる形で「null²」を残すべく、クラウドファンディングは続いている。</p>
]]></description>
      <pubDate>Sat, 04 Oct 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>世界初のAI女優「ティリー・ノーウッド」にハリウッドが揺れる──SAG-AFTRAや著名俳優が「創造性の危機」と警告</title>
      <link>https://ledge.ai/articles/tilly_norwood_ai_actress_controversy_sagaftra</link>
      <description><![CDATA[<p>世界初の「AI女優」と称される Tilly Norwood（ティリー・ノーウッド）が、ハリウッドの俳優や映画俳優組合SAG-AFTRA（全米映画俳優組合・テレビ・ラジオ芸術家連盟）から強い批判を受けている。</p>
<p>SAG-AFTRAは2025年9月30日に「創造性は人間中心であるべき」と声明を<a href="https://www.sagaftra.org/sag-aftra-statement-synthetic-performer">発表</a>した。女優のエミリー・ブラント氏やウーピー・ゴールドバーグ氏も相次いで懸念を表明し、映画・テレビ業界全体に議論が広がっている。</p>
<h2>「AI Commissioner」──ティリー誕生の舞台</h2>
<p>Tilly Norwoodは、ロンドン拠点の制作会社Particle6が2025年9月にYouTube上で公開したコメディスケッチ『AI Commissioner | Comedy Sketch | Particle6』で初登場した。</p>
<p>この作品は、テレビ業界におけるAIの急速な普及を風刺的に描いたもので、AIが脚本作成からキャスティング、予算編成まですべてを自動化する世界を描く。劇中では、AIが生み出したインタラクティブスリラー『I Know What You Streamed Last Summer』に登場する100％AI生成の俳優としてTillyが紹介される。</p>
<p>@<a href="https://www.youtube.com/watch?v=3sVO_j4czYs">YouTube</a></p>
<p>登場人物の1人はTillyについて「僕の言うことを何でも聞いてくれる。恋をしてしまったかもしれない（She’ll do anything I say. I think I’m in love）」と評し、AIによる従順で“最適化された”俳優像を象徴的に表現。
さらに、「“She’ll cry on Graham Norton and be monetized on TikTok by lunchtime.”（彼女はグレアム・ノートンの番組で泣いて、その日の昼までにTikTokで収益化されるだろう）」という台詞が、人間の感情までもがAIによって即座に商業化される未来への皮肉として話題を呼んだ。</p>
<h2>SAG-AFTRAが声明「Tillyは俳優ではない」</h2>
<p>SAG-AFTRAは9月30日に「Statement on Synthetic Performer（合成パフォーマーに関する声明）」を<a href="https://www.sagaftra.org/sag-aftra-statement-synthetic-performer">発表</a>し、次のように明言した。</p>
<p>\u003E“Tilly Norwood is not an actor. Creativity must remain human-centered.”
（ティリー・ノーウッドは俳優ではない。創造性は人間中心であるべきだ。）</p>
<p>声明では、AIによって作られた “合成俳優” が芸術表現を侵食する可能性を指摘し、「経験や感情を持たない存在を“俳優”と呼ぶことは、芸術の根幹を損なう」と警鐘を鳴らした。SAG-AFTRA会長のショーン・アスティン氏もVarietyの取材に対し、「AI倫理と補償問題を正式な交渉テーマとして扱う」と述べている。</p>
<h2>著名俳優からの反発</h2>
<p>女優のエミリー・ブラント氏は、Varietyのポッドキャスト番組でTillyの画像を見せられ、驚きを隠さずこう語った。</p>
<p>\u003E“Good Lord, we’re screwed. That is really, really scary. Come on, agencies, don’t do that. Please stop taking away our human connection.”
「なんてこと、私たちは終わりね。本当に恐ろしいわ。お願い、エージェントはそんなことをやめて。人間のつながりを奪わないで。」</p>
<p>また、取材でTillyを「次のスカーレット・ヨハンソンに」と問われると、「“But we have Scarlett Johansson.”（でも私たちにはスカーレット・ヨハンソンがいる」 と返し、人間俳優の価値を強調した。</p>
<p>俳優・司会者のウーピー・ゴールドバーグ氏も、ABCのトーク番組『The View』（9月30日放送）で次のように発言した。</p>
<p>\u003E“You’re looking at 5,000 actors rolled into one synthetic person. That’s not fair. We all move differently.”
「5,000人の俳優の特性をひとつの合成存在にまとめるなんてフェアじゃない。私たちは皆、動きも表情も違う。」</p>
<h2>開発側の見解：「人間の代替ではなく、芸術表現」</h2>
<p>Tillyを制作したエライン・ファン・デル・フェルデン氏（Eline Van der Velden）は9月28日、ティリーのInstagram（<a href="https://www.instagram.com/tillynorwood/">@tillynorwood</a>）に投稿し、AI俳優の創作意図について次のように述べている。</p>
<p>\u003E “She is not a replacement for a human being, but a creative work — a piece of art.”
「ティリーは人間の代替ではなく、創造的な作品＝ひとつのアートです。」</p>
<p>同氏は、AIを「人間の代わり」ではなく「新しい絵筆のようなツール」と位置づけ、「アニメーションや人形劇、CGIがライブ演技を奪うことなく新しい可能性を開いたように、AIも物語を構築する新たな手段を提供する」と説明した。</p>
<p>\u003E “I’m an actor myself, and nothing — certainly not an AI character — can take away the craft or joy of human performance.”
「私は俳優でもあり、AIキャラクターであっても、人間の演技の技や喜びを奪うことはできません。」</p>
<p>投稿ではさらに、AIを“人間と競わせる存在”ではなく“芸術の新しいジャンルの一部”として評価すべきだと訴えている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/instagram_eline_at_tillynorwood_b80dee2451/instagram_eline_at_tillynorwood_b80dee2451.jpg" alt="instagram eline at tillynorwood.jpg" /></p>
<p>同氏が率いるParticle6は「AI俳優を活用すれば制作コストを最大90％削減できる」と説明し、複数のAIタレントを育てる「デジタル・タレント・ユニバース」構想を掲げている。</p>
<h2>今後の焦点</h2>
<p>各報道や関係者の声明からは、主に次の3点が論点として浮上している。</p>
<ul>
<li>著作権と肖像権：AI俳優の訓練データやモデル構築に使用された素材の扱い</li>
<li>契約・補償問題：AI使用を前提とした俳優契約の新たな枠組み</li>
<li>文化的受容：AIが「演技」を行うことを人々がどう受け入れるか</li>
</ul>
<p>SAG-AFTRAをはじめとする業界団体や俳優たちは、AIが創作活動や雇用に及ぼす影響を注視しており、今後は法制度や契約の整備を含めた議論が進む見通しだ。</p>
]]></description>
      <pubDate>Fri, 03 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google「Gemini for Home」を発表──Nest Cam刷新と新Google Homeスピーカーで “家庭のAI体験” を一新</title>
      <link>https://ledge.ai/articles/google_gemini_for_home_nest_cam_home_speaker</link>
      <description><![CDATA[<p>Googleは2025年10月1日（現地時間）、会話型AI「Gemini」を家庭向けに導入する新戦略「Gemini for Home」を<a href="https://blog.google/products/google-nest/gemini-for-home-launch/">発表</a>した。従来のGoogle Assistantに代わり、Geminiがスマートホームの中核を担う。あわせて「Nest Cam Indoor（有線・第3世代）」「Nest Cam Outdoor（有線・第2世代）」と新しい「Google Homeスピーカー」を公開し、Google Homeアプリのデザインも刷新した。Nest Camは順次各国で販売開始、Google Homeスピーカーは2026年春に発売予定とされる。</p>
<h2>Gemini for Homeとは</h2>
<p>「Gemini for Home」は、家庭内のデバイス横断でより自然な会話と文脈理解に基づく操作を可能にする。照明・センサー・カメラなどの制御に加え、家族ごとのパーソナライズや要約も担う。これによりGoogle Assistantはスマートスピーカー／ディスプレイ上で順次Geminiへ移行する。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Google_Home_Speaker_with_Light_R_width_1000_format_webp_c9825d2c87/Google_Home_Speaker_with_Light_R_width_1000_format_webp_c9825d2c87.webp" alt="Google_Home_Speaker_with_Light_R.width-1000.format-webp.webp" /></p>
<h2>新しいNestデバイス</h2>
<p>GoogleはGemini対応を前提とした新しいNestシリーズを発表した。</p>
<ul>
<li><strong>Nest Cam Indoor（第3世代・有線）／Nest Cam Outdoor（第2世代・有線）</strong> ：2K HDR、低照度性能、広視野などを強化。一部地域では発表当日から販売が始まっており、価格・展開国が案内されている。</li>
<li><strong>Google Homeスピーカー</strong> ：Gemini前提の会話体験と360度オーディオ、サラウンド対応をうたう。発売は2026年春予定（ティーザー段階）。</li>
</ul>
<p>@<a href="https://www.youtube.com/watch?v=xFNVratdz7w">YouTube</a></p>
<h2>Google Homeアプリの刷新</h2>
<p>同時にGoogle Homeアプリも再設計された。新デザインにより、家庭内のすべてのデバイスやカメラ映像を一元管理できる。特に注目されるのは「Gemini Live」で、ユーザーはアプリ内でGeminiと会話しながらデバイスを操作できる。カメラ映像の要約や通知整理など、AIを活用した新しい管理体験が提供される。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/GHA_Hero_Image_Social_width_1200_format_webp_5b687e9699/GHA_Hero_Image_Social_width_1200_format_webp_5b687e9699.webp" alt="GHA_Hero-Image_Social.width-1200.format-webp.webp" /></p>
<h2>今後の展望</h2>
<p>新しいNest製品やGoogle Homeスピーカーの提供開始時期は、各国で順次案内される予定だ。既存のNest製品への対応範囲やアップデート方針も注目点となる。Googleが家庭内AIの新標準としてGeminiを根付かせられるかが、今後の焦点となる。</p>
]]></description>
      <pubDate>Fri, 03 Oct 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ソフトバンクと理研、AI計算基盤と量子コンピュータを相互接続──SINETを介し2025年10月に開始</title>
      <link>https://ledge.ai/articles/softbank_riken_ai_quantum_connection</link>
      <description><![CDATA[<p>2025年9月29日、ソフトバンクと国立研究開発法人理化学研究所（以下、理研）は、学術情報ネットワーク「SINET（サイネット）」を活用し、ソフトバンクのAI計算基盤と理研が運用する量子コンピュータの相互接続を2025年10月に開始すると<a href="https://www.softbank.jp/corp/news/press/sbkk/2025/20250929_01/">発表</a>した。量子コンピュータと大規模AI計算基盤を直結させる国内初の試みであり、研究開発や産業応用における新たな可能性が広がると期待されている。</p>
<h2>相互接続の概要</h2>
<p>今回の取り組みでは、ソフトバンクが構築するAI計算基盤と、理研の計算科学研究センター（R-CCS）が運用する量子コンピュータを、SINETを経由して接続する。これにより、AIによる大規模データ処理と、量子コンピュータ特有の高速かつ効率的な計算を組み合わせた研究が可能になる。</p>
<h2>プロジェクト背景：「JHPC-Quantum」構想</h2>
<p>この相互接続は、日本政府が推進する「JHPC-Quantum（Japan High Performance Computing and Quantum）」プロジェクトの一環として実施される。スーパーコンピュータ「富岳」や先端的なAI計算基盤、量子コンピュータといった計算資源を連携させ、次世代の研究基盤を構築することが狙いだ。</p>
<h2>今後の展望</h2>
<p>両者は、今回の相互接続によって、AI学習と量子計算を融合させた新しい研究手法の実証を進める方針を示している。具体的には、材料科学や創薬、組合せ最適化といった産業応用への活用が想定されている。
ソフトバンクは自社のAIインフラを提供し、理研は量子計算の知見を活かすことで、学術と産業の双方に資する先端計算環境を構築していく考えだ。</p>
]]></description>
      <pubDate>Fri, 03 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Cloudflare、非営利団体と報道機関をAIクローラから防御──Project Galileoに新機能追加</title>
      <link>https://ledge.ai/articles/cloudflare_project_galileo_ai_crawler_protection</link>
      <description><![CDATA[<p>Cloudflareは米国時間2025年9月23日、非営利団体や独立系メディアを対象とするセキュリティ支援プログラム「Project Galileo」を拡張し、AIクローラからのアクセスを監視・制御できる新機能を無償提供すると<a href="https://www.cloudflare.com/press/press-releases/2025/cloudflare-helps-protect-independent-journalism-and-non-profits-from-ai/">発表</a>した。対象は約750の団体で、AI時代におけるコンテンツ保護と収益確保を後押しする狙いがある。</p>
<h2>Project Galileoとは</h2>
<p>Project Galileoは2014年に開始された取り組みで、独立系メディアや人権団体、ジャーナリストらに無償のセキュリティ対策を提供してきた。サイバー攻撃や情報操作から公共性の高い情報源を守ることを目的とし、これまでに数多くの組織を支援している。Cloudflareによれば、2024年5月から2025年3月の間に参加団体への攻撃を1,089億件以上遮断した実績があり、その規模の大きさが示されている。</p>
<h2>新たに追加された機能</h2>
<ul>
<li><strong>Bot Management</strong> ：AIクローラを含むボットを識別し、アクセス状況を可視化。</li>
<li><strong>AI Crawl Control</strong> ：どのAIクローラを許可するか、または拒否するかを団体ごとに制御可能。</li>
</ul>
<p>この拡張により、参加団体はAIサービスによる利用実態を把握し、必要に応じて制限をかけることができる。</p>
<p>Cloudflare共同創設者兼CEOのMatthew Prince氏は発表の中で、「私たちはジャーナリズムを信じている。独立系やローカルニュースの健全性は、インターネット全体、そして社会の健全性と不可分だ」と述べた。</p>
<p>また、メディア支援団体のLION（Local Independent Online News）やInternews Europeなどが、この拡張を歓迎し、持続可能なニュース配信に資するとの期待を表明している。</p>
<h2>今後の展望</h2>
<p>同社は、AIクローラの利用状況を把握し制御できるようにすることで、参加団体がコンテンツ利用に関して交渉力を高められると説明している。また、今回の拡張によって、独立系ニュースや非営利団体が持続可能な活動を続けられるよう支援し、インターネット上の公共性を守ることを目指すとしている。さらに、AI Crawl ControlやBot Managementを無償提供することで、AI時代における報道機関や市民団体の役割を強化し、透明で健全な情報流通を促進するとしている。</p>
]]></description>
      <pubDate>Fri, 03 Oct 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>イーロン・マスク、xAIで“Grokipedia”構築中　『Wikipediaを大幅に上回る』と表明</title>
      <link>https://ledge.ai/articles/elon_musk_xai_grokipedia_vs_wikipedia</link>
      <description><![CDATA[<p>イーロン・マスク氏は9月30日（現地時間）、自身のXアカウントで「<a href="https://x.com/elonmusk/status/1972992095859433671">xAIが“Grokipedia”を構築中</a>」と明かした。Wikipediaを「大幅に上回る改善」と表現し、xAIの目標である「宇宙の理解」に向けた必要な一歩だと位置づけた。</p>
<h2>マスク氏「Wikipediaを大幅に上回る」</h2>
<p>マスク氏は「We are building Grokipedia @xAI. Will be a massive improvement over Wikipedia. Frankly, it is a necessary step towards the xAI goal of understanding the Universe.」（私たちはxAIで「Grokipedia」を構築中です。これはWikipediaを大幅に上回る改善となるでしょう。率直に言って、xAIの目標である“宇宙の理解”に向けた必要な一歩なのです）と投稿。
この発言は、ベンチャー投資家デビッド・サックス氏が「Wikipediaは左派活動家によって偏向している」と批判した投稿に返信する形で行われた。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/grokipedia_5727bb7339/grokipedia_5727bb7339.jpg" alt="grokipedia.jpg" /></p>
<h2>背景にあるWikipedia偏向論争</h2>
<p>Wikipediaの編集方針や中立性をめぐる議論は長年続いてきた。直近ではタッカー・カールソン氏が共同創設者ラリー・サンガー氏を招いたインタビューを公開し、偏向の問題が再び注目を集めた。
マスク氏は今月のAll-In Podcastでも「GrokがWikipediaの誤りを正し、欠けている文脈を補う」と発言しており、今回の「Grokipedia」構想はその延長線上にあるとみられる。</p>
<h2>xAIの目標との関わり</h2>
<p>xAIの公式サイトには「宇宙の理解（Understand the Universe）」を掲げるミッションが明記されている。マスク氏は「Grokipedia」を、この壮大な目標の達成に不可欠な取り組みと強調した。</p>
<h2>「Grokipedia」名義のサイトも登場</h2>
<p>「grokipedia.com」や「grokipedia.fun」といったドメイン名を持つサイトもすでに存在している。しかし、これらは第三者が取得したものであり、xAI公式とは無関係とみられる。現時点で公式なサイトやプロジェクト詳細は公表されておらず、運用体制や公開時期については不明。</p>
<p>マスク氏の投稿は数百万回以上閲覧され、大きな関心を集めている。構想の詳細が明かされていない段階ながら、その動向に多くの注目が集まっている。</p>
]]></description>
      <pubDate>Thu, 02 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ChatGPTに「Instant Checkout」登場──ECサイトの商品をチャット内で直接購入可能に</title>
      <link>https://ledge.ai/articles/chatgpt_instant_checkout_launch</link>
      <description><![CDATA[<p>OpenAIは9月29日（現地時間）、ChatGPT内でECサイトの商品を直接購入できる新機能「Instant Checkout」を<a href="https://openai.com/index/buy-it-in-chatgpt/">発表</a>した。従来は外部リンクに移動していた購入手続きが、ChatGPT上で完結できるようになるという。</p>
<p>@<a href="https://www.youtube.com/watch?v=C6qcZdtIv54">YouTube</a></p>
<h2>機能概要</h2>
<p>「Instant Checkout」を利用すると、ChatGPTの会話画面内で商品購入が完結する。ユーザーはカート追加や外部ブラウザへの遷移を必要とせず、決済までをシームレスに行える。</p>
<p>同社ではChatGPTを「情報検索から購入までをワンストップで実現する場」へと進化させる方針を掲げている。今回の新機能により、生成AIがユーザーの消費行動に直結するハブとなることを狙う。</p>
<h2>仕組みと安全性</h2>
<p>提携するECサイトの商品情報はChatGPT内に直接表示され、購入操作が可能となる。決済処理はChatGPTのセキュアな仕組みを通じて行われ、ユーザーのプライバシーとセキュリティ確保が重視されている。</p>
<p><strong>「Instant Checkout」を支えるエージェント型コマースプロトコルの流れ</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/ACP_POR_1_47d3b99b8f/ACP_POR_1_47d3b99b8f.jpg" alt="ACP_-POR__1.jpg" /></p>
<h2>今後の展開</h2>
<p>OpenAIは、対応するECサイトの拡大を予定している。まずは一部のパートナーから導入を開始し、順次拡大する方針だ。また、開発者や事業者に向けた連携方法の提供も検討されている。</p>
]]></description>
      <pubDate>Thu, 02 Oct 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>学術＆研究2025/9/29 [MON]Google、ロボット操作向けAI基盤モデル「Gemini Robotics 1.5」を発表──行動前に思考し、複雑タスクを実行</title>
      <link>https://ledge.ai/articles/google_gemini_robotics_15_release</link>
      <description><![CDATA[<p>Googleは2025年9月25日、ロボットの操作向けに新たなAIモデル「Gemini Robotics 1.5」を<a href="https://blog.google/intl/ja-jp/company-news/technology/gemini-robotics-15-ai">発表</a>した。Gemini 1.5 Proを基盤としたこのモデルは、ロボットが「行動を起こす前に考える」能力を備えており、従来よりも複雑なマルチステップの作業を遂行できる点が特徴だという。</p>
<h2>新モデル「Gemini Robotics 1.5」とは</h2>
<p>Googleが発表した「Gemini Robotics 1.5」は、同社の大規模言語モデル「Gemini 1.5 Pro」を拡張し、視覚・言語・行動を統合した「Vision-Language-Action（VLA）」モデルとして設計されている。ロボットは環境を理解し、人間の指示に基づいて複数ステップにわたるタスクをこなせるようになった。</p>
<p>@<a href="https://www.youtube.com/watch?v=AMRxbIO04kQ&amp;t=1s">YouTube</a></p>
<h2>思考してから行動する仕組み</h2>
<p>従来のロボティクスAIは与えられた動作を逐次実行することが多かったが、「Gemini Robotics 1.5」は行動前に計画を立てる能力を持つ。これにより、作業の失敗を減らし効率的に遂行できる。たとえば散らかった部屋で物を拾う場合、ロボットは最適な順序や手順を考えてから実行に移す。</p>
<p><strong>「Gemini Robotics 1.5」の仕組み。ロボットはゴミの分別を例に、行動前に思考・計画を立てるプロセスを経て動作する</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Gemini_Robotics_Agentic_System_width_1000_format_webp_75fe64eb0b/Gemini_Robotics_Agentic_System_width_1000_format_webp_75fe64eb0b.jpg" alt="GeminiRobotics-Agentic_System.width-1000.format-webp.jpg" /></p>
<p>@<a href="https://www.youtube.com/watch?v=eDyXEh8XqjM&amp;t=3s">YouTube</a></p>
<h2>応用例と成果</h2>
<p>公開されたデモでは、洗濯物の分類やごみ分別など、家庭やオフィスで想定される作業を自律的に行う様子が披露された。ロボットは周囲を観察しながら判断を下し、マルチステップのタスクを適切に処理することができる。</p>
<p><strong>Gemini Robotics-ER 1.5」は従来モデルよりも学術ベンチマークで高い性能を示した</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Gemini_Robotics_blog_figure_2_250_width_1000_format_webp_a4b168e1a6/Gemini_Robotics_blog_figure_2_250_width_1000_format_webp_a4b168e1a6.jpg" alt="GeminiRobotics-blog-figure-2-250.width-1000.format-webp.jpg" /></p>
<h2>研究開発の背景</h2>
<p>このモデルは、Google DeepMindとGoogle Researchの共同開発による成果だ。Geminiシリーズの能力を物理世界に応用する取り組みの一環であり、従来のロボットAIよりも抽象的な推論や柔軟な対応が可能になった点が強調されている。</p>
<p><strong>物体検出や軌道予測など、多様な認識・推論能力を備える「Gemini Robotics 1.5」</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Screenshot_2025_09_25_at_14_53_4_width_1000_format_webp_c5fae86c2b/Screenshot_2025_09_25_at_14_53_4_width_1000_format_webp_c5fae86c2b.webp" alt="Screenshot_2025-09-25_at_14.53.4.width-1000.format-webp.webp" /></p>
<h2>今後の展望</h2>
<p>Googleは「Gemini Robotics 1.5」を、物理世界における汎用人工知能（AGI）の実現に向けた重要な一歩と位置づけている。単にコマンドに反応するモデルではなく、自ら推論し、計画を立て、ツールを使いこなし、未知の状況にも対応できる自律的なシステムの構築を目指す。</p>
<p>ロボットが知性と器用さを兼ね備え、物理世界の複雑さを乗り越えて人間の生活により役立つ存在となるための基盤づくりであるとし、Googleは研究コミュニティとの協力を継続する方針を示している。また、ロボティクス分野の研究者らが最新の「Gemini Robotics-ER」モデルを活用し、どのような未来を切り拓くのか大いに期待していると結んでいる。</p>
]]></description>
      <pubDate>Mon, 29 Sep 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/9/28 [SUN]Photoshop、外部AIモデルを初統合──Google「Nano Banana」とBlack Forest Labsの「FLUX.1 Kontext」が生成塗りつぶしに対応</title>
      <link>https://ledge.ai/articles/photoshop_generative_fill_gemini_flux_integration</link>
      <description><![CDATA[<p>Adobeは2025年9月25日、Photoshopの「生成塗りつぶし（Generative Fill）」機能に、Googleの画像生成モデル「Gemini 2.5 Flash Image（Nano Banana）」と、Black Forest Labsの「FLUX.1 Kontext [pro]」を統合したと<a href="https://blog.adobe.com/en/publish/2025/09/25/photoshop-beta-expands-generative-fillmore-ai-models-more-possibilities">発表</a>した。これにより、ユーザーはAdobe独自のFireflyモデルに加え、外部AIモデルを切り替えて利用できるようになる。</p>
<h2>外部AIモデルが初めてPhotoshopに統合</h2>
<p>今回の更新は、Photoshopが自社モデル中心から一歩踏み出し、外部の生成AIモデルを取り込む初の事例となる。Adobe公式ブログによれば、Nano BananaとFLUX.1 Kontext [pro]はいずれもPhotoshopベータ版のユーザーが利用可能で、選択範囲を指定したうえでプロンプト入力を行うと、モデルを切り替えて生成や編集を実行できる。
なお、Adobeは昨年動画編集ソフトのPremiere Proにおいて、OpenAIのSoraやRunway、Pika Labsなど外部の動画生成モデルの統合予定を<a href="https://ledge.ai/articles/adobe_premiere_pro_sora">発表</a>しているが、Photoshopへの外部モデル導入は今回が初めてとなる。</p>
<h2>Nano Bananaの特徴</h2>
<p>Googleが開発した「Gemini 2.5 Flash Image（Nano Banana）」は、スタイル変換やグラフィック要素の追加、視覚効果の生成に強みを持つ。Photoshop内ではFireflyと同様の操作感で利用でき、用途に応じたモデルの選択が可能となる。</p>
<p><strong>Googleの「Gemini 2.5 Flash Image（Nano Banana）」を使った生成塗りつぶしの例。衣服や背景を置き換え、黄色い鳥を追加するなど複数の変更を一度に適用できる。</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/media_195028b2e137ad3d6178196faeab0ef497afee9d5_8a49af01ee/media_195028b2e137ad3d6178196faeab0ef497afee9d5_8a49af01ee.jpg" alt="media_195028b2e137ad3d6178196faeab0ef497afee9d5.jpg" /></p>
<h2>FLUX.1 Kontext [pro]の特徴</h2>
<p>一方、Black Forest Labsの「FLUX.1 Kontext [pro]」は、環境や遠近感に調和した生成を得意とする。背景や構図に一貫性を持たせる性能が評価されており、Photoshopでの利用によって「場面全体との整合性」を保ちながらの編集が可能になる。</p>
<p><strong>Black Forest Labsの「FLUX.1 Kontext [pro]」による生成例。画像全体のコンテキストを保ちながら、巨大な毛糸玉を追加するなど背景との調和を重視した編集が可能。</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Adobe_Black_forest_labs_124afb8d27/Adobe_Black_forest_labs_124afb8d27.jpg" alt="Adobe-Black forest labs.jpg" /></p>
<h2>ベータ版での提供と今後の展開</h2>
<p>両モデルは現時点ではPhotoshopのベータ版でのみ利用可能。無料トライアル枠の提供も用意されているが、生成回数には制限がある。正式版への導入時期は明らかにされていない。</p>
]]></description>
      <pubDate>Sun, 28 Sep 2025 23:50:00 GMT</pubDate>
    </item>
  </channel>
</rss>