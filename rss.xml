<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
  <channel>
    <title>Ledge.ai 新着記事</title>
    <link>https://ledge.ai/</link>
    <description>Ledge.ai の最新テクノロジー記事</description>
    <item>
      <title>富士通とジーンクエスト、因果AIで遺伝子と生活習慣の関係を解析──「Fujitsu Kozuchi」活用で新たな知見</title>
      <link>https://ledge.ai/articles/fujitsu_genequest_causal_ai</link>
      <description><![CDATA[<p>富士通株式会社と株式会社ジーンクエスト（DeNAグループ）は2025年10月9日、富士通のAI技術群「Fujitsu Kozuchi（コヅチ）」の中核技術である因果AIを活用し、遺伝子データとライフスタイルデータの関係性を解析した結果、新たな知見を得たと<a href="https://global.fujitsu/ja-jp/pr/news/2025/10/09-01">発表</a>した。</p>
<p>実証では、ジーンクエストが保有する約1万名分の遺伝子情報と生活習慣アンケートデータを対象に、因果AIが両者の間に潜む因果構造を探索。特定の遺伝子多型が睡眠時間や食習慣、BMIなどの生活習慣に与える影響を可視化した。これにより、従来の統計的相関分析では把握しにくかった複雑な要因連鎖を明らかにしたとしている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/sub1_7b9b5b5a0a/sub1_7b9b5b5a0a.png" alt="sub1.png" /></p>
<h2>因果AIで「相関」から「原因と結果」へ</h2>
<p>富士通の因果AIは、膨大な多変量データの中から要因と結果の方向性を自動的に推定する技術。同社が開発した高速因果構造探索アルゴリズムにより、従来の分析手法よりも大規模・高精度な因果モデルを生成できる。富士通はこれを、産業・医療・科学分野を横断するAI基盤「Fujitsu Kozuchi」に搭載し、研究開発支援に活用している。</p>
<h2>生活習慣改善や疾病予防へ応用</h2>
<p>両社は、今回得られた知見を今後のヘルスケアサービスに応用する方針を示している。ジーンクエストは個人向け遺伝子解析サービスの高度化を進め、生活習慣病予防やパーソナライズド医療への展開を目指す。富士通は医療機関や製薬企業との連携を通じ、因果AIを「科学的エビデンス創出を支援する技術」として普及させる考えだ。</p>
<p>富士通は今回の取り組みを「AIによって科学的知見を創出する“サイエンスDX”の推進」と位置づけており、生命科学領域での新たなAI応用事例として注目される。</p>
]]></description>
      <pubDate>Sun, 12 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google、AIがPCを操作する「Gemini 2.5 Computer Use model」を開発者向けに公開──ClaudeやOpenAIモデルを上回る性能を実証</title>
      <link>https://ledge.ai/articles/google_gemini_2_5_computer_use_release</link>
      <description><![CDATA[<p>Google DeepMind は2025年10月7日（米国時間）、AI が実際のコンピューター画面を理解し、クリックや入力などの操作を実行できる新モデル「Gemini 2.5 Computer Use model」を開発者向けにプレビュー提供したと<a href="https://blog.google/technology/google-deepmind/gemini-computer-use-model/">発表</a>した。</p>
<p>Gemini API を通じて利用でき、AI が人間と同様にブラウザやアプリのUI（ユーザーインターフェース）を操作することを可能にする。</p>
<h2>Gemini API に“computer_use”ツールを追加</h2>
<p>今回発表された新モデルは、Gemini 2.5 の機能拡張として API に追加された「computer_use」ツールを用いて動作する。</p>
<p>AI はユーザーからの指示に加え、スクリーンショットと直近の操作履歴を入力として受け取り、次に取るべきアクション（クリック・入力・スクロールなど）を出力。実行結果を再び画面キャプチャとして取得し、目標達成までループ処理を行う。これにより、設定変更やフォーム入力、情報検索など、複数ステップを自律的に完了できる。</p>
<p>Google は公式ブログで、「このモデルはユーザー許可を前提に、安全性と透明性を重視して設計されている」と強調している。</p>
<p><strong>Computer Use model の処理ループ。AI がスクリーンショットと操作履歴をもとに次の行動を生成し、クライアント環境で実行 → 状況を再取得して次の判断へとつなげる</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/CTU_Diagram_RD_4_V01_width_1000_format_webp_b3415f41ee/CTU_Diagram_RD_4_V01_width_1000_format_webp_b3415f41ee.webp" alt="CTU-Diagram-RD4-V01.width-1000.format-webp.webp" /></p>
<h2>プレビュー提供と利用方法</h2>
<p>開発者は Google AI Studio および Vertex AI を通じて Computer Use model にアクセスできる。プレビュー版の段階では主にブラウザ操作に最適化されており、今後はより広範なアプリやデスクトップ環境への対応も検討されているという。</p>
<p>Google は、操作範囲やデータアクセスを制御する仕組みを組み込み、「責任ある自動化（Responsible Automation）」の実現を掲げている。</p>
<h2>ベンチマーク性能：Claude Sonnet 4.5 を上回る</h2>
<p>Google DeepMind は、Gemini 2.5 Computer Use model の性能を複数の標準ベンチマークで検証した。
Browserbase による Online-Mind2Web テストでは 65.7 % の精度を記録し、Claude Sonnet 4.5 や OpenAI Computer-Using Model を上回った。
さらに WebVoyager や AndroidWorld でも高スコアを達成し、実行速度（レイテンシ）でも優位性を示している。</p>
<p><strong>Gemini 2.5 Computer Use model は、Claude Sonnet 4.5 や OpenAI Computer-Using Model に比べ、低レイテンシかつ高精度を示した。</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/CTU_Scatterplot_RD_7_width_1000_format_webp_7e4e545c1c/CTU_Scatterplot_RD_7_width_1000_format_webp_7e4e545c1c.webp" alt="CTU-Scatterplot-RD7.width-1000.format-webp.webp" /></p>
<p><strong>複数ベンチマークで高い精度を記録。特に WebVoyager と AndroidWorld で際立ったスコアを達成した。</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/CTU_Benchmark_Chart_RD_5_V01_width_1000_format_webp_e0982edd33/CTU_Benchmark_Chart_RD_5_V01_width_1000_format_webp_e0982edd33.webp" alt="CTU-Benchmark_Chart-RD5_V01.width-1000.format-webp.webp" /></p>
<h2>動作デモ：AI がブラウザを自律操作</h2>
<p>公式ブログでは、実際の操作デモ動画も公開されている。
動画では AI が画面を認識し、ブラウザ上でリンクをクリックしたり、テキストを入力してタスクを完了する様子が確認できる。</p>
<p>@<a href="https://www.youtube.com/watch?v=_lu-FcPUIfM">YouTube</a></p>
<h2>AI による“手の届く自動化”へ</h2>
<p>今回の発表は、AI が人間の指示をもとに実際のUI を操作できる「エージェント時代」の幕開けを示す。
Google は Computer Use を “次世代の AI アシスタント” 開発の基盤と位置づけており、将来的には業務支援やウェブ操作、アプリ間連携など、より幅広い自動化領域への展開が期待される。</p>
]]></description>
      <pubDate>Sun, 12 Oct 2025 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ジョニー・アイブとサム・アルトマンが語る「AIと人間の新しい関係」──次世代デバイス構想をDevDay 2025で明かす</title>
      <link>https://ledge.ai/articles/openai_jony_ive_ai_device_philosophy</link>
      <description><![CDATA[<p>OpenAIのCEOであるサム・アルトマン氏と、Apple製品のデザインを手がけたジョニー・アイブ氏（LoveFrom代表）は、2025年10月に米サンフランシスコで開催された開発者会議「DevDay 2025」で<a href="https://www.youtube.com/watch?v=7cKbPLzNYws">対談</a>を行い、人間中心のAIデバイス構想を明らかにした。両氏は、テクノロジーがもたらす「過剰な情報と不安」を乗り越え、AIを通じて「穏やかで幸福な体験をもたらす新しいデバイス」を目指すと語った。</p>
<h2>パートナーシップの起点はChatGPT</h2>
<p>アイブ氏は、約30年間在籍したAppleを退社後、デザイナーや建築家などの専門家で構成するチーム「LoveFrom」を設立した。当初は明確な目標を持たずに活動していたが、ChatGPTの登場が転機になったという。
「ChatGPTを見たとき、私たちの目的が結晶化した」とアイブ氏は語る。この出会いが、サム・アルトマン氏との協業を決定づけた。両者は約3年前から新しいAIデバイスの構想を練り始めたという。</p>
<h2>「クラフト」と「ケア」に支えられた創造哲学</h2>
<p>アイブ氏は創造の出発点を「人類への愛」と表現し、ものづくりの根幹に「クラフト（職人技）」と「ケア（思いやり）」を置く。
彼は「人に見えない部分へのこだわりこそが、作り手の誠実さを示す」と述べ、細部まで丁寧に仕上げる姿勢を強調した。さらに「人々は、ケアが込められた製品を直感的に感じ取る。逆に、無頓着さ（ケアレスネス）は容易に見抜かれる」とも語った。</p>
<p>この“ケアの精神”をAIデバイスにも反映させ、「技術のための技術」ではなく、「人の幸福のためのテクノロジー」を設計することを目指すという。</p>
<p>@<a href="https://www.youtube.com/watch?v=7cKbPLzNYws">YouTube</a></p>
<h2>AIがもたらす「人間らしい関係」</h2>
<p>両氏が共通して強調したのは、AIを用いて人とテクノロジーの関係を再構築するというビジョンだ。
アイブ氏は、スマートフォンが2007年に登場して以降、人々がテクノロジーとの関係で「圧倒的な情報量と絶望感」に直面していると指摘。「AIはこの問題を悪化させるものではなく、正面から向き合うチャンスだ」と述べた。</p>
<p>理想とする体験として、彼は「必然性と自明性」「ユーモアと喜び」「穏やかさと幸福感」を挙げる。アルトマン氏も「AIは、人間とテクノロジーの関係を再設計する力を持つ」と応じ、「このプロジェクトは、人がテクノロジーを“感じる”新しい方法を探る実験だ」と語った。</p>
<h2>「誰もが初心者」──AI時代の創造者の課題</h2>
<p>アイブ氏は、急速に進化するAI分野では「誰もが初心者」であり、過去の経験が時に足枷になることもあると述べた。
「AIの勢いがあまりに速く、焦点をどこに定めるかが難しい。しかし、動機が“人類への愛”である限り、進むべき方向は見失わない」と語った。</p>
<h2>理念と現実、その間にある課題</h2>
<p>両氏が掲げた人間中心のビジョンは、AI時代のデバイス設計に新たな指針を与えるものだ。
一方で、<a href="https://www.ft.com/content/58b078be-e0ab-492f-9dbf-c2fe67298dd3">Financial Times</a>などによると、同プロジェクトはハードウェア面の設計やプライバシー制御などで難航しており、思想と実装のギャップにも注目が集まっている。
理念と現実の交差点に立つこの挑戦が、どのような形で結実するのかが問われている。</p>
<h2>「今を変えるチャンスがある」</h2>
<p>両氏は、AIデバイス開発の目的を「人を幸せで穏やかにし、不安を和らげること」に置いている。
アイブ氏はセッションの最後にこう締めくくった。</p>
<p>同氏はAIの可能性を「現状の延長ではなく、根本的な変化をもたらすもの」として位置づけた。</p>
]]></description>
      <pubDate>Sun, 12 Oct 2025 02:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ソフトバンクとオラクル、AI活用のためのソブリンクラウドを共同構築──国内運用基盤「Cloud PF Type A」でデータ主権を確保</title>
      <link>https://ledge.ai/articles/softbank_oracle_sovereign_cloud_pf_type_a</link>
      <description><![CDATA[<p>ソフトバンク株式会社と米オラクルは2025年10月8日、データ主権（ソブリン性）を確保しつつ、AI活用を前提としたクラウド基盤「Cloud PF Type A」を日本国内で運用するための協業を開始したと<a href="https://www.softbank.jp/corp/news/press/sbkk/2025/20251008_01/">発表</a>した。オラクルのクラウド技術「Oracle Alloy」を採用し、ソフトバンクが国内データセンターで独自運用を担う。政府機関や企業が自国の管理下で安全にAIやクラウドを活用できるソブリンクラウドの実現を目指す。</p>
<h2>データ主権を守る「ソブリンクラウド」</h2>
<p>両社が開発する「Cloud PF Type A」は、すべてのデータ処理・保存・管理を日本国内で完結させる構成をとる。国外へのデータ移転を伴わずにクラウドやAIを活用できるようにすることで、経済安全保障やガバナンス上のリスクを最小化する狙いがある。</p>
<p>ソフトバンクは通信事業で培った国内ネットワークと運用ノウハウを活用し、オラクルは独自のクラウド基盤「Oracle Alloy」を通じて技術支援を提供。両社は「日本の社会インフラの一部として信頼されるクラウドを構築する」としている。</p>
<h2>「Cloud PF Type A」を2026年に提供開始</h2>
<p>新たに構築される「Cloud PF Type A」は、2026年4月に東日本拠点、同年10月に西日本拠点での提供を予定している。
この基盤では、Oracle Cloud Infrastructure（OCI）の約200種類のクラウドおよびAIサービスを国内運用で利用可能になる。</p>
<p>主な特徴は次の通り。</p>
<ul>
<li><strong>鍵管理（KMS）</strong> ：Oracle Vaultとソフトバンク独自システムを併用し、暗号鍵を完全に国内で管理。</li>
<li><strong>通信構成</strong> ：「OnePort」や「SmartVPN」などの閉域網接続に対応し、安全なデータ通信を実現。</li>
<li><strong>災害対策</strong> ：東西データセンターの冗長構成を採用し、事業継続（BCP）に対応。</li>
<li><strong>運用支援</strong> ：ソフトバンクがMSP（運用管理代行）を提供し、導入から保守まで一括サポート。</li>
</ul>
<p>この構成により、政府・自治体・企業などがAIモデルの学習や推論を行っても、データが国外へ移動することはない。</p>
<h2>両社のコメント</h2>
<p>ソフトバンクは、自社DCの高いセキュリティ水準に適合したクラウドを提供し、生成AIやGPUを統合して多様な顧客ニーズに応える方針と述べた。オラクルは、Oracle Alloyにより日本国内のデータ主権要件に対応し、OCIの幅広いAI/クラウドを国内DCで利用可能にする取り組みだと説明した。</p>
<h2>国内で広がる“主権クラウド”構想</h2>
<p>欧州を中心に広がる「ソブリンクラウド（主権クラウド）」の潮流は、日本国内でも加速している。
NTT、富士通、日立などが相次いで国産クラウド基盤の整備を進めており、今回のソフトバンクとオラクルの取り組みもその一環と位置づけられる。</p>
<p>両社は今後、生成AIや自然言語処理、画像解析などの高負荷AIワークロードにも対応する予定で、AI時代のデータ主権を支える中核的なインフラを目指す。</p>
]]></description>
      <pubDate>Sat, 11 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Huawei、LLMの精度を保持したまま最大70%メモリ削減できる新手法を発表──コンシューマーGPUでの高精度生成AI実行も視野に</title>
      <link>https://ledge.ai/articles/huawei_sinq_quantization_llm</link>
      <description><![CDATA[<p>中国の大手テクノロジー企業Huawei（華為技術）は2025年9月26日、大規模言語モデル（LLM）を一般的なGPU環境でも高品質に動作させるための新しい量子化技術「Sinkhorn-Normalized Quantization（SINQ）」を<a href="https://www.arxiv.org/abs/2509.22944">発表</a>した。</p>
<h2>Sinkhorn正規化で“再調整なし”を実現</h2>
<p>従来のLLM量子化では、精度を維持するために一部データを用いて再調整（キャリブレーション）を行う必要があった。SINQはその工程を省略し、「再調整なし」で精度を保つ新しい方式だ。</p>
<p>仕組みの中核となるのが、「Sinkhorn-Knoppアルゴリズム」を応用した正規化手法である。モデルの重み行列に対して、行方向と列方向の2つのスケーリングベクトルを設定（dual-scaling）し、両軸の分散を均一化することで、外れ値（outlier）が特定の行や列に偏る問題を防ぐ。この工程により、量子化後の誤差を最小限に抑えられるという。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x1_818acd143f/x1_818acd143f.png" alt="x1.png" /></p>
<h2>精度を保ちながら70％のメモリ削減</h2>
<p>Huaweiの研究チームは、同社のQwen3モデル（1.7B〜235B）やDeepSeek-V2.5（236B）などで実験を行い、SINQが既存手法（RTN、HQQ、Hadamard変換など）を上回る精度を示したと報告している。</p>
<p>具体的には、4bit量子化時にパープレキシティ（文章予測精度）を最大40％改善し、メモリ使用量を最大70％削減。さらに、8GB程度の一般的なGPU上でQwen3-7Bモデルを実行できたとしている。処理時間も高速で、量子化プロセスは従来のRTN法に比べてわずか1.1倍。再調整を伴う手法（AWQやGPTQなど）よりも最大30倍速いという。</p>
<h2>幅広いモデルで動作、非一様量子化とも互換</h2>
<p>SINQは、Qwenシリーズだけでなく、Llama 2・Llama 3・DeepSeek-V3などの異なるモデルにも適用可能。
また、非一様量子化フォーマット（NF4）との併用でも精度を維持しており、調整を行うAWQと組み合わせた「A-SINQ」ではさらに高い性能を達成した。論文では、Mixture-of-Experts（MoE）構造の大型モデルでも安定して動作することが示されている。</p>
<h2>コンシューマーGPUでのLLM実行を視野に</h2>
<p>Huaweiは、SINQを「キャリブレーション不要の汎用量子化手法」と位置づけており、高性能GPUに依存しないLLM運用を可能にする技術として注目されている。論文著者らは、SINQの目的を「メモリ効率と速度を両立し、エッジデバイスでも高品質な生成AIを実行可能にすること」と説明している。</p>
<p>コードはGitHub上で<a href="https://github.com/huawei-csl/SINQ">公開</a>されており、研究者や開発者が自由に評価・応用できる環境が整っている。</p>
]]></description>
      <pubDate>Sat, 11 Oct 2025 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>イーロン・マスク率いるxAI、動画生成AI「Imagine v0.9」を公開──静止画に声と動きを与える“ネイティブ映像生成”モデル</title>
      <link>https://ledge.ai/articles/xai_imagine_v09_release</link>
      <description><![CDATA[<p>イーロン・マスク氏が率いるAI開発企業xAIは2025年10月8日（現地時間）、新たな動画生成モデル「Imagine v0.9」を<a href="https://x.com/xai/status/1975607901571199086">発表</a>した。視覚品質、動き、音声生成などを全面的に改良し、すべてのxAI製品で無料利用が可能となっている。</p>
<p>xAIは公式X（旧Twitter）で「Imagine v0.9は、映像品質・モーション・音声生成などにおいてv0.1から大幅に進化した」と投稿。<a href="https://grok.com/imagine">grok.com/imagine</a> では、Grokプラットフォーム上で同モデルを試せるようになっている。</p>
<h2>音声と映像を同時生成──“編集不要”の体験を掲げる</h2>
<p>xAIは今回の発表で、「Imagine v0.9は音声と映像を同時に生成する“ネイティブ・オーディオ＋ビデオ生成”を実現した」と説明。</p>
<p>\u003E“Imagine v0.9 pushes the boundaries of native audio + video generation, creating cinematic experiences straight out of the box—no editing required.”</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x_AI_Imagine_v0_9_1_dragon_6a3a28422f/x_AI_Imagine_v0_9_1_dragon_6a3a28422f.jpg" alt="xAI Imagine v0-9-1 dragon.jpg" /></p>
<p>投稿では、音声と映像が同期したドラゴンのデモ映像を公開。ユーザーは、生成後の編集作業なしで“完成された動画”を得られるとしている。</p>
<h2>モーション精度とカメラ効果を強化</h2>
<p>Imagine v0.9では、被写体の滑らかな動きとリアリズムを高精度で再現するモーション制御を導入した。
さらに「インテリジェント・フォーカスシフト」と呼ばれる自動焦点移動など、ストーリーテリングに適したダイナミックなカメラ効果も追加された。</p>
<p>\u003E“And lets you add dynamic camera effects like intelligent focus shifts for better storytelling.”</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x_AI_Imagine_v0_9_1_camera_effects_5febfab05e/x_AI_Imagine_v0_9_1_camera_effects_5febfab05e.jpg" alt="xAI Imagine v0-9-1 camera effects.jpg" /></p>
<h2>自然な対話・歌唱・リズムまで再現</h2>
<p>Imagine v0.9では、音声表現の幅も拡大。自然な対話や歌唱を含む“声の演出”が可能になった。</p>
<p>\u003E“v0.9 also brings videos to life with natural dialogue and strong audio-visual harmony.”</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x_AI_Imagine_v0_9_1_natural_dialogue_f30bf3d090/x_AI_Imagine_v0_9_1_natural_dialogue_f30bf3d090.jpg" alt="xAI Imagine v0-9-1 natural dialogue.jpg" /></p>
<p>\u003E“It also brings expressive singing to life with clear vocals and synchronized emotion.”</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x_AI_Imagine_v0_9_1_singing_05b4553dab/x_AI_Imagine_v0_9_1_singing_05b4553dab.jpg" alt="xAI Imagine v0-9-1 singing.jpg" /></p>
<p>xAIはさらに、音と動きを合わせたダンス生成の例として、キャラクター「Ani」の動画を紹介している。</p>
<p>\u003E“Plus good rhythm: here's Ani with smooth dance moves.”</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x_AI_Imagine_v0_9_1_ani_dancing_c19660cb25/x_AI_Imagine_v0_9_1_ani_dancing_c19660cb25.jpg" alt="xAI Imagine v0-9-1 ani dancing.jpg" /></p>
<p>xAIはユーザーからのフィードバックを積極的に求め、モデル改良に反映させる方針を示した。</p>
]]></description>
      <pubDate>Fri, 10 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ソフトバンクグループ、スイスABBのロボティクス事業を約8187億円で買収──AIと自律ロボットの統合を加速</title>
      <link>https://ledge.ai/articles/softbank_acquires_abb_robotics_for_ai_integration</link>
      <description><![CDATA[<p>ソフトバンクグループ株式会社は2025年10月8日、スイスのABB Ltd.（以下ABB）から同社のロボティクス事業を買収する最終契約を締結したと<a href="https://group.softbank/news/press/20251008">発表</a>した。買収総額は53億7500万ドル（約8187億円）で、取引の完了は2026年半ばから後半を予定している。AIとロボティクスを融合させるグローバル戦略の一環として、産業用・協働ロボットの分野で新たな展開を図る。</p>
<h2>AIとロボットを結ぶ「フィジカルAI」構想</h2>
<p>ソフトバンクグループ代表取締役会長兼社長の孫正義氏は、同社の次のフロンティアは「フィジカルAI」にあると述べ、AIの知能とロボティクスの身体を結びつけることで、人類の未来を切り拓く画期的な進化を実現していくと強調した。
同グループはこれまで、SoftBank Robotics GroupやBerkshire Grey、AutoStore、Agile Robots、Skild AIなどに投資しており、今回の買収を通じてAIと実機ロボットの統合をさらに加速させる考えだ。</p>
<h2>ABBの発表：「AI時代のロボティクスを担う最適な拠点」</h2>
<p>ABBは同日発表した<a href="https://new.abb.com/news/detail/129685/abb-to-divest-robotics-division-to-softbank-group">リリース</a>で、ロボティクス事業をソフトバンクに譲渡すると公表。</p>
<p>ABBのCEOであるMorten Wierod氏は、ソフトバンクグループはABBロボティクス事業と従業員にとって最適な新しい拠点となり、AIを基盤とするロボティクスの新時代において事業の成長をさらに後押しするだろうと述べた。</p>
<p>新会社の代表は、現ABB ロボティクス部門社長Marc Segura氏が務める予定で、従業員は約7,000人。2024年の売上は22億7,900万ドルで、ABB全体の売上の約7%を占めている。名称や資本金などの詳細は現時点で未定とされている。</p>
<h2>産業オートメーションから自律ロボットへ</h2>
<p>ABBロボティクスは、AIを活用した協働ロボット（コボット）や産業オートメーションソリューションを展開し、グローバル市場で高いシェアを持つ。
ソフトバンクはこれらの技術をAIプラットフォームと連携させ、製造、物流、医療、サービスなど多様な領域で自律ロボットの社会実装を進める構想を描く。
今回の買収は、AIを「知能」から「行動」へ拡張し、デジタルとフィジカルの融合を現実世界で具現化する取り組みの一環といえる。</p>
<h2>取引の枠組みと今後の見通し</h2>
<p>本取引は、ABBがロボティクス事業を分社化し、新設する持株会社の全株式をソフトバンクグループが取得する形で実施される。両社の取締役会で承認済みであり、各国の規制当局による承認を経て、2026年半ばから後半に完了する見通し。
取引後も両社は協力関係を維持し、ロボティクス分野での技術開発や市場展開を継続していく方針を示している。</p>
]]></description>
      <pubDate>Fri, 10 Oct 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>デロイトの報告書に生成AIのハルシネーションで存在しない文献を引用・参照、豪政府に代金を一部返金──脚注誤りを訂正し再公開、コンサル業界に波紋</title>
      <link>https://ledge.ai/articles/deloitte_ai_refund_australia_report</link>
      <description><![CDATA[<p>コンサルティング大手のデロイト・オーストラリアが、AIを利用して作成した政府向け報告書に誤りが見つかり、オーストラリア連邦政府（雇用・職場関係省＝DEWR）に代金の一部を返金したことが分かった。報告書には存在しない論文や不正確な引用が含まれており、AI生成文書の品質管理をめぐる議論が広がっている。</p>
<p>DEWRは2025年9月26日付で、問題となった報告書「Targeted Compliance Framework（TCF） Assurance Review」と、その概要をまとめた「Statement of Assurance」を更新し、訂正版を公式サイトで公開した。
同省は<a href="https://www.dewr.gov.au/assuring-integrity-targeted-compliance-framework/resources/targeted-compliance-framework-assurance-review-final-report">公式ページ</a>で「この文書は9月26日に更新され、参照と脚注の誤りを訂正した。修正は結論や提言に影響を与えない」と明記している。</p>
<h2>存在しない文献をAIが生成</h2>
<p>調査対象となったのは、雇用支援制度「Targeted Compliance Framework（TCF）」に関する外部監査報告書で、総額約43万9,000豪ドル（約4,200万円）でデロイトに発注されていた。</p>
<p><a href="https://apnews.com/article/australia-ai-errors-deloitte-ab54858680ffc4ae6555b31c8fb987f3">AP通信</a>は、報告書に「実在しない学術論文への参照や、誤った引用が含まれていた」と報じている。また、Financial Times（FT）によると、デロイトは報告書の一部作成でMicrosoftの「Azure OpenAI」ツールを使用していたことを認め、改訂版にはその利用事実が追記されたという。</p>
<p>複数の誤りがAIによるハルシネーション（幻覚）に起因していると<a href="https://www.theguardian.com/australia-news/2025/oct/06/deloitte-to-pay-money-back-to-albanese-government-after-using-ai-in-440000-report">The Guardian</a>が指摘している。</p>
<h2>デロイト、返金を実施</h2>
<p>デロイトが最終支払い分を返金することでオーストラリア政府と合意した。デロイトは声明で「参照と脚注に関する誤りを認識し、修正を完了した」と説明している。DEWR側は、報告書の主要な所見や提言自体に変更はないとしており、「修正は文献参照に限定され、内容の妥当性には影響していない」としている。</p>
<h2>政府の声明と再発防止</h2>
<p>DEWRの事務次官は10月3日付の声明で、「Targeted Compliance Frameworkの透明性と信頼性を高めるため、独立レビューを踏まえて制度改良を進めている」と述べた。
声明では、Deloitteによる報告書も改善プロセスの一部として参照していることが明らかにされ、政府側の対応は継続中とみられる。</p>
<h2>コンサル業界で問われる「AIの信頼性」</h2>
<p>デロイトはAI導入を強化しており、同時期にAnthropicとの提携拡大を発表したと<a href="https://techcrunch.com/2025/10/06/deloitte-goes-all-in-on-ai-despite-having-to-issue-a-hefty-refund-for-use-of-ai/">TechCrunch</a>が報道。一方で、AI生成文書の誤りによって公共契約の信頼性が揺らいでいると指摘した。デロイトは世界的にAIツールを業務へ統合しているが、今回の件は「AIを利用した文書の監査体制」が整備途上であることを浮き彫りにした。</p>
]]></description>
      <pubDate>Fri, 10 Oct 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>GoogleのノーコードAIミニアプリ「Opal」日本を含む15カ国に拡大──ワークフロー可視化と高速化を提供</title>
      <link>https://ledge.ai/articles/google_opal_global_expansion</link>
      <description><![CDATA[<p>Googleは2025年10月7日（現地時間）、自然言語からウェブアプリを作成できるノーコードAIツール「Opal」の提供地域を、日本を含む15カ国へ拡大したと<a href="https://blog.google/technology/google-labs/opal-expansion/">発表</a>した。OpalはGoogle Labsの実験プロジェクトで、コードを書くことなくAIミニアプリを構築できるのが特徴だ。</p>
<h2>米国先行公開からグローバル展開へ</h2>
<p>Opalは7月24日（現地時間）に米国で<a href="https://ledge.ai/articles/google_opal_no_code_ai_tool">初公開</a>。ユーザーが「タスク管理アプリを作成」「画像から色を抽出」といった指示を与えると、AIがHTML/CSS/JavaScriptで構成されたアプリを自動生成する。初期ユーザーの利用が想定以上に高度化したことを受け、今回、日本、韓国、インド、カナダ、ブラジル、アルゼンチン、ベトナム、インドネシア、シンガポール、コロンビア、エルサルバドル、コスタリカ、パナマ、ホンジュラス、パキスタンの15カ国での提供を開始した。</p>
<h2>ワークフローの可視化と高速化</h2>
<p>同時に、Opalの実用性を高める改良が行われた。</p>
<ul>
<li><strong>高度デバッグ</strong> ：ノーコードのまま、ビジュアルエディタ上でワークフローをステップごとに実行・検証。エラーは発生ステップにリアルタイム表示され、原因特定を容易にする。</li>
<li><strong>パフォーマンス改善</strong> ：新規Opal作成の起動時間を短縮。**並列実行（parallel runs）**により複数ステップを同時に処理でき、待機時間を抑える。</li>
</ul>
<h2>クリエイティブから業務効率化まで</h2>
<p>Opalは、個人の創作活動やマーケティング支援、業務プロセスの自動化など、多様な用途に対応する。Googleは「ユーザーが複雑なプロセスを自動化したり、アイデアを素早く形にしたりできるよう支援する」としており、開発者コミュニティはDiscord上でも展開されている。</p>
<p>@<a href="https://youtu.be/g9RBGnz-vqk">YouTube</a></p>
]]></description>
      <pubDate>Thu, 09 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>三井住友FG、社内AI「SMBC-GAI」にRAG機能を搭載──約130万件の社内文書を横断検索可能に</title>
      <link>https://ledge.ai/articles/smbc_gai_rag_internal_search</link>
      <description><![CDATA[<p>三井住友フィナンシャルグループ（SMBCグループ）は2025年10月6日、社内向けAIアシスタントツール「SMBC-GAI」に、RAG（Retrieval-Augmented Generation）技術を活用した社内情報検索機能を新たに搭載したと<a href="https://www.smfg.co.jp/news/pdf/j20251006_01.pdf">発表</a>した。社内規程や業務マニュアルなど約130万件に及ぶファイルをインデックス化し、従業員が一つのインターフェース上で横断的に検索・参照できるようにしたという。</p>
<h2>RAG技術で社内情報を高精度に検索</h2>
<p>RAG（Retrieval-Augmented Generation）とは、AIが関連性の高い情報を検索・参照し、それをもとに自然言語で回答を生成する技術だ。従来の生成AIと比べ、文脈理解と回答の正確性を高めることができる。「SMBC-GAI」ではこの仕組みを活用し、膨大な社内文書を効率的に検索できる環境を実現したという。</p>
<h2>約130万件のファイルをインデックス化</h2>
<p>対象は社内規程、通達、業務マニュアルなど約130万件にのぼるファイル群。RAGによって情報を体系的に整理・索引化することで、従業員は検索から回答生成までを一気通貫で行えるようになった。リリースによると、国内企業のRAG活用事例としては、学習ファイル数および利用人数の両面で「最大級の規模」としている。</p>
<h2>三井住友銀行から導入、グループ展開を視野に</h2>
<p>新機能はまず三井住友銀行の従業員向けに導入され、今後はSMBCグループ各社への展開が検討される。回答生成時には参照元を明示する仕組みも導入され、回答の根拠を追跡できるようになっている。</p>
<h2>金融業務に特化したAI基盤へ</h2>
<p>「SMBC-GAI」は2023年7月のリリース以降、社内の声を取り入れながら順次機能を強化してきた。SMBCグループは今後も「SMBC-GAI」を金融分野における汎用AI基盤へと進化させる方針で、従業員の業務効率化や顧客サービスの高度化につなげていくとしている。</p>
]]></description>
      <pubDate>Thu, 09 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>OpenAIとAMD、6GW規模のAIインフラ契約を締結──次世代GPU「Instinct」複数世代を供給へ</title>
      <link>https://ledge.ai/articles/openai_amd_6gw_ai_infrastructure</link>
      <description><![CDATA[<p>OpenAIと米半導体大手AMDは2025年10月6日（現地時間）、OpenAIの次世代AIインフラ構築に向け、6GW規模の計算能力を備えたシステムを展開する包括的パートナーシップを締結したと<a href="https://openai.com/index/openai-amd-strategic-partnership/">発表</a>した。
複数世代にわたるAMD「Instinct」GPUを活用し、2026年後半に最初の1GW分を導入する計画だ。</p>
<h2>6GW規模の包括的パートナーシップ</h2>
<p>AMDの<a href="https://ir.amd.com/news-events/press-releases/detail/1260/amd-and-openai-announce-strategic-partnership-to-deploy-6-gigawatts-of-amd-gpus">発表</a>によると、今回の契約では、AIトレーニングや推論に最適化されたデータセンター向けGPU「Instinct」シリーズ（MI450および将来世代）をOpenAIに継続的に供給する。これにより、OpenAIは複数世代にわたるAMD GPUを活用し、スケーラブルで高効率なAIインフラを構築する方針だ。</p>
<p>初期段階では、2026年後半に1GW相当の計算能力を展開。数年をかけて最大6GW規模まで拡張する。AMDによれば、この契約は同社のデータセンターGPU事業として過去最大級の供給量にあたるという。</p>
<h2>株式ワラントによる資本提携</h2>
<p>AMDは契約の一環として、OpenAIに対して約1億6千万株分の株式ワラントを発行。これにより、OpenAIは今後、AMDの発行済み株式の最大10%を取得する権利を持つことになる。
このスキームは、長期的な供給関係の安定化と、両社の戦略的連携を強化する目的があると説明されている。</p>
<h2>双方のコメント</h2>
<p>OpenAIは、AMDとの協力が次世代の生成AIシステムをより効率的かつスケーラブルに運用するための重要な一歩であると説明し、今後、AMDのGPUを活用して自社モデルのトレーニングおよび推論を拡張していく考えを示した。AMDのCEOであるリサ・スー氏も、この提携がAI計算基盤の進化を推進し、Instinct GPUの性能と電力効率を最大限に引き出す機会になると強調している。</p>
<h2>AIインフラ競争の新局面</h2>
<p>同提携は、拡大を続ける生成AI需要に対応するための計算資源の長期確保と、複数世代GPUにわたる供給体制を明示した点で、AIインフラ整備の大きな節目となる。両社は今後も、研究開発と運用の両面で協力を継続する方針だ。</p>
]]></description>
      <pubDate>Thu, 09 Oct 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI、ChatGPTの週間利用者が8億人を突破──DevDay 2025でApps SDKやGPT-5 Proなどを発表</title>
      <link>https://ledge.ai/articles/openai_devday2025_chatgpt_800m_wau</link>
      <description><![CDATA[<p>OpenAIのCEOであるサム・アルトマン氏は2025年10月6日（現地時間）、米サンフランシスコで開催された年次開発者会議「DevDay 2025」の基調講演で、対話型AI「ChatGPT」の週間アクティブユーザー数（WAU）が8億人を超えたと<a href="https://www.youtube.com/live/hS1YqcewH0c?si=1psz2A76y_mpcmvn">発表</a>した。2023年のDevDayでは1億人規模とされており、2年間で大幅に拡大したことになる。</p>
<p>アルトマン氏は会場で「ChatGPTの利用は世界中で急速に広がっている」と述べた。OpenAIのDevDay公式ページによれば、同社のプロダクトで開発した開発者は累計400万人、APIプラットフォームの処理量は毎分60億トークンと示されている。</p>
<p>イベントでは、ChatGPTを“アプリプラットフォーム”として拡張する新機能も発表された。ChatGPT内にネイティブアプリを構築できる「<a href="https://openai.com/index/introducing-apps-in-chatgpt/">Apps SDK</a>」プレビューが公開され、Booking.com、Canva、Coursera、Expedia、Figma、Spotify、Zillowによるデモ（対応市場で順次提供）が紹介された。</p>
<p>さらに、コード補助AI「<a href="https://openai.com/index/codex-now-generally-available/">Codex</a>」の一般提供（GA）が始まり、新しいSDKとともに開発者向け機能を強化。モデル面では、高精度な推論能力を備える「GPT-5 Pro」や、動画生成AI「Sora 2」、リアルタイム音声対応の「GPT-Realtime Mini」など、複数の最新モデルが紹介された。</p>
<p>今回の発表は、ChatGPTを単なるチャットツールから開発基盤へと位置づけ直す動きの一環とされる。OpenAIは、Apps SDKを通じて開発者がChatGPT内でアプリを提供できる環境を整えると案内しており、アプリ審査・ディレクトリ公開・マネタイズの詳細も順次共有する方針を示した。</p>
<p>@<a href="https://www.youtube.com/watch?v=hS1YqcewH0c&amp;t=133s">YouTube</a></p>
]]></description>
      <pubDate>Wed, 08 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>LLMが「心の中でイメージ」を描く？──人間の想像課題を超える精度で解答、GPT-5が人間平均を12％上回る</title>
      <link>https://ledge.ai/articles/artificial_phantasia_llm_visual_reasoning</link>
      <description><![CDATA[<p>米ノースイースタン大学の研究チームは2025年9月27日、言語モデル（LLM）が視覚情報なしに、頭の中でイメージを描くような課題を解けることを示した論文「Artificial Phantasia: Evidence for Propositional Reasoning-Based Mental Imagery in Large Language Models」を<a href="https://arxiv.org/abs/2509.23108">発表</a>した。</p>
<p>人間の「心的イメージ（mental imagery）」を模した課題を、テキスト入力だけで解答させた結果、OpenAIのGPT-5とo3モデル群が平均67%の正答率を示し、人間（54.7%）を上回ったという。</p>
<h2>言葉だけで「形」を思い描くタスク</h2>
<p>研究は、認知心理学で半世紀以上議論されてきた「心的イメージが言語的か、それとも視覚的か」という論争をAIで再検証したもの。
参加者には、頭の中で文字や図形を組み合わせて新しい形を作り、それが何に見えるか答える課題が与えられた。</p>
<p>たとえば――</p>
<p><strong>図：心的イメージ課題の一例</strong> ：「大文字のD」を左に90度回転し、「J」を下に組み合わせると傘の形になる。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x1_7_26c25bc70d/x1_7_26c25bc70d.png" alt="x1 (7).png" /></p>
<p>こうした「視覚イメージなしでは解けない」とされてきたタスクを、研究チームは60題（うち48題を新規作成）用意し、LLMに文章だけで解答させた。</p>
<h2>GPT-5とo3が人間を上回る精度</h2>
<p>結果、GPT-5は67.0%、OpenAI o3 Proは66.6%、標準o3は64.1% の正答率を記録。人間の平均54.7%を9〜12%上回り、統計的に有意な差が確認された（p \u003C .00001）。
一方、Claude Sonnet 4やGemini 2.5 Proは40〜46%と低迷し、画像生成を併用した場合はむしろ精度が下がった。研究者は「画像を使わせると推論が乱れ、言語ベースの方が安定する」と分析している。</p>
<h2>「見ずに考える」命題的推論</h2>
<p>論文では、LLMが絵を思い浮かべているのではなく、言語構造に基づいて空間関係を再構築する「命題的推論（propositional reasoning）」を行っていると結論づけている。この結果は、「心的イメージは視覚的でなければならない」とする通説を覆す可能性があり、人間の想像力に関する認知科学の議論にも新たな示唆を与える。</p>
<h2>人間とAIの“アファンタジア”の比較へ</h2>
<p>研究チームは、視覚イメージを持たない「アファンタジア（aphantasia）」の人々も同様の課題をこなせる点に着目。「視覚表象を持たずとも、言語的・構造的な推論でイメージ依存課題を解ける」ことを、AIと人間の両方で確認した形だ。
今後は、アファンタジアの被験者とLLMの思考過程を比較し、「人工的想像力（Artificial Imagination）」の本質を探るとしている。</p>
<p>研究チームはGitHubで実験コードとデータを公開し（subjectivitylab/artificial_phantasia）、今後はマルチモーダルAIや新しい推論ベンチマークへの応用を予定している。</p>
]]></description>
      <pubDate>Wed, 08 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>公共2025/10/6 [MON]デジタル庁、OpenAIと連携　政府共用AI「源内」に同社モデルを導入し行政活用を検討</title>
      <link>https://ledge.ai/articles/digital_agency_openai_genai_collaboration</link>
      <description><![CDATA[<p>デジタル庁は2025年10月2日、米OpenAIと生成AI（人工知能）の活用で連携すると<a href="https://www.digital.go.jp/news/e950673b-73eb-4f65-bf6a-339e4f0e7ef1">発表</a>した。政府職員が安全に生成AIを利用できる共用環境「源内（げんない）」に、OpenAIの大規模言語モデル（LLM）を新たにラインアップとして追加し、職員が業務で直接利用できるようにする方針だ。</p>
<h2>OpenAI選定の背景</h2>
<p>平将明デジタル大臣は翌3日の記者会見で、OpenAIを選定した理由について、最先端のAI研究と展開を進める企業として評価している旨を述べた。
源内では従来から複数のモデルを比較・検証しつつ活用しており、OpenAIのモデル追加により選択肢を拡充し、行政の生産性向上につなげる考えだ。</p>
<p>この協力方針は、2024年9月にOpenAIの最高戦略責任者（CSO）ジェイソン・クォン（Jason Kwon）氏がデジタル庁を訪問した際の会談で確認されたという。平大臣は「生成AIの利活用を推進するうえで重要な一歩」と述べた。</p>
<h2>セキュリティ体制の整備</h2>
<p>行政での生成AI活用にあたっては安全性と信頼性の確保が前提となる。会見の質疑で平大臣は、政府情報システムのためのセキュリティ評価制度「ISMAP（イスマップ）」に言及し、認証の有無により扱える情報の範囲が変わることを説明した。</p>
<p>@<a href="https://www.youtube.com/watch?v=hWf2w9br940">YouTube</a></p>
<p>OpenAI側も同日、自社の公式<a href="https://openai.com/ja-JP/global-affairs/strategic-collaboration-with-japan-digital-agency/">ブログ</a>で日本政府との戦略的協力を発表した。公共分野での活用モデルの共同検討や、ISMAP認証の取得をはじめ安全・安心に資する取り組みを前向きに検討する方針を示している。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/digital_openai_7ada319b72/digital_openai_7ada319b72.jpg" alt="digital openai.jpg" /></p>
]]></description>
      <pubDate>Wed, 08 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>学術＆研究2025/10/6 [MON]オープンLLMの日本語性能でトップ──FLUX、「Flux Japanese LLM」公開　独自手法でQwen2.5を進化</title>
      <link>https://ledge.ai/articles/flux_japanese_llm_release</link>
      <description><![CDATA[<p>国内スタートアップのFLUX株式会社は2025年9月29日、日本語特化の大規模言語モデル「Flux Japanese LLM」を<a href="https://flux.jp/news/1093/">発表</a>した。</p>
<p>同モデルはAlibaba Cloudの大規模言語モデル「Qwen2.5-32B」を基盤に、日本語理解・生成性能を独自の新手法で強化したもので、Open Japanese LLM Leaderboard（通称：LLM勉強会ランキング）で総合スコア第1位（0.7417）を記録したという。</p>
<h2>日本語能力を高める新手法「Precise-tuning」とは</h2>
<p>FLUXは今回のモデル開発にあたり、従来のファインチューニングとは異なる「Precise-tuning（プリサイズチューニング）」手法を導入した。日本語データセット全体でパラメーターを再学習するのではなく、日本語能力強化に必要なネットワーク回路のみを特定して再調整することで、効率的かつ精度の高い言語理解を実現したとしている。</p>
<p><strong>FLUXが開発した「Precise-tuning」手法の概念図</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/1440_810_4_6b464affdc/1440_810_4_6b464affdc.jpg" alt="1440-810-4.jpg" /></p>
<h2>ベンチマークで国内首位に</h2>
<p>同モデルは、LLM勉強会ランキング<a href="https://ledge.ai/articles/open_japanese_llm_leaderboard">オープン日本語LLMリーダーボード</a>において総合スコア0.7417を記録し、他の日本語モデルを上回る評価を得たという。</p>
<p>このランキングは、日本の有志研究者・エンジニアによるコミュニティ「<a href="https://llm-jp.nii.ac.jp/">LLM-jp</a>（通称：LLM勉強会）」が運営しており、複数の日本語LLMを自然言語推論・要約・コード生成などのタスクで比較評価するオープンベンチマークとして知られる。</p>
<p>LLM-jpは、国立情報学研究所（NII）を事務局とする共同研究プロジェクトで、2024年4月にNII内に設立された大規模言語モデル研究開発センター（LLMC）と連携して、「日本語に強いオープンな大規模言語モデル」を開発・評価する活動を進めている。そのため、このランキングは国内の学術・産業両分野で日本語LLMの性能を客観的に測る基準として広く参照されている。</p>
<p><strong>Open Japanese LLM Leaderboardでの評価結果。Flux Japanese LLMが第1位を記録</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/News_main_240718_3_e93e607d1f/News_main_240718_3_e93e607d1f.jpg" alt="News_main_240718-3.jpg" /></p>
<p>モデルはHugging Face上で公開されており、<a href="https://huggingface.co/flux-inc/Flux-Japanese-Qwen2.5-32B-Instruct-V1.0">モデルカード</a>には、自然言語処理・要約・コード生成タスクでの性能指標や学習設計の概要が掲載されている。</p>
<h2>企業・業界別モデル展開へ</h2>
<p>FLUXは、「Flux Japanese LLM」を自社のノーコードAIプラットフォーム群と連携させる計画を進めており、金融業界向けの特化モデル開発も行っている。同社は「AIをすべての人の手に」をミッションに掲げ、企業・研究機関・行政などが安全にLLMを活用できる基盤づくりを目指している。</p>
]]></description>
      <pubDate>Wed, 08 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>日本の「なぞなぞ」でAIの思考力をテスト──人間並みの正答率はGPT-5のみ。JAISTの研究チーム</title>
      <link>https://ledge.ai/articles/jaist_nazonazo_gpt5_benchmark</link>
      <description><![CDATA[<p>2025年9月18日、北陸先端科学技術大学院大学（JAIST）の研究チームは、日本の子ども向け「なぞなぞ（Nazonazo）」を活用し、大規模言語モデル（LLM）の洞察的推論能力を評価する新ベンチマークを開発したことを<a href="https://arxiv.org/abs/2509.14704">発表</a>した。実験の結果、GPT-5のみが人間に匹敵する正答率を示し、他のモデルは大きく下回った。</p>
<h2>飽和する既存ベンチマーク</h2>
<p>AIの能力を測る代表的なベンチマーク（MMLU、GSM8K、HumanEvalなど）は、最先端モデルが80〜90％の高スコアを記録するようになり、モデル間の性能差を明確に測りにくくなっている。OpenAI共同創業者のアンドレイ・カルパシー氏も「評価危機（evaluation crisis）」を指摘していた。</p>
<h2>日本の「なぞなぞ」はハイレベル？</h2>
<p>研究チームは、この「評価危機」を打開する手段として、日本の伝統的な言葉遊びである「なぞなぞ」を採用した。
なぞなぞは短文形式で低コストに新規作成が可能なうえ、専門知識を必要とせず、純粋な洞察力を試せる。また日本語特有の「漢字の分解」「語呂合わせ」「外来語表記」などにより、多様で難度の高い問題を作れる。</p>
<p>例として有名な「パンはパンでも食べられないパンは、なーんだ？」（答え：フライパン）が紹介されているほか、論文では「侍から“人偏”を取ると寺になる」という仕掛けのなぞなぞ（添付図参照）が示されている。</p>
<p><strong>漢字分解を利用したなぞなぞの例。「侍」から「人偏」を取ると「寺」となり、答えは「寺」</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Nazo_Nazo_Benchmar_7a284b9f17/Nazo_Nazo_Benchmar_7a284b9f17.jpg" alt="NazoNazo Benchmar.jpg" /></p>
<h2>英語 “リドル（riddle）” との差異</h2>
<p>英語圏では「RiddleSense」「BRAINTEASER」などのリドル系ベンチマークが存在するが、既に学習データに取り込まれており、GPT-4が98％超の精度で人間を上回るケースもある。
これに対し、日本語なぞなぞは人間でも平均正答率が52.9％にとどまり、AIモデルはさらに苦戦した。論文は「英語リドルはAIにとって容易になりすぎたが、日本語なぞなぞは汚染リスクが小さく、モデルの純粋な推論力を測るのに適している」と位置づけている。</p>
<h2>実験結果</h2>
<p>研究チームは38種類のLLM（GPT-4o、Claude、Gemini、Grok、Llama、DeepSeekなど）と成人126人を比較。</p>
<ul>
<li>人間の平均正答率は52.9％</li>
<li>GPT-5のみが人間平均と同等のスコアを記録</li>
<li>他のモデルは20〜30％台にとどまり、人間の半分程度にすぎなかった</li>
</ul>
<h2>AIが苦手な「最後のひと押し」</h2>
<p>多くのモデルは正解候補を途中で生成するものの、最終的に選べず「検証失敗」に陥るケースが頻発した。人間が持つ「Aha!（ひらめき）」や「これは正しい」という確信度がAIには弱く、洞察課題に特有の“最後のひと押し”が欠けていると指摘される。
また、モデルのパラメータ数の大きさと正答率には相関がなく、「推論型モデル」であることが成績の向上につながっていた。</p>
<h2>今後の展望</h2>
<p>論文は「GPT-5が例外的に人間並みの成績を示したが、他の最先端モデルは依然として人間に及ばない」と結論づけている。研究チームは、さらに難易度を高めた「Nazonazoベンチマーク2」の準備を進めており、今後はAIの“メタ認知的感覚”──正しいと感じる力──の強化が研究の焦点になる見通しだ。</p>
]]></description>
      <pubDate>Wed, 08 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Microsoft 365 Copilotに「Agent Mode」「Office Agent」を導入 ～ “vibe working” でAIによるWord・Excelの自動化を推進</title>
      <link>https://ledge.ai/articles/microsoft365_agent_mode_office_agent_vibe_ai</link>
      <description><![CDATA[<p>Microsoftは2025年9月29日（米国時間）、同社の生成AI搭載ツール「Microsoft 365 Copilot」に、新機能「Agent Mode」および「Office Agent」を導入すると<a href="https://www.microsoft.com/en-us/microsoft-365/blog/2025/09/29/vibe-working-introducing-agent-mode-and-office-agent-in-microsoft-365-copilot/">発表</a>した。これらは「vibe working」と呼ばれる新しい作業体験を掲げ、WordやExcelでの文書作成・データ分析をAIが支援・自動化することを目的としている。</p>
<h2>Agent Mode：Officeアプリ内でのAI自動化</h2>
<p>Agent Modeは、WordやExcelなどのOfficeアプリケーションに組み込まれ、複数ステップにわたる作業をAIと対話しながら進められる機能。</p>
<p>Excelでは「Excel Labs」アドインを通じてプレビュー提供が開始され、数値の分析やグラフ化をAIに任せられる。Wordでは、文書の構成提案や修正作業をAIが継続的に補助する機能が実装され、まずはWeb版から展開される。</p>
<p>@<a href="https://youtu.be/nSqCy-7Qabk">YouTube</a></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Excel_benchmark_FINAL_7b6722975b/Excel_benchmark_FINAL_7b6722975b.webp" alt="Excel-benchmark-FINAL.webp" /></p>
<h2>Office Agent：Copilotチャットから文書やプレゼン生成</h2>
<p>Office Agentは、Copilotのチャット環境で稼働するエージェントで、Anthropicのモデルを搭載している。ユーザーが「レポートをまとめて」「会議資料を作成して」といった意図を伝えると、AIがWord文書やPowerPoint資料を生成・編集する。従来の単発的な応答にとどまらず、業務プロセス全体を遂行する“作業型エージェント”としての役割を担う。</p>
<p>@<a href="https://www.youtube.com/watch?v=NPSnD8-TZjY">YouTube</a></p>
<h2>“vibe working”のコンセプト</h2>
<p>Microsoftはこれらの新機能を総称して「vibe working」と表現している。簡潔な指示を入力するだけでAIが作業を補完し、文書作成やデータ分析の完成度を高めることを狙う。ユーザーはAIを相棒のように扱い、業務をより効率的に進められるという。</p>
<h2>提供条件と展開予定</h2>
<p>新機能は「Microsoft 365 Copilot」ライセンスを持つユーザーに順次展開される。Frontierプログラム参加者向けに先行提供されるケースもあり、初期段階では英語やWeb版が中心。今後は地域やアプリケーションの拡大が予定されている。</p>
]]></description>
      <pubDate>Wed, 08 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Minecraft上で動作するAIチャットボット「CraftGPT」誕生──500万パラメータをレッドストーン回路で構築</title>
      <link>https://ledge.ai/articles/minecraft_ai_chatbot_craftgpt</link>
      <description><![CDATA[<p>人気ゲーム「Minecraft（マインクラフト）」の中で、実際に動作するAIチャットボットが登場した。
開発したのはゲームエンジニア兼クリエイターの Sammyuri氏。2025年10月1日、同氏はYouTubeで「I built ChatGPT with Minecraft redstone!（マインクラフトのレッドストーンでChatGPTを作った）」と題する動画を<a href="https://www.youtube.com/watch?v=VaeI9YgE1o8">公開</a>し、ゲーム内回路だけで大規模言語モデルを構築するプロジェクト「CraftGPT」を披露した。</p>
<h2>レッドストーンで構成されたAIモデル</h2>
<p>「CraftGPT」は、Minecraftのブロックと電気信号（レッドストーン）によって作られた小型の言語モデルだ。
動画では、入力された文字列に対して「Hi! How are you?」などの応答を生成する様子が紹介されている。演算は、ブロックごとに配置された論理回路が信号を受け渡しながら行われ、AIの“思考”を物理的なパルスとして可視化している。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/craft_gpt1_ee0a430c02/craft_gpt1_ee0a430c02.jpg" alt="craft gpt1.jpg" /></p>
<h2>500万パラメータ規模の「ゲーム内AI」</h2>
<p>Sammyuri氏によると、このAIモデルは 約5,087,280パラメータ の構成を持つ。
内部には、自然言語処理で用いられる「トークナイザー」「アテンション層」「行列演算」などの要素を模した回路が組み込まれているという。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/craft_gpt2_cc7dc4a387/craft_gpt2_cc7dc4a387.jpg" alt="craft gpt2.jpg" />
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/craft_gpt3_5fef40fd2e/craft_gpt3_5fef40fd2e.jpg" alt="craft gpt3.jpg" />
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/craft_gpt4_c1f6e020e7/craft_gpt4_c1f6e020e7.jpg" alt="craft gpt4.jpg" /></p>
<p>実装には高速動作環境「MCHPRS（Minecraft High Performance Redstone Server）」が使用され、1トークンの生成に数分を要する。リアルタイム対話には至らないが、AIの計算過程をブロック単位で表現する試みとして注目を集めている。</p>
<h2>巨大な“AI脳”の内部構造</h2>
<p>動画内では、CraftGPTの全体像も公開された。
構造は 約1,020×260×1,656ブロック に及び、総ブロック数はおよそ 4億3800万個。
一つひとつの回路がAIの重みや演算ノードを表しており、まるで“Minecraft上に広がる巨大な脳”のような光景を成している。
制作者は信号の流れを説明しながら、「AIが言葉を生み出す瞬間を、目に見える形で再現した」と語っている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/craft_gpt5_9140b45f1a/craft_gpt5_9140b45f1a.jpg" alt="craft gpt5.jpg" /></p>
<h2>実験的かつ教育的な意義</h2>
<p>CraftGPTは、実用的なAIチャットではなく、AIの内部構造を学び・観察するためのプロジェクトだ。
Sammyuri氏は動画の中で、「AIを単なるブラックボックスとして見るのではなく、その仕組みを楽しみながら理解できるものにしたかった」と述べている。
ゲームを舞台にしたこの試みは、AI技術と創造的表現の融合を示す作品としても評価されている。</p>
<p>YouTubeのコメント欄には、「AIの脳の中を歩いているようだ」「ここまで精密な回路を組むとは信じられない」といった驚きの声が寄せられている。</p>
<p>@<a href="https://www.youtube.com/watch?v=VaeI9YgE1o8">YouTube</a></p>
]]></description>
      <pubDate>Wed, 08 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI、次世代動画生成モデル「Sora 2」を発表──自分や友人が出演する動画を生成できるiOSアプリ「Sora」も米国とカナダで同時公開</title>
      <link>https://ledge.ai/articles/sora2_openai_ios_app_launch</link>
      <description><![CDATA[<p>OpenAIは2025年9月30日、最新の動画・音声生成モデル「Sora 2」を<a href="https://openai.com/index/sora-2/">発表</a>した。</p>
<p>物理挙動の正確さや映像の写実性が大幅に向上し、音声を同期して生成できる点が特徴。同日には、このモデルを利用できるiOS向けアプリ「Sora」も公開され、米国とカナダで招待制による提供が始まった。日本での提供時期は明らかにされていない。</p>
<p>@<a href="https://youtu.be/lEcg6AJ6DVY?si=aS3u22digXd5ZVY8">YouTube</a></p>
<h2>Sora 2の性能</h2>
<p>Sora2は、従来の「Sora」モデルを基盤に開発された動画・音声生成AIである。
OpenAIが公開した<a href="https://openai.com/index/sora-2-system-card/">システムカード</a>によれば、より正確な物理シミュレーション、長尺映像における一貫性、幅広いスタイルへの対応を実現。さらに音声生成を統合し、映像にナレーションや環境音を付与できる。</p>
<p>生成可能な映像は最大20秒とされるが、ReutersやThe Vergeなど複数のメディアは「アプリ上では10秒程度に制限されている」と報じている。</p>
<p>@<a href="https://www.youtube.com/watch?v=1PaoWKvcJP0">YouTube</a></p>
<h2>iOSアプリ「Sora」の提供</h2>
<p>同日に公開されたiOS向けアプリ「Sora」では、ユーザーがAI生成動画を作成・共有できる。
提供開始は米国とカナダで、アクセスは招待制。アプリ内で通知登録を行うことで順次利用可能となる。AndroidユーザーはWeb版の “sora.com” からアクセスできる仕組みだ。Sora2は当初無料で利用できるが、計算能力の制限が設けられているとのこと。</p>
<p>アプリの特徴として注目されるのが**「Cameo（カメオ）機能」** だ。ユーザーは自分や友人を動画に登場させられる。OpenAIは、この機能を利用するには本人の同意が必要とし、無断で他人の肖像を使用することはできない設計にしているという。サム・アルトマンCEOも自身のブログで「チームがキャラクターの一貫性に力を注ぎ、友人同士を動画に登場させることが意外なほど魅力的な新しいつながり方になった」と述べている。</p>
<h2>安全性への配慮</h2>
<p>OpenAIは安全設計を重視しており、生成動画には透かしやC2PAメタデータを付与。肖像権の無断利用や公人の生成は禁止され、未成年保護のためのフィルタリングや保護者向けコントロール機能も導入されている。
システムカードに記載された安全性評価では、不適切コンテンツを検出・遮断する精度が96〜99％に達したという。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/introducing_sora2_9ac129aadc/introducing_sora2_9ac129aadc.jpg" alt="introducing sora2.jpg" /></p>
<h2>背景と思想</h2>
<p>同社CEOのサム・アルトマン氏は自身の<a href="https://blog.samaltman.com/sora-2">ブログ</a>で、Soraを「ChatGPT for creativity」と表現。誰もが手軽に動画生成を楽しめる環境を提供する一方で、依存性や誤用のリスクについても懸念を示し、長期的なユーザーの満足や健全な利用を重視する方針を強調した。</p>
<p>また、OpenAIは公式サイトで「<a href="https://openai.com/index/sora-feed-philosophy/">フィード哲学</a>」を公開し、ユーザーが視聴体験を自ら選択できる仕組みを構築するとしている。</p>
<p>@<a href="https://www.youtube.com/watch?v=gzneGhpXwjU&amp;t=137s">YouTube</a></p>
]]></description>
      <pubDate>Wed, 08 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/10/5 [SUN]Sora、著作権方針を修正──Altman氏「日本の創作物に敬意」発言も。権利保護と生成AIの共存を模索</title>
      <link>https://ledge.ai/articles/sora_update_copyright_and_revenue_sharing_oct2025</link>
      <description><![CDATA[<p>OpenAIは、動画生成AI「Sora（ソラ）」の著作権対応を見直す。</p>
<p>同社CEOであるサム・アルトマン氏は2025年10月3日（現地時間）、自身の公式<a href="https://blog.samaltman.com/">ブログ</a>権利者がキャラクター生成をより細かく管理できる新機能と、収益分配制度の導入計画を発表した。背景には、Soraリリース後に浮上した知的財産権への懸念と、世界中で広がった批判がある。</p>
<h2>Soraをめぐる著作権論争</h2>
<p>OpenAIは2025年9月末、動画生成AIの最新版「Sora 2」と、米国・カナダ向けのソーシャル型iOSアプリ「Sora」を公開した。
テキストから高精細な映像を生成できる能力が話題を呼ぶ一方で、リリース直後には任天堂の「マリオ」や「ピカチュウ」など既存キャラクターに酷似した動画がSNS上で多数共有され、著作権保護の観点から懸念が相次いだ。一部の海外メディアは「任天堂の訴訟を招く可能性がある」と報じ、AIによる無断再現がどこまで許容されるのかが議論となった。</p>
<p>さらに、リリース直前に報じられた「権利者がオプトアウト（除外申請）しない限り自作品が扱われる」という運用観測も反発を招き、“同意なき取り込み”への不信感が増幅した。</p>
<p>この一連の騒動を受け、OpenAIはSoraの利用方針を再検討。今回の発表で、権利者による生成コントロール機能の導入と収益分配制度の構築を正式に打ち出した。</p>
<h2>権利者が生成内容を制御可能に</h2>
<p>アルトマン氏は、「Sora」の今後の方針として、権利者がキャラクター生成の可否や利用範囲を細かく指定できる新機能を追加する考えを示した。
これは、従来の「opt-in for likeness（本人類似モデル許諾）」を拡張したもので、より柔軟で公平な管理を可能にするという。</p>
<p>同氏は次のように述べている。</p>
<p>\u003E“We will give rightsholders more granular control over generation of characters... but want the ability to specify how their characters can be used (including not at all).”
（キャラクター生成について、権利者がより細かく制御できるようにし、使用方法を自ら指定できるようにします。まったく使用を許可しない選択も可能です。）</p>
<p>同氏によると、権利者の多くはSoraを通じた“インタラクティブなファンフィクション（参加型二次創作）”の可能性に期待を寄せつつも、利用のあり方を自ら決めたいと考えているという。OpenAIは、そうした多様な方針を尊重しつつ、全ての権利者に公平な標準を適用する姿勢を示した。</p>
<h2>収益分配モデルの試験導入へ</h2>
<p>OpenAIは、ユーザーによる動画生成量が予想を上回っていることを踏まえ、生成に使用されたキャラクターの権利者に収益を還元する制度を導入する計画も明らかにした。
アルトマン氏は「試行錯誤を重ねながら早期に開始する」とし、経済的な利益と新しい関係構築の双方を実現したい考えを示している。</p>
<p>\u003E “We are going to try sharing some of this revenue with rightsholders who want their characters generated by users.”
（ユーザーによって生成されるキャラクターを許可した権利者に対し、その一部の収益を共有することを試みます。）</p>
<h2>日本の創作文化に敬意を表明</h2>
<p>アルトマン氏は投稿の中で、「日本の創作物は非常に素晴らしい」と述べ、「ユーザーと日本のコンテンツとの深い結びつきに感銘を受けている」と言及した。
Soraをめぐる議論の中心に日本のコンテンツ産業があったことを踏まえ、文化的背景への理解を示した形だ。
今回の発表は、単なる技術的修正にとどまらず、文化と生成AIの関係を再定義する試みともいえる。</p>
<h2>GPT-5も同日にアップデート、安全性を強化</h2>
<p>同日、OpenAIは<a href="https://help.openai.com/en/articles/9624314-model-release-notes">Model Release Notes</a>を更新し、GPT-5 Instantモデルにメンタルヘルス対応の新機能を追加した。感情的または心理的なストレスをより正確に検知し、必要に応じて現実世界の支援リソースへ誘導できるようになったという。</p>
<p>こうした改良は、アルトマン氏が強調する「高速な改善サイクル」の一環として、Soraを含む同社製品全体に順次展開される見通しだ。</p>
<h2>今後の展望</h2>
<p>アルトマン氏は、「ChatGPT初期のように高頻度の改善を続けていく」と述べ、Soraを中心にプロダクト全体の改善を加速させる考えを示した。生成AIと著作権をめぐる議論が国際的に広がるなか、今回の発表は創作支援と権利保護の両立を図る “新しい共存モデル” として注目される。</p>
]]></description>
      <pubDate>Wed, 08 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>エンタメ＆アート2025/10/6 [MON]世界初のAI女優「ティリー・ノーウッド」にハリウッドが揺れる──SAG-AFTRAや著名俳優が「創造性の危機」と警告</title>
      <link>https://ledge.ai/articles/tilly_norwood_ai_actress_controversy_sagaftra</link>
      <description><![CDATA[<p>世界初の「AI女優」と称される Tilly Norwood（ティリー・ノーウッド）が、ハリウッドの俳優や映画俳優組合SAG-AFTRA（全米映画俳優組合・テレビ・ラジオ芸術家連盟）から強い批判を受けている。</p>
<p>SAG-AFTRAは2025年9月30日に「創造性は人間中心であるべき」と声明を<a href="https://www.sagaftra.org/sag-aftra-statement-synthetic-performer">発表</a>した。女優のエミリー・ブラント氏やウーピー・ゴールドバーグ氏も相次いで懸念を表明し、映画・テレビ業界全体に議論が広がっている。</p>
<h2>「AI Commissioner」──ティリー誕生の舞台</h2>
<p>Tilly Norwoodは、ロンドン拠点の制作会社Particle6が2025年9月にYouTube上で公開したコメディスケッチ『AI Commissioner | Comedy Sketch | Particle6』で初登場した。</p>
<p>この作品は、テレビ業界におけるAIの急速な普及を風刺的に描いたもので、AIが脚本作成からキャスティング、予算編成まですべてを自動化する世界を描く。劇中では、AIが生み出したインタラクティブスリラー『I Know What You Streamed Last Summer』に登場する100％AI生成の俳優としてTillyが紹介される。</p>
<p>@<a href="https://www.youtube.com/watch?v=3sVO_j4czYs">YouTube</a></p>
<p>登場人物の1人はTillyについて「僕の言うことを何でも聞いてくれる。恋をしてしまったかもしれない（She’ll do anything I say. I think I’m in love）」と評し、AIによる従順で“最適化された”俳優像を象徴的に表現。
さらに、「“She’ll cry on Graham Norton and be monetized on TikTok by lunchtime.”（彼女はグレアム・ノートンの番組で泣いて、その日の昼までにTikTokで収益化されるだろう）」という台詞が、人間の感情までもがAIによって即座に商業化される未来への皮肉として話題を呼んだ。</p>
<h2>SAG-AFTRAが声明「Tillyは俳優ではない」</h2>
<p>SAG-AFTRAは9月30日に「Statement on Synthetic Performer（合成パフォーマーに関する声明）」を<a href="https://www.sagaftra.org/sag-aftra-statement-synthetic-performer">発表</a>し、次のように明言した。</p>
<p>\u003E“Tilly Norwood is not an actor. Creativity must remain human-centered.”
（ティリー・ノーウッドは俳優ではない。創造性は人間中心であるべきだ。）</p>
<p>声明では、AIによって作られた “合成俳優” が芸術表現を侵食する可能性を指摘し、「経験や感情を持たない存在を“俳優”と呼ぶことは、芸術の根幹を損なう」と警鐘を鳴らした。SAG-AFTRA会長のショーン・アスティン氏もVarietyの取材に対し、「AI倫理と補償問題を正式な交渉テーマとして扱う」と述べている。</p>
<h2>著名俳優からの反発</h2>
<p>女優のエミリー・ブラント氏は、Varietyのポッドキャスト番組でTillyの画像を見せられ、驚きを隠さずこう語った。</p>
<p>\u003E“Good Lord, we’re screwed. That is really, really scary. Come on, agencies, don’t do that. Please stop taking away our human connection.”
「なんてこと、私たちは終わりね。本当に恐ろしいわ。お願い、エージェントはそんなことをやめて。人間のつながりを奪わないで。」</p>
<p>また、取材でTillyを「次のスカーレット・ヨハンソンに」と問われると、「“But we have Scarlett Johansson.”（でも私たちにはスカーレット・ヨハンソンがいる」 と返し、人間俳優の価値を強調した。</p>
<p>俳優・司会者のウーピー・ゴールドバーグ氏も、ABCのトーク番組『The View』（9月30日放送）で次のように発言した。</p>
<p>\u003E“You’re looking at 5,000 actors rolled into one synthetic person. That’s not fair. We all move differently.”
「5,000人の俳優の特性をひとつの合成存在にまとめるなんてフェアじゃない。私たちは皆、動きも表情も違う。」</p>
<h2>開発側の見解：「人間の代替ではなく、芸術表現」</h2>
<p>Tillyを制作したエライン・ファン・デル・フェルデン氏（Eline Van der Velden）は9月28日、ティリーのInstagram（<a href="https://www.instagram.com/tillynorwood/">@tillynorwood</a>）に投稿し、AI俳優の創作意図について次のように述べている。</p>
<p>\u003E “She is not a replacement for a human being, but a creative work — a piece of art.”
「ティリーは人間の代替ではなく、創造的な作品＝ひとつのアートです。」</p>
<p>同氏は、AIを「人間の代わり」ではなく「新しい絵筆のようなツール」と位置づけ、「アニメーションや人形劇、CGIがライブ演技を奪うことなく新しい可能性を開いたように、AIも物語を構築する新たな手段を提供する」と説明した。</p>
<p>\u003E “I’m an actor myself, and nothing — certainly not an AI character — can take away the craft or joy of human performance.”
「私は俳優でもあり、AIキャラクターであっても、人間の演技の技や喜びを奪うことはできません。」</p>
<p>投稿ではさらに、AIを“人間と競わせる存在”ではなく“芸術の新しいジャンルの一部”として評価すべきだと訴えている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/instagram_eline_at_tillynorwood_b80dee2451/instagram_eline_at_tillynorwood_b80dee2451.jpg" alt="instagram eline at tillynorwood.jpg" /></p>
<p>同氏が率いるParticle6は「AI俳優を活用すれば制作コストを最大90％削減できる」と説明し、複数のAIタレントを育てる「デジタル・タレント・ユニバース」構想を掲げている。</p>
<h2>今後の焦点</h2>
<p>各報道や関係者の声明からは、主に次の3点が論点として浮上している。</p>
<ul>
<li>著作権と肖像権：AI俳優の訓練データやモデル構築に使用された素材の扱い</li>
<li>契約・補償問題：AI使用を前提とした俳優契約の新たな枠組み</li>
<li>文化的受容：AIが「演技」を行うことを人々がどう受け入れるか</li>
</ul>
<p>SAG-AFTRAをはじめとする業界団体や俳優たちは、AIが創作活動や雇用に及ぼす影響を注視しており、今後は法制度や契約の整備を含めた議論が進む見通しだ。</p>
]]></description>
      <pubDate>Wed, 08 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Amazonの配送ドローン2機が墜落　米当局が調査開始──アリゾナ州トールソンで事故発生</title>
      <link>https://ledge.ai/articles/amazon_drone_crash_tolleson</link>
      <description><![CDATA[<p>アメリカ連邦航空局（FAA）は2025年10月1日（現地時間）、アリゾナ州トールソンで発生したAmazonの配送ドローン2機の墜落事故について、調査を開始したと<a href="https://www.faa.gov/newsroom/statements/accident_incidents">発表</a>した。FAAによると、事故による負傷者は報告されていない。</p>
<p>FAAの公式声明では、現地時間10月1日午前10時ごろ、Amazon Prime Airが運用する新型ドローン「MK-30」2機がトールソン市内で墜落したことを確認。国家運輸安全委員会（NTSB）と協力し、操縦手順、飛行経路、気象条件など複数の要因を調査する方針を示している。</p>
<p><a href="https://www.reuters.com/business/retail-consumer/ntsb-faa-probe-crashes-two-amazon-delivery-drones-2025-10-02/">Reuters</a>によると、墜落した2機はいずれも商用配送用のPrime Air機で、現場のクレーンのブーム部分に衝突した後、地上に墜落したという。Amazonはこの事故を受けてトールソン地区でのドローン配送を一時停止し、「安全性を最優先にし、関係当局と協力して原因を調べている」とコメントしている。</p>
<p>AmazonのPrime Airは、2022年にカリフォルニア州ロックフォードとテキサス州カレッジステーションでサービスを開始。2024年からアリゾナ州トールソンにも拡大し、最新型ドローン「MK-30」による商用配送を行っていた。Reutersは、今回の事故がMK-30の初期運用段階で発生したと報じている。</p>
<p>FAAは「商用ドローン運用に関する安全基準の適用を確認し、今後の事故防止に向けた対応を進める」としており、調査結果は後日公表される見込みだ。</p>
]]></description>
      <pubDate>Wed, 08 Oct 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google ResearchとDeepMind、「StreetViewAI」を発表──視覚障害者がAIと対話しながらストリートビューを“歩く”</title>
      <link>https://ledge.ai/articles/streetviewai_accessible_navigation</link>
      <description><![CDATA[<p>Google ResearchとGoogle DeepMindの研究チームは2025年9月28日（米国時間）、視覚障害者がAIと対話しながらGoogleストリートビュー上を探索できるツール「StreetViewAI」をACM国際会議UIST 2025で<a href="https://dl.acm.org/doi/10.1145/3746059.3747756">発表</a>した。視覚障害者がAIと対話しながらGoogleストリートビュー上を探索できるツールを開発し、マルチモーダルAIを活用した新しいアクセシビリティ体験を提示している。</p>
<h2>AIが「見る」街を、言葉で伝える</h2>
<p>StreetViewAIは、ストリートビュー画像・地図情報・地理メタデータなどを統合して、AIが環境を自然言語で説明するシステムだ。
会話の文脈を理解し、ユーザーが「右側には何がありますか？」「この角を曲がるとどうなりますか？」と尋ねると、AIが視覚と位置の情報をもとに答える。</p>
<p>AIが“街の目”となり、利用者は言葉のガイドによって世界中の街を“歩く”ことができる。</p>
<h2>背景：地理情報アクセシビリティの壁を超えて</h2>
<p>地図やストリートビューは視覚情報中心の体験であり、視覚障害者には利用のハードルが高い。
これまでの音声ガイドはランドマークの名称や距離といった部分情報にとどまり、空間全体の理解を支える仕組みは十分ではなかった。</p>
<p>StreetViewAIは、AIが空間の意味と文脈を理解し、自然な対話で伝えることを目指す。Googleが蓄積してきた地理空間理解技術に、DeepMindのマルチモーダルモデル技術を組み合わせた成果だという。</p>
<h2>システム構成：マルチモーダルAIによる空間理解</h2>
<p>StreetViewAIは、Googleのマルチモーダル大規模言語モデル（MLLM）を中核に、Street View画像、地図メタデータ、ナビゲーション履歴、周辺施設情報を統合し、文脈に沿った説明を生成する。</p>
<p>たとえば、ユーザーが「前に進んで」と音声指示を出すと、AIが実際に次のパノラマへ移動し、新たな環境を分析してリアルタイムに説明を更新する。この動作を支えるのが「StreetViewAI Control System」であり、AIが空間を“歩きながら”理解する構造を実現している。</p>
<p><strong>StreetViewAIの全体構成。ユーザーが音声で質問すると、マルチモーダルAIが地図情報と画像を統合して回答する</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/uist25_162_fig2_6f6636db4f/uist25_162_fig2_6f6636db4f.jpg" alt="uist25-162-fig2.jpg" /></p>
<h2>AI Describer：街の「語り手」としてのAI</h2>
<p>StreetViewAIの中核モジュールの一つが「AI Describer」だ。これは視覚障害者支援向けの“説明者AI”で、ストリートビュー画像から状況を自然言語で描写する。さらに、観光案内モード（Tour Guide Prompt）に切り替えると、文化的背景や観光情報を加味した解説も行える。</p>
<p>例として、渋谷のスクランブル交差点の場面では、標準モードでは「前方に大きな横断歩道と点字ブロックがあります」と伝えるが、ツアーガイドモードでは「ここは渋谷スクランブル交差点。世界で最も人通りが多い交差点の一つです」と説明する。</p>
<p><strong>対話インターフェースの例。ユーザーの質問（信号・横断歩道・看板の文字など）に対し、画像と地理メタデータに基づいて応答する</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/uist25_162_fig4_1c434bb912/uist25_162_fig4_1c434bb912.jpg" alt="uist25-162-fig4.jpg" /></p>
<h2>多様な都市での適用と検証</h2>
<p>研究チームは、シアトル、サンフランシスコ、ニューヨークなど複数都市のストリートビューで実験を実施。StreetViewAIは、都市構造や文化的文脈が異なる環境でも一貫した説明を生成でき、地域固有のランドマークや道路構造にも柔軟に対応した。
また、視覚障害当事者11名との評価を通じて、POI調査や遠隔での経路検討を支援する有用性も確認された。
こうした結果から、地域横断的なスケーラビリティと実利用の可能性が示されたと論文は述べている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/uist25_162_fig6_ffec020f07/uist25_162_fig6_ffec020f07.jpg" alt="uist25-162-fig6.jpg" /></p>
<h2>世界中の街を「声で歩く」未来へ</h2>
<p>StreetViewAIは、100以上の国・2200億枚超のストリートビュー画像のカバレッジを背景に、地域や言語を問わず動作する設計だ。現在、シアトル、ニューヨーク、東京、サンフランシスコ、ダブリンなど複数の都市でテストされており、地理的スケーラビリティを確認中だ。</p>
<p>今後は、Google MapsやAndroidナビゲーション、教育用アプリへの応用が期待される。Google Researchは論文で、「StreetViewAIは、アクセシビリティAIと地理空間AIの融合の一歩」と位置づけている。</p>
]]></description>
      <pubDate>Tue, 07 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>国立国会図書館×NII、官庁出版物30万点のテキストデータ提供で合意　日本発LLM開発を後押し</title>
      <link>https://ledge.ai/articles/ndl_nii_textdata_llm_collaboration</link>
      <description><![CDATA[<p>国立国会図書館（NDL）は2025年10月1日、国立情報学研究所（NII）に対して、同館が保有する官庁出版物などのテキストデータを提供することで合意したと<a href="https://www.ndl.go.jp/jp/news/fy2025/251001_01.html">発表</a>した。NIIが進める大規模言語モデル（LLM）の構築・研究を支援することを目的とし、日本語モデルの精度向上と公共データの再活用を促す取り組みとなる。</p>
<h2>約30万点の官庁出版物を対象</h2>
<p>提供されるのは、国立国会図書館が所蔵する官庁出版物を中心とした約30万点の資料のテキストデータ。主に1995年以前に刊行された図書・雑誌・官報などが対象で、OCR（光学式文字認識）により全文をデジタル化したものとなる。NIIはこれらのデータを研究用として活用し、日本語大規模言語モデルの構築や性能評価に役立てる。</p>
<h2>公共アーカイブとAI研究の連携</h2>
<p>今回の合意は、公共機関が保有する文献データを学術研究やAI開発に活用する新たな枠組みを示すもの。NDLはこれまでも、デジタルアーカイブ化やメタデータ公開などを通じて情報の利活用を推進してきたが、AI研究への提供は初の本格的な事例となる。</p>
<h2>日本語モデル開発を後押し</h2>
<p>NIIでは、研究機関や大学向けの日本語LLM開発を進めており、今回の提供データが学習素材として活用されることで、公共情報に基づく透明性の高いAIモデルの開発が期待される。
NDLは今後も、デジタル化資料の活用促進を通じて、学術研究や社会的知の発展に寄与していく方針を示している。</p>
]]></description>
      <pubDate>Tue, 07 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>KyoHA、国産ヒューマノイド試作プロジェクトを始動──早稲田大学・テムザック・村田製作所らが連携</title>
      <link>https://ledge.ai/articles/kyoha_humanoid_prototype_project_2025</link>
      <description><![CDATA[<p>一般社団法人「京都ヒューマノイドアソシエーション（KyoHA）」は2025年10月3日、国産ヒューマノイドの開発を目的とした「国産ヒューマノイド試作プロジェクト」を正式に始動したことを<a href="https://www.tmsuk.co.jp/topics/7608/">発表</a>した。
理事長は早稲田大学理工学術院の高西淳夫教授。中心メンバーとして株式会社テムザック、株式会社村田製作所、SREホールディングス株式会社が参画し、2026年3月を目標に全高約170cmの初期プロトタイプを公開する計画だ。</p>
<h2>京都発の産学連携プロジェクト</h2>
<p>KyoHAは、ヒューマノイド研究の第一人者である高西教授を中心に、産学官が協働する新しい枠組みとして発足。テムザックは全体構想および試作をリードし、村田製作所はセンシング・通信技術を、SREホールディングスはAI領域のディレクションを担う。</p>
<p>プロジェクトにはこのほか、マブチモーター、KYB、ヒーハイスト精工、OIST（沖縄科学技術大学院大学）など、国内の主要技術企業・研究機関が名を連ねている。</p>
<h2>技術分担と参画企業の役割</h2>
<p><strong>各社の技術分担：テムザックが全体設計を統括し、村田製作所はセンシング、マブチモーターとKYBは駆動系を担当する</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Slide1_1024x432_e6f41719e7/Slide1_1024x432_e6f41719e7.jpg" alt="Slide1-1024x432.jpg" /></p>
<p>KyoHAは開発領域を「技術全体」「ハード機体」「センシング」「AI」「アクチュエータ」「活動推進」に分類。
たとえば、テムザックは全体構想と機体設計、村田製作所はセンシング部品の開発と供給、マブチモーターとKYBは駆動系（モーター・油圧ユニット）の開発を担う。AI領域では、SREホールディングスとOISTが共同でディレクションを行い、AIプラットフォームや学習環境との連携を進めるという。</p>
<h2>初期プロトタイプは「ベースモデル」から2系統へ</h2>
<p>プロジェクトの第一段階では、汎用部品を活用した「ベースモデル」と呼ばれる初期型を試作し、ヒューマノイドモデルの基礎構築と技術課題の抽出を進める。</p>
<p><strong>ベースモデルを基礎に、2つの方向性（パワー重視／俊敏性重視）でモデル展開を進める</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Slide2_1024x460_9e41c8d6f9/Slide2_1024x460_9e41c8d6f9.jpg" alt="Slide2-1024x460.jpg" /></p>
<h2>京都・けいはんな学研都市で実証</h2>
<p>試作および検証の拠点は、京都市とけいはんな学研都市に設けられる予定だ。
産業界と研究機関、自治体が一体となって開発と実証を進め、地域発のイノベーション拠点として機能させる。
プロジェクトでは、災害対応や介護支援、研究開発など多様な分野での社会実装を視野に入れており、京都から全国への技術波及を目指している。</p>
<h2>国内産業の連携と再構築へ</h2>
<p>KyoHAは、ロボット産業の基盤を国内で完結できる形に再構築することを掲げている。
部品調達からソフトウェア開発、組立・実証までを国内企業が連携して担う体制を整え、安定したサプライチェーンと技術継承の仕組みを築く考えだ。
今後も参画企業や研究機関を広く募り、オープンな協創ネットワークを通じて“国産ヒューマノイド”の開発体制を強化していくとしている。</p>
]]></description>
      <pubDate>Tue, 07 Oct 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>iRobotの共同創業者 ロドニー・ブルックス氏、「人型ロボットの器用さはまだ数十年先」──触覚なきAIを批判、“車輪付きポスト人型”の未来を予見</title>
      <link>https://ledge.ai/articles/rodney_brooks_humanoid_dexterity_post_humanoid_future</link>
      <description><![CDATA[<p>ロボット工学の第一人者であり、ルンバを開発したiRobotの共同創業者として知られるロドニー・ブルックス氏が、2025年9月26日付の<a href="https://rodneybrooks.com/why-todays-humanoids-wont-learn-dexterity/">公式ブログ</a>{target=\</p>
]]></description>
      <pubDate>Sun, 05 Oct 2025 23:50:00 GMT</pubDate>
    </item>
  </channel>
</rss>