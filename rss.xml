<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
  <channel>
    <title>Ledge.ai 新着記事</title>
    <link>https://ledge.ai/</link>
    <description>Ledge.ai の最新テクノロジー記事</description>
    <item>
      <title>東京大学・ソフトバンク・LINEヤフー、「Beyond AI技術研究組合」を設立──AI研究成果の事業化を加速</title>
      <link>https://ledge.ai/articles/beyond_ai_technology_consortium_tokyo_univ_softbank_line_yahoo</link>
      <description><![CDATA[<p>東京大学とソフトバンク、LINEヤフーは2025年10月10日、AI（人工知能）研究の成果を迅速に社会実装・事業化するための新たな産学連携組織「Beyond AI技術研究組合」を9月19日に設立したと<a href="https://www.softbank.jp/corp/news/press/sbkk/2025/20251010_02/">発表</a>した。</p>
<p>同組合は、経済産業省の「CIP（コーポレート・イノベーション・プラットフォーム）」制度を活用し、AI研究から事業化までを一貫して推進する体制を構築するという。</p>
<h2>研究と事業化の橋渡しを強化</h2>
<p>東京大学とソフトバンク、LINEヤフーの3者は、2019年設立の「Beyond AI研究推進機構（BAIR）」を通じてAI分野の基礎研究を進めてきた。今回の「Beyond AI技術研究組合」は、その研究成果を社会や産業へ迅速に還元するための組織で、知的財産や人材育成、資金調達などの面から事業化を支援する「プラットフォーム型CIP」として位置づけられる。</p>
<h2>組織概要と体制</h2>
<p>組合の設立日は2025年9月19日。代表理事は東京大学総長の藤井輝夫氏が務め、理事にはソフトバンク代表取締役社長の宮川潤一氏、LINEヤフー代表取締役CEOの出澤剛氏が就任した。本部は東京都文京区の東京大学内に設置され、AI関連技術の社会実装を加速させるための中核拠点として運営される。</p>
<h2>今後の展開</h2>
<p>同組合では、Beyond AI研究推進機構で進めてきた基礎研究テーマを引き継ぎ、生成AIや自律システム、社会AI（Society AI）などの分野を中心に、企業や行政との連携による実証実験を進めていく。
また、大学発の研究成果をスタートアップ創出や産業連携につなげる「AI事業化アクセラレーション拠点」としての役割も担い、日本におけるAIエコシステムの強化を目指す。</p>
]]></description>
      <pubDate>Wed, 15 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Microsoft、自社設計の画像生成モデル「MAI-Image-1」を発表──フォトリアルな表現と高速生成を両立</title>
      <link>https://ledge.ai/articles/microsoft_mai_image_1_announcement</link>
      <description><![CDATA[<p>MicrosoftのAI部門であるMicrosoft AIは2025年10月14日（米国時間）、同社が初めて自社で設計・開発した画像生成モデル「MAI-Image-1」を<a href="https://microsoft.ai/news/introducing-mai-image-1-debuting-in-the-top-10-on-lmarena/">発表</a>した。</p>
<p>フォトリアルな質感や構図の整った出力に加え、既存モデルと比べて生成速度を大幅に向上させた点が特徴で、今後はCopilotやBing Image CreatorなどMicrosoft製AIサービスへの順次統合を予定している。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/sand_1_07e63f2f03/sand_1_07e63f2f03.jpg" alt="sand-1.jpg" /></p>
<h2>Microsoft初の完全社内開発モデル</h2>
<p>MAI-Image-1は、Microsoft AIが完全に社内で設計・訓練した初の画像生成モデルであり、既存の外部モデルを基盤とせず独自開発された。研究チームは、効率的な学習パイプラインとデータ選別技術を構築し、独自の高品質データセットを用いて訓練を行ったという。</p>
<p>モデルの評価は、画像生成AIの国際ベンチマーク「<a href="https://lmarena.ai/leaderboard/text-to-image">LMArena</a>」で実施され、初登場にしてトップ10入りを果たした。</p>
<h2>フォトリアルな描写と生成速度の両立</h2>
<p>公式ブログによると、MAI-Image-1は「現実に近い光表現」「安定した構図」「被写体の一貫性」に優れており、特に人間や風景のレンダリング品質が高いとされる。また、内部最適化により生成処理を高速化し、既存の大規模モデルよりも短時間で高解像度の画像を生成可能にした。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/crosswalk_7e79e8b8d5/crosswalk_7e79e8b8d5.jpg" alt="crosswalk.jpg" /></p>
<p>Microsoft AIは、テキストから画像を生成するプロンプト理解力の向上にも重点を置き、指示の意図や文脈をより正確に反映するアルゴリズムを採用したという。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/roadrunner_HQ_f5373a8b23/roadrunner_HQ_f5373a8b23.jpg" alt="roadrunner-HQ.jpg" /></p>
<h2>今後の展開：CopilotやDesignerに統合へ</h2>
<p>MAI-Image-1は今後、Copilot、Designer、Bing Image Creatorなど、同社の生成AIプロダクト群に順次統合される予定。Microsoftはこれにより、「安全で信頼性の高い画像生成を誰もが利用できる環境を提供する」としている。</p>
<p>発表ではまた、MAI-Image-1を同社の新しい社内モデル群「MAI-」シリーズの第1弾と位置づけており、今後は動画生成や3D生成分野への展開も視野に入れている。</p>
]]></description>
      <pubDate>Wed, 15 Oct 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI、Broadcomと戦略的提携──自社設計AIアクセラレーターを10GW規模で展開へ</title>
      <link>https://ledge.ai/articles/openai_broadcom_ai_accelerator_10gw_partnership</link>
      <description><![CDATA[<p>OpenAIは2025年10月13日（米国時間）、米半導体大手Broadcomとの戦略的提携を<a href="https://openai.com/index/openai-and-broadcom-announce-strategic-collaboration/">発表</a>した。両社は、OpenAIが設計したAIアクセラレーターとBroadcomのネットワークソリューションを組み合わせ、総計10ギガワット（GW）規模のAIクラスターを共同で構築する。展開は2026年後半に開始し、2029年末までの完了を予定している。（<a href="https://jp.broadcom.com/company/news/product-releases/63631">Broadcomのリリース</a>）</p>
<h2>10GW規模のAIアクセラレーターを共同開発・展開</h2>
<p>発表によるとOpenAIは自社でAIアクセラレーターとシステムを設計し、Broadcomが開発・展開を担う。ラックにはBroadcomのEthernet、PCIe、光接続などのソリューションを採用し、完全にEthernetベースで構築される。これにより、スケールアップ（拡張）とスケールアウト（分散拡張）の両面で柔軟なAIインフラを実現するという。</p>
<p>両社は、AIアクセラレーターおよびBroadcomのネットワーク技術を組み込んだラックを展開するための契約書（term sheet）を締結済み。すでに長期にわたる共同開発・供給の枠組みを構築しており、今後はOpenAIの自社施設およびパートナーデータセンターを中心に順次導入を進める。</p>
<h2>アルトマン氏「AIの可能性を引き出すための重要な一歩」</h2>
<p>OpenAIの共同創業者兼CEOであるサム・アルトマン氏は次のように述べた。
「Broadcomとの提携は、AIの潜在能力を解き放ち、人々や企業に実際の恩恵をもたらすための重要なステップです。自社アクセラレーターの開発は、AIの最前線を押し広げ、人類全体に利益を届けるための取り組みでもあります。」</p>
<p>また、共同創業者兼プレジデントのグレッグ・ブロックマン氏も、「自社設計チップによって、これまでのモデル開発で得た知見をハードウェアに直接組み込み、新たなレベルの知性と能力を引き出す」とコメントしている。</p>
<h2>Broadcom「AGIへの道を拓く協業」と強調</h2>
<p>Broadcomの社長兼CEOホック・タン氏は、「OpenAIとの協業は汎用人工知能（AGI）実現に向けた転換点だ」と述べ、10GW規模の次世代アクセラレーターとネットワークシステムの共同開発が「AIの未来への道を切り開く」と強調した。</p>
<p>同社の半導体ソリューション事業プレジデントであるチャーリー・カワス氏（Ph.D.）も、「オープンでスケーラブル、かつ電力効率の高いAIクラスター設計において新たな業界基準を打ち立てる」と述べ、Ethernetベースのアプローチがコストと性能の両立を可能にすると説明した。</p>
<h2>OpenAI、週8億人の利用基盤を背景に次世代インフラを整備</h2>
<p>OpenAIは現在、ChatGPTをはじめとする製品群を通じて、週あたり8億人を超えるアクティブユーザーを抱える。今回のBroadcomとの連携により、AIインフラの自社開発と最適化を進め、同社が掲げる「AGIの恩恵をすべての人に届ける」という使命の実現を加速させる狙いだ。</p>
]]></description>
      <pubDate>Tue, 14 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>政府、OpenAIに著作権侵害防止を要請──「Sora 2」問題で平デジタル相は“オプトイン方式”を提言</title>
      <link>https://ledge.ai/articles/openai_sora2_government_copyright_request_oct2025</link>
      <description><![CDATA[<p>OpenAIの動画生成AI「Sora 2」による日本のアニメ作品に酷似した映像がSNS上で拡散している問題を受け、政府が対応に乗り出した。城内実内閣府特命担当大臣（知的財産戦略・クールジャパン戦略担当）は10月10日の<a href="https://www.gov-online.go.jp/press_conferences/minister_of_state/202510/video-303104.html">記者会見</a>で、OpenAIに対し著作権侵害となる行為を行わないよう要請したと明らかにした。</p>
<p>城内大臣は「アニメや漫画は世界の人々を魅了し続ける、我が国が世界に誇る宝」と述べ、知的財産権の保護を重視する姿勢を強調。要請は内閣府の知的財産戦略推進事務局からオンラインで直接行われたという。記者質問の内容から、実施時期は10月上旬で、Sora 2による“酷似動画”が相次いだ直後とみられる。</p>
<p>一方、平将明デジタル大臣は10月12日、TBS番組でAIの学習段階における権利処理の在り方について言及した。「OpenAIには、きちんと権利処理をしていただくようお願いしている」と述べたうえで、「AIの学習データについても、事前の同意を得るオプトイン方式が望ましい」と発言。AI事業者に対し、無断利用ではなく同意制に基づくデータ利用の仕組みを導入するよう求めた。</p>
<p>これに先立つ10月3日、OpenAIのCEOであるサム・アルトマン氏は、動画生成AI「Sora 2」に関連する著作権保護と収益分配制度に関する方針をブログで明らかにしていた。同氏は「試行錯誤を重ねながら早期に開始する」と述べ、経済的な利益と新しい関係構築の双方を実現したい考えを示している。</p>
<p>政府は今後もAI事業者に対し、著作権および文化的資産の保護を重視した対応を求める方針を示している。AIによる創作支援が拡大するなかで、学習データの扱いと権利保護の両立が国際的な課題となりつつある。</p>
]]></description>
      <pubDate>Tue, 14 Oct 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>AIがソフトウェアの脆弱性を自動修正──Google DeepMind、新エージェント「CodeMender」を発表</title>
      <link>https://ledge.ai/articles/deepmind_codemender_ai_vulnerability_fix</link>
      <description><![CDATA[<p>Google DeepMindは2025年10月10日、ソフトウェアの脆弱性を自動的に検出・修正するAIエージェント「CodeMender」を<a href="https://deepmind.google/discover/blog/introducing-codemender-an-ai-agent-for-code-security/">発表</a>した。</p>
<p>同エージェントは大規模言語モデル「Gemini Deep Think」を基盤とし、深層推論を活用して安全で安定したコード改変を行う。DeepMindは「AIが自ら脆弱性を理解し、修正案を提案・検証する」という新しいセキュリティサイクルを構築したと説明している。</p>
<h2>72件の脆弱性を自動修正</h2>
<p>CodeMenderはすでにオープンソースソフトウェアの実環境でテストされており、これまでに72件のセキュリティ修正を開発元のリポジトリへ統合した。対象コードには約450万行規模の大規模プロジェクトも含まれる。
DeepMindによれば、修正対象の一例として画像処理ライブラリ「libwebp」でのメモリ管理エラーがあり、AIによる自動改修によってバッファオーバーフロー耐性が向上したという。</p>
<h2>「反応型」と「先制型」AIによる修正</h2>
<p>CodeMenderは、既知の脆弱性に即応する反応型（reactive）と、将来的に問題を起こす可能性のある構造を事前に書き換える先制型（proactive）の2モードを備える。</p>
<p>内部では、静的解析・動的解析・ファジングなどを組み合わせた検査手法を使用し、修正案の妥当性をGemini Deep Thinkが多段階で検証。AIが生成したパッチは必ず人間エンジニアのレビューを経る仕組みで、誤修正を防ぐ安全策も講じられている。</p>
<p><strong>図：CodeMenderの自動修正フロー。LLMエージェントが脆弱性を検出し、Validatorが修正案を検証した後、人間によるパッチレビューを経てコードリポジトリへ統合する。</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/fixing_vulnerabilities_4975259e2f/fixing_vulnerabilities_4975259e2f.jpg" alt="fixing vulnerabilities.jpg" /></p>
<h2>セキュリティ運用の自動化へ</h2>
<p>DeepMindは、「CodeMenderはAIによるソフトウェアセキュリティの自動化を推進する重要な一歩」と位置づけている。今後はGoogleのSecure AI Framework（SAIF）やAI Vulnerability Reward Program（AI VRP）など、既存のセキュリティ施策と連携して展開を進める予定だ。
また、オープンソース開発者コミュニティとの協力を通じて、より多くのプロジェクトでのAI自動修正を実用化していくとしている。</p>
]]></description>
      <pubDate>Tue, 14 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>リコー、推論性能を強化した日本語LLMを発表──GPT-5相当の性能で金融業務に特化</title>
      <link>https://ledge.ai/articles/ricoh_reasoning_llm_gpt5_equivalent_20251010</link>
      <description><![CDATA[<p>リコーは2025年10月10日、推論（）性能を追加搭載した日本語大規模言語モデル（LLM）を<a href="https://jp.ricoh.com/release/2025/1010_1">発表</a>した。</p>
<p>このモデルは700億パラメータ規模のオンプレミス対応型LLMで、金融業務に特化して開発されたもの。同社によれば、多段推論能力（Chain-of-Thoughts: CoT）の導入により、融資稟議など専門的な業務遂行力を強化し、代表的な日本語ベンチマークでOpenAIの「GPT-5」と同等レベルの性能を確認したという。</p>
<h2>金融分野に特化した推論能力</h2>
<p>リコーは、有価証券報告書などの公開データを活用し、金融分野に特有の専門知識を追加学習したうえで、多段推論能力（Chain-of-Thoughts: CoT）を導入。複数の情報を論理的ステップに分解し、融資稟議業務などの複雑な判断を支援する能力を強化した。</p>
<p>このモデルはオンプレミス環境で運用できる「プライベートLLM」として提供され、企業が自社データを安全に学習させることが可能だとしている。</p>
<h2>ベンチマーク評価でGPT-5と同等スコア</h2>
<p>発表資料によると、代表的な日本語ベンチマークで次の結果を得た。
評価には「ELYZA-tasks-100」「Japanese MT-Bench」「japanese-lm-fin-harness」およびリコー独自の融資稟議ベンチマークが用いられた。</p>
<ul>
<li><strong>ELYZA-tasks-100</strong> ：複雑な指示タスク（要約、対話、意図の理解など）</li>
<li><strong>Japanese MT-Bench</strong> ：マルチターン対話能力</li>
<li><strong>japanese-lm-fin-harness</strong> ：金融分野における感情分析・証券知識・試験問題など</li>
<li><strong>融資稟議ベンチマーク</strong> ：企業・財務・信用の総合評価</li>
</ul>
<p>評価の結果、リコーの「Llama-3.3-Ricoh-70B-20251001」は日本語タスク全般でGPT-5に匹敵するスコアを示し、金融ベンチマークでは同規模以上のオープンソースモデルを上回ったとされる。</p>
<p><strong>ベンチマークツールにおける他モデルとの比較結果</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/_d289815140/_d289815140.jpg" alt="ベンチマークツールにおける他モデルとの比較結果.jpg" /></p>
<h2>高精度化を支える独自技術</h2>
<p>リコーのモデルは、Meta社の「Llama-3.3-70B-Instruct」を基に、日本語性能を強化した「Llama-3.3-Swallow-70B-v0.4」をベースモデルとして構築。独自のインストラクション・チューニングや、Chat Vectorを用いたモデルマージ技術、独自カリキュラムを組み合わせることで高精度化を実現した。大規模ながら省コストで動作し、GPUリソースの少ない環境でも運用できる点が特徴とされる。</p>
<h2>今後の展開</h2>
<p>リコーは今後、金融以外にも製造業・医療などの業種特化型モデルを順次開発するとしている。
同社は「使える・使いこなせるAI」の提供を掲げ、企業のデジタルトランスフォーメーション（DX）を支援していく方針だ。</p>
]]></description>
      <pubDate>Tue, 14 Oct 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google、企業向けAIの“入口”「Gemini Enterprise」発表──従業員全員にGoogle AIを届ける統合プラットフォーム</title>
      <link>https://ledge.ai/articles/google_gemini_enterprise_ai_platform_launch</link>
      <description><![CDATA[<p>Googleは米国時間2025年10月9日、企業向けの新しいAIプラットフォーム「Gemini Enterprise（ジェミニ・エンタープライズ）」を<a href="https://blog.google/products/google-cloud/gemini-enterprise-sundar-pichai/">発表</a>した。
職場におけるAI活用の“入口（front door）”として位置づけられ、従業員全員が直感的なチャットインターフェースを通じて、GoogleのAI機能を安全かつ統合的に利用できる環境を提供する。</p>
<h2>部門単位のAIから、全社横断のAIへ</h2>
<p>Googleは発表の冒頭で、「AIは働き方、ビジネス運営、顧客体験のすべてを変革する歴史的機会をもたらす」としながらも、従来のAI活用が部門ごとに孤立していた課題を指摘した。Gemini Enterpriseは、この分断を解消し、ワークフローと従業員をつなぐ包括的なAIプラットフォームとして開発された。</p>
<p>Google Cloudによると、同社の強みは「信頼性の高いAIインフラ」「Google DeepMindによる先駆的研究」「多用途なGeminiモデルファミリー」の3層構造にある。このフルスタックAI戦略が、企業におけるAI変革を支える中核基盤になるという。</p>
<h2>6つの中核コンポーネントを統合</h2>
<p>Gemini Enterpriseは、以下の6つの要素を単一のインターフェースで統合する：</p>
<ul>
<li><strong>最新のGeminiモデル</strong> ：あらゆる業務タスクの“頭脳”として、高度な推論と生成を実現。</li>
<li><strong>ノーコード・ワークベンチ</strong> ：非エンジニアでもデータ分析やエージェント連携が可能。</li>
<li><strong>事前構築エージェント</strong> ：リサーチやデータインサイトなどの専門業務に即対応。</li>
<li><strong>企業データとの安全な接続</strong> ：Google WorkspaceやMicrosoft 365、Salesforce、SAPなどにシームレス接続。</li>
<li><strong>統合ガバナンス</strong> ：すべてのエージェントを一元的に可視化・保護・監査。</li>
<li><strong>オープンなエコシステム</strong> ：10万社を超えるパートナーによる拡張性を確保。</li>
</ul>
<p>Googleはこれにより、「単一タスクの効率化を超え、ワークフロー全体を自動化する」としている。</p>
<h2>業務アプリとの統合と新機能</h2>
<p>Gemini EnterpriseはGoogle Workspaceとも密接に連携する。
テキスト・画像・動画・音声を理解・生成できる初のマルチモーダルエージェントが導入され、文書作成や会議運営などの作業を支援。
「Google Vids」によるAI生成動画の作成機能や、「Google Meet」でのリアルタイム音声翻訳機能も提供される。
後者は発話者のトーンや表現を反映し、自然な多言語コミュニケーションを可能にするという。</p>
<p>さらに、「データサイエンスエージェント（プレビュー）」が発表された。
データ取り込みから探索、モデルトレーニングの自動化までを担い、VodafoneやWalmartなどの企業が既に活用している。</p>
<h2>導入事例の拡大と実用成果</h2>
<p>Googleは、Banco BV、Klarna、Mercedes-Benz、Swarovskiなどの導入事例を紹介した。
Banco BVでは、分析作業の自動化により、マネージャーが新規ビジネス開拓に注力できるようになった。Mercedes-BenzはGeminiを用いてドライバーと自然な会話ができる自動車内AIアシスタントを構築している。</p>
<p>@<a href="https://www.youtube.com/watch?v=ijqTReRzG8M&amp;t=26s">YouTube</a></p>
<p>日本企業では、メルカリがGoogle AIをコールセンターに導入。
AI主導のカスタマーサービス体験を実現し、業務量を20％削減、ROI（投資収益率）を500％向上させる見込みとしている。</p>
<h2>開発者×エージェント経済──A2AとAP2で拡張</h2>
<p>Gemini Enterpriseは企業だけでなく、開発者にも開かれたプラットフォームとして進化する。
すでに100万人以上が利用する「Gemini CLI」に加え、AIをコマンドラインから拡張できる「Gemini CLI Extensions」を導入。
さらに、開発者やISV（独立系ソフトウェアベンダー）がエージェントを構築・販売・収益化できる「エージェントエコノミー」の構想を発表した。</p>
<p>この仕組みを支えるのが、</p>
<ul>
<li><strong>Agent2Agent Protocol（A2A）</strong> ：エージェント間通信を標準化する新プロトコル</li>
<li><strong>Agent Payments Protocol（AP2）</strong> ：エージェントによる安全な金融取引を実現する決済標準
の2つである。AP2は、American Express、Mastercard、PayPalなど100社超のパートナーと共同で策定された。</li>
</ul>
<h2>学習・導入支援プログラム</h2>
<p>Googleは、AI人材育成と現場導入支援の両面から変革を後押しする。
新たに全従業員が無料でAIスキルを学べる「Google Skills」を開設し、開発者向けの教育プログラム「Gemini Enterprise Agent Ready（GEAR）」を開始した。
さらに、顧客企業に伴走して導入支援を行うAIエンジニアチーム「Team Delta」も新設している。</p>
<p>@<a href="https://www.youtube.com/watch?v=Qbix0BOPcgE">YouTube</a></p>
<h2>AIの“入口”としての意義</h2>
<p>Googleは、AIの未来を「オープンで協力的なエコシステム」にあるとし、Box、Workday、ServiceNowなど主要企業との連携を拡大している。クロスプラットフォームでのワークフロー連携や導入支援、検証済みエージェントの発見、収益化の仕組みなど、10万社超のパートナー基盤を活用し、AI導入を全層で支援する方針だ。</p>
<p>\u003E「Gemini Enterpriseは、Google AIの最高の機能をすべての従業員とワークフローに届ける“職場AIの新たな入口”です」（Google Cloud公式ブログより）</p>
<p>Googleは、企業にとってのAI導入を「一部の実験」から「全員の変革」へと引き上げる転換点として、この新プラットフォームを位置づけている。</p>
]]></description>
      <pubDate>Mon, 13 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>公共2025/10/13 [MON]AIとの出会い、早すぎる？──Pew調査で明らかになった子どものチャットボット利用の低年齢化</title>
      <link>https://ledge.ai/articles/pew_children_ai_chatbot_early_use</link>
      <description><![CDATA[<p>米国の調査機関Pew Research Centerは2025年10月8日、保護者3,251人を対象にした全国調査レポート「How parents manage screen time for kids」を<a href="https://www.pewresearch.org/internet/2025/10/08/how-parents-manage-screen-time-for-kids/">発表</a>した。報告によると、5〜7歳の子どもの3％、8〜10歳の7％、11〜12歳の15％が、ChatGPTやGeminiなどのAIチャットボットを利用した経験があると回答。生成AIとの接触が、幼児期を含む低年齢層にまで広がりつつあることが明らかになった。</p>
<h2>スクリーンタイム調査で浮かび上がる「AIの早期接触」</h2>
<p>この調査は、子どものスクリーンタイム管理やデジタル機器利用をテーマとするもので、ゲームやSNSと並び、AIチャットボットの利用状況も尋ねている。</p>
<p>Pew Researchは「幼児では少数だが、年齢が上がるにつれてAI利用が明確に増加している」と指摘。AIが“スクリーン利用”の一形態として、家庭生活の中に入り込み始めている様子が浮かび上がった。</p>
<p><strong>12歳以下の子どものデバイス利用（12歳以下、米国）</strong>：TV 90％、タブレット 68％、スマートフォン 61％など。AIチャットボットは全体で8％。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/PI_2025_10_08_parents_kids_screens_0_01_540d7def3a/PI_2025_10_08_parents_kids_screens_0_01_540d7def3a.webp" alt="PI_2025.10.08_parents-kids-screens_0-01.webp" /></p>
<p>Pew Researchは「幼児では少数だが、年齢が上がるにつれてAI利用が明確に増加している」と指摘。AIが“スクリーン利用”の新たな形態として、家庭生活の中に入り込み始めている様子が浮かび上がった。</p>
<h2>ChatGPTを“友達”と語る子どもも──親世代の戸惑い</h2>
<p>レポートには、10歳の子どもを持つ親のコメントとして、
\u003E「My child talks about ChatGPT like it’s a friend. I’m not sure how much I should worry about that.」
（うちの子はChatGPTのことを友達のように話す。どの程度心配すべきかわからない）</p>
<p>という声が掲載されている。AIを単なるツールではなく、対話相手として受け入れる子どもが現れ始めており、親世代との認識差も表れている。</p>
<h2>理解が追いつかない保護者、監督と教育のはざまで</h2>
<p>Pew Researchは、AIツールをめぐって「多くの親がまだ理解を深めている段階にある」と報告。一部の保護者は、子どものAI利用実態の把握に課題を感じているとの記述もある。</p>
<p>調査では、AIチャットボットを「学習支援」や「創作のきっかけ」として肯定的に見る一方で、内容の信頼性や年齢に応じた利用ルールの整備を求める声も寄せられた。</p>
<p><strong>年齢別にみたデバイス利用状況（米国）</strong>：5–7歳 3％／8–10歳 7％／11–12歳 15％。年齢上昇とともに利用が増える傾向。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/PI_2025_10_08_parents_kids_screens_0_02_1058158cf7/PI_2025_10_08_parents_kids_screens_0_02_1058158cf7.webp" alt="PI_2025.10.08_parents-kids-screens_0-02.webp" /></p>
<h2>家庭に入り込むAI、問われる「年齢相応の付き合い方」</h2>
<p>報告書では、AIチャットボットを
\u003E“conversational tools such as ChatGPT, Gemini, or similar products that allow users to generate text responses through typed prompts（ChatGPT、Gemini、またはユーザーが入力されたプロンプトを通してテキスト応答を生成できる類似製品のような対話ツール）”</p>
<p>と定義し、会話型の生成AI全般を対象にしている。スクリーンタイムという枠組みの中で、AIチャットボットが独立の利用カテゴリーとして扱われている。Pew Researchは、今後の課題として「家庭でのAI利用に関する年齢相応のガイドラインづくり」の必要性を指摘した。</p>
<h2>調査概要</h2>
<ul>
<li>発表日：2025年10月8日</li>
<li>調査名：How parents manage screen time for kids</li>
<li>実施機関：Pew Research Center（米国ワシントンD.C.）</li>
<li>対象：米国の保護者 3,251人（オンライン調査）</li>
<li>実施期間：2025年5月〜6月</li>
<li>調査対象年齢：5〜12歳の子ども</li>
<li>調査目的：家庭におけるスクリーンタイム管理とテクノロジー利用の実態把握</li>
</ul>
]]></description>
      <pubDate>Mon, 13 Oct 2025 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>富士通とジーンクエスト、因果AIで遺伝子と生活習慣の関係を解析──「Fujitsu Kozuchi」活用で新たな知見</title>
      <link>https://ledge.ai/articles/fujitsu_genequest_causal_ai</link>
      <description><![CDATA[<p>富士通株式会社と株式会社ジーンクエストは2025年10月9日、富士通のAI技術群「Fujitsu Kozuchi（コヅチ）」の中核技術である因果AIを活用し、遺伝子データとライフスタイルデータの関係性を解析した結果、新たな知見を得たと<a href="https://global.fujitsu/ja-jp/pr/news/2025/10/09-01">発表</a>した。</p>
<p>実証では、同意が得られた約4,000名分の遺伝子情報と生活習慣アンケートデータを対象に、因果AIが両者の間に潜む因果構造を探索。特定の遺伝子多型が睡眠時間や食習慣、BMIなどの生活習慣に与える影響を可視化した。これにより、従来の統計的相関分析では把握しにくかった複雑な要因連鎖を明らかにしたとしている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/sub1_7b9b5b5a0a/sub1_7b9b5b5a0a.png" alt="sub1.png" /></p>
<h2>因果AIで「相関」から「原因と結果」へ</h2>
<p>富士通の因果AIは、膨大な多変量データの中から要因と結果の方向性を自動的に推定する技術。同社が開発した高速因果構造探索アルゴリズムにより、従来の分析手法よりも大規模・高精度な因果モデルを生成できる。富士通はこれを、産業・医療・科学分野を横断するAI基盤「Fujitsu Kozuchi」に搭載し、研究開発支援に活用している。</p>
<h2>生活習慣改善や疾病予防へ応用</h2>
<p>両社は、今回得られた知見を今後のヘルスケアサービスに応用する方針を示している。ジーンクエストは個人向け遺伝子解析サービスの高度化を進め、生活習慣病予防やパーソナライズド医療への展開を目指す。富士通は医療機関や製薬企業との連携を通じ、因果AIを「科学的エビデンス創出を支援する技術」として普及させる考えだ。</p>
<p>富士通は今回の取り組みを「AIによって科学的知見を創出する“サイエンスDX”の推進」と位置づけており、生命科学領域での新たなAI応用事例として注目される。</p>
]]></description>
      <pubDate>Sun, 12 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>iRobotの共同創業者 ロドニー・ブルックス氏、「人型ロボットの器用さはまだ数十年先」──触覚なきAIを批判、“車輪付きポスト人型”の未来を予見</title>
      <link>https://ledge.ai/articles/rodney_brooks_humanoid_dexterity_post_humanoid_future</link>
      <description><![CDATA[<p>ロボット工学の第一人者であり、ルンバを開発したiRobotの共同創業者として知られるロドニー・ブルックス氏が、2025年9月26日付の<a href="https://rodneybrooks.com/why-todays-humanoids-wont-learn-dexterity/">公式ブログ</a>{target=\</p>
]]></description>
      <pubDate>Sun, 12 Oct 2025 04:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/10/11 [SAT]ソフトバンクとオラクル、AI活用のためのソブリンクラウドを共同構築──国内運用基盤「Cloud PF Type A」でデータ主権を確保</title>
      <link>https://ledge.ai/articles/softbank_oracle_sovereign_cloud_pf_type_a</link>
      <description><![CDATA[<p>ソフトバンク株式会社と米オラクルは2025年10月8日、データ主権（ソブリン性）を確保しつつ、AI活用を前提としたクラウド基盤「Cloud PF Type A」を日本国内で運用するための協業を開始したと<a href="https://www.softbank.jp/corp/news/press/sbkk/2025/20251008_01/">発表</a>した。オラクルのクラウド技術「Oracle Alloy」を採用し、ソフトバンクが国内データセンターで独自運用を担う。政府機関や企業が自国の管理下で安全にAIやクラウドを活用できるソブリンクラウドの実現を目指す。</p>
<h2>データ主権を守る「ソブリンクラウド」</h2>
<p>両社が開発する「Cloud PF Type A」は、すべてのデータ処理・保存・管理を日本国内で完結させる構成をとる。国外へのデータ移転を伴わずにクラウドやAIを活用できるようにすることで、経済安全保障やガバナンス上のリスクを最小化する狙いがある。</p>
<p>ソフトバンクは通信事業で培った国内ネットワークと運用ノウハウを活用し、オラクルは独自のクラウド基盤「Oracle Alloy」を通じて技術支援を提供。両社は「日本の社会インフラの一部として信頼されるクラウドを構築する」としている。</p>
<h2>「Cloud PF Type A」を2026年に提供開始</h2>
<p>新たに構築される「Cloud PF Type A」は、2026年4月に東日本拠点、同年10月に西日本拠点での提供を予定している。
この基盤では、Oracle Cloud Infrastructure（OCI）の約200種類のクラウドおよびAIサービスを国内運用で利用可能になる。</p>
<p>主な特徴は次の通り。</p>
<ul>
<li><strong>鍵管理（KMS）</strong> ：Oracle Vaultとソフトバンク独自システムを併用し、暗号鍵を完全に国内で管理。</li>
<li><strong>通信構成</strong> ：「OnePort」や「SmartVPN」などの閉域網接続に対応し、安全なデータ通信を実現。</li>
<li><strong>災害対策</strong> ：東西データセンターの冗長構成を採用し、事業継続（BCP）に対応。</li>
<li><strong>運用支援</strong> ：ソフトバンクがMSP（運用管理代行）を提供し、導入から保守まで一括サポート。</li>
</ul>
<p>この構成により、政府・自治体・企業などがAIモデルの学習や推論を行っても、データが国外へ移動することはない。</p>
<h2>両社のコメント</h2>
<p>ソフトバンクは、自社DCの高いセキュリティ水準に適合したクラウドを提供し、生成AIやGPUを統合して多様な顧客ニーズに応える方針と述べた。オラクルは、Oracle Alloyにより日本国内のデータ主権要件に対応し、OCIの幅広いAI/クラウドを国内DCで利用可能にする取り組みだと説明した。</p>
<h2>国内で広がる“主権クラウド”構想</h2>
<p>欧州を中心に広がる「ソブリンクラウド（主権クラウド）」の潮流は、日本国内でも加速している。
NTT、富士通、日立などが相次いで国産クラウド基盤の整備を進めており、今回のソフトバンクとオラクルの取り組みもその一環と位置づけられる。</p>
<p>両社は今後、生成AIや自然言語処理、画像解析などの高負荷AIワークロードにも対応する予定で、AI時代のデータ主権を支える中核的なインフラを目指す。</p>
]]></description>
      <pubDate>Sat, 11 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/10/10 [FRI]イーロン・マスク率いるxAI、動画生成AI「Imagine v0.9」を公開──静止画に声と動きを与える“ネイティブ映像生成”モデル</title>
      <link>https://ledge.ai/articles/xai_imagine_v09_release</link>
      <description><![CDATA[<p>イーロン・マスク氏が率いるAI開発企業xAIは2025年10月8日（現地時間）、新たな動画生成モデル「Imagine v0.9」を<a href="https://x.com/xai/status/1975607901571199086">発表</a>した。視覚品質、動き、音声生成などを全面的に改良し、すべてのxAI製品で無料利用が可能となっている。</p>
<p>xAIは公式X（旧Twitter）で「Imagine v0.9は、映像品質・モーション・音声生成などにおいてv0.1から大幅に進化した」と投稿。<a href="https://grok.com/imagine">grok.com/imagine</a> では、Grokプラットフォーム上で同モデルを試せるようになっている。</p>
<h2>音声と映像を同時生成──“編集不要”の体験を掲げる</h2>
<p>xAIは今回の発表で、「Imagine v0.9は音声と映像を同時に生成する“ネイティブ・オーディオ＋ビデオ生成”を実現した」と説明。</p>
<p>\u003E“Imagine v0.9 pushes the boundaries of native audio + video generation, creating cinematic experiences straight out of the box—no editing required.”</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x_AI_Imagine_v0_9_1_dragon_6a3a28422f/x_AI_Imagine_v0_9_1_dragon_6a3a28422f.jpg" alt="xAI Imagine v0-9-1 dragon.jpg" /></p>
<p>投稿では、音声と映像が同期したドラゴンのデモ映像を公開。ユーザーは、生成後の編集作業なしで“完成された動画”を得られるとしている。</p>
<h2>モーション精度とカメラ効果を強化</h2>
<p>Imagine v0.9では、被写体の滑らかな動きとリアリズムを高精度で再現するモーション制御を導入した。
さらに「インテリジェント・フォーカスシフト」と呼ばれる自動焦点移動など、ストーリーテリングに適したダイナミックなカメラ効果も追加された。</p>
<p>\u003E“And lets you add dynamic camera effects like intelligent focus shifts for better storytelling.”</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x_AI_Imagine_v0_9_1_camera_effects_5febfab05e/x_AI_Imagine_v0_9_1_camera_effects_5febfab05e.jpg" alt="xAI Imagine v0-9-1 camera effects.jpg" /></p>
<h2>自然な対話・歌唱・リズムまで再現</h2>
<p>Imagine v0.9では、音声表現の幅も拡大。自然な対話や歌唱を含む“声の演出”が可能になった。</p>
<p>\u003E“v0.9 also brings videos to life with natural dialogue and strong audio-visual harmony.”</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x_AI_Imagine_v0_9_1_natural_dialogue_f30bf3d090/x_AI_Imagine_v0_9_1_natural_dialogue_f30bf3d090.jpg" alt="xAI Imagine v0-9-1 natural dialogue.jpg" /></p>
<p>\u003E“It also brings expressive singing to life with clear vocals and synchronized emotion.”</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x_AI_Imagine_v0_9_1_singing_05b4553dab/x_AI_Imagine_v0_9_1_singing_05b4553dab.jpg" alt="xAI Imagine v0-9-1 singing.jpg" /></p>
<p>xAIはさらに、音と動きを合わせたダンス生成の例として、キャラクター「Ani」の動画を紹介している。</p>
<p>\u003E“Plus good rhythm: here's Ani with smooth dance moves.”</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x_AI_Imagine_v0_9_1_ani_dancing_c19660cb25/x_AI_Imagine_v0_9_1_ani_dancing_c19660cb25.jpg" alt="xAI Imagine v0-9-1 ani dancing.jpg" /></p>
<p>xAIはユーザーからのフィードバックを積極的に求め、モデル改良に反映させる方針を示した。</p>
]]></description>
      <pubDate>Fri, 10 Oct 2025 23:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ソフトバンクグループ、スイスABBのロボティクス事業を約8187億円で買収──AIと自律ロボットの統合を加速</title>
      <link>https://ledge.ai/articles/softbank_acquires_abb_robotics_for_ai_integration</link>
      <description><![CDATA[<p>ソフトバンクグループ株式会社は2025年10月8日、スイスのABB Ltd.（以下ABB）から同社のロボティクス事業を買収する最終契約を締結したと<a href="https://group.softbank/news/press/20251008">発表</a>した。買収総額は53億7500万ドル（約8187億円）で、取引の完了は2026年半ばから後半を予定している。AIとロボティクスを融合させるグローバル戦略の一環として、産業用・協働ロボットの分野で新たな展開を図る。</p>
<h2>AIとロボットを結ぶ「フィジカルAI」構想</h2>
<p>ソフトバンクグループ代表取締役会長兼社長の孫正義氏は、同社の次のフロンティアは「フィジカルAI」にあると述べ、AIの知能とロボティクスの身体を結びつけることで、人類の未来を切り拓く画期的な進化を実現していくと強調した。
同グループはこれまで、SoftBank Robotics GroupやBerkshire Grey、AutoStore、Agile Robots、Skild AIなどに投資しており、今回の買収を通じてAIと実機ロボットの統合をさらに加速させる考えだ。</p>
<h2>ABBの発表：「AI時代のロボティクスを担う最適な拠点」</h2>
<p>ABBは同日発表した<a href="https://new.abb.com/news/detail/129685/abb-to-divest-robotics-division-to-softbank-group">リリース</a>で、ロボティクス事業をソフトバンクに譲渡すると公表。</p>
<p>ABBのCEOであるMorten Wierod氏は、ソフトバンクグループはABBロボティクス事業と従業員にとって最適な新しい拠点となり、AIを基盤とするロボティクスの新時代において事業の成長をさらに後押しするだろうと述べた。</p>
<p>新会社の代表は、現ABB ロボティクス部門社長Marc Segura氏が務める予定で、従業員は約7,000人。2024年の売上は22億7,900万ドルで、ABB全体の売上の約7%を占めている。名称や資本金などの詳細は現時点で未定とされている。</p>
<h2>産業オートメーションから自律ロボットへ</h2>
<p>ABBロボティクスは、AIを活用した協働ロボット（コボット）や産業オートメーションソリューションを展開し、グローバル市場で高いシェアを持つ。
ソフトバンクはこれらの技術をAIプラットフォームと連携させ、製造、物流、医療、サービスなど多様な領域で自律ロボットの社会実装を進める構想を描く。
今回の買収は、AIを「知能」から「行動」へ拡張し、デジタルとフィジカルの融合を現実世界で具現化する取り組みの一環といえる。</p>
<h2>取引の枠組みと今後の見通し</h2>
<p>本取引は、ABBがロボティクス事業を分社化し、新設する持株会社の全株式をソフトバンクグループが取得する形で実施される。両社の取締役会で承認済みであり、各国の規制当局による承認を経て、2026年半ばから後半に完了する見通し。
取引後も両社は協力関係を維持し、ロボティクス分野での技術開発や市場展開を継続していく方針を示している。</p>
]]></description>
      <pubDate>Fri, 10 Oct 2025 07:50:00 GMT</pubDate>
    </item>
    <item>
      <title>デロイトの報告書に生成AIのハルシネーションで存在しない文献を引用・参照、豪政府に代金を一部返金──脚注誤りを訂正し再公開、コンサル業界に波紋</title>
      <link>https://ledge.ai/articles/deloitte_ai_refund_australia_report</link>
      <description><![CDATA[<p>コンサルティング大手のデロイト・オーストラリアが、AIを利用して作成した政府向け報告書に誤りが見つかり、オーストラリア連邦政府（雇用・職場関係省＝DEWR）に代金の一部を返金したことが分かった。報告書には存在しない論文や不正確な引用が含まれており、AI生成文書の品質管理をめぐる議論が広がっている。</p>
<p>DEWRは2025年9月26日付で、問題となった報告書「Targeted Compliance Framework（TCF） Assurance Review」と、その概要をまとめた「Statement of Assurance」を更新し、訂正版を公式サイトで公開した。
同省は<a href="https://www.dewr.gov.au/assuring-integrity-targeted-compliance-framework/resources/targeted-compliance-framework-assurance-review-final-report">公式ページ</a>で「この文書は9月26日に更新され、参照と脚注の誤りを訂正した。修正は結論や提言に影響を与えない」と明記している。</p>
<h2>存在しない文献をAIが生成</h2>
<p>調査対象となったのは、雇用支援制度「Targeted Compliance Framework（TCF）」に関する外部監査報告書で、総額約43万9,000豪ドル（約4,200万円）でデロイトに発注されていた。</p>
<p><a href="https://apnews.com/article/australia-ai-errors-deloitte-ab54858680ffc4ae6555b31c8fb987f3">AP通信</a>は、報告書に「実在しない学術論文への参照や、誤った引用が含まれていた」と報じている。また、Financial Times（FT）によると、デロイトは報告書の一部作成でMicrosoftの「Azure OpenAI」ツールを使用していたことを認め、改訂版にはその利用事実が追記されたという。</p>
<p>複数の誤りがAIによるハルシネーション（幻覚）に起因していると<a href="https://www.theguardian.com/australia-news/2025/oct/06/deloitte-to-pay-money-back-to-albanese-government-after-using-ai-in-440000-report">The Guardian</a>が指摘している。</p>
<h2>デロイト、返金を実施</h2>
<p>デロイトが最終支払い分を返金することでオーストラリア政府と合意した。デロイトは声明で「参照と脚注に関する誤りを認識し、修正を完了した」と説明している。DEWR側は、報告書の主要な所見や提言自体に変更はないとしており、「修正は文献参照に限定され、内容の妥当性には影響していない」としている。</p>
<h2>政府の声明と再発防止</h2>
<p>DEWRの事務次官は10月3日付の声明で、「Targeted Compliance Frameworkの透明性と信頼性を高めるため、独立レビューを踏まえて制度改良を進めている」と述べた。
声明では、Deloitteによる報告書も改善プロセスの一部として参照していることが明らかにされ、政府側の対応は継続中とみられる。</p>
<h2>コンサル業界で問われる「AIの信頼性」</h2>
<p>デロイトはAI導入を強化しており、同時期にAnthropicとの提携拡大を発表したと<a href="https://techcrunch.com/2025/10/06/deloitte-goes-all-in-on-ai-despite-having-to-issue-a-hefty-refund-for-use-of-ai/">TechCrunch</a>が報道。一方で、AI生成文書の誤りによって公共契約の信頼性が揺らいでいると指摘した。デロイトは世界的にAIツールを業務へ統合しているが、今回の件は「AIを利用した文書の監査体制」が整備途上であることを浮き彫りにした。</p>
]]></description>
      <pubDate>Fri, 10 Oct 2025 01:50:00 GMT</pubDate>
    </item>
    <item>
      <title>LLMが「心の中でイメージ」を描く？──人間の想像課題を超える精度で解答、GPT-5が人間平均を12％上回る</title>
      <link>https://ledge.ai/articles/artificial_phantasia_llm_visual_reasoning</link>
      <description><![CDATA[<p>米ノースイースタン大学の研究チームは2025年9月27日、言語モデル（LLM）が視覚情報なしに、頭の中でイメージを描くような課題を解けることを示した論文「Artificial Phantasia: Evidence for Propositional Reasoning-Based Mental Imagery in Large Language Models」を<a href="https://arxiv.org/abs/2509.23108">発表</a>した。</p>
<p>人間の「心的イメージ（mental imagery）」を模した課題を、テキスト入力だけで解答させた結果、OpenAIのGPT-5とo3モデル群が平均67%の正答率を示し、人間（54.7%）を上回ったという。</p>
<h2>言葉だけで「形」を思い描くタスク</h2>
<p>研究は、認知心理学で半世紀以上議論されてきた「心的イメージが言語的か、それとも視覚的か」という論争をAIで再検証したもの。
参加者には、頭の中で文字や図形を組み合わせて新しい形を作り、それが何に見えるか答える課題が与えられた。</p>
<p>たとえば――</p>
<p><strong>図：心的イメージ課題の一例</strong> ：「大文字のD」を左に90度回転し、「J」を下に組み合わせると傘の形になる。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x1_7_26c25bc70d/x1_7_26c25bc70d.png" alt="x1 (7).png" /></p>
<p>こうした「視覚イメージなしでは解けない」とされてきたタスクを、研究チームは60題（うち48題を新規作成）用意し、LLMに文章だけで解答させた。</p>
<h2>GPT-5とo3が人間を上回る精度</h2>
<p>結果、GPT-5は67.0%、OpenAI o3 Proは66.6%、標準o3は64.1% の正答率を記録。人間の平均54.7%を9〜12%上回り、統計的に有意な差が確認された（p \u003C .00001）。
一方、Claude Sonnet 4やGemini 2.5 Proは40〜46%と低迷し、画像生成を併用した場合はむしろ精度が下がった。研究者は「画像を使わせると推論が乱れ、言語ベースの方が安定する」と分析している。</p>
<h2>「見ずに考える」命題的推論</h2>
<p>論文では、LLMが絵を思い浮かべているのではなく、言語構造に基づいて空間関係を再構築する「命題的推論（propositional reasoning）」を行っていると結論づけている。この結果は、「心的イメージは視覚的でなければならない」とする通説を覆す可能性があり、人間の想像力に関する認知科学の議論にも新たな示唆を与える。</p>
<h2>人間とAIの“アファンタジア”の比較へ</h2>
<p>研究チームは、視覚イメージを持たない「アファンタジア（aphantasia）」の人々も同様の課題をこなせる点に着目。「視覚表象を持たずとも、言語的・構造的な推論でイメージ依存課題を解ける」ことを、AIと人間の両方で確認した形だ。
今後は、アファンタジアの被験者とLLMの思考過程を比較し、「人工的想像力（Artificial Imagination）」の本質を探るとしている。</p>
<p>研究チームはGitHubで実験コードとデータを公開し（subjectivitylab/artificial_phantasia）、今後はマルチモーダルAIや新しい推論ベンチマークへの応用を予定している。</p>
]]></description>
      <pubDate>Wed, 08 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>オープンLLMの日本語性能でトップ──FLUX、「Flux Japanese LLM」公開　独自手法でQwen2.5を進化</title>
      <link>https://ledge.ai/articles/flux_japanese_llm_release</link>
      <description><![CDATA[<p>国内スタートアップのFLUX株式会社は2025年9月29日、日本語特化の大規模言語モデル「Flux Japanese LLM」を<a href="https://flux.jp/news/1093/">発表</a>した。</p>
<p>同モデルはAlibaba Cloudの大規模言語モデル「Qwen2.5-32B」を基盤に、日本語理解・生成性能を独自の新手法で強化したもので、Open Japanese LLM Leaderboard（通称：LLM勉強会ランキング）で総合スコア第1位（0.7417）を記録したという。</p>
<h2>日本語能力を高める新手法「Precise-tuning」とは</h2>
<p>FLUXは今回のモデル開発にあたり、従来のファインチューニングとは異なる「Precise-tuning（プリサイズチューニング）」手法を導入した。日本語データセット全体でパラメーターを再学習するのではなく、日本語能力強化に必要なネットワーク回路のみを特定して再調整することで、効率的かつ精度の高い言語理解を実現したとしている。</p>
<p><strong>FLUXが開発した「Precise-tuning」手法の概念図</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/1440_810_4_6b464affdc/1440_810_4_6b464affdc.jpg" alt="1440-810-4.jpg" /></p>
<h2>ベンチマークで国内首位に</h2>
<p>同モデルは、LLM勉強会ランキング<a href="https://ledge.ai/articles/open_japanese_llm_leaderboard">オープン日本語LLMリーダーボード</a>において総合スコア0.7417を記録し、他の日本語モデルを上回る評価を得たという。</p>
<p>このランキングは、日本の有志研究者・エンジニアによるコミュニティ「<a href="https://llm-jp.nii.ac.jp/">LLM-jp</a>（通称：LLM勉強会）」が運営しており、複数の日本語LLMを自然言語推論・要約・コード生成などのタスクで比較評価するオープンベンチマークとして知られる。</p>
<p>LLM-jpは、国立情報学研究所（NII）を事務局とする共同研究プロジェクトで、2024年4月にNII内に設立された大規模言語モデル研究開発センター（LLMC）と連携して、「日本語に強いオープンな大規模言語モデル」を開発・評価する活動を進めている。そのため、このランキングは国内の学術・産業両分野で日本語LLMの性能を客観的に測る基準として広く参照されている。</p>
<p><strong>Open Japanese LLM Leaderboardでの評価結果。Flux Japanese LLMが第1位を記録</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/News_main_240718_3_e93e607d1f/News_main_240718_3_e93e607d1f.jpg" alt="News_main_240718-3.jpg" /></p>
<p>モデルはHugging Face上で公開されており、<a href="https://huggingface.co/flux-inc/Flux-Japanese-Qwen2.5-32B-Instruct-V1.0">モデルカード</a>には、自然言語処理・要約・コード生成タスクでの性能指標や学習設計の概要が掲載されている。</p>
<h2>企業・業界別モデル展開へ</h2>
<p>FLUXは、「Flux Japanese LLM」を自社のノーコードAIプラットフォーム群と連携させる計画を進めており、金融業界向けの特化モデル開発も行っている。同社は「AIをすべての人の手に」をミッションに掲げ、企業・研究機関・行政などが安全にLLMを活用できる基盤づくりを目指している。</p>
]]></description>
      <pubDate>Wed, 08 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google、AIがPCを操作する「Gemini 2.5 Computer Use model」を開発者向けに公開──ClaudeやOpenAIモデルを上回る性能を実証</title>
      <link>https://ledge.ai/articles/google_gemini_2_5_computer_use_release</link>
      <description><![CDATA[<p>Google DeepMind は2025年10月7日（米国時間）、AI が実際のコンピューター画面を理解し、クリックや入力などの操作を実行できる新モデル「Gemini 2.5 Computer Use model」を開発者向けにプレビュー提供したと<a href="https://blog.google/technology/google-deepmind/gemini-computer-use-model/">発表</a>した。</p>
<p>Gemini API を通じて利用でき、AI が人間と同様にブラウザやアプリのUI（ユーザーインターフェース）を操作することを可能にする。</p>
<h2>Gemini API に“computer_use”ツールを追加</h2>
<p>今回発表された新モデルは、Gemini 2.5 の機能拡張として API に追加された「computer_use」ツールを用いて動作する。</p>
<p>AI はユーザーからの指示に加え、スクリーンショットと直近の操作履歴を入力として受け取り、次に取るべきアクション（クリック・入力・スクロールなど）を出力。実行結果を再び画面キャプチャとして取得し、目標達成までループ処理を行う。これにより、設定変更やフォーム入力、情報検索など、複数ステップを自律的に完了できる。</p>
<p>Google は公式ブログで、「このモデルはユーザー許可を前提に、安全性と透明性を重視して設計されている」と強調している。</p>
<p><strong>Computer Use model の処理ループ。AI がスクリーンショットと操作履歴をもとに次の行動を生成し、クライアント環境で実行 → 状況を再取得して次の判断へとつなげる</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/CTU_Diagram_RD_4_V01_width_1000_format_webp_b3415f41ee/CTU_Diagram_RD_4_V01_width_1000_format_webp_b3415f41ee.webp" alt="CTU-Diagram-RD4-V01.width-1000.format-webp.webp" /></p>
<h2>プレビュー提供と利用方法</h2>
<p>開発者は Google AI Studio および Vertex AI を通じて Computer Use model にアクセスできる。プレビュー版の段階では主にブラウザ操作に最適化されており、今後はより広範なアプリやデスクトップ環境への対応も検討されているという。</p>
<p>Google は、操作範囲やデータアクセスを制御する仕組みを組み込み、「責任ある自動化（Responsible Automation）」の実現を掲げている。</p>
<h2>ベンチマーク性能：Claude Sonnet 4.5 を上回る</h2>
<p>Google DeepMind は、Gemini 2.5 Computer Use model の性能を複数の標準ベンチマークで検証した。
Browserbase による Online-Mind2Web テストでは 65.7 % の精度を記録し、Claude Sonnet 4.5 や OpenAI Computer-Using Model を上回った。
さらに WebVoyager や AndroidWorld でも高スコアを達成し、実行速度（レイテンシ）でも優位性を示している。</p>
<p><strong>Gemini 2.5 Computer Use model は、Claude Sonnet 4.5 や OpenAI Computer-Using Model に比べ、低レイテンシかつ高精度を示した。</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/CTU_Scatterplot_RD_7_width_1000_format_webp_7e4e545c1c/CTU_Scatterplot_RD_7_width_1000_format_webp_7e4e545c1c.webp" alt="CTU-Scatterplot-RD7.width-1000.format-webp.webp" /></p>
<p><strong>複数ベンチマークで高い精度を記録。特に WebVoyager と AndroidWorld で際立ったスコアを達成した。</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/CTU_Benchmark_Chart_RD_5_V01_width_1000_format_webp_e0982edd33/CTU_Benchmark_Chart_RD_5_V01_width_1000_format_webp_e0982edd33.webp" alt="CTU-Benchmark_Chart-RD5_V01.width-1000.format-webp.webp" /></p>
<h2>動作デモ：AI がブラウザを自律操作</h2>
<p>公式ブログでは、実際の操作デモ動画も公開されている。
動画では AI が画面を認識し、ブラウザ上でリンクをクリックしたり、テキストを入力してタスクを完了する様子が確認できる。</p>
<p>@<a href="https://www.youtube.com/watch?v=_lu-FcPUIfM">YouTube</a></p>
<h2>AI による“手の届く自動化”へ</h2>
<p>今回の発表は、AI が人間の指示をもとに実際のUI を操作できる「エージェント時代」の幕開けを示す。
Google は Computer Use を “次世代の AI アシスタント” 開発の基盤と位置づけており、将来的には業務支援やウェブ操作、アプリ間連携など、より幅広い自動化領域への展開が期待される。</p>
]]></description>
      <pubDate>Wed, 08 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>GoogleのノーコードAIミニアプリ「Opal」日本を含む15カ国に拡大──ワークフロー可視化と高速化を提供</title>
      <link>https://ledge.ai/articles/google_opal_global_expansion</link>
      <description><![CDATA[<p>Googleは2025年10月7日（現地時間）、自然言語からウェブアプリを作成できるノーコードAIツール「Opal」の提供地域を、日本を含む15カ国へ拡大したと<a href="https://blog.google/technology/google-labs/opal-expansion/">発表</a>した。OpalはGoogle Labsの実験プロジェクトで、コードを書くことなくAIミニアプリを構築できるのが特徴だ。</p>
<h2>米国先行公開からグローバル展開へ</h2>
<p>Opalは7月24日（現地時間）に米国で<a href="https://ledge.ai/articles/google_opal_no_code_ai_tool">初公開</a>。ユーザーが「タスク管理アプリを作成」「画像から色を抽出」といった指示を与えると、AIがHTML/CSS/JavaScriptで構成されたアプリを自動生成する。初期ユーザーの利用が想定以上に高度化したことを受け、今回、日本、韓国、インド、カナダ、ブラジル、アルゼンチン、ベトナム、インドネシア、シンガポール、コロンビア、エルサルバドル、コスタリカ、パナマ、ホンジュラス、パキスタンの15カ国での提供を開始した。</p>
<h2>ワークフローの可視化と高速化</h2>
<p>同時に、Opalの実用性を高める改良が行われた。</p>
<ul>
<li><strong>高度デバッグ</strong> ：ノーコードのまま、ビジュアルエディタ上でワークフローをステップごとに実行・検証。エラーは発生ステップにリアルタイム表示され、原因特定を容易にする。</li>
<li><strong>パフォーマンス改善</strong> ：新規Opal作成の起動時間を短縮。**並列実行（parallel runs）**により複数ステップを同時に処理でき、待機時間を抑える。</li>
</ul>
<h2>クリエイティブから業務効率化まで</h2>
<p>Opalは、個人の創作活動やマーケティング支援、業務プロセスの自動化など、多様な用途に対応する。Googleは「ユーザーが複雑なプロセスを自動化したり、アイデアを素早く形にしたりできるよう支援する」としており、開発者コミュニティはDiscord上でも展開されている。</p>
<p>@<a href="https://youtu.be/g9RBGnz-vqk">YouTube</a></p>
]]></description>
      <pubDate>Wed, 08 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>学術＆研究2025/10/11 [SAT]Huawei、LLMの精度を保持したまま最大70%メモリ削減できる新手法を発表──コンシューマーGPUでの高精度生成AI実行も視野に</title>
      <link>https://ledge.ai/articles/huawei_sinq_quantization_llm</link>
      <description><![CDATA[<p>中国の大手テクノロジー企業Huawei（華為技術）は2025年9月26日、大規模言語モデル（LLM）を一般的なGPU環境でも高品質に動作させるための新しい量子化技術「Sinkhorn-Normalized Quantization（SINQ）」を<a href="https://www.arxiv.org/abs/2509.22944">発表</a>した。</p>
<h2>Sinkhorn正規化で“再調整なし”を実現</h2>
<p>従来のLLM量子化では、精度を維持するために一部データを用いて再調整（キャリブレーション）を行う必要があった。SINQはその工程を省略し、「再調整なし」で精度を保つ新しい方式だ。</p>
<p>仕組みの中核となるのが、「Sinkhorn-Knoppアルゴリズム」を応用した正規化手法である。モデルの重み行列に対して、行方向と列方向の2つのスケーリングベクトルを設定（dual-scaling）し、両軸の分散を均一化することで、外れ値（outlier）が特定の行や列に偏る問題を防ぐ。この工程により、量子化後の誤差を最小限に抑えられるという。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x1_818acd143f/x1_818acd143f.png" alt="x1.png" /></p>
<h2>精度を保ちながら70％のメモリ削減</h2>
<p>Huaweiの研究チームは、同社のQwen3モデル（1.7B〜235B）やDeepSeek-V2.5（236B）などで実験を行い、SINQが既存手法（RTN、HQQ、Hadamard変換など）を上回る精度を示したと報告している。</p>
<p>具体的には、4bit量子化時にパープレキシティ（文章予測精度）を最大40％改善し、メモリ使用量を最大70％削減。さらに、8GB程度の一般的なGPU上でQwen3-7Bモデルを実行できたとしている。処理時間も高速で、量子化プロセスは従来のRTN法に比べてわずか1.1倍。再調整を伴う手法（AWQやGPTQなど）よりも最大30倍速いという。</p>
<h2>幅広いモデルで動作、非一様量子化とも互換</h2>
<p>SINQは、Qwenシリーズだけでなく、Llama 2・Llama 3・DeepSeek-V3などの異なるモデルにも適用可能。
また、非一様量子化フォーマット（NF4）との併用でも精度を維持しており、調整を行うAWQと組み合わせた「A-SINQ」ではさらに高い性能を達成した。論文では、Mixture-of-Experts（MoE）構造の大型モデルでも安定して動作することが示されている。</p>
<h2>コンシューマーGPUでのLLM実行を視野に</h2>
<p>Huaweiは、SINQを「キャリブレーション不要の汎用量子化手法」と位置づけており、高性能GPUに依存しないLLM運用を可能にする技術として注目されている。論文著者らは、SINQの目的を「メモリ効率と速度を両立し、エッジデバイスでも高品質な生成AIを実行可能にすること」と説明している。</p>
<p>コードはGitHub上で<a href="https://github.com/huawei-csl/SINQ">公開</a>されており、研究者や開発者が自由に評価・応用できる環境が整っている。</p>
]]></description>
      <pubDate>Wed, 08 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Minecraft上で動作するAIチャットボット「CraftGPT」誕生──500万パラメータをレッドストーン回路で構築</title>
      <link>https://ledge.ai/articles/minecraft_ai_chatbot_craftgpt</link>
      <description><![CDATA[<p>人気ゲーム「Minecraft（マインクラフト）」の中で、実際に動作するAIチャットボットが登場した。
開発したのはゲームエンジニア兼クリエイターの Sammyuri氏。2025年10月1日、同氏はYouTubeで「I built ChatGPT with Minecraft redstone!（マインクラフトのレッドストーンでChatGPTを作った）」と題する動画を<a href="https://www.youtube.com/watch?v=VaeI9YgE1o8">公開</a>し、ゲーム内回路だけで大規模言語モデルを構築するプロジェクト「CraftGPT」を披露した。</p>
<h2>レッドストーンで構成されたAIモデル</h2>
<p>「CraftGPT」は、Minecraftのブロックと電気信号（レッドストーン）によって作られた小型の言語モデルだ。
動画では、入力された文字列に対して「Hi! How are you?」などの応答を生成する様子が紹介されている。演算は、ブロックごとに配置された論理回路が信号を受け渡しながら行われ、AIの“思考”を物理的なパルスとして可視化している。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/craft_gpt1_ee0a430c02/craft_gpt1_ee0a430c02.jpg" alt="craft gpt1.jpg" /></p>
<h2>500万パラメータ規模の「ゲーム内AI」</h2>
<p>Sammyuri氏によると、このAIモデルは 約5,087,280パラメータ の構成を持つ。
内部には、自然言語処理で用いられる「トークナイザー」「アテンション層」「行列演算」などの要素を模した回路が組み込まれているという。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/craft_gpt2_cc7dc4a387/craft_gpt2_cc7dc4a387.jpg" alt="craft gpt2.jpg" />
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/craft_gpt3_5fef40fd2e/craft_gpt3_5fef40fd2e.jpg" alt="craft gpt3.jpg" />
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/craft_gpt4_c1f6e020e7/craft_gpt4_c1f6e020e7.jpg" alt="craft gpt4.jpg" /></p>
<p>実装には高速動作環境「MCHPRS（Minecraft High Performance Redstone Server）」が使用され、1トークンの生成に数分を要する。リアルタイム対話には至らないが、AIの計算過程をブロック単位で表現する試みとして注目を集めている。</p>
<h2>巨大な“AI脳”の内部構造</h2>
<p>動画内では、CraftGPTの全体像も公開された。
構造は 約1,020×260×1,656ブロック に及び、総ブロック数はおよそ 4億3800万個。
一つひとつの回路がAIの重みや演算ノードを表しており、まるで“Minecraft上に広がる巨大な脳”のような光景を成している。
制作者は信号の流れを説明しながら、「AIが言葉を生み出す瞬間を、目に見える形で再現した」と語っている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/craft_gpt5_9140b45f1a/craft_gpt5_9140b45f1a.jpg" alt="craft gpt5.jpg" /></p>
<h2>実験的かつ教育的な意義</h2>
<p>CraftGPTは、実用的なAIチャットではなく、AIの内部構造を学び・観察するためのプロジェクトだ。
Sammyuri氏は動画の中で、「AIを単なるブラックボックスとして見るのではなく、その仕組みを楽しみながら理解できるものにしたかった」と述べている。
ゲームを舞台にしたこの試みは、AI技術と創造的表現の融合を示す作品としても評価されている。</p>
<p>YouTubeのコメント欄には、「AIの脳の中を歩いているようだ」「ここまで精密な回路を組むとは信じられない」といった驚きの声が寄せられている。</p>
<p>@<a href="https://www.youtube.com/watch?v=VaeI9YgE1o8">YouTube</a></p>
]]></description>
      <pubDate>Wed, 08 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ジョニー・アイブとサム・アルトマンが語る「AIと人間の新しい関係」──次世代デバイス構想をDevDay 2025で明かす</title>
      <link>https://ledge.ai/articles/openai_jony_ive_ai_device_philosophy</link>
      <description><![CDATA[<p>OpenAIのCEOであるサム・アルトマン氏と、Apple製品のデザインを手がけたジョニー・アイブ氏（LoveFrom代表）は、2025年10月に米サンフランシスコで開催された開発者会議「DevDay 2025」で<a href="https://www.youtube.com/watch?v=7cKbPLzNYws">対談</a>を行い、人間中心のAIデバイス構想を明らかにした。両氏は、テクノロジーがもたらす「過剰な情報と不安」を乗り越え、AIを通じて「穏やかで幸福な体験をもたらす新しいデバイス」を目指すと語った。</p>
<h2>パートナーシップの起点はChatGPT</h2>
<p>アイブ氏は、約30年間在籍したAppleを退社後、デザイナーや建築家などの専門家で構成するチーム「LoveFrom」を設立した。当初は明確な目標を持たずに活動していたが、ChatGPTの登場が転機になったという。
「ChatGPTを見たとき、私たちの目的が結晶化した」とアイブ氏は語る。この出会いが、サム・アルトマン氏との協業を決定づけた。両者は約3年前から新しいAIデバイスの構想を練り始めたという。</p>
<h2>「クラフト」と「ケア」に支えられた創造哲学</h2>
<p>アイブ氏は創造の出発点を「人類への愛」と表現し、ものづくりの根幹に「クラフト（職人技）」と「ケア（思いやり）」を置く。
彼は「人に見えない部分へのこだわりこそが、作り手の誠実さを示す」と述べ、細部まで丁寧に仕上げる姿勢を強調した。さらに「人々は、ケアが込められた製品を直感的に感じ取る。逆に、無頓着さ（ケアレスネス）は容易に見抜かれる」とも語った。</p>
<p>この“ケアの精神”をAIデバイスにも反映させ、「技術のための技術」ではなく、「人の幸福のためのテクノロジー」を設計することを目指すという。</p>
<p>@<a href="https://www.youtube.com/watch?v=7cKbPLzNYws">YouTube</a></p>
<h2>AIがもたらす「人間らしい関係」</h2>
<p>両氏が共通して強調したのは、AIを用いて人とテクノロジーの関係を再構築するというビジョンだ。
アイブ氏は、スマートフォンが2007年に登場して以降、人々がテクノロジーとの関係で「圧倒的な情報量と絶望感」に直面していると指摘。「AIはこの問題を悪化させるものではなく、正面から向き合うチャンスだ」と述べた。</p>
<p>理想とする体験として、彼は「必然性と自明性」「ユーモアと喜び」「穏やかさと幸福感」を挙げる。アルトマン氏も「AIは、人間とテクノロジーの関係を再設計する力を持つ」と応じ、「このプロジェクトは、人がテクノロジーを“感じる”新しい方法を探る実験だ」と語った。</p>
<h2>「誰もが初心者」──AI時代の創造者の課題</h2>
<p>アイブ氏は、急速に進化するAI分野では「誰もが初心者」であり、過去の経験が時に足枷になることもあると述べた。
「AIの勢いがあまりに速く、焦点をどこに定めるかが難しい。しかし、動機が“人類への愛”である限り、進むべき方向は見失わない」と語った。</p>
<h2>理念と現実、その間にある課題</h2>
<p>両氏が掲げた人間中心のビジョンは、AI時代のデバイス設計に新たな指針を与えるものだ。
一方で、<a href="https://www.ft.com/content/58b078be-e0ab-492f-9dbf-c2fe67298dd3">Financial Times</a>などによると、同プロジェクトはハードウェア面の設計やプライバシー制御などで難航しており、思想と実装のギャップにも注目が集まっている。
理念と現実の交差点に立つこの挑戦が、どのような形で結実するのかが問われている。</p>
<h2>「今を変えるチャンスがある」</h2>
<p>両氏は、AIデバイス開発の目的を「人を幸せで穏やかにし、不安を和らげること」に置いている。
アイブ氏はセッションの最後にこう締めくくった。</p>
<p>同氏はAIの可能性を「現状の延長ではなく、根本的な変化をもたらすもの」として位置づけた。</p>
]]></description>
      <pubDate>Wed, 08 Oct 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>世界初のAI女優「ティリー・ノーウッド」にハリウッドが揺れる──SAG-AFTRAや著名俳優が「創造性の危機」と警告</title>
      <link>https://ledge.ai/articles/tilly_norwood_ai_actress_controversy_sagaftra</link>
      <description><![CDATA[<p>世界初の「AI女優」と称される Tilly Norwood（ティリー・ノーウッド）が、ハリウッドの俳優や映画俳優組合SAG-AFTRA（全米映画俳優組合・テレビ・ラジオ芸術家連盟）から強い批判を受けている。</p>
<p>SAG-AFTRAは2025年9月30日に「創造性は人間中心であるべき」と声明を<a href="https://www.sagaftra.org/sag-aftra-statement-synthetic-performer">発表</a>した。女優のエミリー・ブラント氏やウーピー・ゴールドバーグ氏も相次いで懸念を表明し、映画・テレビ業界全体に議論が広がっている。</p>
<h2>「AI Commissioner」──ティリー誕生の舞台</h2>
<p>Tilly Norwoodは、ロンドン拠点の制作会社Particle6が2025年9月にYouTube上で公開したコメディスケッチ『AI Commissioner | Comedy Sketch | Particle6』で初登場した。</p>
<p>この作品は、テレビ業界におけるAIの急速な普及を風刺的に描いたもので、AIが脚本作成からキャスティング、予算編成まですべてを自動化する世界を描く。劇中では、AIが生み出したインタラクティブスリラー『I Know What You Streamed Last Summer』に登場する100％AI生成の俳優としてTillyが紹介される。</p>
<p>@<a href="https://www.youtube.com/watch?v=3sVO_j4czYs">YouTube</a></p>
<p>登場人物の1人はTillyについて「僕の言うことを何でも聞いてくれる。恋をしてしまったかもしれない（She’ll do anything I say. I think I’m in love）」と評し、AIによる従順で“最適化された”俳優像を象徴的に表現。
さらに、「“She’ll cry on Graham Norton and be monetized on TikTok by lunchtime.”（彼女はグレアム・ノートンの番組で泣いて、その日の昼までにTikTokで収益化されるだろう）」という台詞が、人間の感情までもがAIによって即座に商業化される未来への皮肉として話題を呼んだ。</p>
<h2>SAG-AFTRAが声明「Tillyは俳優ではない」</h2>
<p>SAG-AFTRAは9月30日に「Statement on Synthetic Performer（合成パフォーマーに関する声明）」を<a href="https://www.sagaftra.org/sag-aftra-statement-synthetic-performer">発表</a>し、次のように明言した。</p>
<p>\u003E“Tilly Norwood is not an actor. Creativity must remain human-centered.”
（ティリー・ノーウッドは俳優ではない。創造性は人間中心であるべきだ。）</p>
<p>声明では、AIによって作られた “合成俳優” が芸術表現を侵食する可能性を指摘し、「経験や感情を持たない存在を“俳優”と呼ぶことは、芸術の根幹を損なう」と警鐘を鳴らした。SAG-AFTRA会長のショーン・アスティン氏もVarietyの取材に対し、「AI倫理と補償問題を正式な交渉テーマとして扱う」と述べている。</p>
<h2>著名俳優からの反発</h2>
<p>女優のエミリー・ブラント氏は、Varietyのポッドキャスト番組でTillyの画像を見せられ、驚きを隠さずこう語った。</p>
<p>\u003E“Good Lord, we’re screwed. That is really, really scary. Come on, agencies, don’t do that. Please stop taking away our human connection.”
「なんてこと、私たちは終わりね。本当に恐ろしいわ。お願い、エージェントはそんなことをやめて。人間のつながりを奪わないで。」</p>
<p>また、取材でTillyを「次のスカーレット・ヨハンソンに」と問われると、「“But we have Scarlett Johansson.”（でも私たちにはスカーレット・ヨハンソンがいる」 と返し、人間俳優の価値を強調した。</p>
<p>俳優・司会者のウーピー・ゴールドバーグ氏も、ABCのトーク番組『The View』（9月30日放送）で次のように発言した。</p>
<p>\u003E“You’re looking at 5,000 actors rolled into one synthetic person. That’s not fair. We all move differently.”
「5,000人の俳優の特性をひとつの合成存在にまとめるなんてフェアじゃない。私たちは皆、動きも表情も違う。」</p>
<h2>開発側の見解：「人間の代替ではなく、芸術表現」</h2>
<p>Tillyを制作したエライン・ファン・デル・フェルデン氏（Eline Van der Velden）は9月28日、ティリーのInstagram（<a href="https://www.instagram.com/tillynorwood/">@tillynorwood</a>）に投稿し、AI俳優の創作意図について次のように述べている。</p>
<p>\u003E “She is not a replacement for a human being, but a creative work — a piece of art.”
「ティリーは人間の代替ではなく、創造的な作品＝ひとつのアートです。」</p>
<p>同氏は、AIを「人間の代わり」ではなく「新しい絵筆のようなツール」と位置づけ、「アニメーションや人形劇、CGIがライブ演技を奪うことなく新しい可能性を開いたように、AIも物語を構築する新たな手段を提供する」と説明した。</p>
<p>\u003E “I’m an actor myself, and nothing — certainly not an AI character — can take away the craft or joy of human performance.”
「私は俳優でもあり、AIキャラクターであっても、人間の演技の技や喜びを奪うことはできません。」</p>
<p>投稿ではさらに、AIを“人間と競わせる存在”ではなく“芸術の新しいジャンルの一部”として評価すべきだと訴えている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/instagram_eline_at_tillynorwood_b80dee2451/instagram_eline_at_tillynorwood_b80dee2451.jpg" alt="instagram eline at tillynorwood.jpg" /></p>
<p>同氏が率いるParticle6は「AI俳優を活用すれば制作コストを最大90％削減できる」と説明し、複数のAIタレントを育てる「デジタル・タレント・ユニバース」構想を掲げている。</p>
<h2>今後の焦点</h2>
<p>各報道や関係者の声明からは、主に次の3点が論点として浮上している。</p>
<ul>
<li>著作権と肖像権：AI俳優の訓練データやモデル構築に使用された素材の扱い</li>
<li>契約・補償問題：AI使用を前提とした俳優契約の新たな枠組み</li>
<li>文化的受容：AIが「演技」を行うことを人々がどう受け入れるか</li>
</ul>
<p>SAG-AFTRAをはじめとする業界団体や俳優たちは、AIが創作活動や雇用に及ぼす影響を注視しており、今後は法制度や契約の整備を含めた議論が進む見通しだ。</p>
]]></description>
      <pubDate>Wed, 08 Oct 2025 05:50:00 GMT</pubDate>
    </item>
  </channel>
</rss>