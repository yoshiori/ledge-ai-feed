<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
  <channel>
    <title>Ledge.ai 新着記事</title>
    <link>https://ledge.ai/</link>
    <description>Ledge.ai の最新テクノロジー記事</description>
    <item>
      <title>ビジネス2025/12/12 [FRI]OpenAI、「エンタープライズAIの現状 2025」公開──ChatGPT Enterpriseで1日40〜60分短縮、導入企業は100万社超に</title>
      <link>https://ledge.ai/articles/enterprise_ai_2025_openai_report</link>
      <description><![CDATA[<p>OpenAIは2025年12月8日、企業におけるAI活用の最新動向をまとめたレポート「The state of enterprise AI 2025」を<a href="https://openai.com/index/the-state-of-enterprise-ai-2025-report/">発表</a>した。調査では、ChatGPT Enterpriseの利用が業務の中心に組み込まれつつあり、従業員が1日あたり平均40〜60分の作業時間を節約していることが確認された。</p>
<p>同レポートは企業向け利用データと、約100社・9,000人を対象にした調査を基に作成されており、AI導入が「実験段階」から「本格運用」へと移行している現状を示す内容となっている。（詳細なレポートは<a href="https://cdn.openai.com/pdf/7ef17d82-96bf-4dd1-9df2-228f7f377a29/the-state-of-enterprise-ai_2025-report.pdf">こちら</a>のPDFから閲覧可能）</p>
<p>レポートによれば、OpenAIはすでに100万以上のビジネス顧客を抱え、ChatGPT Enterpriseの席数は700万を超えた。メッセージ数やカスタムGPTの利用も急増しており、特に独自ワークフローの自動化やナレッジ検索など、企業固有の業務に深くAIが統合されているという。</p>
<p>さらに、AI活用の“深さ”と生産性の向上には明確な相関があることも示された。AIを集中的に利用する従業員ほど、週あたりの時間節約量が大きい傾向が見られた。</p>
<p><strong>■ AIの利用量が多いほど、週あたりの節約時間が増加する：</strong> 週10時間以上を節約しているグループは、ほとんど節約していない層の約8倍のクレジットを利用
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Productivity_gains_increase_with_intensity_of_AI_use_22bd78f29d/Productivity_gains_increase_with_intensity_of_AI_use_22bd78f29d.jpg" alt="Productivity gains increase with intensity of AI use.jpg" /></p>
<p>業種別に見ると、AI導入の速度と規模にも大きな差が浮き彫りになった。特にテクノロジー業界が突出しており、利用規模も成長率も最大だった。一方で、ヘルスケアや製造、建設など、従来デジタル化が進みにくいとされていた業界でも急速に導入が進んでいる。</p>
<p><strong>■ 業種別：AI利用規模と前年比成長率</strong> テクノロジーが規模・成長率とも最大。ヘルスケア・製造が急伸し、金融・プロフェッショナルサービスは高い安定利用を示す
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/AI_adoption_by_industry_b7cdebd753/AI_adoption_by_industry_b7cdebd753.jpg" alt="AI adoption by industry.jpg" /></p>
<p>企業間でも、AI活用の深度には明確な“格差”が存在する。特に「フロンティアワーカー」と呼ばれる AI 活用上位5％の従業員は、AIへの依存度が全く異なる。フロンティア層は高度な分析や推論タスクを日常的にこなし、中位層との間に大きな使用量の差が生じていることが確認された。</p>
<p><strong>■ フロンティアワーカーと中央値ユーザーの利用格差</strong> メッセージ量は6倍、データ分析関連のメッセージは16倍と、格差は高度なタスクほど拡大
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Worker_usage_gaps_widen_with_more_advanced_tools_95836d5bc8/Worker_usage_gaps_widen_with_more_advanced_tools_95836d5bc8.jpg" alt="Worker usage gaps widen with more advanced tools.jpg" /></p>
<p>OpenAIは総括として、AIを幅広い業務で深く活用できる企業ほど、時間短縮だけでなく新しい業務遂行能力を獲得していると指摘する。AIが単なる生産性向上ツールから、企業の競争力を左右する「中心インフラ」へと位置づけが変わりつつあることが、調査により浮き彫りとなった。</p>
<p>単なる効率化ツールから、企業基盤となるインフラへ──AI導入のフェーズは次の段階へ移り始めている。</p>
]]></description>
      <pubDate>Fri, 12 Dec 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>【AI歴史年表】AIはダートマス会議から数えて来年で70周年！起源から生成AI革命までのAI全史を振り返る──Ledge.ai年末年始特集「&apos;25 to &apos;26」から注目コンテンツを特別公開！</title>
      <link>https://ledge.ai/articles/70year_history_of_ai_from_the_dartmouth_conference</link>
      <description><![CDATA[<p>:::button
<a href="https://25to26.ledge.ai/">Ledge.ai年末年始特集サイトはこちら</a>
:::</p>
<p>2026年は、人工知能（AI）研究が正式に始動した歴史的な瞬間、1956年のダートマス会議から70周年という記念すべき節目を迎える。この70年間、AIは期待と幻滅の波を乗り越え、ついに人類の創造性を拡張する「生成AI」の時代へと到達した。この壮大な進化の軌跡を、各時代のエポックメイキングな出来事とともに紹介する。</p>
<h2>1. AIの誕生、最初の挫折と基礎構築 (1956–1979)</h2>
<p>人工知能（AI）は、1956年のダートマス会議でJ.マッカーシーらによって正式に分野として確立された。このダートマス会議で若手の中心となったJ.マッカーシー、M.ミンスキー、A.ニューウェルの3人はいずれも1927年生まれ、30歳を少し過ぎたところだ。ダートマス会議後、この3人はそれぞれスタンフォード大学、MIT、カーネギーメロン大学で活動し、AI研究の世界的な拠点が形成されていく。</p>
<p>初期の成功として、1958年にはF.ローゼンブラットが脳を模倣した初の学習可能モデルであるパーセプトロンを発表し、1966年にはJ.ワイゼンバウムが初の対話型システムであるELIZAを開発した。また、NNの学習においては、1967年に甘利俊一が確率的勾配降下法という後のディープラーニングの基礎となる最適化手法を発表するなど、技術的な基盤も築かれ始めていた。</p>
<p>しかし、この楽観的なブームは短期間で終焉を迎える。1969年、M.ミンスキーらがパーセプトロンの限界証明を行い、単層NNでは複雑な問題が解けないことを示唆した結果、AI研究への資金が大幅に削減され、最初の「冬の時代」が到来した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/01_70_AI_2ddf249970/01_70_AI_2ddf249970.jpg" alt="表01_70AI.jpg" /></p>
<p>この停滞期においても、後のAIの土台となる研究は継続された。日本では、1972年に甘利俊一が脳の記憶を模倣した連想記憶モデルを発表し、1979年には福島邦彦が、後の畳み込みニューラルネットワーク（CNN）の原型となるネオコグニトロンという階層型のNNモデルを開発した。この時期の日本の研究者の貢献は、AIの次の飛躍に向けた重要な種を蒔いたと言える。</p>
<p>:::button
<a href="https://25to26.ledge.ai/">Ledge.ai年末年始特集サイトはこちら</a>
:::</p>
<h2>2. AI研究の転換期における三つの潮流 (1980–1996)</h2>
<p>1980年代から1990年代前半にかけて、AI研究は、1970年代の停滞期を脱するため、異なる哲学に基づいた三つの潮流が並立した。</p>
<p>まず、記号主義（知識ベース）AIの流れを極限まで推し進めようとする試みがあった。その代表が1984年にD.レナートによって開始されたCycプロジェクトである。このプロジェクトは、人間が持つ膨大な常識をすべて手作業で知識ベースに構築し、究極のエキスパートシステムを実現することを目指した。これは、記号主義AIの可能性を探る壮大な挑戦であったが、同時に知識を形式化し獲得することの難しさを浮き彫りにした。</p>
<p>次に、長らく停滞していたニューラルネットワーク（NN）研究が息を吹き返した。この復活は、1982年にJ.ホップフィールドが、甘利俊一の先行研究と同系統の連想記憶モデルを、統計物理学の手法を用いて再発見したことに端を発する。これにより、NNが「記憶」のメカニズムを持つことが示唆された。決定的なブレイクスルーとなったのは、1986年にG.ヒントンらが多層NNを効率的に学習させる誤差逆伝播法（Backpropagation）を普及させたことである。この手法の登場は、NNが単層の限界を乗り越え、複雑なパターン認識を扱えるようになる第2次NNブームを牽引した。</p>
<p>そして第三の潮流として、従来の複雑な推論中心のAIに異を唱える行動ベースAIが登場した。1991年、iRobotの創業者でもあるR.ブルックスは、包摂アーキテクチャという新しい考え方を提唱し、その具体例として小型六本足ロボットのGenghisを開発した。これは、中央の知識ベースを持たず、環境からのセンサー情報に基づいて直接行動することで、現実世界でのタスク実行を重視するアプローチである。この研究は、AI研究の主流を、抽象的な推論から知覚と行動の統合へとシフトさせるきっかけとなり、その後のロボット工学に大きな影響を与えた。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/02_70_AI_204dc3e658/02_70_AI_204dc3e658.jpg" alt="表02_70AI.jpg" /></p>
<p>このように、1980年代から90年代前半は、知識ベースの限界と挑戦、NNの劇的な復活、そして現実世界指向の新しいパラダイムの誕生という、複数の試行錯誤を通じて、後のAI発展の基礎が築かれた重要な転換期であったと言える。</p>
<p>:::button
<a href="https://25to26.ledge.ai/">Ledge.ai年末年始特集サイトはこちら</a>
:::</p>
<h2>3. データと計算力によるAIの夜明け～ディープラーニングの衝撃 (1997–2016)</h2>
<p>1990年代後半から2010年代にかけてのAI研究は、インターネットの普及によるデータの爆発的な増加と、計算機能力の飛躍的な向上という二つの外部要因に強く支えられた。この時期、AIは「知識ベース」から「データ駆動」へとパラダイムを完全に転換し、特定のタスクで人間の能力を超える成果を上げ始めた。</p>
<p>まず、AIは特定の知的ゲームにおいて、人間を凌駕する能力を示した。1997年には、IBMのDeep Blueがチェスの世界チャンピオン、ガルリ・カスパロフに勝利し、「探索と計算」に特化したAIの能力を世界に示した。この頃、インターネットの本格的な普及は、AI研究の間接的な基盤を構築していた。Web上の膨大な情報（ビッグデータ）を分析するデータマイニングや統計的手法が発展し、AI研究も経験的なデータから知識を抽出する方向に傾倒していった。</p>
<p>AIに真のブレイクスルーをもたらしたのは、ニューラルネットワーク（NN）の進化であった。2006年、G.ヒントンらがディープラーニングを提唱し、多層NNを効率的に学習させる手法（深層化）に成功した。これは、増大するインターネット上のビッグデータを扱うために、極めて重要な進歩であった。この技術の有効性は、様々な分野で証明され始めた。2011年には、IBMのWatsonが、膨大な非構造化データ（書籍、記事など）から答えを導き出す能力により、米国の人気クイズ番組『ジェパディ！』で歴代チャンピオンに勝利した。そして2012年、ヒントンらが開発したAlexNetが、大規模な画像認識コンテスト（ILSVRC 2012）で圧倒的な性能を見せつけ、ディープラーニングが画像認識の主流技術となることを決定づけた。</p>
<p>この時期のAI研究の集大成となったのが、Google DeepMindによるAlphaGoの成功である。2016年、AlphaGoは、人間の直感と深い洞察力が求められる囲碁において、世界トップ棋士であるイ・セドル九段に勝利した。この勝利は、チェスのような「探索」だけでなく、「直感的な判断」が必要とされる領域でもAIが人間を超越したことを意味し、ディープラーニングと強化学習を組み合わせたAIが、人類の「知性の最後の砦」の一つを突破した歴史的な瞬間であった。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/03_70_AI_00677c2be5/03_70_AI_00677c2be5.jpg" alt="表03_70AI.jpg" /></p>
<p>この1997年から2016年にかけて、AIはインターネットによって供給されるデータと、高性能なGPUによって可能になった計算力を武器に、ディープラーニングという核技術を獲得し、次の「生成AI」時代への道筋を明確に作ったのである。</p>
<p>:::button
<a href="https://25to26.ledge.ai/">Ledge.ai年末年始特集サイトはこちら</a>
:::</p>
<h2>4. 生成AI革命の勃発 (2017–2022)</h2>
<p>2017年から2022年の期間は、AI研究史上最も劇的な変革期であり、技術的なブレイクスルーと、それによる生成AI革命の勃発が特徴である。</p>
<p>この変革の起点は、2017年にGoogleが発表したTransformerモデルにあった。このモデルは、入力データ内の重要度を把握する「注意機構（Attention）」を採用し、計算を並列処理できるようになったため、大規模で深いネットワークの学習を可能にし、大規模言語モデル（LLM）の時代の扉を開いた。</p>
<p>このアーキテクチャを基盤に、2018年にOpenAIがGenerative Pre-trained Transformer（GPT）を開発し、LLMの基礎を築いた。さらに2020年には、モデルを大きくするほど性能が向上するというスケーリング則が確立され、LLMの巨大化戦略が主流となった。また、同年には拡散モデルが実用化され、高精度な画像生成AIの道も開かれた。</p>
<p>そして2022年、AIは一気に社会へ浸透した。LLMの推論能力を飛躍的に高める思考の連鎖（CoT）などの手法が開発される一方、Midjourneyなどの対話型画像生成AIが普及した。極めつけは、同年後半にOpenAIからリリースされたChatGPTである。人間と遜色ない自然な対話能力を持つChatGPTは、リリース後わずか約2か月で月間アクティブユーザー数1億人を突破し、生成AIブームを世界中に巻き起こした。</p>
<p>一方で、技術の急速な発展に伴い、AIの倫理と安全性に関する議論も本格化した。2017年にはアシロマ会議が開かれ、AIの安全な開発と利用に向けた「アシロマAI 23原則」が策定されたことも、この時期の重要な出来事である。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/04_70_AI_28c3fdc49a/04_70_AI_28c3fdc49a.jpg" alt="表04_70AI.jpg" /></p>
<p>この5年間で、AIは「認識」から「創造」の領域へと能力を拡張し、人類の生活を一変させる新たなステージへと進んだのである。</p>
<p>:::button
<a href="https://25to26.ledge.ai/">Ledge.ai年末年始特集サイトはこちら</a>
:::</p>
<h2>5. 70周年の展望：AIと人類の未来 (2023年～)</h2>
<h3>5-1. 2023～2025年におけるAIと人類の状況</h3>
<p>2023年から2025年は、生成AI（Generative AI）技術が社会全体に急速に浸透した「AIの実用化元年」とも呼ぶべき変革期であった。特に大規模言語モデル（LLM）の進化により、AIは単なる自動化ツールから、人間の協働者や代行者へとその役割を急速に拡大した時期である。</p>
<p>日常生活においては、AIはスマートフォンや家電に深く組み込まれ、ルーティン作業の自動化、情報検索の高度化、そして個別化された健康管理の提供を通じて、人々の生活効率を向上させた。教育分野では、AIチューターや個別学習プログラムの利用が一般化し、生徒一人ひとりの進捗に合わせたカスタマイズ教育が主流となった一方で、教師は教材作成や評価の負担が軽減され、より対話的な指導に注力できるようになった。エンターテイメント領域では、AIによる画像、音楽、動画の生成が爆発的に増加し、コンテンツ制作の民主化が進んだ。また、AIを活用したパーソナライズされたゲーム体験や、没入型のMR/VRコンテンツも普及した。</p>
<p>ビジネスにおいては、AI導入が業務効率を大幅に向上させ、産業構造の再編を促した。ここでは利用形態に明確な対比が見られた。一つは、コーディングや文書作成などの専門作業において人間の作業を支援・加速するコパイロット型AIであり、これは人間の意思決定が最終的に介在する協調的な形態である。もう一つは、人間からの指示を基に複数のタスクを自律的に計画・実行し、ビジネスプロセスや顧客対応を代行・自動化するAIエージェントの進化である。この対比は、業務におけるAIの自律性の度合いを示す重要な指標となった。</p>
<p>政治においては、AIによる情報分析と政策立案支援が進み、行政の効率化が図られた。しかし同時に、AIが生成するディープフェイクや誤情報が選挙や世論形成に与える影響が重大な社会問題となり、各国でAIの倫理的利用と規制に関する議論が加速した。この時期、人類はAIの利便性を享受しつつも、その倫理性、安全性、社会への影響に対する向き合い方を確立する過渡期にあるのが現状である。</p>
<h3>5-2. 未来のAI：相乗効果と応用のグランドビジョン</h3>
<p>未来のAI技術 (AI_future) の進化は、現在の技術の単なる延長ではない。それは、基礎技術の「積」による指数関数的な相乗効果と、応用領域の「和」による社会的な価値の最大化によって実現されるビジョンである。
Ledge.aiでは、このビジョンを、以下のように定式化して考えている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/ai70th_formula_ff6d7ead4c/ai70th_formula_ff6d7ead4c.png" alt="ai70th_formula.png" /></p>
<h3>■SYNERGY（相乗効果：積の力）による性能の飛躍</h3>
<p>現在のAI技術 (AI_current) は、四つの主要な基礎技術が掛け合わされる（積Π）ことで、その性能を劇的に高める。これらの技術は、それぞれがAIの抱える限界を突破する鍵となる。</p>
<ul>
<li><strong>量子コンピューター (Quantum) :</strong> AIの処理速度と複雑な問題解決能力に演算能力のブレイクスルーをもたらし、現行のスーパーコンピューターでは不可能な領域の学習と計算を可能にする。</li>
<li><strong>Web3 技術 (Web3):</strong> ブロックチェーンや分散型台帳技術により、AIが扱うデータと意思決定プロセスに透明性と信頼性を与え、分散化された環境での安全なAI連携を実現する。</li>
<li><strong>核融合エネルギー (Fusion):</strong> ほぼ無限かつクリーンなエネルギー源を提供することで、大規模な計算資源の制約を完全に緩和し、膨大なデータを用いた学習（超大規模モデル）を経済的かつ環境負荷なく実行可能にする。</li>
<li><strong>データインフラ (DataInfra):</strong> 5G/6Gや次世代ストレージ技術が実現する高速・大容量データ処理基盤が、AIのリアルタイムな学習と推論を支える。</li>
</ul>
<p>これらの技術が個別に進化するのではなく、相互に作用し合う（積）ことで、AIはこれまでにないレベルの知性を獲得する。</p>
<h3>■APPLICATION（応用価値：和の力）による社会実装</h3>
<p>性能が飛躍的に向上したAIは、様々な応用領域へ展開され、その価値を社会へ還元する。これらの応用領域は、AIの価値を社会的効用として積み重ねていく（和Σ）役割を担う。</p>
<ul>
<li><strong>Robotics (ロボティクス):</strong> 高度な知性を持つAIが、物理的な世界で活動するロボットと統合され、自動化・遠隔操作・協調作業を飛躍的に進化させる。</li>
<li><strong>MR (複合現実):</strong> AIがMR環境を分析・最適化し、人間とAIが直感的かつシームレスに連携する新たなインターフェースと作業空間を提供する。</li>
<li><strong>Autonomous Driving (自動運転):</strong> 複雑で予測不可能な環境においても、AIがリアルタイムに安全な判断を下し、交通システム全体を最適化することで社会インフラを革新する。</li>
<li><strong>Social Engineering (社会システムへの適用):</strong> 都市計画、医療、教育などの大規模な社会システムにAIが組み込まれ、データの分析と最適化を通じて社会全体の効率と公平性を向上させる。</li>
</ul>
<p>結論として、未来のAIは、基礎技術の「積」によって知性の限界を超え、応用領域の「和」を通じて私たちの生活、産業、そして社会構造そのものを根本から変革するグランドビジョンである。</p>
<p>金融分野でのいわゆる”AIバブル”は、早晩弾ける可能性がある。しかし、AI技術は着実な進歩が予想される。このようなグランドビジョンのもと、人類とAIの未来を創造していただければ幸いである。</p>
<p>:::button
<a href="https://25to26.ledge.ai/">Ledge.ai年末年始特集サイトはこちら</a>
:::</p>
<h2>おわりに</h2>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/25to26_thumb_ced6fcf9a3/25to26_thumb_ced6fcf9a3.png" alt="25to26_thumb.png" /></p>
<p>Ledge.ai年末年始特集では、読者に向けて、2025年のAI関連の重要トレンドを振り返り、また2026年以降のAIの展望について発信している。新しい年へ動き出すための情報が詰まっているので、ぜひ以下ボタンより特集サイトをご覧いただきたい。</p>
<p><strong>【コンテンツ情報】</strong>
無料登録を行うと、これらすべての記事を閲覧できるようになる。
■ 特別インタビュー
京都賞受賞のAIのパイオニア甘利俊一先生をはじめ、量子コンピュータ/量子機械学習からロボット基盤モデル、話題の”PLURALITY”、NVIDIA、AMDなど、”いま読んでおくべき”インタビューが満載
■ 2025年のAI動向総ざらい
■ 厳選注目記事49本</p>
<p>:::button
<a href="https://25to26.ledge.ai/">Ledge.ai年末年始特集サイトはこちら</a>
:::</p>
]]></description>
      <pubDate>Thu, 11 Dec 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Adobe、ChatGPTでPhotoshop・Express・Acrobatを提供開始──対話だけで画像編集やPDF作業が可能に</title>
      <link>https://ledge.ai/articles/adobe_apps_for_chatgpt_photoshop_express_acrobat</link>
      <description><![CDATA[<p>Adobe、ChatGPTでPhotoshop・Express・Acrobatを提供開始──対話だけで画像編集やPDF作業が可能に</p>
<p>Adobeは2025年12月10日（米国時間）、ChatGPT上で同社の主要アプリを利用できる「Adobe Apps for ChatGPT」の提供を開始したと<a href="https://news.adobe.com/ja/news/2025/12/20251211-adobe-photosop-express-acrobat-chatgpt">発表</a>した。対象となるのは「Adobe Photoshop」「Adobe Express」「Adobe Acrobat」の3製品で、ChatGPTとの会話を通じて画像編集やデザイン作成、PDF編集などを直接実行できる。</p>
<p>@<a href="https://www.youtube.com/watch?v=uouNjFuJ3QU">YouTube</a></p>
<p>同社によると、ChatGPT上で「この画像の背景をぼかしてほしい」「このPDFの文章を修正してほしい」といった自然文で指示するだけで、各アプリの機能が呼び出され、編集が行われる。従来のようにアプリを起動して操作する必要はなく、会話の文脈を理解したうえでタスクを実行する点が特徴だ。</p>
<p>Photoshopでは、明るさやコントラストの調整、オブジェクトの編集、グリッチやグローといったクリエイティブエフェクトの適用などが可能となる。画質を保ったまま編集でき、スライダーなどの直感的なUIを用いた調整にも対応する。</p>
<p><strong>■「背景にクリエイティブな効果を追加して」と指示するだけで、Photoshopが編集を実行する</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/media_157c8bf8df2f28f50a647a42c5d3d9ef436a1faa5_168c1e2ea5/media_157c8bf8df2f28f50a647a42c5d3d9ef436a1faa5_168c1e2ea5.jpg" alt="media_157c8bf8df2f28f50a647a42c5d3d9ef436a1faa5.jpg" /></p>
<p>Adobe Expressでは、テンプレートを活用したデザイン作成やテキスト・画像の差し替え、アニメーションの追加などをChatGPT上で行える。編集内容を対話的に修正しながら仕上げていくことができ、デザイン経験の少ないユーザーでも扱いやすい構成となっている。</p>
<p><strong>■ Adobe Expressでは、ChatGPTとの対話を通じてテンプレート選択やデザイン作成が可能になる</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/media_15d631a2f2b908e1ebffd15a9cd75d6d591838062_9927f6fc0a/media_15d631a2f2b908e1ebffd15a9cd75d6d591838062_9927f6fc0a.jpg" alt="media_15d631a2f2b908e1ebffd15a9cd75d6d591838062.jpg" /></p>
<p>Acrobatでは、PDFの編集やテキスト・表の抽出、複数ファイルの統合、圧縮、形式変換などに対応する。レイアウトや体裁を維持したまま編集できるほか、機密情報の修正といった用途にも利用できるという。</p>
<p><strong>■ ChatGPT上でPDFを直接編集。Adobe Acrobatの編集機能を対話形式で利用できる</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/media_1fe39d2bd0158dc797544d0d694b43d51bc6bea35_5644bc6742/media_1fe39d2bd0158dc797544d0d694b43d51bc6bea35_5644bc6742.jpg" alt="media_1fe39d2bd0158dc797544d0d694b43d51bc6bea35.jpg" /></p>
<p>利用環境はChatGPTのWeb版、デスクトップ版、iOS版で、料金は無料。利用にあたってはAdobeアカウントとの連携が必要となるが、無料のAdobe IDで利用できる。Adobe ExpressはAndroidでもすでに利用可能で、PhotoshopとAcrobatのAndroid対応は近日中を予定している。</p>
<p>なお、ChatGPT上で利用できるのは基本的な編集機能に限られ、高度な編集や最終的な仕上げが必要な場合は、ChatGPTから各Adobeアプリにシームレスに移行して作業を継続できる。</p>
<p>Adobeは本取り組みを、同社が進めるエージェント型AI戦略の一環と位置付けている。Model Context Protocol（MCP）を活用し、ユーザーの意図を理解した上で適切な操作を実行する対話型体験の拡張を進めてきた。2025年には「Acrobat AI Assistant」や「Photoshop AI Assistant」なども発表しており、今回のChatGPT連携はその延長線上にある。</p>
<p>Adobeのデジタルメディア事業部門プレジデントを務めるデイビッド・ワドワーニ氏は、「世界中のChatGPTユーザーに、PhotoshopやAcrobat、Expressの機能を直接届けられることをうれしく思う。人々が自分の言葉だけで、簡単にアイデアを形にできるようになる」とコメントしている。</p>
]]></description>
      <pubDate>Thu, 11 Dec 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>学術＆研究2025/12/12 [FRI]諸説あるAGIの定義をスタンフォード大などが明確化、「教養ある成人」がモデルケース──GPT-4は27%、GPT-5は約6割に到達と試算</title>
      <link>https://ledge.ai/articles/agi_definition_stanford_gpt4_27_gpt5_60</link>
      <description><![CDATA[<p>スタンフォード大学やUCバークレー、MITなどの研究者を含む国際チームは、AIの到達度を測る新しい枠組みを提示した論文「A Definition of AGI」を<a href="https://arxiv.org/abs/2510.18212v3">公開</a>した。これまで曖昧だったAGI（汎用人工知能）の定義について、同チームは「教養ある成人（well-educated adult）の認知の幅と熟達度にマッチするAI」と明確化。加えて、人間の知能研究で広く使われるCHC（Cattell-Horn-Carroll）理論に基づき、10種類の認知ドメインでAIモデルを評価する「AGIスコア」を提案した。</p>
<p>今回の枠組みに基づく試算では、GPT-4は27%、GPT-5は約57〜58%に到達しているとされる（100%が「教養ある成人の平均レベル」に相当）。論文のPDF版では57%、公式サイトや解説資料では58%と表記されており、約6割前後の水準と整理できる。</p>
<h2>AGIをめぐる“動くゴールポスト”問題を解消へ</h2>
<p>研究チームが強調するのは、AGIという用語の曖昧さだ。企業・研究者・メディアで異なる意味合いで使われてきた結果、実際にどの能力を満たせばAGIと呼べるのかが明確でなく、「モデルが進化するたびにゴールが後ろにずれる」として批判もあった。</p>
<p>Center for AI Safety（CAIS）はニュースレターの中で、この曖昧さが研究目標、社会的リスク評価、政策議論を複雑化してきたと指摘している。今回の提案は、こうした状況に対し「包括的でテスト可能な定義」を与える試みと位置づけられる。</p>
<h2>10の認知ドメインを10%ずつ評価する「AGIスコア」</h2>
<p>論文が提示する枠組みでは、人間の認知能力を10の中核ドメインに分解し、それぞれを「10%」の重みで評価する。ドメインは以下の通り。</p>
<ul>
<li>一般知識</li>
<li>読解・文章</li>
<li>数学</li>
<li>推論（問題解決・抽象化）</li>
<li>作業記憶・注意制御</li>
<li>長期記憶の保存と検索</li>
<li>視覚処理</li>
<li>聴覚処理</li>
<li>処理速度</li>
<li>中央実行系（複数能力の統合）</li>
</ul>
<p>研究チームはこれらの領域を、人間の心理測定バッテリー（知能検査など）の形式に合わせて評価タスク化。AIモデルを人間の標準スコアにマッピングしたうえで、各分野を合成して「AGIスコア（0〜100%）」として算出する。</p>
<p><strong>■ AGIを構成する10の認知ドメイン（CHC理論に基づく）：</strong> 一般知識・読解・数学・推論・記憶・知覚・速度など、人間の認知を広範にカバーし、各領域を10%ずつ評価してAGIスコアを算出する。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x2_8_1eb9f5c414/x2_8_1eb9f5c414.png" alt="x2 (8).png" /></p>
<p>この枠組みの特徴は、一部の領域だけ優れていてもAGIとは見なされない点にある。たとえば「読解や知識」では人間レベルであっても、「長期記憶」や「視覚処理」が極端に低ければ、スコアは上がらない。</p>
<h2>GPT-5は約6割──強い領域と“ほぼゼロ”の領域が共存</h2>
<p>論文および公式サイトのデータによると、GPT-4とGPT-5には以下のような特徴がある。</p>
<ul>
<li>GPT-4：AGIスコア27%</li>
<li>GPT-5：AGIスコア約57〜58%（約6割）</li>
</ul>
<p><strong>■ GPT-4 と GPT-5 の10ドメイン能力比較：</strong> GPT-5 は読解・数学・推論などで高スコアを示す一方、長期記憶や視覚・聴覚処理などの基盤能力は依然として低く、“ギザギザ”なプロファイルが特徴となっている。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x1_5c3108a876/x1_5c3108a876.png" alt="x1.png" /></p>
<p>高いスコアを示すのは「一般知識」「読解・文章」「数学」「推論」といったデータ駆動型の領域で、人間を上回るケースもある。一方で、</p>
<ul>
<li>長期記憶の保存・検索</li>
<li>視覚的推論</li>
<li>聴覚処理</li>
<li>処理速度（リアルタイム性）</li>
</ul>
<p>といった基盤的能力では、多くのモデルが極めて低いスコアにとどまっており、ほぼゼロに近い領域も存在する。</p>
<p>研究チームは、現行の大規模言語モデルは「部分的には人間を超えるが、能力の分布がギザギザで、総合的な認知としては成人レベルに達していない」と整理している。</p>
<h2>政策・規制議論への影響も──共通の“物差し”が登場</h2>
<p>AGIの定義が明確化されたことで、今後は次のような領域で議論が進む可能性がある。</p>
<ul>
<li><strong>米上院で進むAI安全法案の議論：</strong> 規制対象となるAIモデルをどの基準で選定するか、共通の指標が求められていた。</li>
<li><strong>企業のロードマップ策定：</strong> 「AGIにどれだけ近いか」を定量的に示せることで、開発の透明性や説明責任が高まる可能性。</li>
<li><strong>研究コミュニティでの基準化：</strong> 進捗を測る複数のベンチマークが乱立してきた中で、総合的な認知評価として採用される可能性。</li>
</ul>
<p>ただし研究チームは、今回の枠組みが「最終版の定義」ではなく、今後の議論の叩き台であることも強調している。</p>
<h2>今後の焦点：長期記憶・知覚・実世界タスク</h2>
<p>GPT-5が約6割に到達した一方、AGI達成には大きなギャップが残されている。特に論文が指摘するのは、次のような領域だ。</p>
<ul>
<li>長期記憶の安定した形成と参照</li>
<li>視覚・聴覚などマルチモーダル知覚の統合</li>
<li>現実世界での連続的タスク遂行（エージェント的行動）</li>
</ul>
<p>研究チームは、これらの領域が改善すれば、AGIスコアが急速に伸びる可能性があるとしている。</p>
]]></description>
      <pubDate>Thu, 11 Dec 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Anthropic、AIエージェント開発における”スキル”重視の新設計思想を提示──「IQ300数学の天才から経験豊富な税務専門家へ」、スキル中心アーキテクチャの全体像を解説</title>
      <link>https://ledge.ai/articles/anthropic_ai_agents_build_skills_paradigm</link>
      <description><![CDATA[<p>Anthropicに所属するBarry Zhang氏とMahesh Murag氏は、2025年11月21日に開催された開発者向けイベント「AI Engineer Code Summit」で<a href="https://www.youtube.com/watch?v=CEvIs9y1uog">講演</a>を行い、AIエージェント開発における新たな設計思想を提示した。講演の模様は、同イベントを主催する開発者コミュニティ「AI Engineer」のYouTubeチャンネルで、12月9日に動画として公開されている。</p>
<p>@<a href="https://www.youtube.com/watch?v=CEvIs9y1uog&amp;t=183s">YouTube</a></p>
<p>両氏は講演の中で、現在主流となりつつある「AIエージェント」を多数構築するアプローチに疑問を投げかけ、「エージェントではなくスキルを構築すべきだ」と主張した。汎用的な知性を持つAIエージェントが、実務において専門家として振る舞うことの難しさを課題として挙げ、その解決策として「スキル」という新しい概念を提示している。</p>
<p>具体的には、業務に必要な手続き的知識を「スキル」として明示的にパッケージ化し、エージェントが必要に応じて段階的に読み込むアーキテクチャを紹介した。このスキル中心のアプローチは、AIエージェントの実用性や拡張性を高めるだけでなく、専門知識の再利用や共有、さらにはAI自身による継続的な学習の基盤となる可能性がある。本記事では、AI Engineer Code Summitで語られた講演内容をもとに、Anthropicの研究者が示したAIエージェント開発の新パラダイムと、その全体像を整理する。</p>
<h2>現代のAIエージェントが抱える課題──「優秀だが経験不足」というジレンマ</h2>
<p>AIエージェントは近年、推論能力やツール利用能力の向上によって急速に注目を集めている。一方で、実務の現場では「期待したほど使えない」と感じられるケースも少なくない。講演で両氏は、その理由を分かりやすい比喩で説明した。</p>
<p><strong>■「IQ300の数学の天才」と「経験豊富な税務専門家」の対比で示される、AIエージェントの課題</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/genius_iq300_1526483afb/genius_iq300_1526483afb.jpg" alt="①genius iq300.jpg" /></p>
<p>「あなたの税務処理を任せるなら、IQ300の数学の天才と、長年の経験を持つ税務専門家のどちらを選びますか？」という問いだ。多くの場合、選ばれるのは後者だろう。税務のような専門業務では、生の知性よりも、確立された手続きを一貫して実行できる経験の方が重要だからだ。</p>
<p>現在のAIエージェントは、この「IQ300の天才」に近い存在だと両氏は指摘する。高い汎用知性を備えている一方で、実務に必要な前提知識や暗黙知を最初から持っているわけではなく、専門家としての経験値が不足している。その結果、以下のような課題に直面する。</p>
<ul>
<li>業務特有の文脈や前提を十分に理解できない</li>
<li>専門的な手順やノウハウを効率よく身につけられない</li>
<li>特定のチームやユーザーと長期間協業しても、その経験が蓄積されない</li>
</ul>
<p>こうした課題を踏まえ、Anthropicは従来のエージェント構築の考え方そのものを見直す必要があると提起した。</p>
<h2>Anthropicが提唱する解決策──「エージェント」ではなく「スキル」という発想</h2>
<p>両氏が提示した解決策の中心にあるのが、「スキル」という概念だ。これは、エージェントそのものを高度化するのではなく、エージェントに付与する専門知識の持たせ方を再設計するという発想に基づいている。</p>
<p><strong>■ Anthropicが示すAIスタックの整理。スキルはアプリケーション層に位置づけられる</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/moving_up_the_stack_f254f9be32/moving_up_the_stack_f254f9be32.jpg" alt="②moving up the stack.jpg" /></p>
<p>Anthropicの考えでは、エージェントはあくまで汎用的な推論主体として位置づけられるべき存在だ。専門家としての振る舞いは、エージェント自身がその場で学習するのではなく、事前に整理された手続き的知識を「スキル」として与えることで実現する。</p>
<p>このアプローチにより、エージェントは未知の分野を即興的に推論するのではなく、すでに確立された専門家のやり方に沿って行動できるようになる。結果として、実務で求められる一貫性や再現性を確保しやすくなるという。</p>
<h2>「スキル」とは何か──手続き的知識をフォルダとして扱う設計</h2>
<p>講演で定義された「スキル」は、エージェントのための構成可能な手続き的知識をパッケージ化した、整理されたファイル群である。具体的には、単なるフレームワークや抽象APIではなく、誰もが理解できる「フォルダ」という形で実装される。</p>
<p><strong>■ スキルは特別なフレームワークではなく、手続き的知識をまとめたフォルダとして定義される</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/skills_are_just_folders_fe2866b20f/skills_are_just_folders_fe2866b20f.jpg" alt="③skills are just folders.jpg" /></p>
<p>スキルの中には、以下のような要素が含まれる。</p>
<ul>
<li>スキルの目的や使い方を記述したドキュメント</li>
<li>実行可能なスクリプトやテンプレート</li>
<li>補助的な設定ファイルや例</li>
</ul>
<p>この設計の特徴は、特別なツールや専用環境を必要としない点にある。Gitによるバージョン管理や、クラウドストレージでの共有など、既存の開発者ワークフローと自然に統合できる。</p>
<p>また、スキルはランタイム時に段階的に読み込まれる。最初にエージェントへ提示されるのは、スキルの存在を示すメタデータのみで、必要と判断された場合にのみ詳細な指示やファイルが読み込まれる。この仕組みにより、コンテキストウィンドウの消費を抑えつつ、多数のスキルを併用することが可能になる。</p>
<h2>急速に拡大するスキルエコシステム──基盤・サードパーティ・エンタープライズ</h2>
<p>スキルの仕組みは、すでに活発なエコシステムを形成しつつある。講演によれば、公開から短期間で数千のスキルがコミュニティによって作成されているという。</p>
<p><strong>■ スキルは「基盤」「パートナー」「エンタープライズ」の3層で構成されるエコシステムを形成している</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/the_skills_ecosystem_d01ca1d0cb/the_skills_ecosystem_d01ca1d0cb.jpg" alt="④the skills ecosystem.jpg" /></p>
<p>スキルは大きく、次の3つに分類できる。</p>
<p>まず「基盤スキル」は、エージェントに新たな汎用能力や特定分野の能力を付与するものだ。プロ品質のオフィス文書作成や、科学研究向けデータ解析などが例として挙げられた。</p>
<p>次に「サードパーティスキル」は、外部企業が自社製品とAIエージェントを連携させるために開発したスキルだ。ブラウザ自動化ツールや、ナレッジ管理サービスと連携するスキルが紹介された。</p>
<p>最後に「エンタープライズスキル」は、企業やチーム内で蓄積された業務ノウハウをエージェントに教えるためのものだ。社内独自の業務プロセスや、チーム固有の開発ルールをスキルとして共有することで、組織全体の生産性向上につなげられる。</p>
<p>講演では、特に非エンジニアの専門家がスキルを作成している点が注目すべき動きとして紹介された。金融、法務、採用などの分野で、専門知識を持つ人々が自らAIを強化していることは、このアプローチの有効性を示す事例といえる。</p>
<h2>スキルを前提にした汎用エージェントのアーキテクチャ</h2>
<p>スキルは、AIエージェント全体のアーキテクチャの中で重要な役割を担う。講演では、汎用エージェントを構成する要素として、次の4点が示された。</p>
<ul>
<li>モデルの思考と入出力を管理するエージェントループ</li>
<li>ファイル操作やコード実行を可能にするランタイム環境</li>
<li>外部APIやデータと接続するMCP（Model Context Protocol）サーバー</li>
<li>手続き的専門知識を提供するスキルライブラリ</li>
</ul>
<p><strong>■ MCPサーバー、エージェント、ファイルシステム上のスキルの関係を示した汎用エージェントの基本構成</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/skills_the_complete_picture_900b270496/skills_the_complete_picture_900b270496.jpg" alt="⑤skills the complete picture.jpg" /></p>
<p>この構成により、エージェントは汎用性を保ったまま、スキルとMCPを組み合わせることで特定の業務や業界に適応できる。Anthropicは、金融やライフサイエンスといった分野向けにサービスを迅速に展開できた事例を紹介し、このアーキテクチャの柔軟性を示した。</p>
<h2>スキルの共有と自己生成──Anthropicが描く今後の展望</h2>
<p>Anthropicは、スキルを単なる機能拡張ではなく、AI能力を共有・進化させる基盤として位置づけている。スキルが複雑化するにつれ、テストやバージョン管理、依存関係の明示といったソフトウェア開発の手法を取り入れる重要性も指摘された。</p>
<p><strong>■ スキルは評価・バージョン管理・構成可能性を備え、ソフトウェアとして進化していく</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/exploring_how_skills_evolve_484db0b197/exploring_how_skills_evolve_484db0b197.jpg" alt="⑥exploring how skills evolve.jpg" /></p>
<p>さらに講演では、AI自身がスキルを生成する可能性にも言及された。Claudeはすでに「スキルクリエイター」として、ユーザーのためにスキルを作成できるとされており、今後は対話を通じて学んだ手続き的知識をスキルとして保存する方向性が示されている。</p>
<p>こうした仕組みが実現すれば、AIが学んだ内容は一時的な記憶ではなく、再利用可能な知識資産として蓄積されることになる。Anthropicが掲げる「あなたと30日間働いたClaudeは、初日のClaudeより優れている」という目標に向けて、スキルは重要な役割を果たすと位置づけられている。</p>
]]></description>
      <pubDate>Thu, 11 Dec 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Anthropic、AIエージェント標準「MCP」をLinux Foundation傘下へ移管──Agentic AIの共通基盤化へ</title>
      <link>https://ledge.ai/articles/anthropic_mcp_linux_foundation_aaif_transfer</link>
      <description><![CDATA[<p>Anthropicは2025年12月10日、同社が開発してきたAIエージェント向けの標準仕様「Model Context Protocol（MCP）」を、Linux Foundation傘下に新設された「Agentic AI Foundation（AAIF）」へ移管すると<a href="https://www.anthropic.com/news/donating-the-model-context-protocol-and-establishing-of-the-agentic-ai-foundation">発表</a>した。</p>
<p>Anthropicはこれまで、対話型AI「Claude」を中心としたエコシステムの中でMCPを提唱・整備してきた。エージェントが自律的にタスクを実行する「Agentic AI」への関心が高まる中、MCPはその基盤技術の一つとして注目されている。</p>
<p>今回の移管により、MCPの仕様管理や将来の拡張は、Linux Foundation傘下のAAIFが担う。AAIFは、AIエージェント分野における共通基盤の整備と標準化を目的として新設された組織で、複数の企業や開発者コミュニティが参加する中立的な運営体制を採る。</p>
<p>Linux Foundationによると、AAIFはMCPのほか、Blockが開発したエージェント関連プロジェクト「goose」や、OpenAIが提唱するエージェント仕様「AGENTS.md」などの初期プロジェクトを受け入れ、AIエージェント分野における共通基盤の整備を進めるとしている<a href="https://aaif.io/press/linux-foundation-announces-the-formation-of-the-agentic-ai-foundation-aaif-anchored-by-new-project-contributions-including-model-context-protocol-mcp-goose-and-agents-md/">Linux Foundationの発表</a>。</p>
<p>Anthropicは、MCPの創始者として引き続き技術的な貢献を行う一方、主導権はコミュニティ側に委ねられる。</p>
<p>Linux Foundationは、LinuxやKubernetesなど、業界横断的なソフトウェア基盤を中立的に運営してきた実績を持つ。AI分野ではすでに「LF AI &amp; Data」を通じてオープンな研究・開発を支援しており、今回のAAIF設立とMCP受け入れは、Agentic AIを巡る標準化の動きを一段進めるものとなる。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/aaif_social_media_image_e440cd3697/aaif_social_media_image_e440cd3697.webp" alt="aaif_social media image.webp" /></p>
<p>AIエージェントを巡っては、各社が独自の実装やフレームワークを競う一方で、相互運用性やベンダーロックインへの懸念も指摘されてきた。Anthropicが自社主導で開発してきたMCPをLinux Foundation傘下へ移管したことは、競争領域と共通基盤を切り分け、エコシステム全体の拡大を優先する姿勢を示す動きといえる。</p>
]]></description>
      <pubDate>Thu, 11 Dec 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ChatGPTが選んだ38銘柄ファンド、2年半で+66.1%に──英国人気投信10本の平均+41.36%を引き離す</title>
      <link>https://ledge.ai/articles/chatgpt_fund_performance_2025_update</link>
      <description><![CDATA[<p>英国の金融比較サイト Finder が実施している「<a href="https://www.finder.com/uk/share-trading/share-trading-research/ai-investing">ChatGPTファンド</a>」の概念実験で、2025年10月28日時点の最新パフォーマンスが明らかになった。同ファンドは、生成AI「ChatGPT」が選んだ38銘柄で構成される仮想ポートフォリオで、運用開始から約2年半の累計リターンは +66.1%。ベンチマークとしている英国の人気投資信託トップ10本の平均リターン +41.36% を大きく上回り、開始以来 99% の営業日でアウトパフォーム したという。</p>
<p>Finderは2023年3月、ChatGPTに「借入が少なく、持続的成長と競争優位性を持つ企業」を条件として提示し、株式ポートフォリオの作成を依頼した。ChatGPTは「概念上の演習」であることを前提に38銘柄を選定。このリストをもとに、同社は等金額の仮想ファンドを組成し、Interactive Investor 上で最も人気のある投資信託10本とパフォーマンスを比較する継続実験を開始した。</p>
<p>同年5月にFinderが公表したプレスリリースでは、開始8週間時点でChatGPTファンドが +0.48%、人気投信10本の平均が -0.78% となり、初期段階から優位に立っていたことが示されている。</p>
<p>今回の最新アップデートでは、2年半にわたる追跡期間のうち、ChatGPTファンドが人気投信10本の平均を下回った日はわずか1%。選定銘柄には、Microsoft、Alphabet、Meta、NVIDIA、TSMC、ASML、Johnson &amp; Johnson、PepsiCo など、大型株を中心とした多国籍企業が並ぶ。</p>
<p>Finderは同実験について「ChatGPTは2021年までの訓練データを持つ言語モデルであり、リアルタイム市場データに基づく投資判断を行っているわけではない」と明記。ChatGPTの回答が投資助言に該当するものではないこと、またこのファンド自体は実在せず、あくまでAIの応答を可視化する概念実験である点を繰り返し注意喚起している。</p>
<p>同社は、実験の目的を「AIが人間のファンドマネージャーと比較してどのようなポートフォリオを組むかを検証すること」と説明。今回の結果は、2年半の長期観測においても、ChatGPTが選んだポートフォリオが英国の人気投信平均を継続して上回り続けていることを示すものとなった。</p>
]]></description>
      <pubDate>Thu, 11 Dec 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ディズニーとOpenAI、生成AIに公式キャラクターを解禁──「Sora」でミッキーやスター・ウォーズの動画生成が可能に</title>
      <link>https://ledge.ai/articles/disney_openai_sora_character_license</link>
      <description><![CDATA[<p>2025年12月11日、ウォルト・ディズニー・カンパニーとOpenAIは3年間のライセンス契約を締結したと<a href="https://thewaltdisneycompany.com/disney-openai-sora-agreement/">発表</a>した。</p>
<p>これにより、OpenAIの動画生成AI「Sora」において、ディズニー、マーベル、ピクサー、スター・ウォーズなど、200以上の公式キャラクターを用いた短編動画を生成できるようになるという。対象にはキャラクター本体に加え、衣装、小道具、乗り物、象徴的な環境なども含まれる。</p>
<p>本契約は、生成AIが世界的エンターテインメント企業のキャラクターIPを正式に扱う大規模な事例の一つとなる。両社は、著作権とブランド保護を前提とした「責任あるAI利用」を掲げ、キャラクターの不適切な利用を防ぐための安全策を講じるとしている。</p>
<h2>ディズニー、生成AIに公式キャラクターを解禁</h2>
<p>今回の契約により、ディズニーが保有する主要フランチャイズのキャラクターが、OpenAIの動画生成AI「Sora」に正式にライセンス提供される。対象には、ディズニーのオリジナルキャラクターをはじめ、マーベル、ピクサー、スター・ウォーズといった世界的に認知度の高いブランドが含まれる。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/disney_and_sora_82d3de54a5/disney_and_sora_82d3de54a5.jpg" alt="disney and sora.jpg" /></p>
<p>生成AIによるキャラクター表現をめぐっては、これまで権利侵害の懸念から非公式な利用が問題視される場面も多かった。今回の契約は、公式ライセンスのもとで生成AIを活用する枠組みを明示したものとなる。</p>
<h2>「Sora」でミッキーやスター・ウォーズの動画生成が可能に</h2>
<p>Soraでは、ユーザーが入力したプロンプトに基づき、公式キャラクターを用いた短い動画を生成できるようになる。OpenAIとディズニーは、キャラクターの扱いについて不適切な利用を防ぐための安全策を講じると説明している。</p>
<p>あわせて、同様のキャラクターIPは、静止画生成機能である「ChatGPT Images」でも利用可能になるとしている。</p>
<h2>俳優の顔や声は含めず、キャラクターに限定</h2>
<p>今回のライセンス契約では、生成対象をキャラクターIPに限定している点も明確にされている。実写作品に出演する俳優本人の容姿や声、実在の人物を想起させる表現は含まれない。</p>
<p>両社は、こうした制限を通じて、俳優や声優の肖像権・パブリシティ権への配慮を含む、権利保護と安全性を重視した設計であることを強調している。</p>
<h2>生成された動画はDisney+にも並ぶ予定</h2>
<p>生成された動画の一部については、動画配信サービス「Disney+」での配信も予定されている。具体的な配信形態や選定基準は明らかにされていないが、ユーザーが生成したコンテンツが公式配信プラットフォームに掲載される可能性が示された形だ。</p>
<p>一般向けの展開は2026年初頭を見込んでおり、Soraで制作されたコンテンツがどのように視聴体験へ組み込まれるのかが注目される。</p>
<h2>ライセンスにとどまらず、ディズニーはOpenAIに出資</h2>
<p>今回の提携は、ライセンス契約にとどまらない。ディズニーはOpenAIに対し10億ドルを出資し、あわせて追加株式の購入権も取得した。</p>
<p>ディズニーは、OpenAIのAPIを活用し、映画、テレビ、テーマパーク、消費者向けプロダクト、そしてDisney+を含む体験の構築を進めるとしており、OpenAIにとっても、世界最大級のエンターテインメント企業が主要顧客となる。</p>
<h2>エンタメ大手が踏み出した「公式×生成AI」の一歩</h2>
<p>ディズニーとOpenAIの契約は、生成AIによるキャラクター表現をめぐり、公式ライセンスの枠組みを示した事例となった。非公式利用や訴訟リスクが先行してきた分野において、権利者とAI開発企業が協調する形での活用モデルが提示された格好だ。</p>
<p>両社は今後も、著作権やブランド保護を前提としたAI活用を進めるとしており、生成AIがエンターテインメント制作や体験の在り方にどのような変化をもたらすのか、引き続き注目される。</p>
]]></description>
      <pubDate>Thu, 11 Dec 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>アシモフもびっくり？ 中国EngineAI、人型ロボット「T800」がCEOを蹴り倒す映像公開──CG疑惑を巡り“体を張った実演”</title>
      <link>https://ledge.ai/articles/engineai_t800_ceo_kick_video_cg_suspicions</link>
      <description><![CDATA[<p>中国のロボット企業 EngineAI（众擎机器人） は2025年12月6日、人型ロボット「T800」が同社の創業者で最高経営責任者（CEO）の趙同陽（Zhao Tongyang）氏をキックで倒す様子を収めた映像を<a href="https://www.instagram.com/reel/DR7C3kMEz4b/">公開</a>した。映像はInstagramやXなどの投稿サイトを通じて公開され、SNS上で広がっていた「CGではないか」という疑念に対応するため、CEO自らが実演に臨んだ形だ。</p>
<p>公開された動画では、防具を着用した趙氏がT800の前に立ち、ロボットが繰り出した前方キックを受けて地面に倒れる様子が確認できる。映像内ではキックの威力の強さが強調されており、公開直後から中国国内外のSNSで急速に拡散した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/enginai_ceo_651eec7e25/enginai_ceo_651eec7e25.jpg" alt="enginai ceo.jpg" /></p>
<p>この映像が公開される前、EngineAIはT800がキックや宙返りのような動作を見せるデモ動画を複数公開していた。動きの滑らかさやダイナミックさから、SNS上では「実写ではなくCGIではないか」といった指摘や懐疑的な声が相次いでいた。こうした疑念に反論する意図で、CEOが自らロボットのキックを受ける実演映像を公開したと米Business Insider等が報じている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/enginai_no_aigc_nocgi_f4537fb037/enginai_no_aigc_nocgi_f4537fb037.jpg" alt="enginai no aigc nocgi.jpg" /></p>
<p>映像はInstagramをはじめとする複数の投稿サイトで転載・拡散され、海外メディアが相次いで取り上げた。多くの報道では、ロボット技術の進展そのものに加え、CGや高度な映像編集が一般化した時代において、実機であることをどのように証明するかという点に注目が集まっている。</p>
<p>近年、人型ロボットの開発を巡っては、各国企業が実機デモ映像を積極的に公開する一方で、その真偽や編集の有無が議論になるケースも増えている。EngineAIの今回の対応は、第三者による検証ではなく、CEO本人が体を張る形で“実在性”を示そうとした点で、興味深い事例といえる。</p>
<p>人型ロボットが人間に危害を加える映像は、SF作家アイザック・アシモフが提唱した「ロボット工学三原則」で描かれてきた理想像とは対照的だ。三原則では、ロボットは人間に危害を加えてはならないとされてきた。一方、現実のロボット開発では、技術の実在性や性能を示すために、こうした象徴的な実演が行われるケースが現れた。</p>
<p>技術的な進歩と同時に、安全性や検証方法、情報公開のあり方が問われる人型ロボット。一連の映像は、ロボット技術の進化とともに、社会がどのようにその信頼性を評価していくのかという課題を浮き彫りにしている。</p>
]]></description>
      <pubDate>Thu, 11 Dec 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>AIで“裸眼3D”が進化──上海AI研究所らが超広視野角ディスプレイ「EyeReal」を開発、Natureに発表</title>
      <link>https://ledge.ai/articles/eyereal_ai_glasses_free_3d_ultrawide_display_nature</link>
      <description><![CDATA[<p>上海人工智能実験室と復旦大学の研究チームは、専用メガネなしで広視野角の3D映像を表示できる新方式「EyeReal」を開発し、科学誌 Nature に<a href="https://www.nature.com/articles/s41586-025-09752-y">発表</a>した。従来のホログラフィック方式や自動立体（autostereoscopic）ディスプレイが抱えていた物理的制約を、AIと多層LCD構造を組み合わせた新たな光場生成手法で克服した点が特徴だ。</p>
<h2>従来の裸眼3D方式の限界を超えるアプローチ</h2>
<p>過去の裸眼3D技術は、視野角や画面サイズなどのトレードオフが大きな課題だった。ホログラフィック方式は高密度の光学情報を扱える一方、視域が極めて小さく、実用性が限定される。一方、ビューセグメント型・ビューデンス型の自動立体ディスプレイは、視点数や視域が固定され、観察者が自由に動いた際に自然な視差変化を再現することが難しかった。</p>
<p>EyeReal は、こうした従来方式を図示しつつ、観察者の目の周囲に最適化された光場をリアルタイム生成するというアプローチを採用する。</p>
<p><strong>従来方式（a〜c）が視野角や表示領域に制約を抱える中、EyeReal（d）は観察者の眼のまわりで光場を最適化し、超広視野角の裸眼3D表示を可能にする</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/41586_2025_9752_Fig1_7a43aeeee1/41586_2025_9752_Fig1_7a43aeeee1.jpg" alt="41586_2025_9752_Fig1.jpg" /></p>
<h2>EyeReal の構造：多層LCD × AIによる光場最適化</h2>
<p>研究チームは、市販の液晶パネルを複数枚重ね合わせた構造を採用。光源、偏光板（縦・横）、液晶層の組み合わせにより、各ピクセルからの位相（フェーズ）を精密に制御する。</p>
<p>RGB-D センサーが観察者の両眼位置と姿勢（6D pose）を取得し、ニューラルネットワークがその位置に最適な「位相パターン」を生成する。これにより、従来の光学系では物理的に困難だった複雑な光場を、液晶ディスプレイのみで再現できる。</p>
<p><strong>多層LCD、偏光板、RGB-D センサー、ニューラルネットワークで構成される EyeReal の光場生成システム。両眼位置の推定から位相パターン生成までをリアルタイムで行う</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/41586_2025_9752_Fig2_d941741a66/41586_2025_9752_Fig2_d941741a66.jpg" alt="41586_2025_9752_Fig2.jpg" /></p>
<h2>動きに応じて自然に変化する視差と焦点表現</h2>
<p>実験では、都市景観や屋内シーンなどを用いて、左右の眼に届く画像を検証した。EyeReal が生成した左右画像は、Ground truth（正解画像）と高い一致率を示し、水平・垂直・前後方向の動きに対して自然なモーションパララックスを維持した。</p>
<p>さらに、前景・後景の被写界深度（焦点表現）も再現可能で、奥行き感の自然さが向上している。</p>
<p><strong>EyeReal の再現画像（Prediction）は、Ground truth と高い一致を示し、上下左右・前後の動きに伴う視差変化を自然に再現。前景・後景の焦点表現も維持されている</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/41586_2025_9752_Fig3_c6449eee0f/41586_2025_9752_Fig3_c6449eee0f.jpg" alt="41586_2025_9752_Fig3.jpg" /></p>
<h2>超広視野角100度超、50Hzリアルタイム処理</h2>
<p>定量評価では以下の性能が確認された：</p>
<ul>
<li><strong>視野角：100度超</strong> ：観察者が大きく移動しても PSNR・SSIM が高値を維持した</li>
<li><strong>未知視点に対する一般化性能（Generalization）</strong> ：学習データに存在しない視点からでも安定した画質を確保</li>
<li><strong>動作速度：50.2 Hz</strong> ：多層LCDとAI処理によるシステムとしては高速で、リアルタイム裸眼3Dに求められる水準を満たす</li>
</ul>
<p>奥行き層ごとの PSNR 曲線や視野マップも示され、前景から背景まで一貫した再現品質を持つことが確認された。</p>
<p><strong>EyeReal の定量評価。超広視野角でも高い PSNR/SSIM を維持し、未知視点に対しても一般化。50Hzを超える処理速度によりリアルタイム裸眼3Dとして実用レベルに到達している</strong></p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/41586_2025_9752_Fig4_e554e7ef93/41586_2025_9752_Fig4_e554e7ef93.jpg" alt="41586_2025_9752_Fig4.jpg" /></p>
<p>論文では、エンターテインメント用途にとどまらず、医学画像の立体視、建築設計、教育、科学データの立体構造可視化など、多様な分野への応用可能性が示されている。</p>
<p>今後の課題としては、複数ユーザー同時視聴への拡張、消費電力の最適化、さらなる高速化などが挙げられる。
研究チームは、本方式が「裸眼3Dディスプレイの物理的限界をAIで超える新しい方向性を提示した」としている。</p>
]]></description>
      <pubDate>Thu, 11 Dec 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>月額3万6400円の「Google AI Ultra」に最強推論モード──Geminiアプリに、思考するAI「Deep Think」モードの実装</title>
      <link>https://ledge.ai/articles/gemini_3_deep_think_google_ai_ultra_launch</link>
      <description><![CDATA[<p>Googleは2025年12月4日（現地時間）、Geminiアプリに新たな推論モード「Gemini 3 Deep Think」を追加したと<a href="https://blog.google/products/gemini/gemini-3-deep-think/">発表</a>した。</p>
<p>同社はブログで、「Today, we’re rolling out Gemini 3 Deep Think mode to Google AI Ultra subscribers in the Gemini app.」と述べており、最上位サブスクリプションプラン「Google AI Ultra」の加入者を対象に、Geminiアプリで順次ロールアウトしている。利用するには、モデル選択で「Gemini 3 Pro」を選び、プロンプト入力欄から「Deep Think」モードをオンにする。</p>
<h2>高難度課題向けに推論能力を強化</h2>
<p>公開された「Gemini 3 Deep Think」は、回答を出力する前に複数の推論ステップを挟むことで、従来よりも深い推論ができるよう設計されたモードだ。Googleはブログで、同モードが「meaningful improvement in reasoning capabilities」を提供し、複雑な数学・科学・論理推論に取り組む場面を想定していると説明している。</p>
<p>具体的には、高度な並列推論（advanced parallel reasoning）を用いて複数の仮説を同時に探索するアプローチを採用することで、問題解決の過程を強化しているという。これにより、単に次の単語を即座に予測するのではなく、複数の候補を比較しながら解答に至るプロセスを踏めるとしている。</p>
<p>Googleは、Gemini 3 Deep Thinkが高難度ベンチマークで次のようなスコアを記録したと紹介している。</p>
<ul>
<li>Humanity’s Last Exam：ツールなしで 41.0%</li>
<li>ARC-AGI-2：コード実行ありで 45.1%</li>
</ul>
<p>同社は、これらの結果について「industry leading」「unprecedented（前例のない）」と表現しており、深い推論が求められるベンチマークにおいて業界トップレベルの性能を示したと位置づけている。また、今回のGemini 3 Deep Thinkは、国際数学オリンピック（IMO）や国際大学対抗プログラミングコンテスト（ICPC）World Finalsで「gold-medal standard」とされる水準に達した「Gemini 2.5 Deep Think」の系譜にあると説明されている。</p>
<h2>利用対象は「Google AI Ultra」加入者</h2>
<p>日本向けの公式プランページでは、Google AI Ultraの料金は月額3万6400円（税込）と案内されている。現時点（2025年12月8日）では、GeminiアプリでのDeep ThinkおよびGemini Agentについては「米国のみ、英語のみで利用可能」とされており、機能レベルでは提供地域に制限が設けられている。</p>
]]></description>
      <pubDate>Thu, 11 Dec 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Googleら、マルチエージェントAIの「スケーリング原理」を定量化──数を増やせば賢くなる、は普遍則ではなかった</title>
      <link>https://ledge.ai/articles/google_agent_scaling_law_science</link>
      <description><![CDATA[<p>Google ResearchおよびGoogle DeepMind、MITの研究チームは2025年12月9日、AIエージェントシステムの性能がどのようにスケールするかを定量的に分析した研究論文「Towards a Science of Scaling Agent Systems」をプレプリントサーバーarXiv上で<a href="https://arxiv.org/abs/2512.08296">発表</a>した。</p>
<p>複数のAIエージェントを協調させることで性能が向上するという従来の直感に対し、この研究では、エージェント数の増加が必ずしも性能向上につながらないことを、体系的な実験と分析によって示している。</p>
<h2>エージェントAIに「スケーリング則」は存在するのか</h2>
<p>大規模言語モデル（LLM）では、モデルサイズや計算量の拡大に伴って性能が向上する「スケーリング則」が広く知られている。一方、複数のLLMを組み合わせたエージェントAIについては、どのような条件で性能が伸びるのかについて明確な理論は確立されておらず、設計は経験則に依存してきた。</p>
<p>研究チームは、エージェント数や協調構造と性能の関係を体系的に理解することを目的に、本研究を実施した。</p>
<h2>180通りの構成を用いた大規模評価</h2>
<p>研究では、単一エージェント構成（SAS）と4種類のマルチエージェント構成（MAS）を含む、計5つのアーキテクチャを対象に評価を行った。複数の大規模言語モデルを用い、金融分析、Web操作、計画立案、作業実行などのタスクを実行している。トークン予算やツール環境を統制することで、アーキテクチャの違いが性能に与える影響を比較できるよう設計されている。</p>
<p>評価対象となった4つのベンチマークを平均すると、マルチエージェント構成による性能変化は平均で-3.5%となり、エージェントを追加することが必ずしも全体性能を押し上げないことが示された。</p>
<h2>明らかになった3つの原理</h2>
<p>分析の結果、研究チームは、エージェントAIの性能が単純な「数」ではなく、協調構造とタスク特性の相互作用によって決まることを示す、複数のスケーリング原理を明らかにした。</p>
<p><strong>エージェント構成（SAS／MAS）とモデル性能の関係：</strong>
単一エージェント（SAS）と複数エージェント（MAS）の各構成における性能変化を、OpenAI GPT、Google Gemini、Anthropic Claude の各モデルで比較すると、モデルや協調構造によって、エージェント追加が性能向上につながる場合と、逆に性能が低下する場合があることが確認された。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/x1_b8d45ed289/x1_b8d45ed289.png" alt="x1.png" /></p>
<h3>協調には「コスト」が伴う</h3>
<p>エージェント間の協調にはコストが伴うことが確認された。情報共有や調整が増えるほどオーバーヘッドが発生し、特にツール利用が多い環境では、固定された計算・トークン予算の中でこの負荷が性能低下につながるケースがあった。</p>
<h3>性能には「飽和点」が存在する</h3>
<p>性能の飽和点が存在することが示された。単一エージェント（SAS）の基準性能が一定水準を超えると、追加エージェントによる改善効果は急速に小さくなる。論文では、SASの性能が <strong>約45%</strong> を超えた領域では、協調による効果が統計的に有意に逓減し、場合によっては性能悪化に転じることが示されている。</p>
<h3>誤差の増幅はアーキテクチャで大きく異なる</h3>
<p>誤差がどの程度増幅されるかは、エージェントの協調構造に強く依存することも明らかになった。
独立型のマルチエージェント構成では、各エージェントの判断誤差が累積・増幅されやすい一方、中央統合型の構成では、こうした誤差が比較的抑制される傾向が確認された。</p>
<h2>マルチエージェントは万能ではない</h2>
<p>研究では、並列化しやすいタスクではマルチエージェント構成が有効な場合がある一方、逐次性の高いタスクでは、多くの構成で性能低下が見られた。</p>
<p>研究チームは、タスク特性とアーキテクチャの組み合わせによって性能を予測できるモデルも提示しており、どの構成が適切かを事前に判断できる可能性を示している。</p>
]]></description>
      <pubDate>Thu, 11 Dec 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>学術＆研究2025/12/14 [SUN]Google、AIに「長期記憶」を与える新アーキテクチャ「Titans」と設計理論のフレームワーク「MIRAS」を発表</title>
      <link>https://ledge.ai/articles/google_ai_long_term_memory_titans_miras</link>
      <description><![CDATA[<p>Google Researchは2025年12月4日、AIモデルに長期的な記憶能力を持たせるための新たなアーキテクチャ「Titans」と、その設計思想を統一的に説明するフレームワーク「MIRAS」を<a href="https://research.google/blog/titans-miras-helping-ai-have-long-term-memory/">発表</a>した。</p>
<p>Transformerを中心とする既存の大規模言語モデルが抱えてきた「長い文脈を扱うほど計算コストが増大し、重要な情報を保持しにくい」という課題に対し、推論時に記憶を学習・更新するという新しいアプローチを提示している。</p>
<h2>Transformerの限界と「記憶」の再定義</h2>
<p>TransformerはAttention機構によって高精度な依存関係のモデリングを可能にしてきた一方、計算量がコンテキスト長の二乗に比例するという制約を持つ。このため、極端に長い文書や時系列データを扱う際には効率面・性能面の両方で課題があった。</p>
<p>Google Researchは、Attentionを「短期記憶」、それを補完する仕組みとして「長期記憶」を明確に分離して設計する必要があると位置づけた。</p>
<h2>推論時に学習する「Titans」の中核</h2>
<p>Titansの中心となるのは「Neural Long-Term Memory Module」と呼ばれる長期記憶モジュールだ。このモジュールは、再学習を行うことなく、推論時に入力を受け取りながら記憶を更新する。単なるキー・バリューキャッシュとは異なり、過去の情報をモデル内部のパラメータとして蓄積できる点が特徴だ。</p>
<p><strong>■ Titansの全体構成：</strong> Attentionによる短期記憶に加え、推論時に更新されるNeural Memory（長期記憶）を統合。入力の重要度に応じて記憶を更新しつつ、固定パラメータとは独立して動作する。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Titans_1_Overview_width_1250_e2f30e7bc8/Titans_1_Overview_width_1250_e2f30e7bc8.png" alt="Titans-1-Overview.width-1250.png" /></p>
<p>Titansでは、長期記憶の統合方法として以下の3つの構成を提示している。</p>
<ul>
<li><strong>MAC（Memory as a Context）：</strong> 記憶を文脈としてAttentionに渡す方式</li>
<li><strong>MAG（Memory as a Gate）：</strong> 記憶によってAttention出力を制御する方式</li>
<li><strong>MAL（Memory as a Layer）：</strong> 記憶を独立したレイヤーとして組み込む方式</li>
</ul>
<p>特にMAC構成は、長距離依存関係を扱うタスクで高い性能を示したという。</p>
<h2>「驚き」に基づく選択的記憶と忘却</h2>
<p>Titansはすべての情報を無差別に記憶するのではなく、予測誤差が大きい、いわゆる「驚き（surprise）」の高いトークンを優先的に長期記憶へ反映する設計を採用している。
また、weight decayやモメンタムを用いた更新により、不要になった情報を忘却できる仕組みも備える。これにより、従来の線形RNNやゲート型モデルでは難しかった「完全な記憶の消去」も可能になるという。</p>
<h2>2Mトークン超でも性能を維持</h2>
<p>論文では、言語モデリング、常識推論、needle-in-a-haystackタスク、DNA解析、時系列予測など幅広いベンチマークで評価を実施。BABILongやRULERといった長文タスクでは、GPT-4やRAGを併用した大規模モデルを含む既存手法を上回る結果を示した。
有効なコンテキスト長は200万トークンを超えても性能が維持されることが確認されている。</p>
<h2>設計理論「MIRAS」が示す統一的枠組み</h2>
<p>MIRASは、Titansを含むさまざまなシーケンスモデルを「連想記憶システム」として捉え直すためのフレームワークだ。</p>
<p>設計要素を</p>
<ol>
<li>記憶構造</li>
<li>想起のための目的関数（attentional bias）</li>
<li>保持・忘却を制御するゲート</li>
<li>記憶の学習アルゴリズム</li>
</ol>
<p>の4点に整理し、Transformerや線形RNN、Titansを同一の理論枠組みで説明できるとしている。</p>
<p><strong>■ MIRASフレームワークの概念図：</strong> 記憶構造、想起基準（attentional bias）、保持・忘却（retention gate）、学習アルゴリズムの4要素で、TitansやTransformerを含むシーケンスモデルを統一的に説明する。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/miras_framework_47bb139255/miras_framework_47bb139255.jpg" alt="miras framework.jpg" /></p>
<p>MIRASを基に、Moneta、Yaad、Memoraといった新たなモデル群も提案されており、タスク特性に応じた記憶設計の可能性が示された。</p>
<h2>推論時学習という新たな方向性</h2>
<p>Google Researchは、TitansをMIRASフレームワークの具体例の一つと位置づけている。外部検索に依存するRAGや、巨大な固定コンテキストを用意する方法とは異なり、「推論時に学習する記憶」を内包したAI設計が、今後の長文・長期推論の重要な方向性となる可能性を示した形だ。</p>
]]></description>
      <pubDate>Thu, 11 Dec 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google、ディズニー要請受けAI生成動画を削除──OpenAIとは対照的な対応に</title>
      <link>https://ledge.ai/articles/google_disney_ai_generated_videos_removed_variety</link>
      <description><![CDATA[<p>Googleが、ディズニーからの削除要請を受け、ディズニーキャラクターを描いたAI生成動画を数十本削除していたことが分かった。米<a href="https://variety.com/2025/film/news/google-removes-ai-videos-disney-characters-cease-desist-1236608015/">Variety</a>が2025年12月12日に報じた。</p>
<p>Varietyによると、削除されたのはYouTube上に投稿されていたAI生成動画で、ミッキーマウスやマーベル作品、スター・ウォーズ、『ザ・シンプソンズ』など、ディズニー傘下のキャラクターが描写されていたという。ディズニーはこれらの動画について、著作権侵害の可能性があるとして、Googleに対し正式な削除要請（cease and desist）を送付したとされる。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/variety_disney_google_4a027dbfad/variety_disney_google_4a027dbfad.jpg" alt="variety disney google.jpg" /></p>
<p>GoogleはVarietyの取材に対し、当該動画を削除した事実を認めている。一方で、削除対象の詳細や、今後の対応方針については明らかにしていない。</p>
<p>ディズニーは近年、生成AIによる自社キャラクターの無断利用に対して警戒を強めている。Varietyは、今回の削除要請が単発の対応ではなく、AI生成コンテンツ全般に対する管理を強化する動きの一環である可能性を指摘している。</p>
<p>こうした対応は、ディズニーが生成AIの活用自体を一律に拒否しているわけではない点と対照的だ。ディズニーは2025年12月11日、OpenAIと3年間のライセンス契約を締結し、動画生成AI「Sora」において、ディズニー、マーベル、ピクサー、スター・ウォーズなど200以上の公式キャラクターの利用を<a href="https://ledge.ai/articles/disney_openai_sora_character_license">認めている</a>。</p>
<p>Varietyが伝える今回のGoogleの事例は、同じ生成AIを巡る動きでありながら、企業ごとに異なる対応が取られている現状を示している。OpenAIとはライセンス契約を結ぶ一方で、Googleに対しては削除要請という形で対処しており、どの企業が、どの条件下でディズニーの知的財産を扱えるのか、その線引きがより明確になりつつあることがうかがえる。</p>
]]></description>
      <pubDate>Thu, 11 Dec 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google、MCPを全サービスに展開──Gemini時代の「AI×クラウド接続」を標準化</title>
      <link>https://ledge.ai/articles/google_mcp_all_services_gemini_ai_cloud_standardization</link>
      <description><![CDATA[<p>Googleは2025年12月11日、生成AIモデル「Gemini」などのAIエージェントと、同社のクラウドサービスを接続するための共通基盤として、Model Context Protocol（MCP）の公式サポートを開始したと<a href="https://cloud.google.com/blog/products/ai-machine-learning/announcing-official-mcp-support-for-google-services">発表</a>した。Googleは今後、同社が提供するすべてのサービスにおいて、段階的に接続用のMCPサーバをGoogle側が用意・運用する形で提供していく方針だ。</p>
<h2>MCP：AIとサービスをつなぐ共通プロトコル</h2>
<p>MCPは、生成AIやAIエージェントが外部ツールやデータソース、APIと、安全かつ標準化された方法で接続するためのオープンプロトコルである。GoogleはこのMCPを、自社クラウドサービスとAIを結ぶ「共通の接続レイヤー」として公式に採用した。</p>
<p>今回Googleが提供するMCPサーバは、サーバの構築や運用、スケーリングをすべてGoogleが担う仕組みとなっている。開発者は自らサーバを立ち上げたり管理したりする必要がなく、MCPクライアントを通じてGoogleの各種サービスに直接接続できる。認証や権限制御についても、既存のGoogle Cloudのセキュリティ基盤と連携している。</p>
<h2>まずはMapsやBigQueryなど主要サービスから対応</h2>
<p>発表時点でMCPサーバに対応している主なサービスには、Google Maps、BigQuery、Google Compute Engine、Google Kubernetes Engine（GKE）などが含まれる。これらのサービスは、位置情報、データ分析、計算資源、コンテナ管理といった機能を提供しており、AIエージェントが実際の処理や操作を行うための基盤として利用できる。</p>
<p>Googleは、これらの初期対応サービスにとどまらず、今後すべてのGoogleサービスで同様の仕組みを提供する計画を示している。公式ブログでは、今後数カ月以内にCloud Run、Cloud Storage、Spanner、Looker、Pub/Sub、Cloud Logging、Cloud Monitoring、Security Operations（SecOps）、Android Management APIなどにも対応を広げる予定だとしている。</p>
]]></description>
      <pubDate>Thu, 11 Dec 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>ビジネス2025/12/12 [FRI]ChatGPT、新モデル「GPT-5.2」公開──アルトマンCEO「最も賢い一般提供モデル」　推論・コード生成が大幅進化</title>
      <link>https://ledge.ai/articles/gpt_5_2_release_openai_chatgpt_update</link>
      <description><![CDATA[<p>OpenAIは2025年12月11日（現地時間）、ChatGPTおよびAPI向けの新モデル「GPT-5.2」を公開したと<a href="https://openai.com/ja-JP/index/introducing-gpt-5-2/">発表</a>した。同社の公式ブログ「Introducing GPT-5.2」では、推論能力や長文コンテキスト処理、コード生成、知識労働タスクなどの幅広い能力が前世代のGPT-5.1から大幅に向上したと説明している。</p>
<p>GPT-5.2は「 Instant」「Thinking」「Pro」 の3モデルで構成される。Instantは応答速度を重視した軽量モデル、Thinkingは複雑な数学・論理・科学タスクに特化した深い推論モード、Proは拡張コンテキストと高推論能力を備えた上位モデルとして位置付けられる。</p>
<p>ChatGPTでは、新たに「GPT-5.2 Auto」を導入し、InstantとThinkingをタスク内容に応じて自動で切り替える仕様となった。ユーザーは用途に応じてモデルを手動選択する必要がなく、質問の難易度に応じて適切な推論モードが選択される。</p>
<p>性能面では、GPT-5.2 ThinkingはGPT-5.1 Thinkingと比較して、匿名化されたChatGPTのクエリセットにおける「誤りを含む回答」が相対的に38%減少したとされる。加えて、UIコンポーネントの生成や複数ツールを組み合わせてコードを実行する、いわゆる「エージェント型コーディング」が安定し、問題設定から実装、検証までの一連の開発ワークフローを高精度に遂行できるようになったという。長文処理の安定性も向上した。</p>
<p><strong>GPT-5.2 Thinking の主要ベンチマーク結果：</strong> （SWE-Bench Pro、GPQA、ARC-AGI、GDPvalなど）。GPT-5.1比で複数指標が大幅に向上した。
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/G76_Fh4_Wag_AAE_Ec_6b6e96de07/G76_Fh4_Wag_AAE_Ec_6b6e96de07.jpg" alt="G76Fh4WagAAE_Ec.jpg" /></p>
<h3>ARC-AGI-1でSOTAを更新、推論力の“質”を測る指標で高評価</h3>
<p>GPT-5.2の推論性能を示す指標のひとつが「ARC-AGI-1」だ。
ARC-AGI（Abstraction and Reasoning Corpus）は、人間が持つ抽象的な推論能力を測ることを目的に設計されたベンチマークで、単なる知識量やパターン暗記では解けない問題で構成されている。</p>
<p>具体的には
少数の例からルールを推測する
見慣れない問題に対して柔軟に考え方を切り替える</p>
<p>といった能力が求められ、「AIが本当に“考えているか”」を測るテストとして知られている。</p>
<p>このARC-AGI-1（Verified）において、GPT-5.2 Pro（X-High）は90.5%のスコアを記録した。この結果は第三者機関である <a href="https://x.com/arcprize/status/1999182732845547795">ARC Prize</a> によって検証されており、同団体は、1年前に検証された未公開モデルと比較して約390倍の効率改善が達成されたと評価している。</p>
<p>この結果は、GPT-5.2が単に正解率を積み上げたモデルではなく、未知の問題に対して抽象的に考え、解決策を導く能力が大きく向上していることを示している。</p>
<p><strong>ARC-AGI-1における各モデルのスコアとタスクあたりのコスト：</strong> GPT-5.2 Pro（X-High）は90.5%を記録し、推論能力とコスト効率の両面で最高水準に達した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/G76_GG_Zwag_A_Un7ub_b885ba305c/G76_GG_Zwag_A_Un7ub_b885ba305c.jpg" alt="G76GGZwagAUn7ub.jpg" /></p>
<p>知識労働タスクにおける性能も強化された。業界専門家の回答と比較評価する「GDPval」では、GPT-5.2 Thinkingが70.9%、GPT-5.2 Proが74.1%を記録し、前世代のGPT-5（38.8%）を大きく上回った。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/G76_Fi_Hka_AAE_Dmu_H_0ab7a9b29b/G76_Fi_Hka_AAE_Dmu_H_0ab7a9b29b.png" alt="G76FiHkaAAEDmuH.png" /></p>
<p>さらにGPT-5.2 Thinkingでは、スプレッドシートやプレゼンテーション資料など、「整形済みのファイル」を直接生成する機能も強化されている。OpenAIは、人員計画（ワークフォースプランニング）を例に、部門別のコスト計算や集計まで含めたスプレッドシートを自動生成する事例を公開した。</p>
<p><strong>左：GPT-5.2 Thinking が生成した部門別人員計画のスプレッドシート。集計・比較表まで自動生成されている。</strong>
<strong>右：複数部門のコスト、給与、採用費などが整形済み表として出力される。</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/G76_Fiaoag_AAH_i8_side_6039c883db/G76_Fiaoag_AAH_i8_side_6039c883db.jpg" alt="G76FiaoagAAH_i8-side.jpg" /></p>
<p>OpenAI CEOのサム・アルトマン氏は、自身のX（旧Twitter）でGPT-5.2について「現時点で一般提供されている中で最も賢いモデル」と述べ、特に知識労働タスクにおける性能向上を強調した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/GPT_5_2_is_here_by_sama_cbe335e446/GPT_5_2_is_here_by_sama_cbe335e446.jpg" alt="GPT-5-2 is here by sama.jpg" /></p>
<p>安全性面では、GPT-5系で用いられてきた既存の安全性フレームワークを引き継ぎつつ、GPT-5.2向けに調整した新版のシステムカードが公開されている。リスク緩和策や評価項目の更新が行われたとしている。</p>
<p>OpenAIはGPT-5.2を「GPT-5シリーズの中心モデル」と位置づけ、今後のChatGPTおよびAPIの基盤として、継続的な改善を進めていく方針だ。</p>
]]></description>
      <pubDate>Thu, 11 Dec 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>「ルンバ」のiRobotがついに破産手続き開始、中国Picea傘下で事業は継続──アイロボットジャパン「アフターサービスこれまで通り」Chapter 11申請受け声明</title>
      <link>https://ledge.ai/articles/irobot_chapter11_picea_business_continuity</link>
      <description><![CDATA[<p>ロボット掃除機「ルンバ」で知られる米iRobot Corporationは2025年12月、連邦破産法第11章（Chapter 11）に基づく再編手続きを開始したと<a href="https://media.irobot.com/2025-12-14-iRobot-Announces-Strategic-Transaction-to-Drive-Long-Term-Growth-Plan">発表</a>した。</p>
<p>同社は中国の掃除機メーカーPicea Robotics（関連会社Santrum Hong Kongを含む）による買収を受け入れ、債務整理後も事業を継続する方針だ。これを受け、日本法人のアイロボットジャパンは「事業はこれまで通り継続する」との声明を<a href="https://www.irobot-jp.com/press/pdf/20251215_2.pdf">発表</a>している。</p>
<p>iRobotによると、今回のChapter 11申請は清算を目的としたものではなく、事業継続を前提とした再編手続きとして進められる。あらかじめ主要条件を合意した「プレパッケージ型」の再編で、中国Picea Roboticsとの間で締結した再編支援契約（Restructuring Support Agreement）に基づき、裁判所の監督下で手続きを進めるという。</p>
<h2>中国Piceaが全株式を取得、iRobotは非公開化へ</h2>
<p>再編計画では、主要サプライヤーかつ担保権者でもあるPicea Roboticsが、iRobotの普通株式100％を取得する。再編完了後、iRobotはPiceaの完全子会社となり、非公開企業として再スタートを切る見通しだ。再編プロセスの完了は2026年2月上旬を予定している。</p>
<p>米証券取引委員会（SEC）に提出された開示資料によれば、再編に伴い既存の普通株式は消却され、現在の株主は再編後のiRobotの持ち分を取得しない見込みとされている。</p>
<h2>日本法人「事業はこれまで通り」</h2>
<p>iRobotは、Chapter 11手続き中および再編完了後も通常の事業運営を継続すると説明している。これを受け、アイロボットジャパンも公式声明で、製品販売やカスタマーサポート、保証対応、アプリやクラウドサービスについて「事業はこれまで通り」継続すると明らかにした。日本市場のユーザーや販売チャネルへの直接的な影響は限定的とされる。</p>
<h2>これまでの経緯と今回の位置づけ</h2>
<p>iRobotは家庭用ロボット掃除機市場の先駆者として「ルンバ」ブランドを展開してきたが、近年は競争の激化やコスト上昇を背景に、厳しい経営環境が続いていた。Ledge.aiではこれまで、Amazonによる買収計画の白紙撤回や、2025年に入って表面化した事業危機について報じてきた。</p>
<p>今回のChapter 11申請と中国Picea傘下への移行は、そうした流れの延長線上に位置づけられる。iRobotはこの戦略的取引によりバランスシートを立て直し、ロボティクスおよびスマートホーム分野での長期的な成長を目指すとしている。</p>
<p>今後は、裁判所による再編計画の承認を経て、新体制下での事業運営が本格化する見通しだ。</p>
]]></description>
      <pubDate>Thu, 11 Dec 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>AI検索を汚染する「LLM電話番号ポイズニング」──Aurascape、偽コールセンター誘導の新手法を確認</title>
      <link>https://ledge.ai/articles/llm_phone_number_poisoning_aurascape_fake_call_center</link>
      <description><![CDATA[<p>AIチャットボットやAI検索が参照する公開情報を悪用し、利用者を偽のコールセンターへ誘導する新たなサイバー攻撃キャンペーンが確認されたと、サイバーセキュリティ企業が明らかにした。</p>
<p>米国時間2025年12月8日、サイバーセキュリティ企業Aurascapeの研究部門であるAura Labsは、<a href="https://aurascape.ai/llm-search-poisoning-fake-support-numbers/">調査ブログ</a>でこの手法を「LLM Phone-Number Poisoning（LLM電話番号ポイズニング）」と名付け、AI時代特有の詐欺リスクだと警告している。</p>
<p>Aura Labsによると、この攻撃はAIモデルそのものを侵害するものではない。AIが検索や回答生成の際に参照する公開ウェブ情報の層を意図的に汚染し、AIに誤った電話番号を「正規のサポート番号」として出力させる点が特徴だという。</p>
<h2>公開情報を狙う「LLM電話番号ポイズニング」</h2>
<p>LLM電話番号ポイズニングでは、攻撃者が政府機関や大学、企業関連サイトなど、一見すると信頼性が高いドメイン上に、偽のサポート電話番号を含むコンテンツを配置する。これらのページやPDFは、生成AIやAI検索が拾いやすい構造で作られており、いわゆるGEO（Generative Engine Optimization：生成エンジン最適化）やAEO（Answer Engine Optimization：回答エンジン最適化）を意識した設計がなされているという。</p>
<p>その結果、AIチャットボットやAI検索は、こうした偽情報を複数の情報源から統合し、「もっともらしい公式情報」として利用者に提示してしまう。Aura Labsは、この現象を「検索ポイズニングがAIの回答レイヤーにまで拡張された例」だとしている。</p>
<h2>偽の航空会社サポート番号を案内する事例も</h2>
<p>Aura Labsは調査の中で、実際にAI検索や生成AIが航空会社の公式サポート番号として、誤った電話番号を提示する事例を複数確認したとしている。例えば、航空会社名と予約や問い合わせ用の電話番号を検索した場合、AIが公開情報をもとに生成した回答の中に、正規のものと見分けがつきにくい番号が含まれるケースがあったという。</p>
<p>下の画像は、Google検索におけるAI Overviewの表示例だ。航空会社の予約方法として複数の電話番号が提示されており、AIが「公式情報」として整理している様子が分かる。Aura Labsは、このような表示の背後で、公開ウェブ上に混入した偽情報がAIに取り込まれ、結果として偽のサポート番号が案内されるリスクが生じていると指摘している。</p>
<p><strong>Google検索のAI Overviewに表示された航空会社の予約電話番号の例</strong> ：Aura Labsは、公開情報の汚染によって、AIが偽のサポート番号を公式情報として提示する事例を確認していると指摘
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Google_AI_Blog_Image_3_24272c21ee/Google_AI_Blog_Image_3_24272c21ee.png" alt="Google-AI-Blog-Image-3.png" /></p>
<p>こうした挙動は、複数のAI検索・生成AIサービスで確認されており、特定の企業やサービスに限った問題ではないとされる。Aura Labsは、電話番号が一致している点から、組織的なキャンペーンとして実行されている可能性が高いと指摘している。</p>
<h2>ユーザーが直面する詐欺リスク</h2>
<p>偽の電話番号に連絡した利用者は、予約変更や返金手続きを装った詐欺に誘導される恐れがある。具体的には、クレジットカード情報や個人情報の詐取、金銭の送金要求、さらにはリモート操作ツールの導入を求められるといった被害につながる可能性がある。</p>
<p>Aura Labsは、AIが「信頼できる案内役」として認識されつつある現状において、こうした電話番号汚染攻撃の影響は従来の検索スパムよりも深刻になりうると警告している。</p>
]]></description>
      <pubDate>Thu, 11 Dec 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>スマホ単体でLLMをファインチューニング──C++製OSS「MobileFineTuner」、メモリ・電力制約を越える最適化</title>
      <link>https://ledge.ai/articles/mobilefinetuner_llm_finetuning_on_mobile_phones</link>
      <description><![CDATA[<p>スマートフォン上で大規模言語モデル（LLM）のファインチューニングをエンドツーエンドで実行できる統合フレームワークが提案された。</p>
<p>Duke Kunshan University（中国・崑山）およびThe University of Hong Kongの研究チームは、一般的なスマートフォンを対象にLLMを直接ファインチューニング可能にするオープンソースフレームワーク「MobileFineTuner」を開発したと報告した。</p>
<p>研究成果は2025年12月9日に論文「MobileFineTuner: A Unified End-to-End Framework for Fine-Tuning LLMs on Mobile Phones」としてarXivで<a href="https://arxiv.org/abs/2512.08211">公開</a>。コードは、商用可能なApache-2.0ライセンスでGitHub上に公開された。</p>
<p>論文では、GPT-2、Gemma 3、Qwen 2.5といったLLMを対象に、Androidスマートフォン上でのファインチューニングに関する実証結果が示されている（ただし、端末のメモリ条件によって実行できなかった一部のモデルやタスクについては、論文中では“–”として示されている）。</p>
<h2>公開データ枯渇と「スマホ上の私的データ」</h2>
<p>研究チームは、LLM開発を支えてきた高品質な公開データが、2026〜2032年に枯渇する可能性が指摘されている点を背景として挙げる一方で、個人のメッセージ履歴、メモ、アプリ操作ログといった価値の高い私的データの多くはスマートフォン上に存在するが、クラウド送信による学習はプライバシーや規制面で課題が大きいとした。</p>
<p>このギャップを埋める手段として、データを端末外に出さずに学習する「オンデバイス・ファインチューニング」が注目されてきたものの、既存研究の多くはシミュレーション、IoTボード、PC環境に留まり、「一般的なスマートフォン」を実装対象にした枠組みが不足していたと指摘している。</p>
<h2>Python非依存のC++実装という設計判断</h2>
<p>MobileFineTunerの最大の特徴は、フレームワーク全体をC++で実装し、Pythonランタイムに依存しない点にある。
論文では、PyTorchやHugging Face Transformersなどの既存フレームワークがPython前提であり、Androidを含むモバイルOSでは直接利用できないことが、実装上の最大の障壁だったと説明されている。</p>
<p><strong>■ MobileFineTunerの全体構成：</strong> C++ベースのオンモバイル・ファインチューニング基盤の上に、Full-FT/PEFT、評価機構、メモリ・エネルギー最適化が統合されている
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/overview2_f098ba40dd/overview2_f098ba40dd.png" alt="overview2.png" /></p>
<p>MobileFineTunerは、以下の3点を設計原則として掲げる。</p>
<ul>
<li><strong>効率性：</strong> 自動微分やバックプロパゲーションを含む学習処理をC++でネイティブ実装</li>
<li><strong>拡張性：</strong> Full-FT（全パラメータ更新）とPEFT（LoRA）の双方に対応</li>
<li><strong>実用性：</strong> 高水準APIを備え、モバイル上で完結する学習を可能にする</li>
</ul>
<p>これにより、Python仮想環境やVM（例：Termux）に依存せず、モバイルOS上で直接動作する学習基盤として、スマートフォン単体でLLMをファインチューニングできる構成を実現したとしている。</p>
<h2>メモリと電力制約に対応するシステム最適化</h2>
<p>論文では、モバイル端末特有の制約としてRAM容量とバッテリー消費を明確に課題として位置付けている。
一般に、FP16学習では「10億パラメータあたり約16GBのRAM」が必要とされる一方、2025年時点のスマートフォンのRAMは4〜16GBに留まると指摘する。</p>
<p><strong>■ スマートフォンの限られたRAM（4–16GB）に対し、学習時に必要なレイヤのみを「アクティブレイヤ」として保持し、他のパラメータはストレージ側に退避する設計</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/memory_parameters_76e1f8a522/memory_parameters_76e1f8a522.png" alt="memory_parameters.png" /></p>
<p>これに対し、MobileFineTunerは以下の最適化を統合的に実装している。</p>
<ul>
<li><strong>ZeRO着想のパラメータシャーディング：</strong> 学習に必要なパラメータのみをRAMに保持し、非アクティブ部分はストレージへ退避する仕組みを採用</li>
<li><strong>Gradient Accumulation：</strong> マイクロバッチに分割して勾配を蓄積することで、メモリ使用量を抑えつつ学習の安定性を維持</li>
<li><strong>エネルギー認識型スケジューリング：</strong> バッテリー残量を監視し、閾値を下回ると計算頻度を動的に抑制する仕組みを導入</li>
</ul>
<h2>Pixel実機での検証とPyTorchとの比較</h2>
<p>評価では、Google Pixel 8／7 Pro／8 Proといった市販Androidスマートフォンを用い、GPT-2（124M/355M）、Qwen2.5-0.5B、Gemma3（270M/1B）を対象に検証が行われた。</p>
<p>PEFT（LoRA）による検証では、WikiText-2およびMMLUタスクにおいて、PyTorch（サーバー実行）とほぼ同等の損失・PPL・精度が得られたと報告されている。論文は、性能優位性を主張するものではなく、「モバイル上での学習結果が標準的な学習挙動と整合している」点を検証の主眼としている。</p>
<h2>公開形態と今後の焦点</h2>
<p>MobileFineTunerは、論文とともにオープンソースソフトウェアとして公開されており、Android 10以降の端末で動作するとされている。著者らは今後、異種ハードウェア対応や、フェデレーテッド／協調学習との統合を検討するとしている。</p>
<p>研究は、オンデバイスLLM学習を「概念」ではなく実装と実機検証で示した点に特徴があり、スマートフォンを前提としたLLM活用の設計空間を広げるものとして位置付けられる。</p>
]]></description>
      <pubDate>Thu, 11 Dec 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>NTT、次世代都市構想「光の街」始動──IOWNを都市に実装し、本社を2031年に日比谷へ移転</title>
      <link>https://ledge.ai/articles/ntt_iown_hikari_city_hibiya_hq_move_2031</link>
      <description><![CDATA[<p>NTT株式会社、NTTアーバンソリューションズ株式会社、NTT都市開発株式会社は2025年12月8日、次世代情報通信基盤「IOWN」を都市空間に実装し、街がテクノロジーとともに進化し続ける「光の街 powered by IOWN」構想を開始すると<a href="https://group.ntt/jp/newsrelease/2025/12/08/251208a.html">発表</a>した。本構想の第一弾は、2031年10月末に竣工予定の「NTT日比谷タワー」を中心に展開され、NTT本社も同タワーへの移転を予定している。</p>
<p>1961年に旧日本電信電話公社が本社を構えた日比谷の地に再び拠点を置くことで、NTTグループが蓄積してきた技術力を結集し、新たな価値創出と超・低消費電力化を実現することを目指すとしている。</p>
<h2>社会課題の深刻化とIOWNの役割</h2>
<p>日本では少子高齢化による人手不足、地球温暖化、自然災害の頻発に加え、AI・ロボティクスの普及による電力消費の増大が課題となっている。NTTは、膨大なデータを大容量・低遅延・低消費電力で処理できるIOWNを社会基盤として都市に実装することで、災害予測、インフラ制御、企業業務の効率化など、多領域での課題解決を進めるという。</p>
<h2>新しいビジネス・イノベーション</h2>
<p>IOWNが実装されるNTT日比谷タワーでは、世界中の企業とリアルタイムで協働できる環境を整備する。
通信とデータ処理を統合した基盤を活用することで、臨場感のあるコミュニケーションや高度な遠隔業務が可能になる。</p>
<p>また、NTTは業務支援の高度化に向けて、NTT版LLM「tsuzumi 2」や、大規模AI連携技術「AIコンステレーション」をIOWNと組み合わせることで情報検索や資料作成支援、国際的なコラボレーションなど、企業活動を支える多様なアプリケーションの展開を想定している。
将来的には、会議で生まれたアイデアをモデル化したり、必要な情報を即座に提示するなど、働く環境の高度化を支える技術として発展させる方針だ。</p>
<p>将来的な空間・時間を問わないビジネスシーンイメージ
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/251208ab_3cda72f446/251208ab_3cda72f446.jpg" alt="251208ab.jpg" /></p>
<h2>新たなライフスタイル・エンターテインメント</h2>
<p>日比谷タワー低層部に整備される大規模アトリウム「（仮称）Cross Gate」では、壁面・天井一体型の大型LEDビジョンを活用し、世界各地とリアルタイムでつながるイベント空間を形成する。</p>
<p>商業施設との連動イベントや多拠点同時発表会、ライブビューイングなど、多様な活用が可能。将来的には音響XR技術なども組み合わせ、より深い没入体験を提供する。</p>
<p>将来的な（仮称）Cross Gateでのエンターテイメントイメージ（左図：バーチャルアクアリウム、右図：他会場と融合したバスケットボール観戦）
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/251208ac_72eb58dead/251208ac_72eb58dead.jpg" alt="251208ac.jpg" /></p>
<h2>超・低消費電力化によるサステナビリティ</h2>
<p>都市全体の持続可能性に向け、NTTは以下の取り組みを明示している。</p>
<ul>
<li>オフィス部分で「ZEB Ready」を実現（従来比50％以上の省エネ）</li>
<li>光電融合デバイスによる消費電力削減</li>
<li>IOWN×AIの予測制御「Just Enough Energy」で10〜20％のCO₂削減</li>
<li>将来は光量子コンピュータや水素エネルギーの活用も検討</li>
</ul>
<h2>今後の展開</h2>
<p>内幸町一丁目街区は、官民連携や研究機関との共創が可能な次世代スマートシティへと整備が進む。NTT日比谷タワーで得られた知見は、周辺エリア、国内、さらには海外へと段階的に展開される予定だ。</p>
]]></description>
      <pubDate>Thu, 11 Dec 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>OpenAI、新モデル「GPT Image 1.5」搭載の「ChatGPT Images」を公開──GoogleのNano Banana Proに対抗</title>
      <link>https://ledge.ai/articles/openai_chatgpt_images_gpt_image_1_5_release</link>
      <description><![CDATA[<p>OpenAIは2025年12月17日、新しい画像生成モデル「GPT Image 1.5」を搭載した新バージョンの「ChatGPT Images」をリリースしたことを<a href="https://openai.com/index/new-chatgpt-images-is-here/">発表</a>した。新モデルは、ゼロからの画像生成だけでなく、既存画像の編集においてもユーザーの意図をより忠実に反映するよう設計されており、OpenAIはこれを同社の“最も高性能な汎用画像生成モデル”と位置づけている。</p>
<p>公式ブログによると、GPT Image 1.5は「Precise edits that preserve what matters（重要な要素を保ったまま、正確な編集を行う）」ことを重視して開発された。画像の一部を修正する際にも、照明や構図、人物の外見といった重要な視覚的要素を維持しつつ、指示された変更点のみを反映できるという。</p>
<p>@<a href="https://www.youtube.com/watch?v=DPBtd57p5Mg">YouTube</a></p>
<h2>意図に沿った編集を、細部まで</h2>
<p>GPT Image 1.5は、テキスト入力から新たな画像を生成する従来型の使い方に加え、既存の画像をアップロードして編集する用途にも対応する。OpenAIによると、被写体の一部を変更したり、要素を追加・削除したりする際にも、元の画像の文脈や視覚的な整合性を保ったまま編集できるという。</p>
<p>特に、編集時にありがちな意図のずれや不自然な合成を抑え、ユーザーが指示した内容を反映しやすくした点が、今回のアップデートの特徴とされている。</p>
<p><strong>元の素材</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/chatgpt_images_example_1_input_2_side_10deaf7fff/chatgpt_images_example_1_input_2_side_10deaf7fff.jpg" alt="chatgpt-images-example-1-input-2-side.jpg" />
<strong>プロンプト：2人の男性と犬を、子供の誕生日パーティーで退屈そうにしている2000年代のフィルムカメラ風の写真に合成します</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/chatgpt_images_example_1_output_1_9a9c4bee33/chatgpt_images_example_1_output_1_9a9c4bee33.webp" alt="chatgpt-images-example-1-output-1.webp" />
<strong>プロンプト：背景に、物を投げたり叫んだりするカオスな子供たちを追加します。</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/chatgpt_images_example_1_output_2_c4825ffe56/chatgpt_images_example_1_output_2_c4825ffe56.webp" alt="chatgpt-images-example-1-output-2.webp" />
<strong>プロンプト：左側の男性を手描きのレトロアニメスタイルに変更し、犬をぬいぐるみスタイルに変更し、右側の男性と背景の風景はそのままにします。</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/new_save_8e5602dc1a/new_save_8e5602dc1a.webp" alt="new-save.webp" />
<strong>プロンプト：全員に、こんな感じのOpenAI セーターを着せてみましょう。</strong>
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Screenshot_2025_12_12_at_10_23_01a_AM_514132aad4/Screenshot_2025_12_12_at_10_23_01a_AM_514132aad4.webp" alt="Screenshot_2025-12-12_at_10.23.01â__AM.webp" />
<img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/MIX_1_ea42e6aaef/MIX_1_ea42e6aaef.webp" alt="MIX-1.webp" /></p>
<h2>ChatGPT内に「Images」専用の制作空間を追加</h2>
<p>今回のアップデートにあわせて、ChatGPT内には画像生成専用の「Images」体験が導入された。ユーザーはテキストで指示するだけでなく、プリセットのスタイルやトレンドプロンプトを選択することで、簡単に画像生成や編集を試すことができる。</p>
<p>OpenAIは、この専用スペースによって、アイデア探索や試行錯誤をより直感的かつ高速に行えるようになるとしている。画像生成速度は最大で従来比4倍に向上しており、生成中であっても次の画像を作成できるため、待ち時間を減らしながら創作を進められる。</p>
<h2>API提供も開始</h2>
<p>GPT Image 1.5は、ChatGPT内の機能としてだけでなく、APIとしても提供される。これにより、開発者は自社のアプリケーションやサービスに画像生成・編集機能を組み込むことが可能になる。OpenAIは、クリエイティブ用途にとどまらず、業務やプロダクト開発での活用も想定している。</p>
<p>OpenAIは今回のリリースを通じて、テキストと画像を横断するマルチモーダル体験を強化し、ChatGPTを中心とした生成AIの活用領域をさらに広げていく考えだ。</p>
]]></description>
      <pubDate>Thu, 11 Dec 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>サム・アルトマン、OpenAI設立10年を回顧するブログ「Ten years」公開──「想像以上の成果」とAGIへの使命を語る</title>
      <link>https://ledge.ai/articles/sam_altman_openai_ten_years_blog</link>
      <description><![CDATA[<p>OpenAIのCEOであるサム・アルトマン氏は米国時間2025年12月12日、同社設立から10年間を振り返るブログ「Ten years」を公式サイトで<a href="https://openai.com/index/ten-years/">発表</a>した。</p>
<p>ブログでは、2015年の創設から現在に至るまでの歩みを総括するとともに、AGI（汎用人工知能）をめぐる使命や、技術進展が社会にもたらした変化について言及している。</p>
<p>アルトマン氏は、OpenAIが「不確実で小さな始まり」だったことを振り返る。設立当初はごく少人数のチームで、成功が約束されていたわけではなく、大胆で挑戦的な試みだったと説明している。そうした状況から出発したOpenAIが、10年後には世界的なAI研究・プロダクト開発を担う組織へと成長したことを強調した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/the_first_day_of_openai_aa77360ec8/the_first_day_of_openai_aa77360ec8.png" alt="the first day of openai.png" /></p>
<p>この10年間で達成した成果について、アルトマン氏は「自分が当初に思い描いていた以上のものだった」と述べている。特に、ChatGPTをはじめとする生成AIが社会に広く普及し、多くの人々の日常や仕事に影響を与える存在となった点を示唆している。一方で、個々人の生活そのものは10年前と大きく変わっていないとも指摘した。</p>
<p>そのうえで同氏は、社会全体が持つ「可能性の空間（possibility space）」は大きく広がったと表現する。AIの進化によって、新たに実現可能となった選択肢や未来像が増えたことが、この10年の最も重要な変化の一つだと位置づけている。</p>
<p>ブログではまた、OpenAIの中核的な使命である「AGIを全人類の利益になるよう実現する」という目標が改めて強調された。アルトマン氏は、技術的な前進だけでなく、その影響力の大きさを踏まえた慎重さと責任が不可欠であるとの認識を示している。</p>
<p>「Ten years」は、具体的な製品計画や数値目標に踏み込む内容ではなく、これまでの歩みと今後の方向性を整理する回顧録的な位置づけとなっている。アルトマン氏は、次の10年がOpenAIにとって、そしてAI全体にとって、さらに重要な期間になるとの考えをにじませている。</p>
]]></description>
      <pubDate>Thu, 11 Dec 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Spotify、アルゴリズム操作をユーザーに委ねる新機能「Prompted Playlist」発表──自然言語でプレイリスト生成</title>
      <link>https://ledge.ai/articles/spotify_prompted_playlist_algorithm_control</link>
      <description><![CDATA[<p>スウェーデンの音楽配信大手Spotifyは12月10日（現地時間）、ユーザーが音楽推薦アルゴリズムをより詳細に制御できる新機能「Prompted Playlist」を<a href="https://newsroom.spotify.com/2025-12-10/spotify-prompted-playlists-algorithm-gustav-soderstrom/">発表</a>した。ユーザーがテキストで入力した指示（プロンプト）をもとに、AIが条件に沿ったプレイリストを自動生成するという。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/Prompted_Playlist_GIF_102924_6e4c11df83/Prompted_Playlist_GIF_102924_6e4c11df83.gif" alt="Prompted-Playlist-GIF-102924.gif" /></p>
<p>「Prompted Playlist」は、ムードやジャンル、シチュエーション、年代などを自然言語で指定することで、SpotifyのAIがプレイリストを作成する仕組みだ。生成にはユーザーの過去のリスニング履歴や嗜好データが活用され、従来の自動推薦よりも、ユーザーの意図を直接反映できる点が特徴となっている。</p>
<p>ユーザーは、作成されたプレイリストに対して再度プロンプトを入力し、内容を調整することも可能だ。また、プレイリストを定期的に更新する設定も用意されており、毎日や毎週といった更新頻度を指定できる。</p>
<p>Spotifyによると、この機能は、これまで同社の推薦システムに対して指摘されてきた「アルゴリズムが不透明である」という課題に対応する狙いがある。AIによる自動推薦を維持しつつも、ユーザー自身が明示的に方向性を示すことで、より主体的な音楽体験を提供するという。</p>
<p>「Prompted Playlist」は現在、ニュージーランドのSpotify Premiumユーザーを対象にベータ版として提供されている。今後は利用状況を踏まえながら、対応地域や対象ユーザーを段階的に拡大する方針だが、グローバルでの正式提供時期は明らかにされていない。
　</p>
]]></description>
      <pubDate>Thu, 11 Dec 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>米TIME誌、「Person of the Year 2025」に「The Architects of AI」──世界を動かした8人の“AI設計者たち”</title>
      <link>https://ledge.ai/articles/time_person_of_the_year_2025_architects_of_ai</link>
      <description><![CDATA[<p>米TIME誌は2025年12月11日（現地時間）、その年に最も大きな影響を与えた存在として、「The Architects of AI（AIの設計者たち）」を「Person of the Year 2025」に選出したと<a href="https://time.com/7339685/person-of-the-year-2025-ai-architects/">発表</a>した。</p>
<p>選出は特定の1名ではなく、AIを構想し、設計し、社会へと実装してきた中核的な担い手を集合体として評価する選出となった。</p>
<h2>AIが“止められなくなった”年</h2>
<p>TIME誌は、2025年をAIの潜在力が全面的に可視化され、「後戻りも、参加しないという選択もできなくなった年」と位置づける。医療研究や科学分野での発見、生産性の向上、ソフトウェア開発や創作活動の加速など、AIは短期間で広範な領域に浸透した。AIの能力はかつてない速度で向上し、社会のあらゆる場面で存在感を示すようになった。</p>
<h2>就任式の裏で起きていたこと</h2>
<p>象徴的な出来事として、TIME誌は2025年1月のトランプ大統領就任式当日を挙げる。その日、中国の新興AI企業DeepSeekが新モデルを公開し、市場を動揺させた。翌日には、サム・アルトマン氏、ラリー・エリソン氏、孫正義氏らがホワイトハウスで、最大5000億ドル規模のAIデータセンター投資計画「Stargate」を発表した。これらの出来事は、AIを巡る国際競争、巨額投資、官民の利害が交錯する1年の幕開けを象徴していた。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/stargate_project_5b563d54c6/stargate_project_5b563d54c6.jpg" alt="stargate project.jpg" /></p>
<h2>8人が並ぶ理由</h2>
<p>今回の表紙は、特定の英雄を強調する構図ではない。生成AIのモデル開発、計算基盤、研究、社会実装といった異なる役割を担う人物が、同じ高さで横一列に並んで描かれている。TIME誌は、AIの進化が一人の天才や一社の成果ではなく、複数の層にわたる設計判断と意思決定の積み重ねであることを、この集合肖像で示しているという。</p>
<h3>表紙に描かれた顔ぶれ</h3>
<p>TIME誌の表紙には、「Architects of AI」を象徴する人物として、以下の8人が描かれている。</p>
<ul>
<li>Meta CEO：マーク・ザッカーバーグ氏</li>
<li>AMD CEO：リサ・スー氏</li>
<li>xAI創業者：イーロン・マスク氏</li>
<li>NVIDIA CEO：ジェンスン・フアン氏</li>
<li>OpenAI CEO：サム・アルトマン氏</li>
<li>Google DeepMind CEO：デミス・ハサビス氏</li>
<li>Anthropic CEO：ダリオ・アモデイ氏</li>
<li>スタンフォード大学教授／World Labs CEO：フェイフェイ・リー氏</li>
</ul>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/time_magazine_person_of_the_year_ai_2025_003_e2b05e7953/time_magazine_person_of_the_year_ai_2025_003_e2b05e7953.webp" alt="time-magazine-person-of-the-year-ai-2025-003.webp" /></p>
<h2>加速する現実</h2>
<p>2025年、AIの進展は技術領域にとどまらず、経済や政治とも強く結びついた。膨大な電力を消費するデータセンター建設が各地で進み、雇用構造の変化や誤情報の拡散、サイバーリスクの拡大も現実味を帯びた。権力と資本が少数の企業や経営者に集中する状況は、TIME誌が「金ぴか時代以来」と表現するほどの規模に達している。</p>
<h2>かつてPCが選ばれたように</h2>
<p>TIME誌の「Person of the Year」は、個人だけでなく、時代を画した概念を選んできた。1980年代のパーソナルコンピュータ、2006年の「You（あなた）」に続き、今回はAIという新たな時代の中核を担う存在が選ばれた。ソーシャルインターネットの時代から、AIが社会を形作る時代への移行点として、2025年が位置づけられている。</p>
<p>2025年はAIが実験段階や一部の先進的用途を超え、社会インフラの一部として機能し始めた年だった。生成AIは業務効率化やソフトウェア開発、研究活動だけでなく、映像・音楽・文章といった創作領域にも本格的に浸透した。</p>
<p>同時に、AIを巡る倫理、著作権、規制、雇用への影響といった課題も顕在化し、各国政府や国際機関による対応が進んだ。TIME誌は、こうした恩恵とリスクの双方を生み出した中心に「AIの設計者たち」が存在するとして、今年の<a href="https://time.com/7339621/person-of-the-year-2025-ai-architects-choice/">選出理由</a>に挙げている。</p>
<h2>設計者の時代</h2>
<p>同誌は、AIの未来は技術そのものではなく、それを設計し、運用し、社会に組み込む人間の選択に委ねられていると強調する。2025年は、その設計者たちが世界史の表舞台に立った年だった。称賛と不安の双方を伴いながら、AIはもはや一部の分野の話題ではなく、世界を動かす力となっている。</p>
<h2>AI時代を象徴する選出に</h2>
<p>「The Architects of AI」が選ばれたことで、2025年はAIが単なる技術トレンドではなく、社会の基盤を形作る存在として認識された年として刻まれることになった。</p>
<p>TIME誌は今回の選出を通じて、AIの進化が偶然ではなく、数多くの設計上の選択と意思決定の積み重ねによって生まれていることを強調している。AI時代の行方を左右するのは、技術そのものではなく、それを設計し、運用し、社会に組み込む人間であるというメッセージが込められている。</p>
<p>:::box
[関連記事：AIの\</p>
]]></description>
      <pubDate>Thu, 11 Dec 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>チューリング、都内30分の完全自動運転「Tokyo30」を達成　テスラ猛追へ「富岳」級の計算基盤も公開</title>
      <link>https://ledge.ai/articles/turing_tokyo30_full_self_driving_achievement</link>
      <description><![CDATA[<p>完全自動運転を目指すスタートアップのチューリング株式会社は2025年12月1日、同社の技術カンファレンス「<a href="https://www.youtube.com/watch?v=HhQr0SAZs3Y">Turing AI Day 2025</a>」で、東京都内を人間の操作なしで30分以上走行するプロジェクト「Tokyo30」を達成したと発表した。当日は実際の走行映像も公開され、開発中のEnd-to-End（E2E）自動運転システムの挙動が披露された。</p>
<p>同社は創業当初より「We Overtake Tesla（テスラを超える）」をミッションとして掲げており、今回の成果を国産の完全自動運転に向けた重要なマイルストーンの一つと位置づけている。</p>
<h2>「カンブリア爆発」を経て実用域に近づくE2Eモデル</h2>
<p>AI Day の冒頭で公開された映像では、チューリングの実験車両が、市街地の複雑な交通環境下でハンドル・アクセル・ブレーキのすべてをAIが制御し、30分以上連続して走行する様子が確認できた。信号停止、右左折、歩行者や車両との交錯といった場面でも、安定した挙動を維持していた。</p>
<p>同社は創業時から「Day 1 から E2E」を掲げ、カメラ映像を入力とし、単一のニューラルネットワークが走行経路（トラジェクトリ）を直接出力する方式を採用している。データ収集から学習までのパイプラインを整備した段階でモデル性能が急激に向上した時期を、社内では「カンブリア爆発」と呼んでおり、Tokyo30 はその延長線にある成果だと説明した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/turing_ai_day20251_393e19e328/turing_ai_day20251_393e19e328.jpg" alt="turing ai day20251.jpg" /></p>
<h2>富岳の約40%相当の計算基盤が学習を支える</h2>
<p>E2Eモデルの性能向上を支えるのが、大規模GPUクラスタである。チューリングは自社専用の「Gaggle Cluster」を中心に、オンプレミスとクラウドを組み合わせて学習基盤を構築しており、2025年12月時点の総演算性能は、スーパーコンピュータ「富岳」のAI演算性能（FP16換算）の約40%に相当すると説明した。</p>
<p>同社はシリーズAラウンドの1st Closeとして152.7億円（約153億円）を調達しており、多くを計算基盤の拡充やデータ拡張、組織体制の強化に投じる計画だという。AI Day では、今後2年間で計算能力を5〜10倍（最大7 ExaFLOPS規模）へ拡張する方針も示された。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/turing_ai_day20252_608dfde61a/turing_ai_day20252_608dfde61a.jpg" alt="turing ai day20252.jpg" /></p>
<h2>VLAモデル・世界モデルへと発展する次世代アプローチ</h2>
<p>AI Day 2025 では次世代アプローチとして、視覚（Vision）・言語（Language）・行動（Action）を統合する「VLAモデル」や、未知の走行シーンを生成できる「世界モデル（World Model）」の研究状況が紹介された。</p>
<p>チューリングはこれまでに、日本語VLM「Heron」や運転QAデータセット「Stride-QA」「COBRA」を独自に構築しており、今後はこれらの技術を統合した70B級（700億パラメータ級）規模の「フロンティアモデル」を開発。そのうえで、車載向けに蒸留したE2Eモデルを量産レベルで展開する構想を掲げる。</p>
<h2>2030年の完全自動運転の商用化を見据える</h2>
<p>Q&amp;Aセッションでは、2030年前後にハンドルのない完全自動運転車が市場に登場する可能性が高いとの見通しが示された。チューリングは Tesla や Waymo など先行企業の技術進展を踏まえつつ、高速で追随する「セカンドムーバー・アドバンテージ」を戦略の中核に据えている。</p>
<p>短期的には、ドライバーが「安心して身を預けられるレベル」の挙動を実現することを目標とし、介入頻度の削減と危険シーンの排除に取り組むとした。安全性評価については、従来のISO規格だけではE2E型システムの特性を十分に捉えられないとして、3D Gaussian Splatting や世界モデルを活用した新たな検証アプローチの重要性が言及された。</p>
]]></description>
      <pubDate>Thu, 11 Dec 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>Google傘下Waymoのロボタクシー、停車中のスクールバス追い越しに関する安全違反で規制当局（NHTSA）調査―—自動運転システムのソフトウェアを自主リコール</title>
      <link>https://ledge.ai/articles/waymo_robotaxi_school_bus_software_recall</link>
      <description><![CDATA[<p>Google傘下で自動運転事業を手がけるWaymoのロボタクシーが、停車中のスクールバスを追い越す際に交通安全法規を順守していなかったとして、米運輸省道路交通安全局（NHTSA：The National Highway Traffic Safety Administration）が<a href="https://static.nhtsa.gov/odi/inv/2025/INIM-PE25013-30896.pdf">調査</a>を実施している。これは、2025年12月6日の<a href="https://www.reuters.com/world/waymo-issue-recall-over-self-driving-vehicles-driving-past-stopped-school-buses-2025-12-05/">Reuters</a>などの報道から明らかになった。Waymoは問題となった挙動を認め、対象となる自動運転システムのソフトウェアを自主的にリコールするという。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/waymo_reuters_2aa02403a1/waymo_reuters_2aa02403a1.jpg" alt="waymo reuters.jpg" /></p>
<p>NHTSAの調査は、スクールバスの赤色灯や停止アームが作動している状況下で、Waymoのロボタクシーが本来停止すべき場面でも走行を継続したとされる事例を受けて行われている。調査を担当する同局の欠陥調査局（ODI）は、こうした挙動が各州の交通安全法規に抵触する可能性があるとして、Waymoに対して詳細な情報提供を求めている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/nhtsa_waymo_odi_ade67f60cb/nhtsa_waymo_odi_ade67f60cb.jpg" alt="nhtsa waymo odi.jpg" /></p>
<p>調査の過程でWaymoは、特定の条件下において自動運転ソフトウェアがスクールバス周辺の交通ルールを適切に反映できていなかったことを認めた。これを受け、同社は車両の機械的な欠陥ではなく、制御ソフトウェアの問題として是正措置を講じる方針を示し、ソフトウェア更新をリコールとして届け出る対応を取った。</p>
<p>NHTSAによると、今回の件は予備評価（Preliminary Evaluation）の対象となっており、Waymoは指定された期限までに、問題が発生した具体的な事例数、影響を受けた車両、ソフトウェア修正の内容や実施状況などについて説明する必要がある。調査は継続中で、当局は提出された情報をもとに、追加対応の必要性を判断する。</p>
<p>Waymoは、ロボタクシーの安全性を最優先事項として掲げており、規制当局と連携しながらソフトウェアの改善を進めるとしている。今回のリコールは強制措置ではなく、Waymo側の判断による自主的な対応だという。</p>
]]></description>
      <pubDate>Thu, 11 Dec 2025 05:50:00 GMT</pubDate>
    </item>
    <item>
      <title>リコー、「Gemma 3 27B」基盤の日本語LLMを開発──gpt-oss-20b同等性能を達成しつつ、PCサーバで動くオンプレ向けモデルとして企業提供を開始</title>
      <link>https://ledge.ai/articles/ricoh_gemma3_27b_japanese_llm_onprem_release</link>
      <description><![CDATA[<p>リコーは2025年12月8日、Google のオープンモデル「Gemma 3 27B」を基盤に、日本語向けに最適化した大規模言語モデル（LLM）を開発したと<a href="https://jp.ricoh.com/release/2025/1208_1">発表</a>した。同モデルは企業のオンプレミス環境での利用を想定しており、PCサーバ上で動作可能な規模に抑えつつ、日本語ベンチマークで OpenAI の「gpt-oss-20b」と同等スコアを記録したという。</p>
<h2>モデルマージと「Chat Vector」で性能を強化</h2>
<p>リコーは今回、独自のモデルマージ技術を活用し、ベースモデルである Gemma 3 27B に対して複数の「Chat Vector」を統合した。Chat Vectorは、約1万5,000件の指示チューニングデータから抽出した“指示追従能力”を表すベクトルで、これを組み合わせることで追加学習を行わずに対話性能を高める仕組みとなっている。</p>
<p>同社によると、非推論モデルでありながら初期応答性（TTFT）が短く、文書作成などの業務利用に適した特性を持つという。</p>
<p>@<a href="https://www.youtube.com/watch?v=mKftRMFEZYg">YouTube</a></p>
<h2>ベンチマークで「gpt-oss-20b」と同等水準</h2>
<p>評価には「Japanese MT-Bench」と「ELYZA-tasks-100」を使用。Google「gemma-3-27b-it」や Alibaba Cloud「Qwen3-32B」、OpenAI「gpt-oss-20b」などと比較し、リコーのモデルは平均スコアで OpenAI の gpt-oss-20bに近い水準を示した。</p>
<p>MT-Benchはコーディング、抽出、数学、推論、ライティングなど8分野の質問応答を、ELYZA-tasks-100は要約・意図汲み取り・複雑計算など100種類のタスクを評価対象とする。スコアはそれぞれ MT-Benchが10点満点、ELYZAが5点満点で、リコーは比較のため2倍換算した平均値で評価した。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/LLM_cfe88d4fc3/LLM_cfe88d4fc3.jpg" alt="ベンチマークテストリコーLLM.jpg" /></p>
<h2>PCサーバで動作可能な27Bモデル、オンプレ向けに最適化</h2>
<p>モデル規模は270億パラメータと比較的コンパクトで、GPUリソースを大量に必要とする大規模モデルとは異なり、PCサーバなど既存のオンプレ環境で運用できる点が特徴となる。</p>
<p>リコーは同モデルを「低コストで導入できる日本語プライベートLLM」と位置づけており、データ主権やセキュリティ要件の高い企業での利用を想定している。</p>
<h2>PRIMERGYへプリインストール、Difyとともに提供</h2>
<p>2025年12月下旬から、エフサステクノロジーズの「Private AI Platform on PRIMERGY（Very Smallモデル）」に量子化済みモデルをプリインストールした形で提供を開始する。生成AIアプリをノーコードで構築できる「Dify」も組み込まれ、リコージャパンが環境構築済みの形で企業へ導入支援を行う。</p>
<p>Difyを利用すれば、FAQ対応、ナレッジ検索、文書要約などの業務アプリケーションを、専門知識なしに構築できるとしている。</p>
<p><img src="https://storage.googleapis.com/ledge-ai-prd-public-bucket/media/1208_1_60d9384d72/1208_1_60d9384d72.webp" alt="1208_1.webp" /></p>
<p>リコーは2021年から自然言語処理を活用した文書分析サービスを提供し、2022年以降は独自LLM開発を本格化。700億パラメータの大規模モデル、指示追従性を高めたInstructモデル、モデルマージによる高速開発手法など、LLM関連の取り組みを継続してきた。</p>
<p>今回のGemma 3 27Bベースモデルは、その延長線上で「オンプレミスで利用できる高性能日本語LLM」というニーズを反映したものになる。</p>
]]></description>
      <pubDate>Wed, 10 Dec 2025 07:50:00 GMT</pubDate>
    </item>
  </channel>
</rss>